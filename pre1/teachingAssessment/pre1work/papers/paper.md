
Elsevier logo
Journals & Books

Help
Search
My account
Sign in

Download full issue
Search ScienceDirect
Search ScienceDirect

Outline
Highlights
Abstract
Keywords
1. Introduction
2. Literature review
3. The study
4. Methodology
5. Analysis
6. Results
7. Discussion
8. Pedagogical implications for EAP contexts
9. Limitations
10. Conclusions
CRediT authorship contribution statement
Declaration of generative AI and AI-assisted technologies in the writing process
Funding
Conflict of interest
Appendix A. Student Worksheet
Appendix B. Collocations used in the study and examples from students' interactions with ChatGPT
References
Vitae

Show full outline
Figures (2)
Fig. 1. Sample test item measuring academic spoken collocations
Fig. 2. Collocation frequency across time by instructional group
Tables (3)
Table 1
Table 2
Table
Elsevier
Journal of English for Academic Purposes
Volume 80, March 2026, 101638
Journal of English for Academic Purposes
Guided and unguided GenAI tasks for learning academic spoken collocations
Author links open overlay panel
Valentina Morgana
, 
Francesca Poli

Show more

Add to Mendeley

Share

Cite
https://doi.org/10.1016/j.jeap.2026.101638
Get rights and content
Under a Creative Commons license
Open access
Highlights
•
Guided and unguided ChatGPT tasks were compared for learning academic spoken collocations.
•
Unguided AI tasks led to more frequent and accurate collocation use than guided tasks.
•
Task design in AI-mediated EAP learning shapes learner engagement and collocation uptake.
Abstract
This study investigates the impact of AI-assisted task-based language teaching (TBLT) on the acquisition of academic English collocations in spoken discourse. While TBLT has been widely studied, its application to spoken academic collocations remains under-researched. The objectives are (1) to examine whether guided use of a generative AI tool with structured activities enhances collocation learning more effectively than unguided interactions, and (2) to compare AI-assisted instruction with traditional, technology-mediated, non-AI-based approaches in fostering collocational competence. A specialised list of academic spoken collocations was used to inform task design and to identify collocations in learner outputs. Seventy-five B2-level university students in foreign language programs were randomly assigned to one of three groups: an unguided group engaging in open-ended discussions with ChatGPT, a guided group completing structured tasks with AI, and a control group receiving non-AI-based instruction. For the analysis, a subset of 37 participants was examined after data cleaning, producing 259 observations and an average of 8500 words per student. A pretest–posttest design with a delayed posttest was used to assess short- and long-term learning gains, and participant–AI outputs were analysed using linear mixed-effects models. Results indicate that learners in both AI-supported conditions showed higher frequency and accuracy of academic spoken collocations than the non-AI control group. Generally, the unguided interaction with AI was linked to more frequent and accurate use of collocations. Overall, the findings suggest that generative AI can support the development of academic spoken collocations in task-based EAP instruction, highlighting the importance of task design and learner engagement.
Previous article in issue
Next article in issue
Keywords
AI-Mediated learning tasksTask-based language teaching (TBLT)Academic spoken collocationsIncidental vocabulary learningChatGPTLearner autonomy
1. Introduction
The rapid spread of generative artificial intelligence (GenAI), powered by large language models (LLMs), has begun to influence research and practice in English for Academic Purposes (EAP). LLMs are the underlying technologies trained on large-scale language data, while GenAI applications such as ChatGPT allow learners to interact with these models through language generation. In EAP contexts, recent studies have examined how GenAI can support students' academic writing, reading, and AI literacy (Cai et al., 2025; Mo & Crosthwaite, 2025). Far less attention, however, has been paid to how GenAI-mediated tasks can support students’ spoken academic language development. Because spoken academic language is typically developed through classroom tasks, task-based language teaching (TBLT) provides a relevant framework for examining how GenAI is integrated into instruction.
Before the emergence of GenAI, technology-mediated task-based language teaching (TBLT) typically involved tools such as learning management systems, discussion boards, synchronous computer-mediated communication, and multimedia resources (González-Lloret & Ortega, 2014; Ziegler, 2016). These technologies supported task implementation by facilitating interaction, providing access to input, or enabling collaboration, but they did not actively generate language with learners. GenAI tasks differ in this respect: they allow students to interact with an AI system that can model academic discourse, respond to prompts, and offer reformulations in real time. This shift changes not only the technological affordances of tasks but also how students engage with language during task performance.
Because GenAI can act as an interactive task partner, its integration into language teaching has been described as disruptive. It challenges established routines around practice, feedback, and scaffolding by allowing AI to provide support that is traditionally provided by teachers or materials. In higher education, this has renewed debates about whether new technologies merely enhance existing pedagogical approaches or lead to changes in learning outcomes (Kirkwood & Price, 2014; Selwyn, 2016). To address this issue, the present study adopts a student-centred perspective, focusing on how different ways of using GenAI within tasks affect what learners actually do and learn.
Spoken academic discourse is a particularly relevant domain for examining the pedagogical impact of GenAI tasks. In EAP programmes, students are frequently assessed through oral presentations and question-and-answer sessions, where they must structure arguments, express a stance, etc. Importantly, academic presentations are not purely spontaneous speech but rather a hybrid genre that is often planned, drafted, and rehearsed in writing before delivery. Research on academic discourse shows that spoken academic genres share many features with written discourse, including explicit organization and recurrent phraseology (Biber & Conrad, 2019; Biber et al., 2004; Hyland, 2008). As a result, students typically prepare presentations through written scripts, which are later adapted for spoken delivery. Using written GenAI interaction can support this preparation phase, providing opportunities to notice and rehearse spoken academic language. However, this preparation phase is also where many learners struggle to acquire and use appropriate spoken academic collocations.
Despite their importance, spoken academic collocations are often weak even among upper-intermediate learners. Learner corpus studies show that students rely on a small set of common word combinations and have difficulty using appropriate collocations naturally in oral tasks such as presentations (Granger & Bestgen, 2014; Paquot, 2010). This can reduce clarity and fluency in academic speaking, which makes collocational competence an important focus in EAP teaching.
Task-based language teaching (TBLT) provides a clear way to address this issue. In TBLT, learning is built around meaning-focused tasks, where students use language to achieve a clear goal rather than to practise specific forms. This creates natural opportunities for spoken academic collocations to occur and be reused. When tasks also include time for planning and reflection, students can notice useful word combinations while focusing on meaning, rather than studying language in isolation. In the present study, these tasks, for example, include preparing and rehearsing academic presentations and planning and supporting an argument, which closely reflect assessment tasks in EAP programmes. From a second language acquisition (SLA) perspective, this raises an important question: how much guidance learners need during task performance. Some SLA approaches focus on instruction and support, while others focus on learning through use, exposure, and learner initiative (Bybee, 2006; Ellis, 2004; Schmidt, 1990).
This theoretical distinction directly informs the task design adopted in the present study. We define guided and unguided GenAI tasks based on the amount of support students receive during task completion. In guided GenAI tasks, learners work with structured prompts or step-by-step instructions that draw attention to specific aspects of language use. In unguided GenAI tasks, learners complete the same communicative goal using GenAI, but with minimal instructions and more freedom in how they interact with the tool. Both task types are meaning-focused and aligned with EAP goals, but they differ in the amount of support provided during task performance. The difference lies not in whether learners notice language, but in how noticing occurs during task work.
Research shows that collocations develop through repeated use in meaningful tasks, often supported by digital tools (Dang et al., 2017; Szudarski & Carter, 2016). However, GenAI research in EAP has focused mainly on writing and reading, leaving the effects of guided and unguided GenAI tasks on academic spoken collocations largely unexplored (Liu et al., 2025; Sako, 2024). The present study addresses this gap by examining how guided and unguided GenAI-mediated tasks influence students’ learning of academic spoken collocations within a TBLT framework. Learner output is analysed using the Academic Spoken Collocation List (ASCL; Li et al., 2024), a corpus-based resource developed from large spoken academic corpora and validated across university-level speaking genres. In this study, TBLT provides the pedagogical framework, while GenAI functions as a tool embedded within tasks to support task completion and language use.
2. Literature review
2.1. Collocations and collocational competence in EAP academic speaking
Research on collocations in applied linguistics shows that they contribute to natural and fluent language use. Speakers rely on stored multiword units to reduce processing load and support fluent production, particularly in speech (Wray, 2002). In EAP, this line of research has increasingly focused on academic phraseology, that is, recurrent word combinations that help speakers organise content, express stance, and manage interaction in academic contexts (Biber et al., 2004; Hyland, 2008).
More recent work has argued that collocational competence in speaking should be treated as a distinct construct. Xu (2018), drawing on data from L2 speaking assessments, proposed the notion of spoken collocational competence (SCC), defined along three dimensions: accuracy, complexity, and fluency. Using performances from L2 learners in different speaking contexts, Xu showed that collocation use varies systematically across interactional settings and is closely related to perceived oral proficiency. Importantly, the study highlights that collocations function differently in spoken academic contexts than in everyday conversation or written tasks, supporting the need for speaking-specific approaches to collocation research and assessment.
Evidence from studies on academic presentations further underscores the importance of collocations in effective academic speaking. Zareva (2015), analysing recorded academic presentations produced by L1 English speakers and advanced L2 university students, examined the lexical composition of successful presentations and found that effective speakers relied on a dense and varied use of academic vocabulary and recurrent word combinations. The analysis revealed both overlap with written academic vocabulary and features specific to oral delivery, such as formulaic sequences used to structure discourse and guide listeners, reflecting the hybrid nature of academic presentations.
Zareva (2020) analysed noun–noun and adjective–noun collocations in student academic presentations using discourse analysis. The study found that collocations play a key role in structuring arguments and supporting comprehension. However, students often mixed academic and informal collocations, which reduced the perceived academic quality of their presentations.
Related findings emerge from Cribb and Wang's (2021) study of Chinese university students delivering academic presentations in English. Using qualitative and quantitative analyses of presentation transcripts, the authors showed that effective presenters deployed academic vocabulary strategically, with collocational knowledge playing a central role in coherence and clarity. The study also highlighted a broader research gap: while academic vocabulary has been widely studied in relation to reading and writing, far less attention has been paid to how learners use academic vocabulary and collocations in spoken academic genres such as presentations.
Despite their importance, collocations remain problematic for many EAP learners. Learner corpus studies show that L2 speakers tend to underuse collocations, rely on a limited set of high-frequency combinations, or produce non-target-like patterns, particularly in speech (Nesselhauf, 2005; Paquot, 2010). These difficulties are especially visible in academic presentations, where speakers must organise arguments, signal stance, and maintain fluency in real time. At the same time, corpus-based research demonstrates that stronger collocational knowledge is associated with higher oral fluency and proficiency (Uchihara et al., 2021), suggesting that collocations are both difficult to acquire and central to successful academic speaking.
A further challenge in EAP research and pedagogy is that many lexical resources are based on written academic texts. Lists such as the Academic Word List (AWL; Coxhead, 2000) capture important aspects of written academic discourse but offer limited coverage of the interactional and organisational language typical of spoken academic genres. Addressing this gap, Li et al. (2024) developed the Academic Spoken Collocation List (ASCL) using large spoken academic corpora and expert ratings of pedagogical relevance. Their corpus-based study showed that ASCL items provide substantially higher coverage in spoken academic corpora than in written ones, offering empirical evidence that spoken academic collocations constitute a distinct lexical domain.
Taken together, this body of research highlights the central role of collocations in academic presentations, while also pointing to persistent learner difficulties and limitations in existing written-oriented resources. These findings provide a strong rationale for focusing on spoken academic collocations in EAP research and for examining instructional approaches that support learners’ development of collocational competence in academic speaking contexts.
2.2. Task-based language teaching (TBLT) and technology integration in EAP
Task-Based Language Teaching (TBLT) is widely used in EAP because it centres on meaning-focused tasks that resemble academic communication practices, such as presenting arguments (Ellis, 2004). In these tasks, learners use language to achieve a communicative goal, and attention to language form arises during task performance rather than through isolated practice. This approach aligns well with the demands of academic speaking, where learners must organise ideas, express stance, and respond appropriately in real time. In academic settings, tasks that involve planning, performance, and reflection enable learners to organise content, evaluate language choices, and gradually improve accuracy and appropriateness. How these tasks are supported during implementation, therefore, becomes a key factor in shaping learning outcomes.
It is at this point that technology-mediated TBLT becomes relevant. Digital tools can provide additional forms of interaction, input, and feedback during task performance, extending the pedagogical potential of TBLT. For example, Mulyadi et al. (2021) investigated technology-enhanced TBLT with university-level English for Specific Purposes learners using online presentations, role-plays, and group discussions. Their quasi-experimental study showed significant gains in listening comprehension and speaking performance, particularly in role-play tasks. While this study demonstrates the potential of technology-mediated TBLT for oral development, it focuses on overall speaking ability rather than specific linguistic features such as collocations, and it does not examine how different types of task support affect learning.
Broader evidence is provided by Bhandari et al. (2025), whose systematic review shows that technology-mediated TBLT is generally associated with increased engagement and improvements in fluency, accuracy, and complexity. At the same time, the review highlights challenges related to task design, teacher preparation, and the meaningful integration of technology. Notably, many studies report positive outcomes without clearly identifying which aspects of task support drive learning gains.
Overall, existing research suggests that technology-mediated TBLT can support language development in EAP-related speaking contexts, but it has largely focused on general performance outcomes rather than on specific linguistic resources such as academic collocations. In addition, limited attention has been paid to how different levels of task support shape learners’ engagement with language. These gaps point to the need for more focused research on task design variables—such as guided versus unguided task implementation—and their impact on academic spoken language use.
2.3. AI-supported language learning in EAP
Recent advances in generative artificial intelligence, particularly large language models such as ChatGPT, have increased interest in their use for language learning. Compared with earlier technologies, generative AI can generate extended discourse, respond dynamically to learner input, and provide on-demand feedback, making it relevant for task-based learning contexts (Mo & Crosthwaite, 2025; Shin & Lee, 2025; Sok & Shin, 2025). At the same time, research consistently shows that learning outcomes depend on how AI tools are used and which language features are targeted.
In second language acquisition and EAP research, most studies on generative AI have focused on written language, especially academic writing and reading. Mo and Crosthwaite (2025), for example, showed that LLM-generated texts can shape stance and engagement in writing, but may also produce register-inappropriate patterns. Similarly, Cai et al. (2025) demonstrated that human–AI collaborative reading tasks can support comprehension and engagement.
Research explicitly addressing speaking remains limited. Liu et al. (2025) evaluated an AI-powered speaking assessment tool with EAP learners and reported increased learner confidence, but the feedback was largely general and did not target specific linguistic features such as collocations. In a task-based classroom study, Sako (2024) found that integrating ChatGPT into collaborative academic projects supported learners’ collocational awareness and critical thinking, although the study relied mainly on qualitative data and did not isolate the effects of different types of AI support on spoken output.
Across this body of research, methods vary widely, including experimental designs, discourse analysis, and learner perception data. Meta-analyses and systematic reviews (Li et al., 2025; Lyu et al., 2025) report overall positive effects of AI-assisted language learning, but also stress that outcomes are inconsistent and strongly shaped by task design and learner engagement. In particular, many studies do not clearly specify how learners are guided in their interactions with AI tools.Taken together, existing AI-in-EAP research provides limited evidence on how AI-mediated tasks support the learning of spoken academic collocations, particularly regarding how different levels of instructional guidance shape learners’ language use.
2.4. Guided and unguided AI-supported collocation learning
Research on AI-assisted second-language learning indicates that vocabulary development is among the areas most strongly supported by AI tools. A recent meta-analysis by Xu et al. (2025) reports larger effects for vocabulary learning than for other language skills and notes differences between guided and autonomous AI-supported learning. Although both task conditions in the present study were classroom-based, this distinction is relevant for understanding how varying levels of guidance during AI-mediated tasks may shape learning outcomes. Classroom-based research further suggests that guidance can enhance AI-supported vocabulary learning. Feng (2025), for example, found that guided interactive AI lessons directing learners' attention to problem areas led to stronger vocabulary gains and lower cognitive load than less structured approaches. These findings suggest that guidance can help learners focus their attention and use AI support more efficiently. At the same time, unguided AI use is increasingly common, particularly when learners interact with AI tools independently (Sako, 2024). While such use may promote autonomy and exploratory learning, learners’ attention to lexical form can be uneven without instructional support (Schmidt, 1990), especially for complex multiword units such as collocations, which are difficult to notice and acquire incidentally (Wray, 2002).
Despite growing evidence that guidance plays a role in AI-assisted vocabulary learning, few studies have directly compared guided and unguided AI-mediated tasks within the same instructional context, and even fewer have examined their effects on spoken academic collocations. Most existing research focuses on single-word vocabulary or written tasks, leaving open questions about how different levels of guidance shape learners’ noticing and use of collocations during task-based academic speaking preparation. By comparing guided and unguided GenAI-mediated tasks within a TBLT framework, the present study addresses this gap and examines how instructional support influences collocational learning in academic speaking contexts.
3. The study
This study was based on the hypothesis that, even when using AI tools, upper-intermediate (B2) English learners still benefit from guided tasks to better notice and acquire target language features such as academic spoken collocations. To test this, we compared three groups: a guided group, an unguided group, and a non-AI control group. By adopting a mixed-methods approach that combines corpus analysis with quantitative investigations, this study aimed to evaluate the frequency and correct use of academic spoken collocations in students' outputs. Through the systematic analysis of seven learner outputs per participant, ranging from pretest to delayed posttest and including four academic tasks, the study investigated whether generative AI tools can support collocation learning. These tasks were aligned with the students' EAP course requirements and reflected typical academic speaking activities. Specifically, they focused on defining and developing an argument, preparing and rehearsing an academic presentation, responding to pro and con questions, and discussing or debating ideas. All tasks were communicative, goal-oriented, and designed to elicit academic spoken collocations in contexts relevant to students’ coursework (see also 4.3.3).
Two primary research questions guided this investigation:
1.
Can the guided use of ChatGPT, paired with structured learning activities, foster the acquisition of academic spoken collocations more effectively than unguided interactions with ChatGPT?
2.
What is the impact of generative AI tools on the acquisition of academic spoken collocations compared to non-AI-based instruction?
This study used written interactions with ChatGPT to help students learn academic spoken collocations. Although the responses from ChatGPT were written, the activity was designed to support students as they prepared their spoken presentations. Academic speech, such as conference talks or student presentations, is often written in advance and shares many features with formal written language (Biber & Conrad, 2019; Biber et al., 2004). This written-to-be-spoken nature of academic presentations explains why students often prepare notes or scripts in advance, even though the final output is delivered orally. By interacting with ChatGPT, students could see examples of how academic collocations are used and try them out themselves. During task completion, students engaged with ChatGPT by asking for suggestions, examples, or reformulations related to their arguments or presentations. This process allowed them to notice useful phrases and plan their spoken output. Moreover, according to task-based language teaching (Ellis, 2004), planning is an important part of learning. The written interaction with ChatGPT supported both noticing of academic collocations and deliberate planning of how to use them in speech, increasing the likelihood that learners could retrieve and use these collocations during oral performance.
4. Methodology
4.1. Research design
This study employed a mixed-methods, quasi-experimental design to investigate the impact of guided and unguided ChatGPT-mediated learning activities on the acquisition of academic spoken collocations among B2-level university students. A total of 75 students participated in the project; however, the analysis focused on 37 learners who met the inclusion criteria of attending all lessons and completing all seven tasks, ensuring full exposure to the intervention and comparable datasets. All participants were first- or second-year undergraduate students enrolled in a Foreign Languages BA programme, with English as their major and a second language as their minor. English proficiency was verified using the Oxford Placement Test, administered one week prior to the start of the study.
Over a ten-week period, participants completed seven text-based outputs: a pretest, four task-based activities (T1–T4), a posttest, and a delayed posttest (see Table 1). The study compared three instructional conditions: a guided AI group, an unguided AI group, and a non-AI control group. Students interacted with ChatGPT in English throughout the tasks. All three groups completed the same tasks and covered the same course content; the only difference across conditions concerned the type of instructional support provided during task completion. The study was embedded within the existing EAP curriculum, where oral presentations are a core component of assessment, and the focus on academic spoken collocations was therefore aligned with course objectives rather than introduced as an additional intervention (Ellis, 2004; Mackey & Gass, 2022).
Table 1. Research Timeframe.

Week(s)	Activity
Week 1	Pretest
Weeks 2–5	Experimental tasks
Week 6	Posttest
Week 7–9	Regular instruction
Week 10	Delayed posttest
4.2. Participants
Participants were randomly assigned to one of three instructional conditions. The guided group (N = 16) interacted with ChatGPT through structured tasks, including guided prompts, attention-focusing questions, and explicit task objectives designed to encourage the use of target academic collocations. The unguided group (N = 14) completed the same communicative tasks using ChatGPT but with open-ended instructions, allowing learners to interact freely with the tool without prompts designed to focus attention on specific language features.
The control group (N = 7) completed the same tasks without ChatGPT or any other AI tools. Students in this group were allowed to use standard digital resources, such as websites, Google searches, and spell checkers, reflecting typical academic study practices. This design ensured that all groups had comparable opportunities to engage with the same tasks and content, with differences limited to the type of support provided rather than access to learning resources. Each participant completed the pretest, posttest, and delayed posttest, as well as four task-based activities simulating real-world academic speaking assignments (see Section 4.3.3).
Following TBLT principles (Ellis, 2004), target collocations were never explicitly taught or named during the four experimental tasks, and students in all groups were unaware of the specific linguistic focus of the study. Time on task was controlled across conditions (60 min for the four tasks; 90 min each for the pretest, posttest, and delayed posttest). All ChatGPT interactions and final task outputs were saved and transcribed for quantitative analysis.
4.3. Materials
4.3.1. The Academic Spoken Collocation List (ASCL)
The ASCL was used as the reference framework for identifying the use of collocations in learner outputs. Comprising 417 two-word academic collocations derived from the British Academic Spoken English (BASE) corpus, the ASCL captures typical lexical combinations used in spoken academic discourse. The list was developed using a frequency-based mixed-methods approach and validated through linguistic and contextual analysis (Evert, 2008; Gablasova et al., 2017). The ASCL informed task design by guiding the selection of target collocations and was used to identify collocations in learner outputs. In order to maintain ecological validity, a decision was made not to explicitly train ChatGPT on the ASCL collocations. The main aim was to replicate, as closely as possible, real-world tasks and conditions in which learners interact with ChatGPT and receive unmodified responses, rather than using tools specifically trained to elicit specific language features. This approach allowed for a more authentic assessment of learners’ ability to notice, acquire, and use collocations independently.
4.3.2. Pretest, posttest and delayed posttest
The pretest, posttest, and delayed posttest assessed learners' knowledge of academic collocations through both receptive and productive tasks. The tests were identical, with items drawn from spoken academic materials such as student presentations and video lectures to ensure construct validity. We did not use full oral presentations as test data; instead, the tests focused on the language needed for academic presentations so that results could be compared reliably across testing phases. All responses were scored on a binary scale (0–1), providing consistency and reliability across testing occasions. The test comprised two sections: prompting tasks with ChatGPT and meaning-focused exercises. In the prompting section, learners first completed an open-ended (unguided) task by generating prompts and responses for an academic scenario, and then a guided task where they were given structured prompts and had to incorporate ChatGPT's suggestions. These activities supported learners' productive use of collocations typical of spoken academic discourse, as reflected in the final task outputs produced after interaction with ChatGPT. The meaning-focused section included a sentence-completion task (four items) requiring the retrieval of appropriate academic collocations (Fig. 1), and a paraphrasing task (four items) that combined recognition and controlled production. Overall, the section contained eight items plus two interactive tasks, offering a balance of closed- and open-ended formats. Importantly, the test closely resembled the format of the subsequent experimental tasks, thereby ensuring task repetition, which is widely recognized as a facilitator of L2 learning (Bygate, 2013; Ellis, 2009). All groups completed the same test to ensure comparable testing conditions, allowing differences in performance to be attributed to the task phase rather than to variations in test conditions. The structure and content of the tests were intentionally aligned with the academic tasks. This alignment ensured that the tests measured language relevant to and practised during task performance.
Fig. 1
Download: Download high-res image (132KB)
Download: Download full-size image
Fig. 1. Sample test item measuring academic spoken collocations.

4.3.3. Procedure
In line with the principles of TBLT, the four experimental, technology-mediated tasks were communicative, goal-oriented, and designed to elicit academic spoken collocations. Each task focused on a different academic speaking function required in the students’ courses: (1) defining and structuring an academic argument, (2) preparing and rehearsing an academic presentation, (3) responding to pro and con questions, and (4) discussing or debating ideas and responding to questions. Before the treatment, all participants completed the academic spoken collocation pretest. During the treatment, students in the two experimental groups (guided and unguided) took part in four weekly AI-mediated tasks, while the control group performed the same tasks without AI support. Each task was completed in the university computer lab under the supervision of the two researchers. The AI-mediated tasks were completed using four separate ChatGPT chats, as the 4o AI model used at the time of the study did not support content transfer between chats.
In the four weekly sessions, participants in all three groups were asked to produce short academic texts in spoken form. These activities were integrated into regular classroom sessions and mirrored tasks students were required to complete for assessment in their EAP and linguistics courses. Students in the unguided group received general task instructions (e.g., Task 2 “use ChatGPT to help you structure your argument on this topic”) and interacted freely with ChatGPT to brainstorm, organise ideas, or request feedback. By contrast, the guided group followed structured steps: they analysed and rewrote academic samples, asked ChatGPT to enhance them using more natural and formal collocations, and then incorporated those collocations into new outputs. The control group completed the same communicative tasks without access to AI.
The researchers were present during all sessions to supervise the process and provide technical assistance if needed, but did not offer any additional support related to collocations. After the treatment, all participants completed the posttest, and four weeks later, they completed the delayed posttest. An example of an academic task used in the study is provided in Appendix A.
4.4. Data collection
Each participant completed a set of seven text-based outputs as part of their interaction with ChatGPT, for a total of approximately 8500 words each. These included a pretest, followed by four weekly AI-mediated academic tasks (T1–T4), a posttest, and a delayed posttest administered one month after the posttest. Each task was designed around real-life university meaning-making tasks to elicit the use of academic English relevant to university-level speaking contexts. The same discourse functions targeted in the tasks (e.g. argument structure, stance expression, and response management) were also reflected in the test items, ensuring consistency across data collection points.
All interactions and final outputs were transcribed and systematically reviewed by the two researchers, who analysed the anonymized outputs; students signed consent forms and used anonymous profiles to ensure ethical compliance. Data were organized to include the type of collocation, the number of occurrences, and the accuracy of use for each student at every stage of the study (pretest, T1–T4, posttest, delayed posttest). This allowed for both a quantitative overview of collocation use and a detailed, student-specific analysis. The collocational analysis was limited to items found in the ASCL. For each collocation, two variables were recorded: its frequency of occurrence and its accuracy. Accuracy was evaluated on both grammatical and contextual contexts and was coded as a binary value (0 = inaccurate, 1 = accurate). Frequency referred to the raw number of times a given collocation appeared in the student's output. Although responses were written, the tasks elicited collocations intended for spoken academic use, reflecting the planning stage of academic presentations and discussions.
5. Analysis
5.1. Corpus-based collocation extraction
To identify the use of target collocations in learner output, students’ interactions with ChatGPT were saved and organized by activity type: pretest, four task-based activities (T1–T4), posttest, and delayed posttest. Each set of texts was saved as a plain.txt file and uploaded into Sketch Engine for corpus analysis. The full list of 417 collocations from the ASCL was used as the reference. To verify the presence of these collocations in student texts, a manual Corpus Query Language (CQL)–based search was performed within Sketch Engine, using lemma-based matching. We identified candidate tokens of ASCL items by running manual CQL queries in Sketch Engine over our learner-only subcorpus (AI turns excluded). Queries targeted lemmatized pairs in order, allowing up to three intervening tokens and ignoring punctuation. The core pattern was:
where [] matches any token and {0,3} sets the maximum span. We adapted queries as needed (e.g., optional function words/variants, hyphenated forms), inspected the resulting concordance lines, and manually coded each hit as valid vs. spurious before inclusion in counts. This procedure ensures that only contextually appropriate realizations of the target collocations were tallied.
This allowed for accurate identification of collocations regardless of morphological variation. Each instance was checked to ensure alignment with the ASCL criteria and to eliminate false positives (i.e. incorrectly identified target academic collocation). Lastly, each valid occurrence was annotated for participant identity, instructional group, task or test identifier, collocation frequency, and accuracy.
The use of the ASCL as a reference point was intended to ensure that the analysis remained focused on authentic and pedagogically relevant academic collocations. By selecting ASCL-validated collocations in spoken academic contexts, the study avoided the pitfalls of less precise n-gram-based analyses (cf. Simpson-Vlach & Ellis, 2010) and grounded its measures in actual language use.
5.2. Quantitative analysis
Following common practice in applied linguistics research, repeated-measures ANOVA was first used to provide an overall comparison of group and time effects. Linear mixed-effects models were then employed to account for participant-level variability and task-level effects, particularly given the repeated observations and unbalanced group sizes (Baayen et al., 2008; Plonsky & Oswald, 2017). Data from the corpus analysis were analysed using both repeated-measures ANOVA and linear mixed-effects models (LMMs) in Jamovi 2.6.24. To investigate whether guided AI use had a differential effect on collocation acquisition compared to unguided or non-AI instruction, a repeated-measures ANOVA was conducted. Two dependent variables were used: collocation accuracy and collocation frequency. Both were aggregated per participant at each of the three test points (Pretest, Posttest, and Delayed Posttest).
The ANOVA provided a general overview of the test results, examining whether there were significant differences between the scores of the guided, unguided, and non-AI groups. In addition to ANOVA, linear mixed-effects models were used to provide a more detailed analysis of collocation use across all tests and tasks. LMMs also accounted for individual differences in student performance, which is a crucial aspect in second language acquisition and teaching.
6. Results
The analysis focused on two outcome variables: collocation accuracy and collocation frequency, measured across three test points (Pretest, Posttest, Delayed posttest). The results of the study, based on repeated-measures ANOVA and linear mixed-effects models (LMMs), revealed several interesting patterns. Some aligned with initial expectations, while others were unexpected. Generally, students used a relatively restricted range of academic spoken collocations (N = 60), and often, as expected, the more frequent ones (see Table 2 and Appendix B for examples of collocations produced by each group).
Table 2. Examples of collocations participants used in the project.

Group	Example collocation	Total occurrences	Description from the ASCL
Unguided	Talk about	31	Discourse-organising collocation (topic framing)
Unguided	In particular	12	Stance/focus-marking collocation
Unguided	In general	4	Generalising discourse marker
Guided	Talk about	20	Discourse-organising collocation
Guided	In fact	11	Discourse marker (contrast/emphasis)
Guided	Rather different	10	Evaluative stance collocation
Control	Talk about	5	Discourse-organising collocation (limited range)
Control	In fact	4	Discourse marker (isolated use)
6.1. Effectiveness of guided and unguided use of ChatGPT
For collocation accuracy, the repeated-measures ANOVA showed no significant main effect of Time, F(2, 68) = 0.457, p = .635, indicating that accuracy scores remained relatively stable from Pretest to Delayed Posttest across all groups. The interaction between Time and Group was also not significant, F(4, 68) = 0.535, p = .710, suggesting that the three groups followed similar patterns of change over time. However, a significant main effect of Group emerged, F(2, 34) = 5.43, p = .009, indicating overall differences in accuracy across instructional conditions. Post hoc tests revealed that the unguided group showed significantly higher accuracy scores than both the guided (p = .027) and control (p = .029) groups, while no significant difference was found between the guided and control groups. A similar pattern was observed for collocation frequency. Although neither the main effect of Time, F(2, 68) = 1.89, p = .160, nor the Time × Group interaction, F(4, 68) = 1.27, p = .289, reached significance, the main effect of Group was again significant, F(2, 34) = 7.83, p = .002. Post hoc comparisons confirmed that the unguided group produced a significantly higher number of collocations than both the guided (p = .011) and control (p = .004) groups, with no significant difference between the latter two. Q–Q plots confirmed that the residuals for both accuracy and frequency were approximately normally distributed, supporting the validity of the ANOVA results.
Results from the linear mixed-effects models (LMMs) analysis offered additional insight. In intact classroom settings, students may share similar proficiency levels but differ in a range of individual factors. To account for this variability, LMMs were used with Participant included as a random intercept, allowing for individual differences in starting levels regardless of group, task, or time point.
The linear mixed-effects models for accuracy and frequency both revealed significant effects of instructional condition, suggesting an association between AI-mediated tasks on learners’ collocational competence. For accuracy, there was a significant main effect of Group (F(2, 33.7) = 8.785, p < .001), with both the guided (Estimate = 0.2743, p = .024) and unguided (Estimate = 0.4773, p < .001) groups outperforming the control group. The interaction between Time and Group was also significant (F(12, 286.2) = 1.856, p = .040), suggesting that group differences were not uniform across time, although most pairwise comparisons were not individually significant. For frequency, a similar pattern emerged. The Group effect was again significant (F(2, 33.5) = 5.976, p = .006), with the unguided group producing significantly more collocations than the control group (Estimate = 0.4626, p = .002), while the guided group showed a positive trend that did not reach significance (Estimate = 0.2579, p = .069). The interaction between Time and Group was also significant (F(12, 288.0) = 2.546, p = .003), suggesting that group differences in frequency were not constant over time. Fig. 2 shows the collocation frequency across time by group. While changes over time were modest, the figure illustrates that group differences were not uniform across testing points, in line with the significant Time × Group interaction. No significant main effects of Time were found in either model, indicating that observed differences were more closely associated with instructional condition than with time within the duration of the study. In both models, random intercepts for participants accounted for approximately 18–19 % of the variance, confirming the importance of individual differences in collocational use.
Fig. 2
Download: Download high-res image (186KB)
Download: Download full-size image
Fig. 2. Collocation frequency across time by instructional group.

Both the repeated-measures ANOVA and the linear mixed-effects models indicated that the unguided group tended to use academic collocations with higher accuracy and frequency than the other groups. While ANOVA showed clear group differences but no change over time, the LMMs found that group performance did vary across different tasks. LMMs also took into account individual differences between learners, making the analysis more detailed. Using both methods helped confirm the results: ANOVA showed clear group trends, while LMMs gave a deeper look at how performance changed across time and between learners.
Importantly, the non-AI group consistently exhibited the lowest accuracy scores and, in many cases, produced no target collocations. This pattern points to the potential role of AI integration: even when not guided, learners interacting with ChatGPT were more likely, in this dataset, to use and produce academic collocations than those completing traditional language tasks without AI support.
7. Discussion
This study investigated the impact of generative AI, specifically ChatGPT, on the use and acquisition of academic spoken collocations by B2-level EAP learners. Two research questions guided the investigation: (1) whether guided use of ChatGPT paired with structured learning activities fosters the acquisition of academic spoken collocations more effectively than unguided interactions, and (2) whether AI-mediated instruction leads to different outcomes compared to non-AI-based instruction. To address these questions, we compared three groups (guided, unguided, and control/non-AI) across three test points (pretest, posttest, delayed posttest) and four academic tasks (T1–T4), analysing changes in the accuracy and frequency of academic spoken collocations as defined by the ASCL. The findings offer insights into how different forms of AI-mediated task engagement shape collocational development in academic speaking contexts.
7.1. Guided and unguided AI use in academic tasks
The main hypothesis guiding the study was that, although all groups completed the same academic tasks with the same expected outcomes (e.g. preparing an academic presentation or responding to questions), the guided group—who received structured prompts and scaffolding while interacting with ChatGPT—would outperform the unguided and control groups in both accuracy and frequency of academic spoken collocation use. However, results from the repeated-measures ANOVA and the linear mixed-effects models (LMMs) did not fully support this assumption.
These findings need to be interpreted in relation to the specific characteristics of the participants and the learning context. The learners were B2-level EAP students who were already familiar with academic speaking tasks as part of their degree programme. For these learners, unguided interaction with ChatGPT may have provided greater flexibility to explore language while working on tasks such as structuring arguments, rehearsing presentation openings, or formulating responses to questions. This helps explain why the unguided group showed steady development in collocation use over time, despite the lack of explicit scaffolding. However, this pattern should be interpreted with caution, as it reflects the results of a specific group of B2 learners in this particular learning context.
From an SLA perspective, the performance of the unguided group is consistent with usage-based and incidental learning accounts (Bybee, 2006; Ellis, 2004), especially when considered in relation to the task. Rather than learning through exposure alone, learners actively used ChatGPT to test ideas, model academic phrasing, and refine their output during task preparation. In this context, noticing (Schmidt, 1990) was driven by learners’ communicative goals rather than by prompts provided by the task design.
By contrast, the guided group followed more structured steps that explicitly directed attention to language form. While this support may have helped control accuracy, it may also have reduced opportunities for broader experimentation with collocations during task completion. For B2 learners engaged in familiar academic tasks, this level of structure may have constrained learner agency or increased cognitive demands (Skehan, 1998), limiting the range of collocations used in the final outputs.
7.2. AI-mediated vs. non-AI instruction
Beyond differences between guided and unguided AI use, the results also allow a comparison between AI-mediated and non-AI-based instruction. Learners in the control group consistently underperformed in both accuracy and frequency of academic spoken collocations, confirming earlier findings that collocational competence remains a weak point in traditional instruction (Nesselhauf, 2005; Paquot, 2010). In contrast, both AI groups produced richer collocational output, reflecting the affordances of generative AI for modelling, reformulation, and contextualised language support (Mo & Crosthwaite, 2025; Zou et al., 2023). These findings are in line with recent meta-analyses (Li et al., 2025; Lyu et al., 2025), which report positive effects of chatbot-assisted language learning when tasks are interactive and meaning-focused.
The analyses reported here are primarily descriptive and comparative: the linear mixed-effects models were used to identify group-level patterns over time under controlled testing conditions, rather than to explain learning mechanisms. Accordingly, the results for the second research question indicate relative differences across task conditions and complement the more focused analysis of guided versus unguided AI use.
7.3. Development of academic spoken collocations in EAP contexts
Taken together, these findings also shed light on how academic spoken collocations develop in EAP contexts. Although the analysed data were written, the collocations examined were drawn from spoken academic discourse and are typical of presentations and question-and-answer sessions. In EAP settings, academic speaking is rarely fully spontaneous; students typically plan arguments, transitions, and stance expressions in writing before oral delivery. The written task outputs analysed in this study, therefore, represent an important stage in the development of spoken academic competence.
The absence of strong Time effects across groups highlights the difficulty of collocation acquisition within a limited instructional period. Research on formulaic language consistently shows that collocations require extended exposure, repeated retrieval, and sustained use before becoming automatized (Uchihara et al., 2021; Wood, 2006; Wray, 2002). The ten-week study period may not have been long enough to show clear long-term gains, especially in productive use. This aligns with Granger and Bestgen's (2014) observation that even advanced learners rely on a restricted set of collocations, and with Myles' (2015) argument that productive mastery of formulaic language develops slowly over time. Overall, the findings suggest that guidance can improve accuracy, whereas greater use of autonomous AI may support the productive use of academic spoken collocations among upper-intermediate EAP learners, though the results are indicative rather than conclusive.
8. Pedagogical implications for EAP contexts
This study highlights the specific demands of EAP speaking, which go beyond general fluency and include structuring arguments, expressing stance, and managing academic discourse in presentations and discussions (Biber & Barbieri, 2007; Hyland, 2008). These abilities rely heavily on spoken academic collocations, which many B2 learners still underuse (Granger & Bestgen, 2014; Nesselhauf, 2005). Resources such as the ASCL are therefore particularly relevant for EAP instruction, as they target collocations typical of spoken academic registers.
At the same time, the results point to the importance of task design in AI-supported learning. Although guided activities were intended to support learning, the unguided group achieved higher accuracy and frequency of collocation use. For these B2 learners, open-ended interaction with ChatGPT during task preparation (e.g. drafting presentation openings or planning responses to questions) appeared to encourage experimentation and noticing of useful expressions. From a pedagogical perspective, this suggests that explicit instruction in EAP does not necessarily require strictly controlled procedures. It can instead be realised through tasks that make key spoken academic collocations salient while allowing learners autonomy to explore and reuse them with AI support.
9. Limitations
This study has several limitations that should be considered when interpreting the findings. First, the analytic sample was relatively small and uneven across groups, particularly for the control condition, which limits the strength of statistical inference and the generalisability of the results. In addition, the analysis focused on students who completed all tasks and test sessions; therefore, the findings reflect a fully engaged subgroup and may not represent the broader student population.
Second, the study relied on written task outputs that supported the planning and rehearsal of academic speaking, rather than on recorded oral performance. Although this reflects common EAP practice, future studies should examine whether similar patterns emerge in spoken production.
Finally, the study was exploratory and did not include qualitative data on learners’ interactions with ChatGPT. As a result, claims about noticing, autonomy, and learning processes remain inferential. Further research combining quantitative and qualitative methods, with larger and more diverse samples, is needed to better understand how different learners engage with AI-mediated tasks in varied institutional contexts.
10. Conclusions
This study examined whether ChatGPT-supported tasks help B2 EAP learners use academic spoken collocations, and whether guidance influences learning. Overall, learners in the AI conditions produced more frequent and accurate ASCL collocations than the non-AI group, and the unguided AI group showed the strongest performance. This suggests that, for B2 learners working on academic speaking tasks, open-ended interaction with AI can support collocation use during task preparation.
At the same time, gains over time were limited. This confirms that spoken academic collocations are difficult to acquire quickly, even with repeated task engagement. Learning academic spoken collocations takes time and repeated practice, rather than short-term instruction. From a pedagogical perspective, the findings point to the importance of task design rather than the amount of guidance alone. In this study, tasks linked to academic speaking goals, combined with AI support, created opportunities for learners to notice and use spoken academic collocations in their final task outputs. Allowing learners more control during AI interaction may be beneficial at the B2 level. Future research should examine whether such gains transfer to oral performance and how different levels of guidance affect learners at lower proficiency levels.
CRediT authorship contribution statement
Valentina Morgana: Writing – review & editing, Writing – original draft, Methodology, Investigation, Formal analysis, Data curation, Conceptualization. Francesca Poli: Writing – original draft, Methodology, Investigation, Data curation.
Declaration of generative AI and AI-assisted technologies in the writing process
During the preparation of this work, the authors used ChatGPT 4o/5.2 in order to support general language editing and to improve readability and style. After using this tool/service, the authors reviewed and edited the content as needed and take full responsibility for the content of the publication.
Funding
This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.
Conflict of interest
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
Appendix A. Student Worksheet
Task 3: Managing Disagreement in Academic Discussion.
Part 1. Task Description.
You will work on a short academic-style disagreement scenario. Your goal is to prepare language for use in a spoken academic discussion, such as a seminar or a Q&A session.
The final output should be written, but it should be intended for spoken use.
Part 2. Task Instructions.
If you are using ChatGPT (Guided Group).
1.
Read the short dialogue provided by your professor.
2.
Use ChatGPT to revise the dialogue to make it sound more natural and appropriate for an academic discussion.
3.
Compare your original and revised versions.
4.
Identify useful expressions or word combinations introduced by ChatGPT.
5.
Create a new short dialogue on a similar topic, using some of the expressions you noticed.
If you are using ChatGPT (Unguided Group).
1.
Describe the disagreement scenario to ChatGPT.
2.
Interact freely with ChatGPT to:
o
explore ways to express disagreement politely,
o
support an argument,
o
respond to opposing views.
3.
Use the interaction to help you plan how you would speak in an academic discussion.
4.
Write a short dialogue or response that could be used in a spoken academic context.
If you are not using ChatGPT (Non-AI Group).
1.
Watch the short video or read the short text provided by your professor.
2.
Identify expressions used to manage disagreement and respond to opposing views.
3.
Rewrite a short dialogue or response using similar language.
4.
Your text should be suitable for spoken academic interaction.
Part 3. Final Output.
•
Write a short dialogue or response (approximately 120–150 words).
•
The text should be appropriate for spoken academic use, such as:
o
a seminar discussion,
o
a presentation Q&A,
o
an academic debate.
•
Focus on clarity, stance expression, and organization of ideas.
Appendix B. Collocations used in the study and examples from students' interactions with ChatGPT
Collocation	Example
answer to	<no I want to answer to your questions, can I?>
at the beginning	<we could put the possible middle ground at the beginning of the presentation>
different country	<how the phenomenon is projected to develop in the future>
end of	<and arrives at the end of the day with the head exploding>
for example	<For example, a friend's advice or support can help someone feel better>
good idea	<yes, that's a good idea>
good way	<find a good way to frame this argument>
write about	<write about Italian stereotypes, typical food, music etc … >
happen in	<you have to be ready for everything to happen in just a few seconds>
idea about	<the best way to express my idea about a presentation I'm preparing>
in direction	<without incorporating significant changes in their operations which actually go in that direction.>
in fact	<In fact, it has a specific structure where Mexicans put different objects that have a proper meaning>
in general	<Could you please add a section about the evolution of art in general?>
in particular	<but I do not remember liking one in particular>
interested in	<I've been interested in this topic since when I was a kid>
key point	<the key point is that I believe that their help should not be based on telling them “can I help you?”>
make point	<and I want to make the point that accents are not mistakes>
move into	<I wanted to move into some of the negative effects that it can cause>
move to	<Also, many people move to new cities for work or studies>
other way	<mm. an other way? Of cuours [sic] with an academic language>
particular about	<more in particular about the non-democratic regimes>
way of	<I could absolutely tell how harsh this way of living is>
put together	<ok, now put these things together with the presentation>
really interested	<and the global population are not really interested in helping our world out of this crisis>
really interesting	<I think “Should AI-generated art be considered genuine art?” is a really interesting topic!>
same thing	<do the same thing>
solve problem	<which improve their ability to think critically and solve problems efficiently [sic]>
talk about	<I would like to talk about the musical Mamma mia!>
talk more about	<maybe you could talk more about about technological influence>
References
Baayen et al., 2008
R.H. Baayen, D.J. Davidson, D.M. Bates
Mixed-effects modeling with crossed random effects for subjects and items
Journal of Memory and Language, 59 (4) (2008), pp. 390-412, 10.1016/j.jml.2007.12.005
View PDF
View articleView in ScopusGoogle Scholar
Bhandari et al., 2025
L.P. Bhandari, N. Dahal, J.R. Awasthi, S. Dhungana
Technology-mediated task-based language teaching: A systematic review
Cogent Education, 12 (1) (2025), Article 256005, 10.1080/2331186X.2025.2560051
Google Scholar
Biber and Barbieri, 2007
D. Biber, F. Barbieri
Lexical bundles in university spoken and written registers
English for Specific Purposes, 26 (3) (2007), pp. 263-286, 10.1016/j.esp.2006.08.003
View PDF
View articleView in ScopusGoogle Scholar
Biber and Conrad, 2019
D. Biber, S. Conrad
Register, genre, and style
Cambridge University Press (2019)
Google Scholar
Biber et al., 2004
D. Biber, S. Conrad, V. Cortes
If you look at…: Lexical bundles in university teaching and textbooks
Applied Linguistics, 25 (3) (2004), pp. 371-405, 10.1093/applin/25.3.371
View in ScopusGoogle Scholar
Bybee, 2006
J. Bybee
From usage to grammar: The mind's response to repetition
Language, 82 (4) (2006), pp. 711-733, 10.1353/lan.2006.0186
View in ScopusGoogle Scholar
Bygate, 2013
M. Bygate
Effects of task repetition on the structure and control of oral language
M. Bygate, P. Skehan, M. Swain (Eds.), Researching pedagogic tasks: Second language learning, teaching, and testing, Routledge (2013), pp. 23-48
View in ScopusGoogle Scholar
Cai et al., 2025
J. Cai, M. Li, K. Hyland
Human–AI collaborative reading in academic contexts
Journal of English for Academic Purposes, 75 (2025), Article 101506, 10.1016/j.jeap.2025.101506
Google Scholar
Coxhead, 2000
A. Coxhead
A new academic word list
Tesol Quarterly, 34 (2) (2000), pp. 213-238, 10.2307/3587951
View in ScopusGoogle Scholar
Cribb and Wang, 2021
V.M. Cribb, X. Wang
Making academic vocabulary count through strategic deployment in oral presentations by Chinese students of English
Language Learning Journal, 49 (2) (2021), pp. 251-264, 10.1080/09571736.2019.1566396
View in ScopusGoogle Scholar
Dang et al., 2017
T.N.Y. Dang, A. Coxhead, S. Webb
The academic spoken word list
Language Learning, 67 (4) (2017), pp. 959-997, 10.1111/lang.12253
View in ScopusGoogle Scholar
Ellis, 2004
R. Ellis
Task-based language learning and teaching
Oxford University Press (2004)
Google Scholar
Ellis, 2009
R. Ellis
Task-based language teaching: Sorting out the misunderstandings
International Journal of Applied Linguistics, 19 (3) (2009), pp. 221-246, 10.1111/j.1473-4192.2009.0023
View in ScopusGoogle Scholar
Evert, 2008
S. Evert
Corpora and collocations
A. Lüdeling, M. Kytö (Eds.), Corpus linguistics: An international handbook, Mouton de Gruyter (2008), pp. 1212-1248, 10.1515/9783110213881.2.1212
View in ScopusGoogle Scholar
Feng, 2025
L. Feng
Investigating the effects of artificial intelligence-assisted language learning strategies on cognitive load and learning outcomes: A comparative study
Journal of Educational Computing Research, 62 (8) (2025), pp. 1741-1774, 10.1177/07356331241268349
Google Scholar
Gablasova et al., 2017
D. Gablasova, V. Brezina, T. McEnery
Collocations in corpus-based language learning research: Identifying, comparing, and interpreting the evidence
Language Learning, 67 (S1) (2017), pp. 155-179, 10.1111/lang.12225
View in ScopusGoogle Scholar
González-Lloret and Ortega, 2014
M. González-Lloret, L. Ortega (Eds.), Technology-mediated TBLT: Researching technology and tasks, John Benjamins (2014), 10.1075/tblt.6
Google Scholar
Granger and Bestgen, 2014
S. Granger, Y. Bestgen
The use of collocations by intermediate vs. advanced non-native writers: A bigram-based study
International Review of Applied Linguistics in Language Teaching, 52 (3) (2014), pp. 229-252, 10.1515/iral-2014-0011
View in ScopusGoogle Scholar
Hyland, 2008
K. Hyland
Academic clusters: Text patterning in published and postgraduate writing
International Journal of Applied Linguistics, 18 (1) (2008), pp. 41-62, 10.1111/j.1473-4192.2008.00178.x
View in ScopusGoogle Scholar
Kirkwood and Price, 2014
A. Kirkwood, L. Price
Technology-enhanced learning and teaching in higher education: What is ‘enhanced’ and how do we know? A critical literature review
Learning, Media and Technology, 39 (1) (2014), pp. 6-36, 10.1080/17439884.2013.770404
View in ScopusGoogle Scholar
Li et al., 2025
Y. Li, J. Wang, H. Yang
Chatbot-assisted language learning: A meta-analysis
System, 124 (2025), Article 103612, 10.1016/j.system.2025.103612
Google Scholar
Li et al., 2024
L. Li, H. Xu, X. Zhang
Creation and application of an academic spoken collocation list
Tesol Quarterly, 59 (1) (2024), pp. 518-528, 10.1002/tesq.3339
Google Scholar
Liu et al., 2025
X.J. Liu, J. Wang, B. Zou
Evaluating an AI speaking assessment tool: Score accuracy, perceived validity, and oral peer feedback as feedback enhancement
Journal of English for Academic Purposes, 75 (2025), Article 101505, 10.1016/j.jeap.2025.101505
View PDF
View articleView in ScopusGoogle Scholar
Lyu et al., 2025
B. Lyu, C. Lai, J. Guo
Effectiveness of chatbots in improving language learning: A meta‐analysis of comparative studies
International Journal of Applied Linguistics, 35 (2) (2025), pp. 834-851, 10.1111/ijal.12668
Google Scholar
Mackey and Gass, 2022
A. Mackey, S.M. Gass
Second language research: Methodology and design
(3rd ed.), Routledge (2022), 10.4324/9780367809730
Google Scholar
Mo and Crosthwaite, 2025
J. Mo, P. Crosthwaite
Generative AI in EAP: Opportunities and challenges
Journal of English for Academic Purposes, 75 (2025), Article 101520, 10.1016/j.jeap.2025.101520
Google Scholar
Mulyadi et al., 2021
D. Mulyadi, T.D. Wijayatiningsih, C.K.S. Singh, E.F. Prastikawati
Effects of technology enhanced task-based language teaching on learners' listening comprehension and speaking performance
International Journal of Instruction, 14 (3) (2021), pp. 717-736, 10.29333/iji.2021.14342a
View in ScopusGoogle Scholar
Myles, 2015
F. Myles
Second language acquisition theory and learner corpus research
S. Granger, G. Gilquin, F. Meunier (Eds.), The Cambridge handbook of learner corpus research, Cambridge University Press (2015), pp. 309-332, 10.1017/CBO9781139649414.016
View in ScopusGoogle Scholar
Nesselhauf, 2005
N. Nesselhauf
Collocations in a learner corpus
John Benjamins (2005), 10.1075/scl.14
Google Scholar
Paquot, 2010
M. Paquot
Academic vocabulary in learner writing: From extraction to analysis
Bloomsbury Publishing (2010)
Google Scholar
Plonsky and Oswald, 2017
L. Plonsky, F.L. Oswald
Multiple regression as a flexible alternative to ANOVA in L2 research
Studies in Second Language Acquisition, 39 (3) (2017), pp. 579-592, 10.1017/S0272263116000231
View in ScopusGoogle Scholar
Sako, 2024
T. Sako
Enhancing critical thinking through AI-assisted collaborative task-based learning: A case study of prospective teachers in Japan
Journal of English Language Teaching and Linguistics, 9 (2) (2024), pp. 157-170, 10.21462/jeltl.v9i2.1319
Google Scholar
Schmidt, 1990
R. Schmidt
The role of consciousness in second language learning
Applied Linguistics, 11 (2) (1990), pp. 129-158, 10.1093/applin/11.2.129
View in ScopusGoogle Scholar
Selwyn, 2016
N. Selwyn
Education and technology: Key issues and debates
(2nd ed.), Bloomsbury Academic (2016)
Google Scholar
Shin and Lee, 2025
D. Shin, J.H. Lee
Leveraging LLM-Based chatbots for interactional grammar feedback in L2 writing: Opportunities and challenges
Innovation in Language Learning and Teaching (2025), pp. 1-13, 10.1080/17501229.2025.2549037
Google Scholar
Simpson-Vlach and Ellis, 2010
R. Simpson-Vlach, N.C. Ellis
An academic formulas list: New methods in phraseology research
Applied Linguistics, 31 (4) (2010), pp. 487-512, 10.1093/applin/amp058
View in ScopusGoogle Scholar
Skehan, 1998
P. Skehan
A cognitive approach to language learning
Oxford University Press (1998)
Google Scholar
Sok and Shin, 2025
S. Sok, H.W. Shin
Do interactions with ChatGPT influence L2 learners' oral speaking ability, summarization ability, and perceptions of generative AI tasks?
Tesol Quarterly, 59 (2025), pp. S19-S51, 10.1002/tesq.70001
View in ScopusGoogle Scholar
Szudarski and Carter, 2016
P. Szudarski, R. Carter
The role of input flood and input enhancement in EFL learners' acquisition of collocations
International Journal of Applied Linguistics, 26 (2) (2016), pp. 245-266, 10.1111/ijal.12092
View in ScopusGoogle Scholar
Uchihara et al., 2021
T. Uchihara, K. Saito, T. Nakata
To what extent is collocation knowledge associated with oral proficiency? A corpus-based approach
Language Learning, 71 (2) (2021), pp. 461-502, 10.1111/lang.12444
Google Scholar
Wood, 2006
D. Wood
Uses and functions of formulaic sequences in second language speech: An exploration of the foundations of fluency
Canadian Modern Language Review, 63 (1) (2006), pp. 13-33, 10.3138/cmlr.63.1.13
View in ScopusGoogle Scholar
Wray, 2002
A. Wray
Formulaic language and the lexicon
Cambridge University Press, Cambridge (2002)
Google Scholar
Xu, 2018
J. Xu
Measuring “spoken collocational competence” in communicative speaking assessment
Language Assessment Quarterly, 15 (3) (2018), pp. 255-272, 10.1080/15434303.2018.1482900
View in ScopusGoogle Scholar
Xu et al., 2025
G. Xu, A. Yu, L. Liu
A meta-analysis examining AI-assisted L2 learning
International Review of Applied Linguistics in Language Teaching (0) (2025), 10.1515/iral-2024-0213
Google Scholar
Zareva, 2015
A. Zareva
Lexical composition of effective L1 and L2 students' academic presentations
Journal of Applied Linguistics, 6 (1) (2015), pp. 91-110, 10.1558/japl.v6i1.91
Google Scholar
Zareva, 2020
A. Zareva
Collocations in student academic presentations
Speech accommodation in student presentations, Springer International Publishing, Cham (2020), pp. 69-93, 10.1007/978-3-030-37980-3_4
Google Scholar
Ziegler, 2016
N. Ziegler
Taking technology to task: Technology-mediated TBLT and language learning
Annual Review of Applied Linguistics, 36 (2016), pp. 136-163, 10.1017/S0267190516000039
View in ScopusGoogle Scholar
Zou et al., 2023
B. Zou, X. Guan, Y. Shao, P. Chen
Supporting speaking practice by social network-based interaction in AI-assisted language learning
Sustainability, 15 (4) (2023), p. 2872, 10.3390/su15042872
View in ScopusGoogle Scholar
Cited by (0)
Valentina Morgana valentina.morgana@unicatt.it Valentina Morgana is an Assistant Professor of English Language and Linguistics at Università Cattolica del Sacro Cuore in Milan. Her research focuses on technology-mediated Task-Based English Language Teaching (TBLT), with an emphasis on AI, multimodal input, and corpus-based approaches.
Francesca Poli francesca.poli@unicatt.it Francesca Poli is a Postdoc Fellow of English Language and Linguistics at Università Cattolica del Sacro Cuore in Milan. Her research focuses on learner corpora, corpus linguistics, with an emphasis on spoken discourse, multimodality. She's currently studying the impact of AI on second language acquisition.
This article is part of a special issue entitled: Tech in EAP published in Journal of English for Academic Purposes.
© 2026 The Authors. Published by Elsevier Ltd.

Part of special issue
Technology in EAP Language Teaching and Learning
Edited by Professor Tania Zulli (Gabriele d'Annunzio University of Chieti and Pescara, Chieti, , Italy), Professor Jim McKinley (University College London, London, , United Kingdom), Professor Giuseppe Balirano (University of Naples L'Orientale, Napoli, , Italy)
View special issue

Recommended articles
Predicting student performance: A comprehensive review of machine learning, deep learning, and explainable AI approaches
Computers and Education: Artificial Intelligence, Volume 10, 2026, Article 100548
Salma Boujmiraz, …, Ahmed Drissi el maliani
View PDF
Language teacher beliefs and teacher education programs: A 25-year methodological synthesis (2000-2024)
Research Methods in Applied Linguistics, Volume 5, Issue 1, 2026, Article 100299
Farahnaz Faez, …, Ata Ghaderi
Constructing China’s national image through political discourse: A corpus-based diachronic analysis of government work reports (2001–2025)
Applied Corpus Linguistics, Volume 6, Issue 1, 2026, Article 100179
Liai Ma, Peter Crosthwaite
View PDF

Show 3 more articles
Elsevier logo with wordmark
About ScienceDirect
Remote access
Contact and support
Terms and conditions
Privacy policy
Cookie settings
All content on this site: Copyright © 2026 Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the relevant licensing terms apply.

RELX group home page
View PDFFeedback