<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>A Survey on Large Language Model based Autonomous Agents</title>
<!--Generated on Sun Mar  2 04:03:11 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2308.11432v7/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S1" title="In A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S2" title="In A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>LLM-based Autonomous Agent Construction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S2.SS1" title="In 2 LLM-based Autonomous Agent Construction ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Agent Architecture Design</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S2.SS1.SSS1" title="In 2.1 Agent Architecture Design ‣ 2 LLM-based Autonomous Agent Construction ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Profiling Module</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S2.SS1.SSS2" title="In 2.1 Agent Architecture Design ‣ 2 LLM-based Autonomous Agent Construction ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Memory Module</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S2.SS1.SSS3" title="In 2.1 Agent Architecture Design ‣ 2 LLM-based Autonomous Agent Construction ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.3 </span>Planning Module</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S2.SS1.SSS4" title="In 2.1 Agent Architecture Design ‣ 2 LLM-based Autonomous Agent Construction ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.4 </span>Action Module</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S2.SS2" title="In 2 LLM-based Autonomous Agent Construction ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Agent Capability Acquisition</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S3" title="In A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>LLM-based Autonomous Agent Application</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S3.SS1" title="In 3 LLM-based Autonomous Agent Application ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Social Science</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S3.SS2" title="In 3 LLM-based Autonomous Agent Application ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Natural Science</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S3.SS3" title="In 3 LLM-based Autonomous Agent Application ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Engineering</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S4" title="In A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>LLM-based Autonomous Agent Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S4.SS1" title="In 4 LLM-based Autonomous Agent Evaluation ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Subjective Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S4.SS2" title="In 4 LLM-based Autonomous Agent Evaluation ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Objective Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S5" title="In A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Related Surveys</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S6" title="In A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Challenges</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S6.SS1" title="In 6 Challenges ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Role-playing Capability</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S6.SS2" title="In 6 Challenges ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Generalized Human Alignment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S6.SS3" title="In 6 Challenges ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Prompt Robustness</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S6.SS4" title="In 6 Challenges ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Hallucination</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S6.SS5" title="In 6 Challenges ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5 </span>Knowledge Boundary</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S6.SS6" title="In 6 Challenges ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.6 </span>Efficiency</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S7" title="In A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">A Survey on Large Language Model based Autonomous Agents</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lei Wang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chen Ma<span class="ltx_note ltx_role_footnote" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Both authors contribute equally to this paper.</span></span></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xueyang Feng<span class="ltx_note ltx_role_footnote" id="footnotex2"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Both authors contribute equally to this paper.</span></span></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zeyu Zhang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hao Yang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jingsen Zhang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhi-Yuan Chen
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiakai Tang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wayne Xin Zhao
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhewei Wei
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ji-Rong Wen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, 100872, China
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="1.1">Autonomous agents have long been a research focus in academic and industry communities.
Previous research often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and makes the agents hard to achieve human-like decisions.
Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have shown potential in human-level intelligence, leading to a surge in research on LLM-based autonomous agents.
In this paper, we present a comprehensive survey of these studies, delivering a systematic review of LLM-based autonomous agents from a holistic perspective.
We first discuss the construction of LLM-based autonomous agents, proposing a unified framework that encompasses much of previous work.
Then, we present an overview of the diverse applications of LLM-based autonomous agents in social science, natural science, and engineering.
Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents.
Based on the previous studies, we also present several challenges and future directions in this field.
</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Autonomous agent, Large language model, Human-level intelligence
</div>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">Xu Chen
Yankai Lin




<span class="ltx_ERROR undefined" id="p1.1.1">\fcssetup</span>
received = month dd, yyyy,
accepted = month dd, yyyy,
corr-email = xu.chen@ruc.edu.cn;yankailin@ruc.edu.cn,
doi=10.1007/s11704-024-40231-1





</p>
</div>
<figure class="ltx_figure" id="S0.F1">
<p class="ltx_p ltx_align_center" id="S0.F1.1"><span class="ltx_text ltx_framed ltx_framed_rectangle" id="S0.F1.1.1" style="border-color: #000000;padding:0.0pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="421" id="S0.F1.1.1.g1" src="x1.png" width="830"/>
</span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
Illustration of the growth trend in the field of LLM-based autonomous agents.
We present the cumulative number of papers published from January 2021 to August 2023.
We assign different colors to represent various agent categories. For example, a game agent aims to simulate a game-player, while a tool agent mainly focuses on tool using.
For each time period, we provide a curated list of studies with diverse agent categories.
</figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<blockquote class="ltx_quote ltx_epigraph" id="S1.p1.1" style="width:216.81pt; margin-left:auto;;">
<div class="ltx_block ltx_epigraph_text" id="S1.p1.1.1" style="text-align:left; ;">
<p class="ltx_p" id="S1.p1.1.1.1"><span class="ltx_text ltx_font_italic" id="S1.p1.1.1.1.1" style="font-size:90%;">“An autonomous agent is a system situated within and a part of an environment that senses that environment and acts on it, over time, in pursuit of its own agenda and so as to effect what it senses in the future.”</span></p>
</div>
<div class="ltx_block ltx_epigraph_source" id="S1.p1.1.2" style="border-top:solid 0.4pt; text-align:right; ;">
<p class="ltx_p" id="S1.p1.1.2.1"><span class="ltx_text" id="S1.p1.1.2.1.1" style="font-size:90%;">Franklin and Graesser (1997)</span></p>
</div>
</blockquote>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Autonomous agents have long been recognized as a promising approach to achieving artificial general intelligence (AGI), which is expected to accomplish tasks through self-directed planning and actions.
In previous studies, the agents are assumed to act based on simple and heuristic policy functions, and learned in isolated and restricted environments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib6" title="">6</a>]</cite>.
Such assumptions significantly differs from the human learning process, since the human mind is highly complex and individuals can learn from a much wider variety of environments.
Because of these gaps, the agents obtained from previous studies are usually far from replicating human-level decision processes, especially in unconstrained, open-domain settings.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In recent years, large language models (LLMs) have achieved notable successes, demonstrating significant potential to achieve human-like intelligence <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib10" title="">10</a>]</cite>. This capability arises from leveraging comprehensive training datasets alongside a substantial number of model parameters.
Building upon this capability, there has been a growing research area that employs LLMs as central controllers to construct autonomous agents to obtain human-like decision-making capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib17" title="">17</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Compared to reinforcement learning, LLM-based agents possess more comprehensive internal world knowledge, enabling them to perform informed actions even without training on specific domain data. Furthermore, LLM-based agents can offer natural language interfaces for human interaction, providing greater flexibility and enhanced explainability.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Along this direction, researchers have developed numerous promising models (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S0.F1" title="Figure 1 ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_tag">1</span></a> for an overview), where the key idea is to equip LLMs with human capabilities such as memory and planning to make them behave like humans and complete various tasks effectively.
Previously, these models were proposed independently, with limited efforts made to summarize and compare them holistically.
However, we believe that a systematic summary of this rapidly developing field is of great significance for a comprehensive understanding of it and is beneficial in inspiring future research.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In this paper, we conduct a comprehensive survey of the field of LLM-based autonomous agents.
We organize our survey around three key aspects: construction, application, and evaluation of LLM-based autonomous agents.
For agent construction, we focus on two problems, that is,
(1) how to design the agent architecture to better leverage LLMs,
and (2) how to inspire and enhance the agent capability to complete different tasks.
Intuitively, the first problem aims to build the hardware fundamentals for the agent, while the second problem focuses on providing the agent with software resources.
For the first problem, we present a unified agent framework, which can encompass most of the previous studies.
For the second problem, we provide a summary on the commonly-used strategies for agents’ capability acquisition.
In addition to discussing agent construction, we also provide a systematic overview of the applications of LLM-based autonomous agents in social science, natural science, and engineering.
Finally, we delve into the strategies for evaluating LLM-based autonomous agents, focusing on both subjective and objective strategies.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">In summary, this survey conducts a systematic review and establishes comprehensive taxonomies for existing studies in the burgeoning field of LLM-based autonomous agents.
Our focus encompasses three primary areas: the construction of agents, their applications, and methods of evaluation.
Drawing from a wealth of previous studies, we identify various challenges in this field and discuss potential future directions.
We expect that our survey can provide newcomers of LLM-based autonomous agents with a comprehensive background knowledge, and also encourage further groundbreaking studies.</p>
</div>
<figure class="ltx_figure" id="S1.F2">
<p class="ltx_p ltx_align_center" id="S1.F2.1"><span class="ltx_text ltx_framed ltx_framed_rectangle" id="S1.F2.1.1" style="border-color: #000000;padding:0.0pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="375" id="S1.F2.1.1.g1" src="x2.png" width="830"/>
</span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A unified framework for the architecture design of LLM-based autonomous agent.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>LLM-based Autonomous Agent Construction</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">LLM-based autonomous agents are expected to effectively perform diverse tasks by leveraging the human-like capabilities of LLMs.
In order to achieve this goal, there are two significant aspects: (1) which architecture should be designed to better use LLMs and (2) given the designed architecture, how to enable the agent to acquire capabilities for accomplishing specific tasks.
Within the context of architecture design, we contribute a systematic synthesis of existing research, culminating in a comprehensive unified framework.
As for the second aspect, we summarize the strategies for agent capability acquisition based on whether they fine-tune the LLMs.
Comparing LLM-based autonomous agents to traditional machine learning, architecture design is analogous to defining the network structure, while capability acquisition resembles the process of learning network parameters.
In the following sections, we explore these two aspects in greater detail.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Agent Architecture Design</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Recent advancements in LLMs have demonstrated their great potential to accomplish a wide range of tasks in the form of question-answering (QA).
However, building autonomous agents is far from QA, since they need to fulfill specific roles and autonomously perceive and learn from the environment to evolve themselves like humans.
To bridge the gap between traditional LLMs and autonomous agents, a crucial aspect is to design rational agent architectures to assist LLMs in maximizing their capabilities.
Along this direction, previous work has developed a number of modules to enhance LLMs.
In this section, we propose a unified framework to summarize these modules.
Specifically, the overall structure of our framework is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_tag">2</span></a>, which is composed of a profiling module, a memory module, a planning module, and an action module.
The purpose of the profiling module is to identify the role of the agent.
The memory and planning modules place the agent into a dynamic environment, enabling it to recall past behaviors and plan future actions.
The action module is responsible for translating the agent’s decisions into specific outputs.
Within these modules, the profiling module impacts the memory and planning modules, and collectively, these three modules influence the action module.
In the following, we detail these modules.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Profiling Module</h4>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">Autonomous agents typically perform tasks by assuming specific roles, such as coders, teachers, and domain experts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib19" title="">19</a>]</cite>.
The profiling module aims to indicate the profiles of the agent roles, which are usually written into the prompt to influence the behavior of the LLM.
Agent profiles typically encompass basic information such as age, gender, and career <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib20" title="">20</a>]</cite>, as well as psychology information, reflecting the personalities of the agents, and social information, detailing the relationships between agents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib21" title="">21</a>]</cite>.
The choice of information to profile the agent is largely determined by the specific application scenarios.
For instance, if the application aims to study human cognitive process, then the psychology information becomes pivotal.
After identifying the types of profile information, the next important problem is to create specific profiles for the agents.
Existing literature commonly employs the following three strategies.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p2">
<p class="ltx_p" id="S2.SS1.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS1.p2.1.1">Handcrafting Method</span>:
in this method, agent profiles are manually specified. For instance, if one would like to design agents with different personalities, he can use "you are an outgoing person" or "you are an introverted person" to profile the agent.
The handcrafting method has been leveraged in a lot of previous work to specify the agent profiles.
For example, Generative Agent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib22" title="">22</a>]</cite> describes the agent by the information such as name, objectives, and relationships with other agents.
MetaGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib23" title="">23</a>]</cite>, ChatDev <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib18" title="">18</a>]</cite>, and Self-collaboration <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib24" title="">24</a>]</cite> predefine various roles and their corresponding responsibilities in software development, manually assigning distinct profiles to each agent to facilitate collaboration.
PTLLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib25" title="">25</a>]</cite> aims to explore and quantify personality traits displayed in texts generated by LLMs.
This method guides LLMs in generating diverse responses by manually defining various agent characters through the use of personality assessment tools such as IPIP-NEO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib26" title="">26</a>]</cite> and BFI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib27" title="">27</a>]</cite>.
 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib28" title="">28</a>]</cite> studies the toxicity of the LLM output by manually prompting LLMs with different roles, such as politicians, journalists and businesspersons.
In general, the handcrafting method is very flexible, since one can assign any profile information to the agents.
However, it can be also labor-intensive, particularly when dealing with a large number of agents.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p3">
<p class="ltx_p" id="S2.SS1.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS1.p3.1.1">LLM-generation Method</span>:
in this method, agent profiles are automatically generated based on LLMs.
Typically, it begins by indicating the profile generation rules, elucidating the composition and attributes of the agent profiles within the target population.
Then, one can optionally specify several seed agent profiles to serve as few-shot examples.
Finally, LLMs are leveraged to generate all the agent profiles.
For example, RecAgent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib21" title="">21</a>]</cite> first creates seed profiles for a few agents by manually crafting their attributes such as age, gender, personal traits, and movie preferences. Then, it leverages ChatGPT to generate more agent profiles based on the seed information.
This approach significantly reduces the time and effort required to construct agent profiles, particularly for large-scale populations. However, it may lack precise control over the generated profiles, which can result in inconsistencies or deviations from the intended characteristics.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p4">
<p class="ltx_p" id="S2.SS1.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS1.p4.1.1">Dataset Alignment Method</span>:
in this method, the agent profiles are obtained from real-world datasets.
Typically, one can first organize the information about real humans in the datasets into natural language prompts, and then leverage it to profile the agents.
For instance, in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib29" title="">29</a>]</cite>, the authors assign roles to GPT-3 based on the demographic backgrounds (such as race/ethnicity, gender, age, and state of residence) of participants in the American National Election Studies (ANES). They subsequently investigate whether GPT-3 can produce similar results to those of real humans. The dataset alignment method accurately captures the attributes of the real population, thereby making the agent behaviors more meaningful and reflective of real-world scenarios.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremarkx1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic" id="Thmremarkx1.1.1.1">Remark</span></span><span class="ltx_text ltx_font_italic" id="Thmremarkx1.2.2">.</span>
</h6>
<div class="ltx_para" id="Thmremarkx1.p1">
<p class="ltx_p" id="Thmremarkx1.p1.1">While most of the previous work leverage the above profile generation strategies independently, we argue that combining them may yield additional benefits.
For example, in order to predict social developments via agent simulation, one can leverage real-world datasets to profile a subset of the agents, thereby accurately reflecting the current social status.
Subsequently, roles that do not exist in the real world but may emerge in the future can be manually assigned to the other agents, enabling the prediction of future social development.
Beyond this example, one can also flexibly combine the other strategies.
The profile module serves as the foundation for agent design, exerting significant influence on the agent memorization, planning, and action procedures.</p>
</div>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Memory Module</h4>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">The memory module plays a very important role in the agent architecture design.
It stores information perceived from the environment and leverages the recorded memories to facilitate future actions.
The memory module can help the agent to accumulate experiences, self-evolve, and behave in a more consistent, reasonable, and effective manner.
This section provides a comprehensive overview of the memory module, focusing on its structures, formats, and operations.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p2">
<p class="ltx_p" id="S2.SS1.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS2.p2.1.1">Memory Structures</span>: LLM-based autonomous agents often draw inspiration from cognitive science research on human memory processes. Human memory follows a general progression from sensory memory that registers perceptual inputs, to short-term memory that maintains information transiently, to long-term memory that consolidates information over extended periods.
When designing the agent memory structures, researchers take inspiration from these aspects of human memory.
In particular, short-term memory is analogous to the input information within the context window constrained by the transformer architecture.
Long-term memory resembles the external vector storage that agents can rapidly query and retrieve from as needed.
In the following, we introduce two commonly used memory structures based on the short-term and long-term memories.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p3">
<p class="ltx_p" id="S2.SS1.SSS2.p3.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p3.1.m1.1"><semantics id="S2.SS1.SSS2.p3.1.m1.1a"><mo id="S2.SS1.SSS2.p3.1.m1.1.1" xref="S2.SS1.SSS2.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p3.1.m1.1b"><ci id="S2.SS1.SSS2.p3.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p3.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p3.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS2.p3.1.1">Unified Memory</span>.
This structure only simulates the human short-term memory, which is usually realized by in-context learning, and the memory information is directly written into the prompts.
For example, RLP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib30" title="">30</a>]</cite> is a conversation agent, which maintains internal states for the speaker and listener.
During each round of conversation, these states serve as LLM prompts, functioning as the agent’s short-term memory.
SayPlan <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib31" title="">31</a>]</cite> is an embodied agent specifically designed for task planning. In this agent, the scene graphs and environment feedback serve as the agent’s short-term memory, guiding its actions.
CALYPSO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib32" title="">32</a>]</cite> is an agent designed for the game Dungeons &amp; Dragons, which can assist Dungeon Masters in the creation and narration of stories.
Its short-term memory is built upon scene descriptions, monster information, and previous summaries.
DEPS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib33" title="">33</a>]</cite> is also a game agent, developed for Minecraft.
The agent initially generates task plans and then utilizes them to prompt LLMs, which in turn produce actions to complete the task. These plans can be deemed as the agent’s short-term memory.
In practice, implementing short-term memory is straightforward and can enhance an agent’s ability to perceive recent or contextually sensitive behaviors and observations.
However, the limited context window of LLMs restricts incorporating comprehensive memories into prompts, which can impair agent performance. This challenge necessitates LLMs with larger context windows and the ability to handle extended contexts. Consequently, numerous researchers turn to hybrid memory systems to mitigate this issue.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p4">
<p class="ltx_p" id="S2.SS1.SSS2.p4.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p4.1.m1.1"><semantics id="S2.SS1.SSS2.p4.1.m1.1a"><mo id="S2.SS1.SSS2.p4.1.m1.1.1" xref="S2.SS1.SSS2.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p4.1.m1.1b"><ci id="S2.SS1.SSS2.p4.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p4.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p4.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS2.p4.1.1">Hybrid Memory</span>.
This structure explicitly models the human short-term and long-term memories.
The short-term memory temporarily buffers recent perceptions, while long-term memory consolidates important information over time.
For instance, Generative Agent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib20" title="">20</a>]</cite> employs a hybrid memory structure to facilitate agent behaviors.
The short-term memory contains the context information about the agent current situations, while the long-term memory stores the agent past behaviors and thoughts, which can be retrieved according to the current events.
AgentSims <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib34" title="">34</a>]</cite> also implements a hybrid memory architecture.
The information provided in the prompt can be considered as short-term memory.
In order to enhance the storage capacity of memory, the authors propose a long-term memory system that utilizes a vector database, facilitating efficient storage and retrieval.
Specifically, the agent’s daily memories are encoded as embeddings and stored in the vector database.
If the agent needs to recall its previous memories, the long-term memory system retrieves relevant information using embedding similarities.
This process can improve the consistency of the agent’s behavior.
In GITM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib16" title="">16</a>]</cite>, the short-term memory stores the current trajectory, and the long-term memory saves reference plans summarized from successful prior trajectories.
Long-term memory provides stable knowledge, while short-term memory allows flexible planning.
Reflexion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib12" title="">12</a>]</cite> utilizes a short-term sliding window to capture recent feedback and incorporates persistent long-term storage to retain condensed insights. This combination allows for the utilization of both detailed immediate experiences and high-level abstractions.
SCM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib35" title="">35</a>]</cite> selectively activates the most relevant long-term knowledge to combine with short-term memory, enabling reasoning over complex contextual dialogues.
SimplyRetrieve <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib36" title="">36</a>]</cite> utilizes user queries as short-term memory and stores long-term memory using private knowledge bases.
This design enhances the model accuracy while guaranteeing user privacy.
MemorySandbox <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib37" title="">37</a>]</cite> implements long-term and short-term memory to store different objects, which can then be accessed throughout various conversations.
Users can create multiple conversations with different agents on the same canvas, facilitating the sharing of memory objects through a simple drag-and-drop interface.
In practice, integrating both short-term and long-term memories can enhance an agent’s ability for long-range reasoning and accumulation of valuable experiences, which are crucial for accomplishing tasks in complex environments.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremarkx2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic" id="Thmremarkx2.1.1.1">Remark</span></span><span class="ltx_text ltx_font_italic" id="Thmremarkx2.2.2">.</span>
</h6>
<div class="ltx_para" id="Thmremarkx2.p1">
<p class="ltx_p" id="Thmremarkx2.p1.1">Careful readers may find that there may also exist another type of memory structure, that is, only based on the long-term memory.
However, we find that such type of memory is rarely documented in the literature.
Our speculation is that the agents are always situated in continuous and dynamic environments, with consecutive actions displaying a high correlation.
Therefore, the capture of short-term memory is very important and usually cannot be disregarded.</p>
</div>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p5">
<p class="ltx_p" id="S2.SS1.SSS2.p5.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS2.p5.1.1">Memory Formats</span>:
In addition to the memory structure, another perspective to analyze the memory module is based on the formats of the memory storage medium, for example, natural language memory or embedding memory.
Different memory formats possess distinct strengths and are suitable for various applications.
In the following, we introduce several representative memory formats.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p6">
<p class="ltx_p" id="S2.SS1.SSS2.p6.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p6.1.m1.1"><semantics id="S2.SS1.SSS2.p6.1.m1.1a"><mo id="S2.SS1.SSS2.p6.1.m1.1.1" xref="S2.SS1.SSS2.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p6.1.m1.1b"><ci id="S2.SS1.SSS2.p6.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p6.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p6.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS2.p6.1.1">Natural Languages</span>.
In this format, memory information such as the agent behaviors and observations are directly described using raw natural language.
This format possesses several strengths.
Firstly, the memory information can be expressed in a flexible and understandable manner.
Moreover, it retains rich semantic information that can provide comprehensive signals to guide agent behaviors.
In the previous work, Reflexion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib12" title="">12</a>]</cite> stores experiential feedback in natural language within a sliding window.
Voyager <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib38" title="">38</a>]</cite> employs natural language descriptions to represent skills within the Minecraft game, which are directly stored in memory.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p7">
<p class="ltx_p" id="S2.SS1.SSS2.p7.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p7.1.m1.1"><semantics id="S2.SS1.SSS2.p7.1.m1.1a"><mo id="S2.SS1.SSS2.p7.1.m1.1.1" xref="S2.SS1.SSS2.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p7.1.m1.1b"><ci id="S2.SS1.SSS2.p7.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p7.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p7.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS2.p7.1.1">Embeddings</span>.
In this format, memory information is encoded into embedding vectors, which enhances both retrieval and reading efficiency.
For instance, MemoryBank <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib39" title="">39</a>]</cite> encodes each memory segment as an embedding vector and employs a dual-tower dense retrieval model to efficiently retrieve relevant information from past conversations.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p8">
<p class="ltx_p" id="S2.SS1.SSS2.p8.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p8.1.m1.1"><semantics id="S2.SS1.SSS2.p8.1.m1.1a"><mo id="S2.SS1.SSS2.p8.1.m1.1.1" xref="S2.SS1.SSS2.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p8.1.m1.1b"><ci id="S2.SS1.SSS2.p8.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p8.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p8.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS2.p8.1.1">Databases</span>.
In this format, memory information is stored in databases, allowing the agent to manipulate memories efficiently and comprehensively.
For example, ChatDB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib40" title="">40</a>]</cite> uses a database as a symbolic memory module. The agent can utilize SQL statements to precisely add, delete, and modify the memory information.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p9">
<p class="ltx_p" id="S2.SS1.SSS2.p9.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p9.1.m1.1"><semantics id="S2.SS1.SSS2.p9.1.m1.1a"><mo id="S2.SS1.SSS2.p9.1.m1.1.1" xref="S2.SS1.SSS2.p9.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p9.1.m1.1b"><ci id="S2.SS1.SSS2.p9.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p9.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p9.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p9.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS2.p9.1.1">Structured Lists</span>.
In this format, memory information is organized into lists, and the semantic of memory can be conveyed in an efficient and concise manner.
For instance, GITM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib16" title="">16</a>]</cite> stores action lists for sub-goals in a hierarchical tree structure.
The hierarchical structure explicitly captures the relationships between goals and corresponding plans.
RET-LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib41" title="">41</a>]</cite> initially converts natural language sentences into triplet phrases, and subsequently stores them in memory.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremarkx3">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic" id="Thmremarkx3.1.1.1">Remark</span></span><span class="ltx_text ltx_font_italic" id="Thmremarkx3.2.2">.</span>
</h6>
<div class="ltx_para" id="Thmremarkx3.p1">
<p class="ltx_p" id="Thmremarkx3.p1.1">Here we only show several representative memory formats, but it is important to note that there are many uncovered ones, such as the programming code used by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib38" title="">38</a>]</cite>.
Moreover, it should be emphasized that these formats are not mutually exclusive; many models incorporate multiple formats to concurrently harness their respective benefits.
A notable example is the memory module of GITM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib16" title="">16</a>]</cite>, which utilizes a key-value list structure. In this structure, the keys are represented by embedding vectors, while the values consist of raw natural languages.
The use of embedding vectors allows for efficient retrieval of memory records.
By utilizing natural languages, the memory contents become highly comprehensive, enabling more informed agent actions.</p>
</div>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p10">
<p class="ltx_p" id="S2.SS1.SSS2.p10.1">Above, we mainly discuss the internal designs of the memory module. In the following, we turn our focus to memory operations, which are used to interact with external environments.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p11">
<p class="ltx_p" id="S2.SS1.SSS2.p11.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS2.p11.1.1">Memory Operations</span>:
The memory module plays a critical role in allowing the agent to acquire, accumulate, and utilize significant knowledge by interacting with the environment.
The interaction between the agent and the environment is accomplished through three crucial memory operations: memory reading, memory writing, and memory reflection.
In the following, we introduce these operations more in detail.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p12">
<p class="ltx_p" id="S2.SS1.SSS2.p12.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p12.1.m1.1"><semantics id="S2.SS1.SSS2.p12.1.m1.1a"><mo id="S2.SS1.SSS2.p12.1.m1.1.1" xref="S2.SS1.SSS2.p12.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.1.m1.1b"><ci id="S2.SS1.SSS2.p12.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p12.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS2.p12.1.1">Memory Reading</span>.
The objective of memory reading is to extract meaningful information from memory to enhance the agent’s actions.
For example, using the previously successful actions to achieve similar goals <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib16" title="">16</a>]</cite>.
The key of memory reading lies in how to extract valuable information from history actions.
Usually, there are three commonly used criteria for information extraction, that is, the recency, relevance, and importance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib20" title="">20</a>]</cite>.
Memories that are more recent, relevant, and important are more likely to be extracted.
Formally, we conclude the following equation from existing literature for memory information extraction:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S2.E1">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S2.E1X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle m^{*}=\arg\max_{m\in M}\left(\alpha s^{rec}(q,m)+\beta s^{rel}(q%
,m)+\gamma s^{imp}(m)\right)," class="ltx_Math" display="inline" id="S2.E1X.2.1.1.m1.6"><semantics id="S2.E1X.2.1.1.m1.6a"><mrow id="S2.E1X.2.1.1.m1.6.6.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.cmml"><mrow id="S2.E1X.2.1.1.m1.6.6.1.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.cmml"><msup id="S2.E1X.2.1.1.m1.6.6.1.1.4" xref="S2.E1X.2.1.1.m1.6.6.1.1.4.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.4.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.4.2.cmml">m</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.4.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.4.3.cmml">∗</mo></msup><mo id="S2.E1X.2.1.1.m1.6.6.1.1.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.cmml">=</mo><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.2.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.3.cmml">arg</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2a" lspace="0.167em" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.cmml">⁡</mo><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.3.cmml"><munder id="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1.2.cmml">max</mi><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1.3.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1.3.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1.3.2.cmml">m</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1.3.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1.3.1.cmml">∈</mo><mi id="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1.3.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1.3.3.cmml">M</mi></mrow></munder><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2a" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.3.cmml">⁡</mo><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.3.cmml"><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.3.cmml">(</mo><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.cmml"><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.2.cmml">α</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.1.cmml">⁢</mo><msup id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.2.cmml">s</mi><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.3.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.3.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.3.2.cmml">r</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.3.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.3.1.cmml">⁢</mo><mi id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.3.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.3.3.cmml">e</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.3.1a" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.3.1.cmml">⁢</mo><mi id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.3.4" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.3.4.cmml">c</mi></mrow></msup><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.1a" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.1.cmml">⁢</mo><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.4.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.4.1.cmml"><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.4.2.1" stretchy="false" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.4.1.cmml">(</mo><mi id="S2.E1X.2.1.1.m1.1.1" xref="S2.E1X.2.1.1.m1.1.1.cmml">q</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.4.2.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.4.1.cmml">,</mo><mi id="S2.E1X.2.1.1.m1.2.2" xref="S2.E1X.2.1.1.m1.2.2.cmml">m</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.4.2.3" stretchy="false" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.4.1.cmml">)</mo></mrow></mrow><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.1.cmml">+</mo><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.2.cmml">β</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.1.cmml">⁢</mo><msup id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.2.cmml">s</mi><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.3.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.3.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.3.2.cmml">r</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.3.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.3.1.cmml">⁢</mo><mi id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.3.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.3.3.cmml">e</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.3.1a" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.3.1.cmml">⁢</mo><mi id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.3.4" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.3.4.cmml">l</mi></mrow></msup><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.1a" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.1.cmml">⁢</mo><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.4.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.4.1.cmml"><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.4.2.1" stretchy="false" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.4.1.cmml">(</mo><mi id="S2.E1X.2.1.1.m1.3.3" xref="S2.E1X.2.1.1.m1.3.3.cmml">q</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.4.2.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.4.1.cmml">,</mo><mi id="S2.E1X.2.1.1.m1.4.4" xref="S2.E1X.2.1.1.m1.4.4.cmml">m</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.4.2.3" stretchy="false" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.4.1.cmml">)</mo></mrow></mrow><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.1a" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.1.cmml">+</mo><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.2.cmml">γ</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.1.cmml">⁢</mo><msup id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.2.cmml">s</mi><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.3.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.3.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.3.2.cmml">i</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.3.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.3.1.cmml">⁢</mo><mi id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.3.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.3.3.cmml">m</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.3.1a" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.3.1.cmml">⁢</mo><mi id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.3.4" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.3.4.cmml">p</mi></mrow></msup><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.1a" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.1.cmml">⁢</mo><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.4.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.cmml"><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.4.2.1" stretchy="false" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.cmml">(</mo><mi id="S2.E1X.2.1.1.m1.5.5" xref="S2.E1X.2.1.1.m1.5.5.cmml">m</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.4.2.2" stretchy="false" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S2.E1X.2.1.1.m1.6.6.1.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1X.2.1.1.m1.6b"><apply id="S2.E1X.2.1.1.m1.6.6.1.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1"><eq id="S2.E1X.2.1.1.m1.6.6.1.1.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3"></eq><apply id="S2.E1X.2.1.1.m1.6.6.1.1.4.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.4"><csymbol cd="ambiguous" id="S2.E1X.2.1.1.m1.6.6.1.1.4.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.4">superscript</csymbol><ci id="S2.E1X.2.1.1.m1.6.6.1.1.4.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.4.2">𝑚</ci><times id="S2.E1X.2.1.1.m1.6.6.1.1.4.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.4.3"></times></apply><apply id="S2.E1X.2.1.1.m1.6.6.1.1.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2"><arg id="S2.E1X.2.1.1.m1.6.6.1.1.2.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.3"></arg><apply id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2"><apply id="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1">subscript</csymbol><max id="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1.2"></max><apply id="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1.3"><in id="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1.3.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1.3.1"></in><ci id="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1.3.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1.3.2">𝑚</ci><ci id="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1.3.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.1.1.1.1.3.3">𝑀</ci></apply></apply><apply id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1"><plus id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.1"></plus><apply id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2"><times id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.1"></times><ci id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.2">𝛼</ci><apply id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3"><csymbol cd="ambiguous" id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3">superscript</csymbol><ci id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.2">𝑠</ci><apply id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.3"><times id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.3.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.3.1"></times><ci id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.3.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.3.2">𝑟</ci><ci id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.3.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.3.3">𝑒</ci><ci id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.3.4.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.3.3.4">𝑐</ci></apply></apply><interval closure="open" id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.4.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.2.4.2"><ci id="S2.E1X.2.1.1.m1.1.1.cmml" xref="S2.E1X.2.1.1.m1.1.1">𝑞</ci><ci id="S2.E1X.2.1.1.m1.2.2.cmml" xref="S2.E1X.2.1.1.m1.2.2">𝑚</ci></interval></apply><apply id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3"><times id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.1"></times><ci id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.2">𝛽</ci><apply id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3"><csymbol cd="ambiguous" id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3">superscript</csymbol><ci id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.2">𝑠</ci><apply id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.3"><times id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.3.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.3.1"></times><ci id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.3.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.3.2">𝑟</ci><ci id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.3.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.3.3">𝑒</ci><ci id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.3.4.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.3.3.4">𝑙</ci></apply></apply><interval closure="open" id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.4.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.3.4.2"><ci id="S2.E1X.2.1.1.m1.3.3.cmml" xref="S2.E1X.2.1.1.m1.3.3">𝑞</ci><ci id="S2.E1X.2.1.1.m1.4.4.cmml" xref="S2.E1X.2.1.1.m1.4.4">𝑚</ci></interval></apply><apply id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4"><times id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.1"></times><ci id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.2">𝛾</ci><apply id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3"><csymbol cd="ambiguous" id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3">superscript</csymbol><ci id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.2">𝑠</ci><apply id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.3"><times id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.3.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.3.1"></times><ci id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.3.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.3.2">𝑖</ci><ci id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.3.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.3.3">𝑚</ci><ci id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.3.4.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.2.2.1.4.3.3.4">𝑝</ci></apply></apply><ci id="S2.E1X.2.1.1.m1.5.5.cmml" xref="S2.E1X.2.1.1.m1.5.5">𝑚</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1X.2.1.1.m1.6c">\displaystyle m^{*}=\arg\max_{m\in M}\left(\alpha s^{rec}(q,m)+\beta s^{rel}(q%
,m)+\gamma s^{imp}(m)\right),</annotation><annotation encoding="application/x-llamapun" id="S2.E1X.2.1.1.m1.6d">italic_m start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = roman_arg roman_max start_POSTSUBSCRIPT italic_m ∈ italic_M end_POSTSUBSCRIPT ( italic_α italic_s start_POSTSUPERSCRIPT italic_r italic_e italic_c end_POSTSUPERSCRIPT ( italic_q , italic_m ) + italic_β italic_s start_POSTSUPERSCRIPT italic_r italic_e italic_l end_POSTSUPERSCRIPT ( italic_q , italic_m ) + italic_γ italic_s start_POSTSUPERSCRIPT italic_i italic_m italic_p end_POSTSUPERSCRIPT ( italic_m ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(1)</span></td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="S2.SS1.SSS2.p12.16">where <math alttext="q" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p12.2.m1.1"><semantics id="S2.SS1.SSS2.p12.2.m1.1a"><mi id="S2.SS1.SSS2.p12.2.m1.1.1" xref="S2.SS1.SSS2.p12.2.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.2.m1.1b"><ci id="S2.SS1.SSS2.p12.2.m1.1.1.cmml" xref="S2.SS1.SSS2.p12.2.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.2.m1.1c">q</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.2.m1.1d">italic_q</annotation></semantics></math> is the query, for example, the task that the agent should address or the context in which the agent is situated.
<math alttext="M" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p12.3.m2.1"><semantics id="S2.SS1.SSS2.p12.3.m2.1a"><mi id="S2.SS1.SSS2.p12.3.m2.1.1" xref="S2.SS1.SSS2.p12.3.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.3.m2.1b"><ci id="S2.SS1.SSS2.p12.3.m2.1.1.cmml" xref="S2.SS1.SSS2.p12.3.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.3.m2.1c">M</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.3.m2.1d">italic_M</annotation></semantics></math> is the set of all memories.
<math alttext="s^{rec}(\cdot)" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p12.4.m3.1"><semantics id="S2.SS1.SSS2.p12.4.m3.1a"><mrow id="S2.SS1.SSS2.p12.4.m3.1.2" xref="S2.SS1.SSS2.p12.4.m3.1.2.cmml"><msup id="S2.SS1.SSS2.p12.4.m3.1.2.2" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.cmml"><mi id="S2.SS1.SSS2.p12.4.m3.1.2.2.2" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.2.cmml">s</mi><mrow id="S2.SS1.SSS2.p12.4.m3.1.2.2.3" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.3.cmml"><mi id="S2.SS1.SSS2.p12.4.m3.1.2.2.3.2" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.3.2.cmml">r</mi><mo id="S2.SS1.SSS2.p12.4.m3.1.2.2.3.1" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.3.1.cmml">⁢</mo><mi id="S2.SS1.SSS2.p12.4.m3.1.2.2.3.3" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.3.3.cmml">e</mi><mo id="S2.SS1.SSS2.p12.4.m3.1.2.2.3.1a" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.3.1.cmml">⁢</mo><mi id="S2.SS1.SSS2.p12.4.m3.1.2.2.3.4" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.3.4.cmml">c</mi></mrow></msup><mo id="S2.SS1.SSS2.p12.4.m3.1.2.1" xref="S2.SS1.SSS2.p12.4.m3.1.2.1.cmml">⁢</mo><mrow id="S2.SS1.SSS2.p12.4.m3.1.2.3.2" xref="S2.SS1.SSS2.p12.4.m3.1.2.cmml"><mo id="S2.SS1.SSS2.p12.4.m3.1.2.3.2.1" stretchy="false" xref="S2.SS1.SSS2.p12.4.m3.1.2.cmml">(</mo><mo id="S2.SS1.SSS2.p12.4.m3.1.1" lspace="0em" rspace="0em" xref="S2.SS1.SSS2.p12.4.m3.1.1.cmml">⋅</mo><mo id="S2.SS1.SSS2.p12.4.m3.1.2.3.2.2" stretchy="false" xref="S2.SS1.SSS2.p12.4.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.4.m3.1b"><apply id="S2.SS1.SSS2.p12.4.m3.1.2.cmml" xref="S2.SS1.SSS2.p12.4.m3.1.2"><times id="S2.SS1.SSS2.p12.4.m3.1.2.1.cmml" xref="S2.SS1.SSS2.p12.4.m3.1.2.1"></times><apply id="S2.SS1.SSS2.p12.4.m3.1.2.2.cmml" xref="S2.SS1.SSS2.p12.4.m3.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p12.4.m3.1.2.2.1.cmml" xref="S2.SS1.SSS2.p12.4.m3.1.2.2">superscript</csymbol><ci id="S2.SS1.SSS2.p12.4.m3.1.2.2.2.cmml" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.2">𝑠</ci><apply id="S2.SS1.SSS2.p12.4.m3.1.2.2.3.cmml" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.3"><times id="S2.SS1.SSS2.p12.4.m3.1.2.2.3.1.cmml" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.3.1"></times><ci id="S2.SS1.SSS2.p12.4.m3.1.2.2.3.2.cmml" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.3.2">𝑟</ci><ci id="S2.SS1.SSS2.p12.4.m3.1.2.2.3.3.cmml" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.3.3">𝑒</ci><ci id="S2.SS1.SSS2.p12.4.m3.1.2.2.3.4.cmml" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.3.4">𝑐</ci></apply></apply><ci id="S2.SS1.SSS2.p12.4.m3.1.1.cmml" xref="S2.SS1.SSS2.p12.4.m3.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.4.m3.1c">s^{rec}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.4.m3.1d">italic_s start_POSTSUPERSCRIPT italic_r italic_e italic_c end_POSTSUPERSCRIPT ( ⋅ )</annotation></semantics></math>, <math alttext="s^{rel}(\cdot)" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p12.5.m4.1"><semantics id="S2.SS1.SSS2.p12.5.m4.1a"><mrow id="S2.SS1.SSS2.p12.5.m4.1.2" xref="S2.SS1.SSS2.p12.5.m4.1.2.cmml"><msup id="S2.SS1.SSS2.p12.5.m4.1.2.2" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.cmml"><mi id="S2.SS1.SSS2.p12.5.m4.1.2.2.2" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.2.cmml">s</mi><mrow id="S2.SS1.SSS2.p12.5.m4.1.2.2.3" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.3.cmml"><mi id="S2.SS1.SSS2.p12.5.m4.1.2.2.3.2" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.3.2.cmml">r</mi><mo id="S2.SS1.SSS2.p12.5.m4.1.2.2.3.1" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.3.1.cmml">⁢</mo><mi id="S2.SS1.SSS2.p12.5.m4.1.2.2.3.3" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.3.3.cmml">e</mi><mo id="S2.SS1.SSS2.p12.5.m4.1.2.2.3.1a" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.3.1.cmml">⁢</mo><mi id="S2.SS1.SSS2.p12.5.m4.1.2.2.3.4" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.3.4.cmml">l</mi></mrow></msup><mo id="S2.SS1.SSS2.p12.5.m4.1.2.1" xref="S2.SS1.SSS2.p12.5.m4.1.2.1.cmml">⁢</mo><mrow id="S2.SS1.SSS2.p12.5.m4.1.2.3.2" xref="S2.SS1.SSS2.p12.5.m4.1.2.cmml"><mo id="S2.SS1.SSS2.p12.5.m4.1.2.3.2.1" stretchy="false" xref="S2.SS1.SSS2.p12.5.m4.1.2.cmml">(</mo><mo id="S2.SS1.SSS2.p12.5.m4.1.1" lspace="0em" rspace="0em" xref="S2.SS1.SSS2.p12.5.m4.1.1.cmml">⋅</mo><mo id="S2.SS1.SSS2.p12.5.m4.1.2.3.2.2" stretchy="false" xref="S2.SS1.SSS2.p12.5.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.5.m4.1b"><apply id="S2.SS1.SSS2.p12.5.m4.1.2.cmml" xref="S2.SS1.SSS2.p12.5.m4.1.2"><times id="S2.SS1.SSS2.p12.5.m4.1.2.1.cmml" xref="S2.SS1.SSS2.p12.5.m4.1.2.1"></times><apply id="S2.SS1.SSS2.p12.5.m4.1.2.2.cmml" xref="S2.SS1.SSS2.p12.5.m4.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p12.5.m4.1.2.2.1.cmml" xref="S2.SS1.SSS2.p12.5.m4.1.2.2">superscript</csymbol><ci id="S2.SS1.SSS2.p12.5.m4.1.2.2.2.cmml" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.2">𝑠</ci><apply id="S2.SS1.SSS2.p12.5.m4.1.2.2.3.cmml" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.3"><times id="S2.SS1.SSS2.p12.5.m4.1.2.2.3.1.cmml" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.3.1"></times><ci id="S2.SS1.SSS2.p12.5.m4.1.2.2.3.2.cmml" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.3.2">𝑟</ci><ci id="S2.SS1.SSS2.p12.5.m4.1.2.2.3.3.cmml" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.3.3">𝑒</ci><ci id="S2.SS1.SSS2.p12.5.m4.1.2.2.3.4.cmml" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.3.4">𝑙</ci></apply></apply><ci id="S2.SS1.SSS2.p12.5.m4.1.1.cmml" xref="S2.SS1.SSS2.p12.5.m4.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.5.m4.1c">s^{rel}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.5.m4.1d">italic_s start_POSTSUPERSCRIPT italic_r italic_e italic_l end_POSTSUPERSCRIPT ( ⋅ )</annotation></semantics></math> and <math alttext="s^{imp}(\cdot)" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p12.6.m5.1"><semantics id="S2.SS1.SSS2.p12.6.m5.1a"><mrow id="S2.SS1.SSS2.p12.6.m5.1.2" xref="S2.SS1.SSS2.p12.6.m5.1.2.cmml"><msup id="S2.SS1.SSS2.p12.6.m5.1.2.2" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.cmml"><mi id="S2.SS1.SSS2.p12.6.m5.1.2.2.2" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.2.cmml">s</mi><mrow id="S2.SS1.SSS2.p12.6.m5.1.2.2.3" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.3.cmml"><mi id="S2.SS1.SSS2.p12.6.m5.1.2.2.3.2" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.3.2.cmml">i</mi><mo id="S2.SS1.SSS2.p12.6.m5.1.2.2.3.1" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.3.1.cmml">⁢</mo><mi id="S2.SS1.SSS2.p12.6.m5.1.2.2.3.3" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.3.3.cmml">m</mi><mo id="S2.SS1.SSS2.p12.6.m5.1.2.2.3.1a" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.3.1.cmml">⁢</mo><mi id="S2.SS1.SSS2.p12.6.m5.1.2.2.3.4" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.3.4.cmml">p</mi></mrow></msup><mo id="S2.SS1.SSS2.p12.6.m5.1.2.1" xref="S2.SS1.SSS2.p12.6.m5.1.2.1.cmml">⁢</mo><mrow id="S2.SS1.SSS2.p12.6.m5.1.2.3.2" xref="S2.SS1.SSS2.p12.6.m5.1.2.cmml"><mo id="S2.SS1.SSS2.p12.6.m5.1.2.3.2.1" stretchy="false" xref="S2.SS1.SSS2.p12.6.m5.1.2.cmml">(</mo><mo id="S2.SS1.SSS2.p12.6.m5.1.1" lspace="0em" rspace="0em" xref="S2.SS1.SSS2.p12.6.m5.1.1.cmml">⋅</mo><mo id="S2.SS1.SSS2.p12.6.m5.1.2.3.2.2" stretchy="false" xref="S2.SS1.SSS2.p12.6.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.6.m5.1b"><apply id="S2.SS1.SSS2.p12.6.m5.1.2.cmml" xref="S2.SS1.SSS2.p12.6.m5.1.2"><times id="S2.SS1.SSS2.p12.6.m5.1.2.1.cmml" xref="S2.SS1.SSS2.p12.6.m5.1.2.1"></times><apply id="S2.SS1.SSS2.p12.6.m5.1.2.2.cmml" xref="S2.SS1.SSS2.p12.6.m5.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p12.6.m5.1.2.2.1.cmml" xref="S2.SS1.SSS2.p12.6.m5.1.2.2">superscript</csymbol><ci id="S2.SS1.SSS2.p12.6.m5.1.2.2.2.cmml" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.2">𝑠</ci><apply id="S2.SS1.SSS2.p12.6.m5.1.2.2.3.cmml" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.3"><times id="S2.SS1.SSS2.p12.6.m5.1.2.2.3.1.cmml" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.3.1"></times><ci id="S2.SS1.SSS2.p12.6.m5.1.2.2.3.2.cmml" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.3.2">𝑖</ci><ci id="S2.SS1.SSS2.p12.6.m5.1.2.2.3.3.cmml" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.3.3">𝑚</ci><ci id="S2.SS1.SSS2.p12.6.m5.1.2.2.3.4.cmml" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.3.4">𝑝</ci></apply></apply><ci id="S2.SS1.SSS2.p12.6.m5.1.1.cmml" xref="S2.SS1.SSS2.p12.6.m5.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.6.m5.1c">s^{imp}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.6.m5.1d">italic_s start_POSTSUPERSCRIPT italic_i italic_m italic_p end_POSTSUPERSCRIPT ( ⋅ )</annotation></semantics></math> are the scoring functions for measuring the recency, relevance, and importance of the memory <math alttext="m" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p12.7.m6.1"><semantics id="S2.SS1.SSS2.p12.7.m6.1a"><mi id="S2.SS1.SSS2.p12.7.m6.1.1" xref="S2.SS1.SSS2.p12.7.m6.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.7.m6.1b"><ci id="S2.SS1.SSS2.p12.7.m6.1.1.cmml" xref="S2.SS1.SSS2.p12.7.m6.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.7.m6.1c">m</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.7.m6.1d">italic_m</annotation></semantics></math>, with higher scores indicating more recent, more relevant, and more important memories respectively.
These scoring functions can be implemented using various methods, for example, <math alttext="s^{rel}(q,m)" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p12.8.m7.2"><semantics id="S2.SS1.SSS2.p12.8.m7.2a"><mrow id="S2.SS1.SSS2.p12.8.m7.2.3" xref="S2.SS1.SSS2.p12.8.m7.2.3.cmml"><msup id="S2.SS1.SSS2.p12.8.m7.2.3.2" xref="S2.SS1.SSS2.p12.8.m7.2.3.2.cmml"><mi id="S2.SS1.SSS2.p12.8.m7.2.3.2.2" xref="S2.SS1.SSS2.p12.8.m7.2.3.2.2.cmml">s</mi><mrow id="S2.SS1.SSS2.p12.8.m7.2.3.2.3" xref="S2.SS1.SSS2.p12.8.m7.2.3.2.3.cmml"><mi id="S2.SS1.SSS2.p12.8.m7.2.3.2.3.2" xref="S2.SS1.SSS2.p12.8.m7.2.3.2.3.2.cmml">r</mi><mo id="S2.SS1.SSS2.p12.8.m7.2.3.2.3.1" xref="S2.SS1.SSS2.p12.8.m7.2.3.2.3.1.cmml">⁢</mo><mi id="S2.SS1.SSS2.p12.8.m7.2.3.2.3.3" xref="S2.SS1.SSS2.p12.8.m7.2.3.2.3.3.cmml">e</mi><mo id="S2.SS1.SSS2.p12.8.m7.2.3.2.3.1a" xref="S2.SS1.SSS2.p12.8.m7.2.3.2.3.1.cmml">⁢</mo><mi id="S2.SS1.SSS2.p12.8.m7.2.3.2.3.4" xref="S2.SS1.SSS2.p12.8.m7.2.3.2.3.4.cmml">l</mi></mrow></msup><mo id="S2.SS1.SSS2.p12.8.m7.2.3.1" xref="S2.SS1.SSS2.p12.8.m7.2.3.1.cmml">⁢</mo><mrow id="S2.SS1.SSS2.p12.8.m7.2.3.3.2" xref="S2.SS1.SSS2.p12.8.m7.2.3.3.1.cmml"><mo id="S2.SS1.SSS2.p12.8.m7.2.3.3.2.1" stretchy="false" xref="S2.SS1.SSS2.p12.8.m7.2.3.3.1.cmml">(</mo><mi id="S2.SS1.SSS2.p12.8.m7.1.1" xref="S2.SS1.SSS2.p12.8.m7.1.1.cmml">q</mi><mo id="S2.SS1.SSS2.p12.8.m7.2.3.3.2.2" xref="S2.SS1.SSS2.p12.8.m7.2.3.3.1.cmml">,</mo><mi id="S2.SS1.SSS2.p12.8.m7.2.2" xref="S2.SS1.SSS2.p12.8.m7.2.2.cmml">m</mi><mo id="S2.SS1.SSS2.p12.8.m7.2.3.3.2.3" stretchy="false" xref="S2.SS1.SSS2.p12.8.m7.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.8.m7.2b"><apply id="S2.SS1.SSS2.p12.8.m7.2.3.cmml" xref="S2.SS1.SSS2.p12.8.m7.2.3"><times id="S2.SS1.SSS2.p12.8.m7.2.3.1.cmml" xref="S2.SS1.SSS2.p12.8.m7.2.3.1"></times><apply id="S2.SS1.SSS2.p12.8.m7.2.3.2.cmml" xref="S2.SS1.SSS2.p12.8.m7.2.3.2"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p12.8.m7.2.3.2.1.cmml" xref="S2.SS1.SSS2.p12.8.m7.2.3.2">superscript</csymbol><ci id="S2.SS1.SSS2.p12.8.m7.2.3.2.2.cmml" xref="S2.SS1.SSS2.p12.8.m7.2.3.2.2">𝑠</ci><apply id="S2.SS1.SSS2.p12.8.m7.2.3.2.3.cmml" xref="S2.SS1.SSS2.p12.8.m7.2.3.2.3"><times id="S2.SS1.SSS2.p12.8.m7.2.3.2.3.1.cmml" xref="S2.SS1.SSS2.p12.8.m7.2.3.2.3.1"></times><ci id="S2.SS1.SSS2.p12.8.m7.2.3.2.3.2.cmml" xref="S2.SS1.SSS2.p12.8.m7.2.3.2.3.2">𝑟</ci><ci id="S2.SS1.SSS2.p12.8.m7.2.3.2.3.3.cmml" xref="S2.SS1.SSS2.p12.8.m7.2.3.2.3.3">𝑒</ci><ci id="S2.SS1.SSS2.p12.8.m7.2.3.2.3.4.cmml" xref="S2.SS1.SSS2.p12.8.m7.2.3.2.3.4">𝑙</ci></apply></apply><interval closure="open" id="S2.SS1.SSS2.p12.8.m7.2.3.3.1.cmml" xref="S2.SS1.SSS2.p12.8.m7.2.3.3.2"><ci id="S2.SS1.SSS2.p12.8.m7.1.1.cmml" xref="S2.SS1.SSS2.p12.8.m7.1.1">𝑞</ci><ci id="S2.SS1.SSS2.p12.8.m7.2.2.cmml" xref="S2.SS1.SSS2.p12.8.m7.2.2">𝑚</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.8.m7.2c">s^{rel}(q,m)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.8.m7.2d">italic_s start_POSTSUPERSCRIPT italic_r italic_e italic_l end_POSTSUPERSCRIPT ( italic_q , italic_m )</annotation></semantics></math> can be calculated using vector similarity measures between query and memory embeddings.
It should be noted that <math alttext="s^{imp}" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p12.9.m8.1"><semantics id="S2.SS1.SSS2.p12.9.m8.1a"><msup id="S2.SS1.SSS2.p12.9.m8.1.1" xref="S2.SS1.SSS2.p12.9.m8.1.1.cmml"><mi id="S2.SS1.SSS2.p12.9.m8.1.1.2" xref="S2.SS1.SSS2.p12.9.m8.1.1.2.cmml">s</mi><mrow id="S2.SS1.SSS2.p12.9.m8.1.1.3" xref="S2.SS1.SSS2.p12.9.m8.1.1.3.cmml"><mi id="S2.SS1.SSS2.p12.9.m8.1.1.3.2" xref="S2.SS1.SSS2.p12.9.m8.1.1.3.2.cmml">i</mi><mo id="S2.SS1.SSS2.p12.9.m8.1.1.3.1" xref="S2.SS1.SSS2.p12.9.m8.1.1.3.1.cmml">⁢</mo><mi id="S2.SS1.SSS2.p12.9.m8.1.1.3.3" xref="S2.SS1.SSS2.p12.9.m8.1.1.3.3.cmml">m</mi><mo id="S2.SS1.SSS2.p12.9.m8.1.1.3.1a" xref="S2.SS1.SSS2.p12.9.m8.1.1.3.1.cmml">⁢</mo><mi id="S2.SS1.SSS2.p12.9.m8.1.1.3.4" xref="S2.SS1.SSS2.p12.9.m8.1.1.3.4.cmml">p</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.9.m8.1b"><apply id="S2.SS1.SSS2.p12.9.m8.1.1.cmml" xref="S2.SS1.SSS2.p12.9.m8.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p12.9.m8.1.1.1.cmml" xref="S2.SS1.SSS2.p12.9.m8.1.1">superscript</csymbol><ci id="S2.SS1.SSS2.p12.9.m8.1.1.2.cmml" xref="S2.SS1.SSS2.p12.9.m8.1.1.2">𝑠</ci><apply id="S2.SS1.SSS2.p12.9.m8.1.1.3.cmml" xref="S2.SS1.SSS2.p12.9.m8.1.1.3"><times id="S2.SS1.SSS2.p12.9.m8.1.1.3.1.cmml" xref="S2.SS1.SSS2.p12.9.m8.1.1.3.1"></times><ci id="S2.SS1.SSS2.p12.9.m8.1.1.3.2.cmml" xref="S2.SS1.SSS2.p12.9.m8.1.1.3.2">𝑖</ci><ci id="S2.SS1.SSS2.p12.9.m8.1.1.3.3.cmml" xref="S2.SS1.SSS2.p12.9.m8.1.1.3.3">𝑚</ci><ci id="S2.SS1.SSS2.p12.9.m8.1.1.3.4.cmml" xref="S2.SS1.SSS2.p12.9.m8.1.1.3.4">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.9.m8.1c">s^{imp}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.9.m8.1d">italic_s start_POSTSUPERSCRIPT italic_i italic_m italic_p end_POSTSUPERSCRIPT</annotation></semantics></math> only reflects the characters of the memory itself, thus it is unrelated to the query <math alttext="q" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p12.10.m9.1"><semantics id="S2.SS1.SSS2.p12.10.m9.1a"><mi id="S2.SS1.SSS2.p12.10.m9.1.1" xref="S2.SS1.SSS2.p12.10.m9.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.10.m9.1b"><ci id="S2.SS1.SSS2.p12.10.m9.1.1.cmml" xref="S2.SS1.SSS2.p12.10.m9.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.10.m9.1c">q</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.10.m9.1d">italic_q</annotation></semantics></math>.
<math alttext="\alpha" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p12.11.m10.1"><semantics id="S2.SS1.SSS2.p12.11.m10.1a"><mi id="S2.SS1.SSS2.p12.11.m10.1.1" xref="S2.SS1.SSS2.p12.11.m10.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.11.m10.1b"><ci id="S2.SS1.SSS2.p12.11.m10.1.1.cmml" xref="S2.SS1.SSS2.p12.11.m10.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.11.m10.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.11.m10.1d">italic_α</annotation></semantics></math>, <math alttext="\beta" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p12.12.m11.1"><semantics id="S2.SS1.SSS2.p12.12.m11.1a"><mi id="S2.SS1.SSS2.p12.12.m11.1.1" xref="S2.SS1.SSS2.p12.12.m11.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.12.m11.1b"><ci id="S2.SS1.SSS2.p12.12.m11.1.1.cmml" xref="S2.SS1.SSS2.p12.12.m11.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.12.m11.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.12.m11.1d">italic_β</annotation></semantics></math> and <math alttext="\gamma" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p12.13.m12.1"><semantics id="S2.SS1.SSS2.p12.13.m12.1a"><mi id="S2.SS1.SSS2.p12.13.m12.1.1" xref="S2.SS1.SSS2.p12.13.m12.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.13.m12.1b"><ci id="S2.SS1.SSS2.p12.13.m12.1.1.cmml" xref="S2.SS1.SSS2.p12.13.m12.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.13.m12.1c">\gamma</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.13.m12.1d">italic_γ</annotation></semantics></math> are balancing parameters.
By assigning them with different values, one can obtain various memory reading strategies.
For example, by setting <math alttext="\alpha=\gamma=0" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p12.14.m13.1"><semantics id="S2.SS1.SSS2.p12.14.m13.1a"><mrow id="S2.SS1.SSS2.p12.14.m13.1.1" xref="S2.SS1.SSS2.p12.14.m13.1.1.cmml"><mi id="S2.SS1.SSS2.p12.14.m13.1.1.2" xref="S2.SS1.SSS2.p12.14.m13.1.1.2.cmml">α</mi><mo id="S2.SS1.SSS2.p12.14.m13.1.1.3" xref="S2.SS1.SSS2.p12.14.m13.1.1.3.cmml">=</mo><mi id="S2.SS1.SSS2.p12.14.m13.1.1.4" xref="S2.SS1.SSS2.p12.14.m13.1.1.4.cmml">γ</mi><mo id="S2.SS1.SSS2.p12.14.m13.1.1.5" xref="S2.SS1.SSS2.p12.14.m13.1.1.5.cmml">=</mo><mn id="S2.SS1.SSS2.p12.14.m13.1.1.6" xref="S2.SS1.SSS2.p12.14.m13.1.1.6.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.14.m13.1b"><apply id="S2.SS1.SSS2.p12.14.m13.1.1.cmml" xref="S2.SS1.SSS2.p12.14.m13.1.1"><and id="S2.SS1.SSS2.p12.14.m13.1.1a.cmml" xref="S2.SS1.SSS2.p12.14.m13.1.1"></and><apply id="S2.SS1.SSS2.p12.14.m13.1.1b.cmml" xref="S2.SS1.SSS2.p12.14.m13.1.1"><eq id="S2.SS1.SSS2.p12.14.m13.1.1.3.cmml" xref="S2.SS1.SSS2.p12.14.m13.1.1.3"></eq><ci id="S2.SS1.SSS2.p12.14.m13.1.1.2.cmml" xref="S2.SS1.SSS2.p12.14.m13.1.1.2">𝛼</ci><ci id="S2.SS1.SSS2.p12.14.m13.1.1.4.cmml" xref="S2.SS1.SSS2.p12.14.m13.1.1.4">𝛾</ci></apply><apply id="S2.SS1.SSS2.p12.14.m13.1.1c.cmml" xref="S2.SS1.SSS2.p12.14.m13.1.1"><eq id="S2.SS1.SSS2.p12.14.m13.1.1.5.cmml" xref="S2.SS1.SSS2.p12.14.m13.1.1.5"></eq><share href="https://arxiv.org/html/2308.11432v7#S2.SS1.SSS2.p12.14.m13.1.1.4.cmml" id="S2.SS1.SSS2.p12.14.m13.1.1d.cmml" xref="S2.SS1.SSS2.p12.14.m13.1.1"></share><cn id="S2.SS1.SSS2.p12.14.m13.1.1.6.cmml" type="integer" xref="S2.SS1.SSS2.p12.14.m13.1.1.6">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.14.m13.1c">\alpha=\gamma=0</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.14.m13.1d">italic_α = italic_γ = 0</annotation></semantics></math>, many studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib30" title="">30</a>]</cite> only consider the relevance score <math alttext="s^{rel}" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p12.15.m14.1"><semantics id="S2.SS1.SSS2.p12.15.m14.1a"><msup id="S2.SS1.SSS2.p12.15.m14.1.1" xref="S2.SS1.SSS2.p12.15.m14.1.1.cmml"><mi id="S2.SS1.SSS2.p12.15.m14.1.1.2" xref="S2.SS1.SSS2.p12.15.m14.1.1.2.cmml">s</mi><mrow id="S2.SS1.SSS2.p12.15.m14.1.1.3" xref="S2.SS1.SSS2.p12.15.m14.1.1.3.cmml"><mi id="S2.SS1.SSS2.p12.15.m14.1.1.3.2" xref="S2.SS1.SSS2.p12.15.m14.1.1.3.2.cmml">r</mi><mo id="S2.SS1.SSS2.p12.15.m14.1.1.3.1" xref="S2.SS1.SSS2.p12.15.m14.1.1.3.1.cmml">⁢</mo><mi id="S2.SS1.SSS2.p12.15.m14.1.1.3.3" xref="S2.SS1.SSS2.p12.15.m14.1.1.3.3.cmml">e</mi><mo id="S2.SS1.SSS2.p12.15.m14.1.1.3.1a" xref="S2.SS1.SSS2.p12.15.m14.1.1.3.1.cmml">⁢</mo><mi id="S2.SS1.SSS2.p12.15.m14.1.1.3.4" xref="S2.SS1.SSS2.p12.15.m14.1.1.3.4.cmml">l</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.15.m14.1b"><apply id="S2.SS1.SSS2.p12.15.m14.1.1.cmml" xref="S2.SS1.SSS2.p12.15.m14.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p12.15.m14.1.1.1.cmml" xref="S2.SS1.SSS2.p12.15.m14.1.1">superscript</csymbol><ci id="S2.SS1.SSS2.p12.15.m14.1.1.2.cmml" xref="S2.SS1.SSS2.p12.15.m14.1.1.2">𝑠</ci><apply id="S2.SS1.SSS2.p12.15.m14.1.1.3.cmml" xref="S2.SS1.SSS2.p12.15.m14.1.1.3"><times id="S2.SS1.SSS2.p12.15.m14.1.1.3.1.cmml" xref="S2.SS1.SSS2.p12.15.m14.1.1.3.1"></times><ci id="S2.SS1.SSS2.p12.15.m14.1.1.3.2.cmml" xref="S2.SS1.SSS2.p12.15.m14.1.1.3.2">𝑟</ci><ci id="S2.SS1.SSS2.p12.15.m14.1.1.3.3.cmml" xref="S2.SS1.SSS2.p12.15.m14.1.1.3.3">𝑒</ci><ci id="S2.SS1.SSS2.p12.15.m14.1.1.3.4.cmml" xref="S2.SS1.SSS2.p12.15.m14.1.1.3.4">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.15.m14.1c">s^{rel}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.15.m14.1d">italic_s start_POSTSUPERSCRIPT italic_r italic_e italic_l end_POSTSUPERSCRIPT</annotation></semantics></math> for memory reading.
By assigning <math alttext="\alpha=\beta=\gamma=1.0" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p12.16.m15.1"><semantics id="S2.SS1.SSS2.p12.16.m15.1a"><mrow id="S2.SS1.SSS2.p12.16.m15.1.1" xref="S2.SS1.SSS2.p12.16.m15.1.1.cmml"><mi id="S2.SS1.SSS2.p12.16.m15.1.1.2" xref="S2.SS1.SSS2.p12.16.m15.1.1.2.cmml">α</mi><mo id="S2.SS1.SSS2.p12.16.m15.1.1.3" xref="S2.SS1.SSS2.p12.16.m15.1.1.3.cmml">=</mo><mi id="S2.SS1.SSS2.p12.16.m15.1.1.4" xref="S2.SS1.SSS2.p12.16.m15.1.1.4.cmml">β</mi><mo id="S2.SS1.SSS2.p12.16.m15.1.1.5" xref="S2.SS1.SSS2.p12.16.m15.1.1.5.cmml">=</mo><mi id="S2.SS1.SSS2.p12.16.m15.1.1.6" xref="S2.SS1.SSS2.p12.16.m15.1.1.6.cmml">γ</mi><mo id="S2.SS1.SSS2.p12.16.m15.1.1.7" xref="S2.SS1.SSS2.p12.16.m15.1.1.7.cmml">=</mo><mn id="S2.SS1.SSS2.p12.16.m15.1.1.8" xref="S2.SS1.SSS2.p12.16.m15.1.1.8.cmml">1.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.16.m15.1b"><apply id="S2.SS1.SSS2.p12.16.m15.1.1.cmml" xref="S2.SS1.SSS2.p12.16.m15.1.1"><and id="S2.SS1.SSS2.p12.16.m15.1.1a.cmml" xref="S2.SS1.SSS2.p12.16.m15.1.1"></and><apply id="S2.SS1.SSS2.p12.16.m15.1.1b.cmml" xref="S2.SS1.SSS2.p12.16.m15.1.1"><eq id="S2.SS1.SSS2.p12.16.m15.1.1.3.cmml" xref="S2.SS1.SSS2.p12.16.m15.1.1.3"></eq><ci id="S2.SS1.SSS2.p12.16.m15.1.1.2.cmml" xref="S2.SS1.SSS2.p12.16.m15.1.1.2">𝛼</ci><ci id="S2.SS1.SSS2.p12.16.m15.1.1.4.cmml" xref="S2.SS1.SSS2.p12.16.m15.1.1.4">𝛽</ci></apply><apply id="S2.SS1.SSS2.p12.16.m15.1.1c.cmml" xref="S2.SS1.SSS2.p12.16.m15.1.1"><eq id="S2.SS1.SSS2.p12.16.m15.1.1.5.cmml" xref="S2.SS1.SSS2.p12.16.m15.1.1.5"></eq><share href="https://arxiv.org/html/2308.11432v7#S2.SS1.SSS2.p12.16.m15.1.1.4.cmml" id="S2.SS1.SSS2.p12.16.m15.1.1d.cmml" xref="S2.SS1.SSS2.p12.16.m15.1.1"></share><ci id="S2.SS1.SSS2.p12.16.m15.1.1.6.cmml" xref="S2.SS1.SSS2.p12.16.m15.1.1.6">𝛾</ci></apply><apply id="S2.SS1.SSS2.p12.16.m15.1.1e.cmml" xref="S2.SS1.SSS2.p12.16.m15.1.1"><eq id="S2.SS1.SSS2.p12.16.m15.1.1.7.cmml" xref="S2.SS1.SSS2.p12.16.m15.1.1.7"></eq><share href="https://arxiv.org/html/2308.11432v7#S2.SS1.SSS2.p12.16.m15.1.1.6.cmml" id="S2.SS1.SSS2.p12.16.m15.1.1f.cmml" xref="S2.SS1.SSS2.p12.16.m15.1.1"></share><cn id="S2.SS1.SSS2.p12.16.m15.1.1.8.cmml" type="float" xref="S2.SS1.SSS2.p12.16.m15.1.1.8">1.0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.16.m15.1c">\alpha=\beta=\gamma=1.0</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.16.m15.1d">italic_α = italic_β = italic_γ = 1.0</annotation></semantics></math>, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib20" title="">20</a>]</cite> equally weights all the above three metrics to extract information from memory.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p13">
<p class="ltx_p" id="S2.SS1.SSS2.p13.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p13.1.m1.1"><semantics id="S2.SS1.SSS2.p13.1.m1.1a"><mo id="S2.SS1.SSS2.p13.1.m1.1.1" xref="S2.SS1.SSS2.p13.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p13.1.m1.1b"><ci id="S2.SS1.SSS2.p13.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p13.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p13.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p13.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS2.p13.1.1">Memory Writing</span>.
The purpose of memory writing is to store information about the perceived environment in memory.
Storing valuable information in memory provides a foundation for retrieving informative memories in the future, enabling the agent to act more efficiently and rationally.
During the memory writing process, there are two potential problems that should be carefully addressed.
On one hand, it is crucial to address how to store information that is similar to existing memories (<em class="ltx_emph ltx_font_italic" id="S2.SS1.SSS2.p13.1.2">i.e.</em>, memory duplicated).
On the other hand, it is important to consider how to remove information when the memory reaches its storage limit (<em class="ltx_emph ltx_font_italic" id="S2.SS1.SSS2.p13.1.3">i.e.</em>, memory overflow).
In the following, we discuss these problems more in detail.
(1) <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS2.p13.1.4">Memory Duplicated</span>.
To incorporate similar information, people have developed various methods for integrating new and previous records.
For instance, in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib16" title="">16</a>]</cite>, the successful action sequences related to the same sub-goal are stored in a list.
Once the size of the list reaches N(=5), all the sequences in it are condensed into a unified plan solution using LLMs.
The original sequences in the memory are replaced with the newly generated one.
Augmented LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib42" title="">42</a>]</cite> aggregates duplicate information via count accumulation, avoiding redundant storage.
(2) <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS2.p13.1.5">Memory Overflow</span>.
In order to write information into the memory when it is full, people design different methods to delete existing information to continue the memorizing process.
For example, in ChatDB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib40" title="">40</a>]</cite>, memories can be explicitly deleted based on user commands.
RET-LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib41" title="">41</a>]</cite> uses a fixed-size buffer for memory, overwriting the oldest entries in a first-in-first-out (FIFO) manner.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p14">
<p class="ltx_p" id="S2.SS1.SSS2.p14.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p14.1.m1.1"><semantics id="S2.SS1.SSS2.p14.1.m1.1a"><mo id="S2.SS1.SSS2.p14.1.m1.1.1" xref="S2.SS1.SSS2.p14.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p14.1.m1.1b"><ci id="S2.SS1.SSS2.p14.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p14.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p14.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p14.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS2.p14.1.1">Memory Reflection</span>.
Memory reflection emulates humans’ ability to witness and evaluate their own cognitive, emotional, and behavioral processes.
When adapted to agents, the objective is to provide agents with the capability to independently summarize and infer more abstract, complex and high-level information.
More specifically, in Generative Agent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib20" title="">20</a>]</cite>, the agent has the capability to summarize its past experiences stored in memory into broader and more abstract insights.
To begin with, the agent generates three key questions based on its recent memories.
Then, these questions are used to query the memory to obtain relevant information.
Building upon the acquired information, the agent generates five insights, which reflect the agent high-level ideas.
For example, the low-level memories “Klaus Mueller is writing a research paper”, “Klaus Mueller is engaging with a librarian to further his research”, and “Klaus Mueller is conversing with Ayesha Khan
about his research” can induce the high-level insight “Klaus Mueller is dedicated to his research”.
In addition, the reflection process can occur hierarchically, meaning that the insights can be generated based on existing insights.
In GITM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib16" title="">16</a>]</cite>, the actions that successfully accomplish the sub-goals are stored in a list.
When the list contains more than five elements, the agent summarizes them into a common and abstract pattern and replaces all the elements.
In ExpeL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib43" title="">43</a>]</cite>, two approaches are introduced for the agent to acquire reflection. Firstly, the agent compares successful or failed trajectories within the same task. Secondly, the agent learns from a collection of successful trajectories to gain experiences.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p15">
<p class="ltx_p" id="S2.SS1.SSS2.p15.1">A significant distinction between traditional LLMs and the agents is that the latter must possess the capability to learn and complete tasks in dynamic environments.
If we consider the memory module as responsible for managing the agents’ past behaviors, it becomes essential to have another significant module that can assist the agents in planning their future actions.
In the following, we present an overview of how researchers design the planning module.</p>
</div>
<figure class="ltx_figure" id="S2.F3">
<p class="ltx_p ltx_align_center" id="S2.F3.1"><span class="ltx_text ltx_framed ltx_framed_rectangle" id="S2.F3.1.1" style="border-color: #000000;padding:0.0pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="266" id="S2.F3.1.1.g1" src="x3.png" width="747"/>
</span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Comparison between the strategies of single-path and multi-path reasoning. LMZSP is the model proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib44" title="">44</a>]</cite>.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>Planning Module</h4>
<div class="ltx_para" id="S2.SS1.SSS3.p1">
<p class="ltx_p" id="S2.SS1.SSS3.p1.1">When faced with a complex task, humans tend to deconstruct it into simpler subtasks and solve them individually.
The planning module aims to empower the agents with such human capability, which is expected to make the agent behave more reasonably, powerfully, and reliably.
In specific, we summarize existing studies based on whether the agent can receive feedback in the planing process, which are detailed as follows:</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS3.p2">
<p class="ltx_p" id="S2.SS1.SSS3.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS3.p2.1.1">Planning without Feedback</span>:
In this method, the agents do not receive feedback that can influence its future behaviors after taking actions.
In the following, we present several representative strategies.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS3.p3">
<p class="ltx_p" id="S2.SS1.SSS3.p3.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S2.SS1.SSS3.p3.1.m1.1"><semantics id="S2.SS1.SSS3.p3.1.m1.1a"><mo id="S2.SS1.SSS3.p3.1.m1.1.1" xref="S2.SS1.SSS3.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p3.1.m1.1b"><ci id="S2.SS1.SSS3.p3.1.m1.1.1.cmml" xref="S2.SS1.SSS3.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p3.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS3.p3.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS3.p3.1.1">Single-path Reasoning</span>.
In this strategy, the final task is decomposed into several intermediate steps.
These steps are connected in a cascading manner, with each step leading to only one subsequent step.
LLMs follow these steps to achieve the final goal.
Specifically, Chain of Thought (CoT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib45" title="">45</a>]</cite> proposes inputting reasoning steps for solving complex problems into the prompt. These steps serve as examples to inspire LLMs to plan and act in a step-by-step manner.
In this method, the plans are created based on the inspiration from the examples in the prompts.
Zero-shot-CoT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib46" title="">46</a>]</cite> enables LLMs to generate task reasoning processes by prompting them with trigger sentences like "think step by step".
Unlike CoT, this method does not incorporate reasoning steps as examples in the prompts.
Re-Prompting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib47" title="">47</a>]</cite> involves checking whether each step meets the necessary prerequisites before generating a plan.
If a step fails to meet the prerequisites, it introduces a prerequisite error message and prompts the LLM to regenerate the plan.
ReWOO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib48" title="">48</a>]</cite> introduces a paradigm of separating plans from external observations, where the agents first generate plans and obtain observations independently, and then combine them together to derive the final results.
HuggingGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib13" title="">13</a>]</cite> first decomposes the task into many sub-goals, and then solves each of them based on Huggingface.
Different from CoT and Zero-shot-CoT, which outcome all the reasoning steps in a one-shot manner, ReWOO and HuggingGPT produce the results by accessing LLMs multiply times.
SWIFTSAGE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib49" title="">49</a>]</cite>, inspired by the dual-process theory of human cognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib50" title="">50</a>]</cite>, combines the power of both SWIFT and SAGE modules for planning in complex interactive tasks. The SWIFT module provides quick responses based on learned patterns, while the SAGE module, using large language models, conducts in-depth planning by asking key questions and generating action sequences to ensure successful task completion.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS3.p4">
<p class="ltx_p" id="S2.SS1.SSS3.p4.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S2.SS1.SSS3.p4.1.m1.1"><semantics id="S2.SS1.SSS3.p4.1.m1.1a"><mo id="S2.SS1.SSS3.p4.1.m1.1.1" xref="S2.SS1.SSS3.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p4.1.m1.1b"><ci id="S2.SS1.SSS3.p4.1.m1.1.1.cmml" xref="S2.SS1.SSS3.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p4.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS3.p4.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS3.p4.1.1">Multi-path Reasoning</span>.
In this strategy, the reasoning steps for generating the final plans are organized into a tree-like structure.
Each intermediate step may have multiple subsequent steps.
This approach is analogous to human thinking, as individuals may have multiple choices at each reasoning step.
In specific, Self-consistent CoT (CoT-SC) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib51" title="">51</a>]</cite> believes that each complex problem has multiple ways of thinking to deduce the final answer.
Thus, it starts by employing CoT to generate various reasoning paths and corresponding answers. Subsequently, the answer with the highest frequency is chosen as the final output.
Tree of Thoughts (ToT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib52" title="">52</a>]</cite> is designed to generate plans using a tree-like reasoning structure. In this approach, each node in the tree represents a "thought," which corresponds to an intermediate reasoning step. The selection of these intermediate steps is based on the evaluation of LLMs. The final plan is generated using either the breadth-first search (BFS) or depth-first search (DFS) strategy.
Comparing with CoT-SC, which generates all the planed steps together, ToT needs to query LLMs for each reasoning step.
In RecMind <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib53" title="">53</a>]</cite>, the authors designed a self-inspiring mechanism, where the discarded historical information in the planning process is also leveraged to derive new reasoning steps.
In GoT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib54" title="">54</a>]</cite>, the authors expand the tree-like reasoning structure in ToT to graph structures, resulting in more powerful prompting strategies.
In AoT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib55" title="">55</a>]</cite>, the authors design a novel method to enhance the reasoning processes of LLMs by incorporating algorithmic examples into the prompts.
Remarkably, this method only needs to query LLMs for only one or a few times.
In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib44" title="">44</a>]</cite>, the LLMs are leveraged as zero-shot planners.
At each planning step, they first generate multiple possible next steps, and then determine the final one based on their distances to admissible actions.
 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib56" title="">56</a>]</cite> further improves  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib44" title="">44</a>]</cite> by incorporating examples that are similar to the queries in the prompts.
RAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib57" title="">57</a>]</cite> builds a world model to simulate the potential benefits of different plans based on Monte Carlo Tree Search (MCTS), and then, the final plan is generated by aggregating multiple MCTS iterations.
To enhance comprehension, we provide an illustration comparing the strategies of single-path and multi-path reasoning in Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S2.F3" title="Figure 3 ‣ 2.1.2 Memory Module ‣ 2.1 Agent Architecture Design ‣ 2 LLM-based Autonomous Agent Construction ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS3.p5">
<p class="ltx_p" id="S2.SS1.SSS3.p5.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S2.SS1.SSS3.p5.1.m1.1"><semantics id="S2.SS1.SSS3.p5.1.m1.1a"><mo id="S2.SS1.SSS3.p5.1.m1.1.1" xref="S2.SS1.SSS3.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p5.1.m1.1b"><ci id="S2.SS1.SSS3.p5.1.m1.1.1.cmml" xref="S2.SS1.SSS3.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p5.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS3.p5.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS3.p5.1.1">External Planner</span>.
Despite the demonstrated power of LLMs in zero-shot planning, effectively generating plans for domain-specific problems remains highly challenging.
To address this challenge, researchers turn to external planners.
These tools are well-developed and employ efficient search algorithms to rapidly identify correct, or even optimal, plans.
In specific, LLM+P <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib58" title="">58</a>]</cite> first transforms the task descriptions into formal Planning Domain Definition Languages (PDDL), and then it uses an external planner to deal with the PDDL. Finally, the generated results are transformed back into natural language by LLMs.
Similarly, LLM-DP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib59" title="">59</a>]</cite> utilizes LLMs to convert the observations, the current world state, and the target objectives into PDDL. Subsequently, this transformed data is passed to an external planner, which efficiently determines the final action sequence.
CO-LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib22" title="">22</a>]</cite> demonstrates that LLMs is good at generating high-level plans, but struggle with low-level control.
To address this limitation, a heuristically designed external low-level planner is employed to effectively execute actions based on high-level plans.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS3.p6">
<p class="ltx_p" id="S2.SS1.SSS3.p6.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS3.p6.1.1">Planning with Feedback</span>:
In many real-world scenarios, the agents need to make long-horizon planning to solve complex tasks.
When facing these tasks, the above planning modules without feedback can be less effective due to the following reasons:
firstly, generating a flawless plan directly from the beginning is extremely difficult as it needs to consider various complex preconditions.
As a result, simply following the initial plan often leads to failure.
Moreover, the execution of the plan may be hindered by unpredictable transition dynamics, rendering the initial plan non-executable.
Simultaneously, when examining how humans tackle complex tasks, we find that individuals may iteratively make and revise their plans based on external feedback.
To simulate such human capability, researchers have designed many planning modules, where the agent can receive feedback after taking actions.
The feedback can be obtained from environments, humans, and models, which are detailed in the following.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS3.p7">
<p class="ltx_p" id="S2.SS1.SSS3.p7.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S2.SS1.SSS3.p7.1.m1.1"><semantics id="S2.SS1.SSS3.p7.1.m1.1a"><mo id="S2.SS1.SSS3.p7.1.m1.1.1" xref="S2.SS1.SSS3.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p7.1.m1.1b"><ci id="S2.SS1.SSS3.p7.1.m1.1.1.cmml" xref="S2.SS1.SSS3.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p7.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS3.p7.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS3.p7.1.1">Environmental Feedback</span>.
This feedback is obtained from the objective world or virtual environment.
For instance, it could be the game’s task completion signals or the observations made after the agent takes an action.
In specific,
ReAct <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib60" title="">60</a>]</cite> proposes constructing prompts using thought-act-observation triplets.
The thought component aims to facilitate high-level reasoning and planning for guiding agent behaviors.
The act represents a specific action taken by the agent.
The observation corresponds to the outcome of the action, acquired through external feedback, such as search engine results.
The next thought is influenced by the previous observations, which makes the generated plans more adaptive to the environment.
Voyager <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib38" title="">38</a>]</cite> makes plans by incorporating three types of environment feedback including the intermediate progress of program execution, the execution error and self-verification results.
These signals can help the agent to make better plans for the next action.
Similar to Voyager, Ghost <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib16" title="">16</a>]</cite> also incorporates feedback into the reasoning and action taking processes.
This feedback encompasses the environment states as well as the success and failure information for each executed action.
SayPlan <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib31" title="">31</a>]</cite> leverages environmental feedback derived from a scene graph simulator to validate and refine its strategic formulations.
This simulator is adept at discerning the outcomes and state transitions subsequent to agent actions, facilitating SayPlan’s iterative recalibration of its strategies until a viable plan is ascertained.
In DEPS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib33" title="">33</a>]</cite>, the authors argue that solely providing information about the completion of a task is often inadequate for correcting planning errors.
Therefore, they propose informing the agent about the detail reasons for task failure, allowing them to more effectively revise their plans.
LLM-Planner <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib61" title="">61</a>]</cite> introduces a grounded re-planning algorithm that dynamically updates plans generated by LLMs when encountering object mismatches and unattainable plans during task completion.
Inner Monologue <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib62" title="">62</a>]</cite> provides three types of feedback to the agent after it takes actions:
(1) whether the task is successfully completed, (2) passive scene descriptions, and (3) active scene descriptions.
The former two are generated from the environments, which makes the agent actions more reasonable.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS3.p8">
<p class="ltx_p" id="S2.SS1.SSS3.p8.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S2.SS1.SSS3.p8.1.m1.1"><semantics id="S2.SS1.SSS3.p8.1.m1.1a"><mo id="S2.SS1.SSS3.p8.1.m1.1.1" xref="S2.SS1.SSS3.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p8.1.m1.1b"><ci id="S2.SS1.SSS3.p8.1.m1.1.1.cmml" xref="S2.SS1.SSS3.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p8.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS3.p8.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS3.p8.1.1">Human Feedback</span>.
In addition to obtaining feedback from the environment, directly interacting with humans is also a very intuitive strategy to enhance the agent planning capability.
The human feedback is a subjective signal.
It can effectively make the agent align with the human values and preferences, and also help to alleviate the hallucination problem.
In Inner Monologue <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib62" title="">62</a>]</cite>, the agent aims to perform high-level natural language instructions in a 3D visual environment.
It is given the capability to actively solicit feedback from humans regarding scene descriptions.
Then, the agent incorporates the human feedback into its prompts, enabling more informed planning and reasoning.
In the above cases, we can see, different types of feedback can be combined to enhance the agent planning capability.
For example, Inner Monologue <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib62" title="">62</a>]</cite> collects both environment and human feedback to facilitate the agent plans.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS3.p9">
<p class="ltx_p" id="S2.SS1.SSS3.p9.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S2.SS1.SSS3.p9.1.m1.1"><semantics id="S2.SS1.SSS3.p9.1.m1.1a"><mo id="S2.SS1.SSS3.p9.1.m1.1.1" xref="S2.SS1.SSS3.p9.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p9.1.m1.1b"><ci id="S2.SS1.SSS3.p9.1.m1.1.1.cmml" xref="S2.SS1.SSS3.p9.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p9.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS3.p9.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS3.p9.1.1">Model Feedback</span>.
Apart from the aforementioned environmental and human feedback, which are external signals, researchers have also investigated the utilization of internal feedback from the agents themselves.
This type of feedback is usually generated based on pre-trained models.
In specific, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib63" title="">63</a>]</cite> proposes a self-refine mechanism.
This mechanism consists of three crucial components: output, feedback, and refinement. Firstly, the agent generates an output. Then, it utilizes LLMs to provide feedback on the output and offer guidance on how to refine it. At last, the output is improved by the feedback and refinement.
This output-feedback-refinement process iterates until reaching some desired conditions.
SelfCheck <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib64" title="">64</a>]</cite> allows agents to examine and evaluate their reasoning steps generated at various stages. They can then correct any errors by comparing the outcomes.
InterAct <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib65" title="">65</a>]</cite> uses different language models (such as ChatGPT and InstructGPT) as auxiliary roles, such as checkers and sorters, to help the main language model avoid erroneous and inefficient actions.
ChatCoT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib66" title="">66</a>]</cite> utilizes model feedback to improve the quality of its reasoning process.
The model feedback is generated by an evaluation module that monitors the agent reasoning steps.
Reflexion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib12" title="">12</a>]</cite> is developed to enhance the agent’s planning capability through detailed verbal feedback.
In this model, the agent first produces an action based on its memory, and then, the evaluator generates feedback by taking the agent trajectory as input.
In contrast to previous studies, where the feedback is given as a scalar value, this model leverages LLMs to provide more detailed verbal feedback, which can provide more comprehensive supports for the agent plans.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremarkx4">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic" id="Thmremarkx4.1.1.1">Remark</span></span><span class="ltx_text ltx_font_italic" id="Thmremarkx4.2.2">.</span>
</h6>
<div class="ltx_para" id="Thmremarkx4.p1">
<p class="ltx_p" id="Thmremarkx4.p1.1">In conclusion, the implementation of planning module without feedback is relatively straightforward. However, it is primarily suitable for simple tasks that only require a small number of reasoning steps.
Conversely, the strategy of planning with feedback needs more careful designs to handle the feedback.
Nevertheless, it is considerably more powerful and capable of effectively addressing complex tasks that involve long-range reasoning.</p>
</div>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.4 </span>Action Module</h4>
<div class="ltx_para" id="S2.SS1.SSS4.p1">
<p class="ltx_p" id="S2.SS1.SSS4.p1.1">The action module is responsible for translating the agent’s decisions into specific outcomes. This module is located at the most downstream position and directly interacts with the environment. It is influenced by the profile, memory, and planning modules. This section introduces the action module from four perspectives: (1) Action goal: what are the intended outcomes of the actions? (2) Action production: how are the actions generated? (3) Action space: what are the available actions? (4) Action impact: what are the consequences of the actions?
Among these perspectives, the first two focus on the aspects preceding the action ("before-action" aspects), the third focuses on the action itself ("in-action" aspect), and the fourth emphasizes the impact of the actions ("after-action" aspect).</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS4.p2">
<p class="ltx_p" id="S2.SS1.SSS4.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS4.p2.1.1">Action Goal</span>:
The agent can perform actions with various objectives.
Here, we present several representative examples:
(1) <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS4.p2.1.2">Task Completion</span>.
In this scenario, the agent’s actions are aimed at accomplishing specific tasks, such as crafting an iron pickaxe in Minecraft <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib38" title="">38</a>]</cite> or completing a function in software development <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib18" title="">18</a>]</cite>.
These actions usually have well-defined objectives, and each action contributes to the completion of the final task.
Actions aimed at this type of goal are very common in existing literature.
(2) <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS4.p2.1.3">Communication</span>.
In this case, the actions are taken to communicate with the other agents or real humans for sharing information or collaboration.
For example, the agents in ChatDev <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib18" title="">18</a>]</cite> may communicate with each other to collectively accomplish software development tasks.
In Inner Monologue <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib62" title="">62</a>]</cite>, the agent actively engages in communication with humans and adjusts its action strategies based on human feedback.
(3) <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS4.p2.1.4">Environment Exploration</span>.
In this example, the agent aims to explore unfamiliar environments to expand its perception and strike a balance between exploring and exploiting.
For instance, the agent in Voyager <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib38" title="">38</a>]</cite> may explore unknown skills in their task completion process and continually refine the skill execution code based on environment feedback through trial and error.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS4.p3">
<p class="ltx_p" id="S2.SS1.SSS4.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS4.p3.1.1">Action Production</span>:
Different from ordinary LLMs, where the model input and output are directly associated, the agent may take actions via different strategies and sources.
In the following, we introduce two types of commonly used action production strategies.
(1) <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS4.p3.1.2">Action via Memory Recollection</span>.
In this strategy, the action is generated by extracting information from the agent memory according to the current task.
The task and the extracted memories are used as prompts to trigger the agent actions.
For example, in Generative Agents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib20" title="">20</a>]</cite>, the agent maintains a memory stream, and before taking each action, it retrieves recent, relevant and important information from the memory steam to guide the agent actions.
In GITM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib16" title="">16</a>]</cite>, in order to achieve a low-level sub-goal, the agent queries its memory to determine if there are any successful experiences related to the task. If similar tasks have been completed previously, the agent invokes the previously successful actions to handle the current task directly.
In collaborative agents such as ChatDev <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib18" title="">18</a>]</cite> and MetaGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib23" title="">23</a>]</cite>, different agents may communicate with each other.
In this process, the conversation history in a dialog is remembered in the agent memories.
Each utterance generated by the agent is influenced by its memory.
(2) <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS4.p3.1.3">Action via Plan Following</span>.
In this strategy, the agent takes actions following its pre-generated plans.
For instance, in DEPS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib33" title="">33</a>]</cite>, for a given task, the agent first makes action plans. If there are no signals indicating plan failure, the agent will strictly adhere to these plans.
In GITM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib16" title="">16</a>]</cite>, the agent makes high-level plans by decomposing the task into many sub-goals.
Based on these plans, the agent takes actions to solve each sub-goal sequentially to complete the final task.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS4.p4">
<p class="ltx_p" id="S2.SS1.SSS4.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS4.p4.1.1">Action Space</span>:
Action space refers to the set of possible actions that can be performed by the agent.
In general, we can roughly divide these actions into two classes: (1) external tools and (2) internal knowledge of the LLMs.
In the following, we introduce these actions more in detail.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS4.p5">
<p class="ltx_p" id="S2.SS1.SSS4.p5.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S2.SS1.SSS4.p5.1.m1.1"><semantics id="S2.SS1.SSS4.p5.1.m1.1a"><mo id="S2.SS1.SSS4.p5.1.m1.1.1" xref="S2.SS1.SSS4.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS4.p5.1.m1.1b"><ci id="S2.SS1.SSS4.p5.1.m1.1.1.cmml" xref="S2.SS1.SSS4.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS4.p5.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS4.p5.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS4.p5.1.1">External Tools</span>.
While LLMs have been demonstrated to be effective in accomplishing a large amount of tasks, they may not work well for the domains which need comprehensive expert knowledge.
In addition, LLMs may also encounter hallucination problems, which are hard to be resolved by themselves.
To alleviate the above problems, the agents are empowered with the capability to call external tools for executing action.
In the following, we present several representative tools which have been exploited in the literature.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS4.p6">
<p class="ltx_p" id="S2.SS1.SSS4.p6.1">(1) <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS4.p6.1.1">APIs</span>.
Leveraging external APIs to complement and expand action space is a popular paradigm in recent years.
For example,
HuggingGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib13" title="">13</a>]</cite> integrates HuggingFace’s vast model ecosystem to tackle complex user tasks.
Similarly, WebGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib67" title="">67</a>]</cite> proposes to automatically generate queries to extract relevant content from external web pages when responding to user request.
TPTU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib68" title="">68</a>]</cite> explores the potential of LLMs to address intricate tasks through strategic task planning and API-based tools.
Gorilla <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib69" title="">69</a>]</cite> introduces a fine-tuned LLM capable of generating precise input arguments for API calls, effectively mitigating hallucination issues during external API usage. ToolFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib15" title="">15</a>]</cite> employs self-supervised learning to determine when and how to invoke external tools, using demonstrations of tool APIs for training. API-Bank <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib70" title="">70</a>]</cite> offers a comprehensive benchmark with a diverse collection of API tools to systematically evaluate tool-augmented LLMs, alongside robust training datasets designed to enhance their integration capabilities.
ToolLLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib14" title="">14</a>]</cite> proposes a tool-use framework encompassing data collection, training, and evaluation, with the resulting fine-tuned model excelling across a wide array of APIs.
RestGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib71" title="">71</a>]</cite> connects LLMs with RESTful APIs, which follow widely accepted standards for web services development, making the resulting program more compatible with real-world applications.
TaskMatrix.AI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib72" title="">72</a>]</cite> connects LLMs with an extensive ecosystem of APIs to support task execution.
At its core lies a multimodal conversational foundational model that interacts with users, understands their goals and context, and then produces executable code for particular tasks.
In essence, these intelligent agents strategically harness external APIs as versatile tools, systematically expanding their action space and transcending the inherent limitations of traditional language models by integrating diverse computational capabilities.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS4.p7">
<p class="ltx_p" id="S2.SS1.SSS4.p7.1">(2) <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS4.p7.1.1">Databases &amp; Knowledge Bases</span>.
Integrating external database or knowledge base enables agents to obtain specific domain information for generating more realistic actions.
For example, ChatDB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib40" title="">40</a>]</cite> employs SQL statements to query databases, facilitating actions by the agents in a logical manner.
MRKL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib73" title="">73</a>]</cite> and OpenAGI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib74" title="">74</a>]</cite> incorporate various expert systems such as knowledge bases and planners to access domain-specific information.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS4.p8">
<p class="ltx_p" id="S2.SS1.SSS4.p8.1">(3) <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS4.p8.1.1">External Models</span>.
Previous studies often utilize external models to expand the range of possible actions.
In comparison to APIs, external models typically handle more complex tasks.
Each external model may correspond to multiple APIs.
For example, ViperGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib75" title="">75</a>]</cite> firstly uses Codex, which is implemented based on language model, to generate Python code from text descriptions, and then executes the code to complete the given tasks.
ChemCrow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib76" title="">76</a>]</cite> is an LLM-based chemical agent designed to perform tasks in organic synthesis, drug discovery, and material design. It utilizes seventeen expert-designed models to assist its operations.
MM-REACT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib77" title="">77</a>]</cite> integrates various external models, such as VideoBERT for video summarization, X-decoder for image generation, and SpeechBERT for audio processing, enhancing its capability in diverse multimodal scenarios.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS4.p9">
<p class="ltx_p" id="S2.SS1.SSS4.p9.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S2.SS1.SSS4.p9.1.m1.1"><semantics id="S2.SS1.SSS4.p9.1.m1.1a"><mo id="S2.SS1.SSS4.p9.1.m1.1.1" xref="S2.SS1.SSS4.p9.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS4.p9.1.m1.1b"><ci id="S2.SS1.SSS4.p9.1.m1.1.1.cmml" xref="S2.SS1.SSS4.p9.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS4.p9.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS4.p9.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS4.p9.1.1">Internal Knowledge</span>.
In addition to utilizing external tools, many agents rely solely on the internal knowledge of LLMs to guide their actions.
We now present several crucial capabilities of LLMs that can support the agent to behave reasonably and effectively.
(1) <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS4.p9.1.2">Planning Capability</span>.
Previous work has demonstrated that LLMs can be used as decent planners to decompose complex tasks into simpler ones <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib45" title="">45</a>]</cite>.
Such a capability of LLMs can be even triggered without incorporating examples in the prompts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib46" title="">46</a>]</cite>.
Building on the planning capability of LLMs, DEPS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib33" title="">33</a>]</cite> develops a Minecraft agent, which can solve complex tasks via sub-goal decomposition.
Similar agents like GITM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib16" title="">16</a>]</cite> and Voyager <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib38" title="">38</a>]</cite> also heavily rely on the planning capability of LLMs to successfully complete various tasks.
(2) <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS4.p9.1.3">Conversation Capability</span>.
LLMs can usually generate high-quality conversations.
This capability enables agents to behave more like humans.
In the previous work, many agents take actions based on the strong conversation capability of LLMs.
For example, in ChatDev <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib18" title="">18</a>]</cite>, different agents can discuss the software development process and reflect on their own behaviors.
In RLP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib30" title="">30</a>]</cite>, the agent can communicate with the listeners based on their potential feedback on the agent’s utterance.
(3) <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS4.p9.1.4">Common Sense Understanding Capability</span>.
Another important capability of LLMs is that they can well comprehend human common sense.
Based on this capability, many agents can simulate human daily life and make human-like decisions.
For example, in Generative Agent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib20" title="">20</a>]</cite>, the agent can accurately understand its current state, the surrounding environment, and summarize high-level ideas based on basic observations.
Without the common sense understanding capability of LLMs, these behaviors cannot be reliably simulated.
Similar conclusions may also apply to RecAgent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib21" title="">21</a>]</cite> and S3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib78" title="">78</a>]</cite>, where the agents focus on simulating user social behaviors.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS4.p10">
<p class="ltx_p" id="S2.SS1.SSS4.p10.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS4.p10.1.1">Action Impact</span>:
Action impact refers to the consequences of an agent’s actions. While the range of possible impacts is vast, we highlight a few key examples for clarity:
(1) <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS4.p10.1.2">Changing Environments.</span>
Agents can directly alter environment states by actions, such as moving their positions, collecting items, constructing buildings, etc.
For instance, in GITM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib16" title="">16</a>]</cite> and Voyager <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib38" title="">38</a>]</cite>, the environments are changed by the actions of the agents in their task completion process.
Specifically, when an agent collects resources—such as harvesting three pieces of wood—the resources disappear from the environment.
(2) <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS4.p10.1.3">Altering Internal States.</span>
Actions taken by the agent can also change the agent itself, including updating memories, forming new plans, acquiring novel knowledge, and more.
For example, in Generative Agents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib20" title="">20</a>]</cite>, memory streams are updated after performing actions within the system.
Similarly, SayCan <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib79" title="">79</a>]</cite> enables agents to take actions to update understandings of the environment.
(3) <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS4.p10.1.4">Triggering New Actions.</span>
In task completion processes, one action often leads to subsequent actions. For example, in Voyager <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib38" title="">38</a>]</cite>, once the agent has gathered the necessary resources, it triggers the construction of buildings.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>
For the profile module, we use ①, ② and ③ to represent the handcrafting method, LLM-generation method, and dataset alignment method, respectively.
For the memory module, we focus on the implementation strategies for memory operation and memory structure.
For memory operation, we use ① and ② to indicate that the model only has read/write operations and has read/write/reflection operations, respectively.
For memory structure, we use ① and ② to represent unified and hybrid memories, respectively.
For the planning module, we use ① and ② to represent planning w/o feedback and w/ feedback, respectively.
For the action module, we use ① and ② to represent that the model does not use tools and use tools, respectively.
For the agent <span class="ltx_text ltx_framed ltx_framed_underline" id="S2.T1.4.1">c</span>apability <span class="ltx_text ltx_framed ltx_framed_underline" id="S2.T1.5.2">a</span>cquisition (CA) strategy, we use ① and ② to represent the methods with and without fine-tuning, respectively.
“-” indicates that the corresponding content is not explicitly discussed in the paper.
</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S2.T1.1" style="width:499.9pt;height:559.1pt;vertical-align:-6.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-18.8pt,20.8pt) scale(0.93,0.93) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.2.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.2.1.1" rowspan="2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.2.1.1.1">
<span class="ltx_p" id="S2.T1.1.1.2.1.1.1.1" style="width:113.8pt;"><span class="ltx_text" id="S2.T1.1.1.2.1.1.1.1.1">Model</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.2.1.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.2.1.2.1">
<span class="ltx_p" id="S2.T1.1.1.2.1.2.1.1" style="width:51.2pt;"><span class="ltx_text" id="S2.T1.1.1.2.1.2.1.1.1">Profile</span></span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S2.T1.1.1.2.1.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">Memory</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.2.1.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.2.1.4.1">
<span class="ltx_p" id="S2.T1.1.1.2.1.4.1.1" style="width:42.7pt;"><span class="ltx_text" id="S2.T1.1.1.2.1.4.1.1.1">Planning</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.2.1.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.2.1.5.1">
<span class="ltx_p" id="S2.T1.1.1.2.1.5.1.1" style="width:34.1pt;"><span class="ltx_text" id="S2.T1.1.1.2.1.5.1.1.1">Action</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.2.1.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.2.1.6.1">
<span class="ltx_p" id="S2.T1.1.1.2.1.6.1.1" style="width:34.1pt;"><span class="ltx_text" id="S2.T1.1.1.2.1.6.1.1.1">CA</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.2.1.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.2.1.7.1">
<span class="ltx_p" id="S2.T1.1.1.2.1.7.1.1" style="width:42.7pt;"><span class="ltx_text" id="S2.T1.1.1.2.1.7.1.1.1">Time</span></span>
</span>
</th>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.3.2">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" id="S2.T1.1.1.3.2.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.3.2.1.1">
<span class="ltx_p" id="S2.T1.1.1.3.2.1.1.1" style="width:51.2pt;"></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S2.T1.1.1.3.2.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.3.2.2.1">
<span class="ltx_p" id="S2.T1.1.1.3.2.2.1.1" style="width:68.3pt;">Operation</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S2.T1.1.1.3.2.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.3.2.3.1">
<span class="ltx_p" id="S2.T1.1.1.3.2.3.1.1" style="width:51.2pt;">Structure</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" id="S2.T1.1.1.3.2.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.3.2.4.1">
<span class="ltx_p" id="S2.T1.1.1.3.2.4.1.1" style="width:42.7pt;"></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" id="S2.T1.1.1.3.2.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.3.2.5.1">
<span class="ltx_p" id="S2.T1.1.1.3.2.5.1.1" style="width:34.1pt;"></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" id="S2.T1.1.1.3.2.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.3.2.6.1">
<span class="ltx_p" id="S2.T1.1.1.3.2.6.1.1" style="width:34.1pt;"></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" id="S2.T1.1.1.3.2.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.3.2.7.1">
<span class="ltx_p" id="S2.T1.1.1.3.2.7.1.1" style="width:42.7pt;"></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.1.4.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.1.4.1.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.4.1.1.1">
<span class="ltx_p" id="S2.T1.1.1.4.1.1.1.1" style="width:113.8pt;">WebGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib67" title="">67</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.1.4.1.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.4.1.2.1">
<span class="ltx_p" id="S2.T1.1.1.4.1.2.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.1.4.1.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.4.1.3.1">
<span class="ltx_p" id="S2.T1.1.1.4.1.3.1.1" style="width:68.3pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.1.4.1.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.4.1.4.1">
<span class="ltx_p" id="S2.T1.1.1.4.1.4.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.1.4.1.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.4.1.5.1">
<span class="ltx_p" id="S2.T1.1.1.4.1.5.1.1" style="width:42.7pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.1.4.1.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.4.1.6.1">
<span class="ltx_p" id="S2.T1.1.1.4.1.6.1.1" style="width:34.1pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.1.4.1.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.4.1.7.1">
<span class="ltx_p" id="S2.T1.1.1.4.1.7.1.1" style="width:34.1pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.1.4.1.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.4.1.8.1">
<span class="ltx_p" id="S2.T1.1.1.4.1.8.1.1" style="width:42.7pt;">12/2021</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.5.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.5.2.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.5.2.1.1">
<span class="ltx_p" id="S2.T1.1.1.5.2.1.1.1" style="width:113.8pt;">SayCan <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib79" title="">79</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.5.2.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.5.2.2.1">
<span class="ltx_p" id="S2.T1.1.1.5.2.2.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.5.2.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.5.2.3.1">
<span class="ltx_p" id="S2.T1.1.1.5.2.3.1.1" style="width:68.3pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.5.2.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.5.2.4.1">
<span class="ltx_p" id="S2.T1.1.1.5.2.4.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.5.2.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.5.2.5.1">
<span class="ltx_p" id="S2.T1.1.1.5.2.5.1.1" style="width:42.7pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.5.2.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.5.2.6.1">
<span class="ltx_p" id="S2.T1.1.1.5.2.6.1.1" style="width:34.1pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.5.2.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.5.2.7.1">
<span class="ltx_p" id="S2.T1.1.1.5.2.7.1.1" style="width:34.1pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.5.2.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.5.2.8.1">
<span class="ltx_p" id="S2.T1.1.1.5.2.8.1.1" style="width:42.7pt;">04/2022</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.6.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.6.3.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.6.3.1.1">
<span class="ltx_p" id="S2.T1.1.1.6.3.1.1.1" style="width:113.8pt;">MRKL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib73" title="">73</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.6.3.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.6.3.2.1">
<span class="ltx_p" id="S2.T1.1.1.6.3.2.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.6.3.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.6.3.3.1">
<span class="ltx_p" id="S2.T1.1.1.6.3.3.1.1" style="width:68.3pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.6.3.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.6.3.4.1">
<span class="ltx_p" id="S2.T1.1.1.6.3.4.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.6.3.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.6.3.5.1">
<span class="ltx_p" id="S2.T1.1.1.6.3.5.1.1" style="width:42.7pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.6.3.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.6.3.6.1">
<span class="ltx_p" id="S2.T1.1.1.6.3.6.1.1" style="width:34.1pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.6.3.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.6.3.7.1">
<span class="ltx_p" id="S2.T1.1.1.6.3.7.1.1" style="width:34.1pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.6.3.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.6.3.8.1">
<span class="ltx_p" id="S2.T1.1.1.6.3.8.1.1" style="width:42.7pt;">05/2022</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.7.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.7.4.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.7.4.1.1">
<span class="ltx_p" id="S2.T1.1.1.7.4.1.1.1" style="width:113.8pt;">Inner Monologue <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib62" title="">62</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.7.4.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.7.4.2.1">
<span class="ltx_p" id="S2.T1.1.1.7.4.2.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.7.4.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.7.4.3.1">
<span class="ltx_p" id="S2.T1.1.1.7.4.3.1.1" style="width:68.3pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.7.4.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.7.4.4.1">
<span class="ltx_p" id="S2.T1.1.1.7.4.4.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.7.4.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.7.4.5.1">
<span class="ltx_p" id="S2.T1.1.1.7.4.5.1.1" style="width:42.7pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.7.4.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.7.4.6.1">
<span class="ltx_p" id="S2.T1.1.1.7.4.6.1.1" style="width:34.1pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.7.4.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.7.4.7.1">
<span class="ltx_p" id="S2.T1.1.1.7.4.7.1.1" style="width:34.1pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.7.4.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.7.4.8.1">
<span class="ltx_p" id="S2.T1.1.1.7.4.8.1.1" style="width:42.7pt;">07/2022</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.8.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.8.5.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.8.5.1.1">
<span class="ltx_p" id="S2.T1.1.1.8.5.1.1.1" style="width:113.8pt;">Social Simulacra <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib80" title="">80</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.8.5.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.8.5.2.1">
<span class="ltx_p" id="S2.T1.1.1.8.5.2.1.1" style="width:51.2pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.8.5.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.8.5.3.1">
<span class="ltx_p" id="S2.T1.1.1.8.5.3.1.1" style="width:68.3pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.8.5.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.8.5.4.1">
<span class="ltx_p" id="S2.T1.1.1.8.5.4.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.8.5.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.8.5.5.1">
<span class="ltx_p" id="S2.T1.1.1.8.5.5.1.1" style="width:42.7pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.8.5.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.8.5.6.1">
<span class="ltx_p" id="S2.T1.1.1.8.5.6.1.1" style="width:34.1pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.8.5.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.8.5.7.1">
<span class="ltx_p" id="S2.T1.1.1.8.5.7.1.1" style="width:34.1pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.8.5.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.8.5.8.1">
<span class="ltx_p" id="S2.T1.1.1.8.5.8.1.1" style="width:42.7pt;">08/2022</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.9.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.9.6.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.9.6.1.1">
<span class="ltx_p" id="S2.T1.1.1.9.6.1.1.1" style="width:113.8pt;">ReAct <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib60" title="">60</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.9.6.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.9.6.2.1">
<span class="ltx_p" id="S2.T1.1.1.9.6.2.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.9.6.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.9.6.3.1">
<span class="ltx_p" id="S2.T1.1.1.9.6.3.1.1" style="width:68.3pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.9.6.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.9.6.4.1">
<span class="ltx_p" id="S2.T1.1.1.9.6.4.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.9.6.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.9.6.5.1">
<span class="ltx_p" id="S2.T1.1.1.9.6.5.1.1" style="width:42.7pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.9.6.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.9.6.6.1">
<span class="ltx_p" id="S2.T1.1.1.9.6.6.1.1" style="width:34.1pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.9.6.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.9.6.7.1">
<span class="ltx_p" id="S2.T1.1.1.9.6.7.1.1" style="width:34.1pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.9.6.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.9.6.8.1">
<span class="ltx_p" id="S2.T1.1.1.9.6.8.1.1" style="width:42.7pt;">10/2022</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.10.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.10.7.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.10.7.1.1">
<span class="ltx_p" id="S2.T1.1.1.10.7.1.1.1" style="width:113.8pt;">MALLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib42" title="">42</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.10.7.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.10.7.2.1">
<span class="ltx_p" id="S2.T1.1.1.10.7.2.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.10.7.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.10.7.3.1">
<span class="ltx_p" id="S2.T1.1.1.10.7.3.1.1" style="width:68.3pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.10.7.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.10.7.4.1">
<span class="ltx_p" id="S2.T1.1.1.10.7.4.1.1" style="width:51.2pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.10.7.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.10.7.5.1">
<span class="ltx_p" id="S2.T1.1.1.10.7.5.1.1" style="width:42.7pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.10.7.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.10.7.6.1">
<span class="ltx_p" id="S2.T1.1.1.10.7.6.1.1" style="width:34.1pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.10.7.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.10.7.7.1">
<span class="ltx_p" id="S2.T1.1.1.10.7.7.1.1" style="width:34.1pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.10.7.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.10.7.8.1">
<span class="ltx_p" id="S2.T1.1.1.10.7.8.1.1" style="width:42.7pt;">01/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.11.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.11.8.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.11.8.1.1">
<span class="ltx_p" id="S2.T1.1.1.11.8.1.1.1" style="width:113.8pt;">DEPS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib33" title="">33</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.11.8.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.11.8.2.1">
<span class="ltx_p" id="S2.T1.1.1.11.8.2.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.11.8.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.11.8.3.1">
<span class="ltx_p" id="S2.T1.1.1.11.8.3.1.1" style="width:68.3pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.11.8.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.11.8.4.1">
<span class="ltx_p" id="S2.T1.1.1.11.8.4.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.11.8.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.11.8.5.1">
<span class="ltx_p" id="S2.T1.1.1.11.8.5.1.1" style="width:42.7pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.11.8.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.11.8.6.1">
<span class="ltx_p" id="S2.T1.1.1.11.8.6.1.1" style="width:34.1pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.11.8.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.11.8.7.1">
<span class="ltx_p" id="S2.T1.1.1.11.8.7.1.1" style="width:34.1pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.11.8.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.11.8.8.1">
<span class="ltx_p" id="S2.T1.1.1.11.8.8.1.1" style="width:42.7pt;">02/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.12.9">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.12.9.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.12.9.1.1">
<span class="ltx_p" id="S2.T1.1.1.12.9.1.1.1" style="width:113.8pt;">Toolformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib15" title="">15</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.12.9.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.12.9.2.1">
<span class="ltx_p" id="S2.T1.1.1.12.9.2.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.12.9.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.12.9.3.1">
<span class="ltx_p" id="S2.T1.1.1.12.9.3.1.1" style="width:68.3pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.12.9.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.12.9.4.1">
<span class="ltx_p" id="S2.T1.1.1.12.9.4.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.12.9.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.12.9.5.1">
<span class="ltx_p" id="S2.T1.1.1.12.9.5.1.1" style="width:42.7pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.12.9.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.12.9.6.1">
<span class="ltx_p" id="S2.T1.1.1.12.9.6.1.1" style="width:34.1pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.12.9.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.12.9.7.1">
<span class="ltx_p" id="S2.T1.1.1.12.9.7.1.1" style="width:34.1pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.12.9.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.12.9.8.1">
<span class="ltx_p" id="S2.T1.1.1.12.9.8.1.1" style="width:42.7pt;">02/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.13.10">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.13.10.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.13.10.1.1">
<span class="ltx_p" id="S2.T1.1.1.13.10.1.1.1" style="width:113.8pt;">Reflexion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib12" title="">12</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.13.10.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.13.10.2.1">
<span class="ltx_p" id="S2.T1.1.1.13.10.2.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.13.10.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.13.10.3.1">
<span class="ltx_p" id="S2.T1.1.1.13.10.3.1.1" style="width:68.3pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.13.10.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.13.10.4.1">
<span class="ltx_p" id="S2.T1.1.1.13.10.4.1.1" style="width:51.2pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.13.10.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.13.10.5.1">
<span class="ltx_p" id="S2.T1.1.1.13.10.5.1.1" style="width:42.7pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.13.10.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.13.10.6.1">
<span class="ltx_p" id="S2.T1.1.1.13.10.6.1.1" style="width:34.1pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.13.10.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.13.10.7.1">
<span class="ltx_p" id="S2.T1.1.1.13.10.7.1.1" style="width:34.1pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.13.10.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.13.10.8.1">
<span class="ltx_p" id="S2.T1.1.1.13.10.8.1.1" style="width:42.7pt;">03/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.14.11">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.14.11.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.14.11.1.1">
<span class="ltx_p" id="S2.T1.1.1.14.11.1.1.1" style="width:113.8pt;">CAMEL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib81" title="">81</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.14.11.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.14.11.2.1">
<span class="ltx_p" id="S2.T1.1.1.14.11.2.1.1" style="width:51.2pt;">① ②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.14.11.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.14.11.3.1">
<span class="ltx_p" id="S2.T1.1.1.14.11.3.1.1" style="width:68.3pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.14.11.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.14.11.4.1">
<span class="ltx_p" id="S2.T1.1.1.14.11.4.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.14.11.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.14.11.5.1">
<span class="ltx_p" id="S2.T1.1.1.14.11.5.1.1" style="width:42.7pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.14.11.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.14.11.6.1">
<span class="ltx_p" id="S2.T1.1.1.14.11.6.1.1" style="width:34.1pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.14.11.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.14.11.7.1">
<span class="ltx_p" id="S2.T1.1.1.14.11.7.1.1" style="width:34.1pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.14.11.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.14.11.8.1">
<span class="ltx_p" id="S2.T1.1.1.14.11.8.1.1" style="width:42.7pt;">03/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.15.12">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.15.12.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.15.12.1.1">
<span class="ltx_p" id="S2.T1.1.1.15.12.1.1.1" style="width:113.8pt;">API-Bank <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib70" title="">70</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.15.12.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.15.12.2.1">
<span class="ltx_p" id="S2.T1.1.1.15.12.2.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.15.12.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.15.12.3.1">
<span class="ltx_p" id="S2.T1.1.1.15.12.3.1.1" style="width:68.3pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.15.12.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.15.12.4.1">
<span class="ltx_p" id="S2.T1.1.1.15.12.4.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.15.12.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.15.12.5.1">
<span class="ltx_p" id="S2.T1.1.1.15.12.5.1.1" style="width:42.7pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.15.12.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.15.12.6.1">
<span class="ltx_p" id="S2.T1.1.1.15.12.6.1.1" style="width:34.1pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.15.12.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.15.12.7.1">
<span class="ltx_p" id="S2.T1.1.1.15.12.7.1.1" style="width:34.1pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.15.12.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.15.12.8.1">
<span class="ltx_p" id="S2.T1.1.1.15.12.8.1.1" style="width:42.7pt;">04/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.16.13">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.16.13.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.16.13.1.1">
<span class="ltx_p" id="S2.T1.1.1.16.13.1.1.1" style="width:113.8pt;">ViperGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib75" title="">75</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.16.13.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.16.13.2.1">
<span class="ltx_p" id="S2.T1.1.1.16.13.2.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.16.13.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.16.13.3.1">
<span class="ltx_p" id="S2.T1.1.1.16.13.3.1.1" style="width:68.3pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.16.13.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.16.13.4.1">
<span class="ltx_p" id="S2.T1.1.1.16.13.4.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.16.13.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.16.13.5.1">
<span class="ltx_p" id="S2.T1.1.1.16.13.5.1.1" style="width:42.7pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.16.13.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.16.13.6.1">
<span class="ltx_p" id="S2.T1.1.1.16.13.6.1.1" style="width:34.1pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.16.13.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.16.13.7.1">
<span class="ltx_p" id="S2.T1.1.1.16.13.7.1.1" style="width:34.1pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.16.13.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.16.13.8.1">
<span class="ltx_p" id="S2.T1.1.1.16.13.8.1.1" style="width:42.7pt;">03/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.17.14">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.17.14.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.17.14.1.1">
<span class="ltx_p" id="S2.T1.1.1.17.14.1.1.1" style="width:113.8pt;">HuggingGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib13" title="">13</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.17.14.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.17.14.2.1">
<span class="ltx_p" id="S2.T1.1.1.17.14.2.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.17.14.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.17.14.3.1">
<span class="ltx_p" id="S2.T1.1.1.17.14.3.1.1" style="width:68.3pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.17.14.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.17.14.4.1">
<span class="ltx_p" id="S2.T1.1.1.17.14.4.1.1" style="width:51.2pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.17.14.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.17.14.5.1">
<span class="ltx_p" id="S2.T1.1.1.17.14.5.1.1" style="width:42.7pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.17.14.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.17.14.6.1">
<span class="ltx_p" id="S2.T1.1.1.17.14.6.1.1" style="width:34.1pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.17.14.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.17.14.7.1">
<span class="ltx_p" id="S2.T1.1.1.17.14.7.1.1" style="width:34.1pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.17.14.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.17.14.8.1">
<span class="ltx_p" id="S2.T1.1.1.17.14.8.1.1" style="width:42.7pt;">03/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.18.15">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.18.15.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.18.15.1.1">
<span class="ltx_p" id="S2.T1.1.1.18.15.1.1.1" style="width:113.8pt;">Generative Agents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib20" title="">20</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.18.15.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.18.15.2.1">
<span class="ltx_p" id="S2.T1.1.1.18.15.2.1.1" style="width:51.2pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.18.15.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.18.15.3.1">
<span class="ltx_p" id="S2.T1.1.1.18.15.3.1.1" style="width:68.3pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.18.15.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.18.15.4.1">
<span class="ltx_p" id="S2.T1.1.1.18.15.4.1.1" style="width:51.2pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.18.15.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.18.15.5.1">
<span class="ltx_p" id="S2.T1.1.1.18.15.5.1.1" style="width:42.7pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.18.15.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.18.15.6.1">
<span class="ltx_p" id="S2.T1.1.1.18.15.6.1.1" style="width:34.1pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.18.15.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.18.15.7.1">
<span class="ltx_p" id="S2.T1.1.1.18.15.7.1.1" style="width:34.1pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.18.15.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.18.15.8.1">
<span class="ltx_p" id="S2.T1.1.1.18.15.8.1.1" style="width:42.7pt;">04/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.19.16">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.19.16.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.19.16.1.1">
<span class="ltx_p" id="S2.T1.1.1.19.16.1.1.1" style="width:113.8pt;">LLM+P <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib58" title="">58</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.19.16.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.19.16.2.1">
<span class="ltx_p" id="S2.T1.1.1.19.16.2.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.19.16.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.19.16.3.1">
<span class="ltx_p" id="S2.T1.1.1.19.16.3.1.1" style="width:68.3pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.19.16.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.19.16.4.1">
<span class="ltx_p" id="S2.T1.1.1.19.16.4.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.19.16.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.19.16.5.1">
<span class="ltx_p" id="S2.T1.1.1.19.16.5.1.1" style="width:42.7pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.19.16.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.19.16.6.1">
<span class="ltx_p" id="S2.T1.1.1.19.16.6.1.1" style="width:34.1pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.19.16.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.19.16.7.1">
<span class="ltx_p" id="S2.T1.1.1.19.16.7.1.1" style="width:34.1pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.19.16.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.19.16.8.1">
<span class="ltx_p" id="S2.T1.1.1.19.16.8.1.1" style="width:42.7pt;">04/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.20.17">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.20.17.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.20.17.1.1">
<span class="ltx_p" id="S2.T1.1.1.20.17.1.1.1" style="width:113.8pt;">ChemCrow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib76" title="">76</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.20.17.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.20.17.2.1">
<span class="ltx_p" id="S2.T1.1.1.20.17.2.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.20.17.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.20.17.3.1">
<span class="ltx_p" id="S2.T1.1.1.20.17.3.1.1" style="width:68.3pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.20.17.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.20.17.4.1">
<span class="ltx_p" id="S2.T1.1.1.20.17.4.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.20.17.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.20.17.5.1">
<span class="ltx_p" id="S2.T1.1.1.20.17.5.1.1" style="width:42.7pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.20.17.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.20.17.6.1">
<span class="ltx_p" id="S2.T1.1.1.20.17.6.1.1" style="width:34.1pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.20.17.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.20.17.7.1">
<span class="ltx_p" id="S2.T1.1.1.20.17.7.1.1" style="width:34.1pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.20.17.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.20.17.8.1">
<span class="ltx_p" id="S2.T1.1.1.20.17.8.1.1" style="width:42.7pt;">04/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.21.18">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.21.18.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.21.18.1.1">
<span class="ltx_p" id="S2.T1.1.1.21.18.1.1.1" style="width:113.8pt;">OpenAGI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib74" title="">74</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.21.18.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.21.18.2.1">
<span class="ltx_p" id="S2.T1.1.1.21.18.2.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.21.18.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.21.18.3.1">
<span class="ltx_p" id="S2.T1.1.1.21.18.3.1.1" style="width:68.3pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.21.18.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.21.18.4.1">
<span class="ltx_p" id="S2.T1.1.1.21.18.4.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.21.18.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.21.18.5.1">
<span class="ltx_p" id="S2.T1.1.1.21.18.5.1.1" style="width:42.7pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.21.18.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.21.18.6.1">
<span class="ltx_p" id="S2.T1.1.1.21.18.6.1.1" style="width:34.1pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.21.18.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.21.18.7.1">
<span class="ltx_p" id="S2.T1.1.1.21.18.7.1.1" style="width:34.1pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.21.18.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.21.18.8.1">
<span class="ltx_p" id="S2.T1.1.1.21.18.8.1.1" style="width:42.7pt;">04/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.22.19">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.22.19.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.22.19.1.1">
<span class="ltx_p" id="S2.T1.1.1.22.19.1.1.1" style="width:113.8pt;">AutoGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib82" title="">82</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.22.19.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.22.19.2.1">
<span class="ltx_p" id="S2.T1.1.1.22.19.2.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.22.19.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.22.19.3.1">
<span class="ltx_p" id="S2.T1.1.1.22.19.3.1.1" style="width:68.3pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.22.19.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.22.19.4.1">
<span class="ltx_p" id="S2.T1.1.1.22.19.4.1.1" style="width:51.2pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.22.19.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.22.19.5.1">
<span class="ltx_p" id="S2.T1.1.1.22.19.5.1.1" style="width:42.7pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.22.19.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.22.19.6.1">
<span class="ltx_p" id="S2.T1.1.1.22.19.6.1.1" style="width:34.1pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.22.19.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.22.19.7.1">
<span class="ltx_p" id="S2.T1.1.1.22.19.7.1.1" style="width:34.1pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.22.19.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.22.19.8.1">
<span class="ltx_p" id="S2.T1.1.1.22.19.8.1.1" style="width:42.7pt;">04/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.23.20">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.23.20.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.23.20.1.1">
<span class="ltx_p" id="S2.T1.1.1.23.20.1.1.1" style="width:113.8pt;">SCM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib35" title="">35</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.23.20.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.23.20.2.1">
<span class="ltx_p" id="S2.T1.1.1.23.20.2.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.23.20.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.23.20.3.1">
<span class="ltx_p" id="S2.T1.1.1.23.20.3.1.1" style="width:68.3pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.23.20.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.23.20.4.1">
<span class="ltx_p" id="S2.T1.1.1.23.20.4.1.1" style="width:51.2pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.23.20.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.23.20.5.1">
<span class="ltx_p" id="S2.T1.1.1.23.20.5.1.1" style="width:42.7pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.23.20.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.23.20.6.1">
<span class="ltx_p" id="S2.T1.1.1.23.20.6.1.1" style="width:34.1pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.23.20.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.23.20.7.1">
<span class="ltx_p" id="S2.T1.1.1.23.20.7.1.1" style="width:34.1pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.23.20.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.23.20.8.1">
<span class="ltx_p" id="S2.T1.1.1.23.20.8.1.1" style="width:42.7pt;">04/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.24.21">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.24.21.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.24.21.1.1">
<span class="ltx_p" id="S2.T1.1.1.24.21.1.1.1" style="width:113.8pt;">Socially Alignment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib83" title="">83</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.24.21.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.24.21.2.1">
<span class="ltx_p" id="S2.T1.1.1.24.21.2.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.24.21.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.24.21.3.1">
<span class="ltx_p" id="S2.T1.1.1.24.21.3.1.1" style="width:68.3pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.24.21.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.24.21.4.1">
<span class="ltx_p" id="S2.T1.1.1.24.21.4.1.1" style="width:51.2pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.24.21.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.24.21.5.1">
<span class="ltx_p" id="S2.T1.1.1.24.21.5.1.1" style="width:42.7pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.24.21.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.24.21.6.1">
<span class="ltx_p" id="S2.T1.1.1.24.21.6.1.1" style="width:34.1pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.24.21.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.24.21.7.1">
<span class="ltx_p" id="S2.T1.1.1.24.21.7.1.1" style="width:34.1pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.24.21.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.24.21.8.1">
<span class="ltx_p" id="S2.T1.1.1.24.21.8.1.1" style="width:42.7pt;">05/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.25.22">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.25.22.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.25.22.1.1">
<span class="ltx_p" id="S2.T1.1.1.25.22.1.1.1" style="width:113.8pt;">GITM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib16" title="">16</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.25.22.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.25.22.2.1">
<span class="ltx_p" id="S2.T1.1.1.25.22.2.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.25.22.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.25.22.3.1">
<span class="ltx_p" id="S2.T1.1.1.25.22.3.1.1" style="width:68.3pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.25.22.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.25.22.4.1">
<span class="ltx_p" id="S2.T1.1.1.25.22.4.1.1" style="width:51.2pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.25.22.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.25.22.5.1">
<span class="ltx_p" id="S2.T1.1.1.25.22.5.1.1" style="width:42.7pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.25.22.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.25.22.6.1">
<span class="ltx_p" id="S2.T1.1.1.25.22.6.1.1" style="width:34.1pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.25.22.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.25.22.7.1">
<span class="ltx_p" id="S2.T1.1.1.25.22.7.1.1" style="width:34.1pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.25.22.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.25.22.8.1">
<span class="ltx_p" id="S2.T1.1.1.25.22.8.1.1" style="width:42.7pt;">05/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.26.23">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.26.23.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.26.23.1.1">
<span class="ltx_p" id="S2.T1.1.1.26.23.1.1.1" style="width:113.8pt;">Voyager <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib38" title="">38</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.26.23.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.26.23.2.1">
<span class="ltx_p" id="S2.T1.1.1.26.23.2.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.26.23.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.26.23.3.1">
<span class="ltx_p" id="S2.T1.1.1.26.23.3.1.1" style="width:68.3pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.26.23.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.26.23.4.1">
<span class="ltx_p" id="S2.T1.1.1.26.23.4.1.1" style="width:51.2pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.26.23.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.26.23.5.1">
<span class="ltx_p" id="S2.T1.1.1.26.23.5.1.1" style="width:42.7pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.26.23.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.26.23.6.1">
<span class="ltx_p" id="S2.T1.1.1.26.23.6.1.1" style="width:34.1pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.26.23.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.26.23.7.1">
<span class="ltx_p" id="S2.T1.1.1.26.23.7.1.1" style="width:34.1pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.26.23.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.26.23.8.1">
<span class="ltx_p" id="S2.T1.1.1.26.23.8.1.1" style="width:42.7pt;">05/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.27.24">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.27.24.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.27.24.1.1">
<span class="ltx_p" id="S2.T1.1.1.27.24.1.1.1" style="width:113.8pt;">Introspective Tips <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib84" title="">84</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.27.24.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.27.24.2.1">
<span class="ltx_p" id="S2.T1.1.1.27.24.2.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.27.24.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.27.24.3.1">
<span class="ltx_p" id="S2.T1.1.1.27.24.3.1.1" style="width:68.3pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.27.24.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.27.24.4.1">
<span class="ltx_p" id="S2.T1.1.1.27.24.4.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.27.24.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.27.24.5.1">
<span class="ltx_p" id="S2.T1.1.1.27.24.5.1.1" style="width:42.7pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.27.24.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.27.24.6.1">
<span class="ltx_p" id="S2.T1.1.1.27.24.6.1.1" style="width:34.1pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.27.24.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.27.24.7.1">
<span class="ltx_p" id="S2.T1.1.1.27.24.7.1.1" style="width:34.1pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.27.24.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.27.24.8.1">
<span class="ltx_p" id="S2.T1.1.1.27.24.8.1.1" style="width:42.7pt;">05/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.28.25">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.28.25.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.28.25.1.1">
<span class="ltx_p" id="S2.T1.1.1.28.25.1.1.1" style="width:113.8pt;">RET-LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib41" title="">41</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.28.25.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.28.25.2.1">
<span class="ltx_p" id="S2.T1.1.1.28.25.2.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.28.25.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.28.25.3.1">
<span class="ltx_p" id="S2.T1.1.1.28.25.3.1.1" style="width:68.3pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.28.25.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.28.25.4.1">
<span class="ltx_p" id="S2.T1.1.1.28.25.4.1.1" style="width:51.2pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.28.25.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.28.25.5.1">
<span class="ltx_p" id="S2.T1.1.1.28.25.5.1.1" style="width:42.7pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.28.25.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.28.25.6.1">
<span class="ltx_p" id="S2.T1.1.1.28.25.6.1.1" style="width:34.1pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.28.25.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.28.25.7.1">
<span class="ltx_p" id="S2.T1.1.1.28.25.7.1.1" style="width:34.1pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.28.25.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.28.25.8.1">
<span class="ltx_p" id="S2.T1.1.1.28.25.8.1.1" style="width:42.7pt;">05/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.29.26">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.29.26.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.29.26.1.1">
<span class="ltx_p" id="S2.T1.1.1.29.26.1.1.1" style="width:113.8pt;">ChatDB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib40" title="">40</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.29.26.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.29.26.2.1">
<span class="ltx_p" id="S2.T1.1.1.29.26.2.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.29.26.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.29.26.3.1">
<span class="ltx_p" id="S2.T1.1.1.29.26.3.1.1" style="width:68.3pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.29.26.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.29.26.4.1">
<span class="ltx_p" id="S2.T1.1.1.29.26.4.1.1" style="width:51.2pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.29.26.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.29.26.5.1">
<span class="ltx_p" id="S2.T1.1.1.29.26.5.1.1" style="width:42.7pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.29.26.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.29.26.6.1">
<span class="ltx_p" id="S2.T1.1.1.29.26.6.1.1" style="width:34.1pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.29.26.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.29.26.7.1">
<span class="ltx_p" id="S2.T1.1.1.29.26.7.1.1" style="width:34.1pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.29.26.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.29.26.8.1">
<span class="ltx_p" id="S2.T1.1.1.29.26.8.1.1" style="width:42.7pt;">06/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.1.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.1.1">
<span class="ltx_p" id="S2.T1.1.1.1.1.1.1" style="width:113.8pt;"><math alttext="S^{3}" class="ltx_Math" display="inline" id="S2.T1.1.1.1.1.1.1.m1.1"><semantics id="S2.T1.1.1.1.1.1.1.m1.1a"><msup id="S2.T1.1.1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S2.T1.1.1.1.1.1.1.m1.1.1.2" xref="S2.T1.1.1.1.1.1.1.m1.1.1.2.cmml">S</mi><mn id="S2.T1.1.1.1.1.1.1.m1.1.1.3" xref="S2.T1.1.1.1.1.1.1.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.1.1.m1.1b"><apply id="S2.T1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.T1.1.1.1.1.1.1.m1.1.1">superscript</csymbol><ci id="S2.T1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S2.T1.1.1.1.1.1.1.m1.1.1.2">𝑆</ci><cn id="S2.T1.1.1.1.1.1.1.m1.1.1.3.cmml" type="integer" xref="S2.T1.1.1.1.1.1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.1.1.m1.1c">S^{3}</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.1.1.m1.1d">italic_S start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib78" title="">78</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.1.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.2.1">
<span class="ltx_p" id="S2.T1.1.1.1.2.1.1" style="width:51.2pt;">③</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.1.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.3.1">
<span class="ltx_p" id="S2.T1.1.1.1.3.1.1" style="width:68.3pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.1.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.4.1">
<span class="ltx_p" id="S2.T1.1.1.1.4.1.1" style="width:51.2pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.1.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.5.1">
<span class="ltx_p" id="S2.T1.1.1.1.5.1.1" style="width:42.7pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.1.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.6.1">
<span class="ltx_p" id="S2.T1.1.1.1.6.1.1" style="width:34.1pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.1.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.7.1">
<span class="ltx_p" id="S2.T1.1.1.1.7.1.1" style="width:34.1pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.1.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.8.1">
<span class="ltx_p" id="S2.T1.1.1.1.8.1.1" style="width:42.7pt;">07/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.30.27">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.30.27.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.30.27.1.1">
<span class="ltx_p" id="S2.T1.1.1.30.27.1.1.1" style="width:113.8pt;">ChatDev <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib18" title="">18</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.30.27.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.30.27.2.1">
<span class="ltx_p" id="S2.T1.1.1.30.27.2.1.1" style="width:51.2pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.30.27.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.30.27.3.1">
<span class="ltx_p" id="S2.T1.1.1.30.27.3.1.1" style="width:68.3pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.30.27.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.30.27.4.1">
<span class="ltx_p" id="S2.T1.1.1.30.27.4.1.1" style="width:51.2pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.30.27.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.30.27.5.1">
<span class="ltx_p" id="S2.T1.1.1.30.27.5.1.1" style="width:42.7pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.30.27.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.30.27.6.1">
<span class="ltx_p" id="S2.T1.1.1.30.27.6.1.1" style="width:34.1pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.30.27.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.30.27.7.1">
<span class="ltx_p" id="S2.T1.1.1.30.27.7.1.1" style="width:34.1pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.30.27.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.30.27.8.1">
<span class="ltx_p" id="S2.T1.1.1.30.27.8.1.1" style="width:42.7pt;">07/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.31.28">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.31.28.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.31.28.1.1">
<span class="ltx_p" id="S2.T1.1.1.31.28.1.1.1" style="width:113.8pt;">ToolLLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib14" title="">14</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.31.28.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.31.28.2.1">
<span class="ltx_p" id="S2.T1.1.1.31.28.2.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.31.28.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.31.28.3.1">
<span class="ltx_p" id="S2.T1.1.1.31.28.3.1.1" style="width:68.3pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.31.28.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.31.28.4.1">
<span class="ltx_p" id="S2.T1.1.1.31.28.4.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.31.28.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.31.28.5.1">
<span class="ltx_p" id="S2.T1.1.1.31.28.5.1.1" style="width:42.7pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.31.28.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.31.28.6.1">
<span class="ltx_p" id="S2.T1.1.1.31.28.6.1.1" style="width:34.1pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.31.28.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.31.28.7.1">
<span class="ltx_p" id="S2.T1.1.1.31.28.7.1.1" style="width:34.1pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.31.28.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.31.28.8.1">
<span class="ltx_p" id="S2.T1.1.1.31.28.8.1.1" style="width:42.7pt;">07/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.32.29">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.32.29.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.32.29.1.1">
<span class="ltx_p" id="S2.T1.1.1.32.29.1.1.1" style="width:113.8pt;">MemoryBank <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib39" title="">39</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.32.29.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.32.29.2.1">
<span class="ltx_p" id="S2.T1.1.1.32.29.2.1.1" style="width:51.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.32.29.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.32.29.3.1">
<span class="ltx_p" id="S2.T1.1.1.32.29.3.1.1" style="width:68.3pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.32.29.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.32.29.4.1">
<span class="ltx_p" id="S2.T1.1.1.32.29.4.1.1" style="width:51.2pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.32.29.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.32.29.5.1">
<span class="ltx_p" id="S2.T1.1.1.32.29.5.1.1" style="width:42.7pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.32.29.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.32.29.6.1">
<span class="ltx_p" id="S2.T1.1.1.32.29.6.1.1" style="width:34.1pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.32.29.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.32.29.7.1">
<span class="ltx_p" id="S2.T1.1.1.32.29.7.1.1" style="width:34.1pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.32.29.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.32.29.8.1">
<span class="ltx_p" id="S2.T1.1.1.32.29.8.1.1" style="width:42.7pt;">07/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.33.30">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S2.T1.1.1.33.30.1" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.33.30.1.1">
<span class="ltx_p" id="S2.T1.1.1.33.30.1.1.1" style="width:113.8pt;">MetaGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib23" title="">23</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S2.T1.1.1.33.30.2" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.33.30.2.1">
<span class="ltx_p" id="S2.T1.1.1.33.30.2.1.1" style="width:51.2pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S2.T1.1.1.33.30.3" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.33.30.3.1">
<span class="ltx_p" id="S2.T1.1.1.33.30.3.1.1" style="width:68.3pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S2.T1.1.1.33.30.4" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.33.30.4.1">
<span class="ltx_p" id="S2.T1.1.1.33.30.4.1.1" style="width:51.2pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S2.T1.1.1.33.30.5" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.33.30.5.1">
<span class="ltx_p" id="S2.T1.1.1.33.30.5.1.1" style="width:42.7pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S2.T1.1.1.33.30.6" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.33.30.6.1">
<span class="ltx_p" id="S2.T1.1.1.33.30.6.1.1" style="width:34.1pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S2.T1.1.1.33.30.7" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.33.30.7.1">
<span class="ltx_p" id="S2.T1.1.1.33.30.7.1.1" style="width:34.1pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S2.T1.1.1.33.30.8" style="padding-top:-0.75pt;padding-bottom:-0.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.33.30.8.1">
<span class="ltx_p" id="S2.T1.1.1.33.30.8.1.1" style="width:42.7pt;">08/2023</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Agent Capability Acquisition</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">In the sections above, we focus mainly on how to design the agent architecture to better harness the capabilities of LLMs to enabling them to accomplish tasks akin to human performance.
The architecture functions as the “hardware” of an agent.
However, relying solely on the hardware is insufficient for achieving effective task performance. This is because the agent may lack the necessary task-specific capabilities, skills, and experiences, which can be regarded as "software" resources. In order to equip the agent with these resources, various strategies have been devised.
Generally, we categorize these strategies into two classes based on whether they require fine-tuning of the LLMs.
Below, we introduce each category in detail.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p2.1.1">Capability Acquisition with Fine-tuning</span>:
A direct approach to enhance agent capabilities for task completion is to fine-tune the model using task-specific datasets. These datasets can be constructed from human annotations, LLM-generated content, or real-world applications. We discuss these methods in detail below.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S2.SS2.p3.1.m1.1"><semantics id="S2.SS2.p3.1.m1.1a"><mo id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><ci id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S2.SS2.p3.1.1">Fine-tuning with Human Annotated Datasets</span>.
To fine-tune the agent, utilizing human annotated datasets is a versatile approach that can be employed in various application scenarios.
In this approach, researchers first design annotation tasks and then recruit workers to complete them.
For example, in CoH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib85" title="">85</a>]</cite>, the authors aim to align LLMs with human values and preferences. Different from the other models, where the human feedback is leveraged in a simple and symbolic manner, this method converts the human feedback into detailed comparison information in the form of natural languages. The LLMs are directly fine-tuned based on these natural language datasets.
In RET-LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib41" title="">41</a>]</cite>, in order to better convert natural languages into structured memory information, the authors fine-tune LLMs based on a human constructed dataset, where each sample is a “triplet-natural language” pair.
In WebShop <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib86" title="">86</a>]</cite>, the authors collect 1.18 million real-world products from amazon.com, and put them onto a simulated e-commerce website, which contains several carefully designed human shopping scenarios. Based on this website, the authors recruit 13 workers to collect a real-human behavior dataset. At last, three methods based on heuristic rules, imitation learning and reinforcement learning are trained based on this dataset. Although the authors do not fine-tune LLM-based agents, we believe that the dataset proposed in this paper holds immense potential to enhance the capabilities of agents in the field of web shopping.
In EduChat <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib87" title="">87</a>]</cite>, the authors aim to enhance the educational functions of LLMs, such as open-domain question answering, essay assessment, Socratic teaching, and emotional support. They fine-tune LLMs based on human annotated datasets that cover various educational scenarios and tasks.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S2.SS2.p4.1.m1.1"><semantics id="S2.SS2.p4.1.m1.1a"><mo id="S2.SS2.p4.1.m1.1.1" xref="S2.SS2.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.1.m1.1b"><ci id="S2.SS2.p4.1.m1.1.1.cmml" xref="S2.SS2.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p4.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S2.SS2.p4.1.1">Fine-tuning with LLM Generated Datasets</span>.
Building human-annotated datasets typically requires recruiting people, which can be costly, especially when dealing with large-scale annotation tasks.
Considering that LLMs can achieve human-like capabilities in a wide range of tasks, a natural idea is using LLMs to accomplish the annotation task.
While the datasets produced from this method can be not as perfect as the human annotated ones, it is much cheaper, and can be leveraged to generate more samples.
For example, in ToolBench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib14" title="">14</a>]</cite>, to enhance the tool-using capability of open-source LLMs, the authors collect 16,464 real-world APIs spanning 49 categories from the RapidAPI Hub. They used these APIs to prompt ChatGPT to generate diverse instructions, covering both single-tool and multi-tool scenarios. Based on the obtained dataset, the authors fine-tune LLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib9" title="">9</a>]</cite>, and obtain significant performance improvement in terms of tool using.
In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib83" title="">83</a>]</cite>, to empower the agent with social capability, the authors design a sandbox, and deploy multiple agents to interact with each other.
Given a social question, the central agent first generates initial responses.
Then, it shares the responses to its nearby agents for collecting their feedback.
Based on the feedback as well as its detailed explanations, the central agent revise its initial responses to make them more consistent with social norms.
In this process, the authors collect a large amount of agent social interaction data, which is then leveraged to fine-tune the LLMs.</p>
</div>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S2.SS2.p5.1.m1.1"><semantics id="S2.SS2.p5.1.m1.1a"><mo id="S2.SS2.p5.1.m1.1.1" xref="S2.SS2.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.1.m1.1b"><ci id="S2.SS2.p5.1.m1.1.1.cmml" xref="S2.SS2.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p5.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S2.SS2.p5.1.1">Fine-tuning with Real-world Datasets</span>.
In addition to building datasets based on human or LLM annotations, directly using real-world datasets to fine-tune the agent is also a common strategy.
For example, in MIND2WEB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib88" title="">88</a>]</cite>, the authors collect a large amount of real-world datasets to enhance the agent capability in the web domain.
In contrast to prior studies, the dataset presented in this paper encompasses diverse tasks, real-world scenarios, and comprehensive user interaction patterns.
Specifically, the authors collect over 2,000 open-ended tasks from 137 real-world websites spanning 31 domains.
Using this dataset, the authors fine-tune LLMs to enhance their performance on web-related tasks such as movie discovery and ticket booking.
Similarly, in SQL-PaLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib89" title="">89</a>]</cite>, researchers fine-tune PaLM-2 using cross-domain, large-scale text-to-SQL datasets, including Spider and BIRD. The resulting model achieves notable performance improvements on text-to-SQL tasks, demonstrating the effectiveness of real-world datasets for domain-specific applications.</p>
</div>
<figure class="ltx_figure" id="S2.F4">
<p class="ltx_p ltx_align_center" id="S2.F4.1"><span class="ltx_text ltx_framed ltx_framed_rectangle" id="S2.F4.1.1" style="border-color: #000000;padding:0.0pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="279" id="S2.F4.1.1.g1" src="x4.png" width="830"/>
</span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Illustration of transitions in strategies for acquiring model capabilities.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p6">
<p class="ltx_p" id="S2.SS2.p6.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p6.1.1">Capability Acquisition without Fine-tuning</span>:
In the era of tradition machine learning, the model capability is mainly acquired by learning from datasets, where the knowledge is encoded into the model parameters.
In the era of LLMs, the model capability can be acquired either by training/fine-tuning the model parameters or designing delicate prompts (<em class="ltx_emph ltx_font_italic" id="S2.SS2.p6.1.2">i.e.</em>, prompt engineering).
In prompt engineering, one needs to write valuable information into the prompts to enhance the model capability or unleash existing LLM capabilities.
In the era of agents, the model capability can be acquired based on three strategies:
(1) model fine-tuning, (2) prompt engineering and (3) designing proper agent evolution mechanisms (we called it as <span class="ltx_text ltx_font_italic" id="S2.SS2.p6.1.3">mechanism engineering</span>).
Mechanism engineering is a broad concept that involves developing specialized modules, introducing novel working rules, and other strategies to enhance agent capabilities.
For clearly understanding the transitions of model capability acquisition strategies, we illustrate them in Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S2.F4" title="Figure 4 ‣ 2.2 Agent Capability Acquisition ‣ 2 LLM-based Autonomous Agent Construction ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_tag">4</span></a>.
In the following, we detail prompting engineering and mechanism engineering.</p>
</div>
<div class="ltx_para" id="S2.SS2.p7">
<p class="ltx_p" id="S2.SS2.p7.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S2.SS2.p7.1.m1.1"><semantics id="S2.SS2.p7.1.m1.1a"><mo id="S2.SS2.p7.1.m1.1.1" xref="S2.SS2.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p7.1.m1.1b"><ci id="S2.SS2.p7.1.m1.1.1.cmml" xref="S2.SS2.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p7.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p7.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S2.SS2.p7.1.1">Prompting Engineering</span>.
Due to the strong language comprehension capabilities, people can directly interact with LLMs using natural languages.
This introduces a novel strategy for enhancing agent capabilities, that is, one can describe the desired capability using natural language and then use it as prompts to influence LLM actions.
For example, in CoT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib45" title="">45</a>]</cite>, in order to empower the agent with the capability for complex task reasoning, the authors present the intermediate reasoning steps as few-shot examples in the prompt.
Similar techniques are also used in CoT-SC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib51" title="">51</a>]</cite> and ToT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib52" title="">52</a>]</cite>.
In RLP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib30" title="">30</a>]</cite>, the authors aim to enhance an agent’s self-awareness in conversations by prompting LLMs with the agent’s beliefs about both its own and the listeners’ mental states. This approach results in more engaging and adaptive utterances. Furthermore, the incorporation of the target mental states of listeners allows the agent to formulate more strategic plans.
Retroformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib90" title="">90</a>]</cite> presents a retrospective model that enables the agent to generate reflections on its past failures.
The reflections are integrated into the prompt of LLMs to guide the agent’s future actions.
Additionally, this model utilizes reinforcement learning to iteratively improve the retrospective model, thereby refining the LLM prompt.</p>
</div>
<div class="ltx_para" id="S2.SS2.p8">
<p class="ltx_p" id="S2.SS2.p8.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S2.SS2.p8.1.m1.1"><semantics id="S2.SS2.p8.1.m1.1a"><mo id="S2.SS2.p8.1.m1.1.1" xref="S2.SS2.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p8.1.m1.1b"><ci id="S2.SS2.p8.1.m1.1.1.cmml" xref="S2.SS2.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p8.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p8.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S2.SS2.p8.1.1">Mechanism Engineering</span>.
Unlike model fine-tuning and prompt engineering, mechanism engineering is a unique strategy to enhance agent capability.
In the following, we present several representative methods of mechanism engineering.</p>
</div>
<div class="ltx_para" id="S2.SS2.p9">
<p class="ltx_p" id="S2.SS2.p9.1">(1) <span class="ltx_text ltx_font_italic" id="S2.SS2.p9.1.1">Trial-and-error.</span>
In this method, the agent first performs an action, and subsequently, a pre-defined critic is invoked to judge the action.
If the action is deemed unsatisfactory, then the agent reacts by incorporating the critic’s feedback.
For example, in RAH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib91" title="">91</a>]</cite>, the agent serves as a user assistant in recommender systems. One of the agent’s crucial roles is to simulate human behavior and generate responses on behalf of the user. To fulfill this objective, the agent first generates a predicted response and then compares it with the real human feedback. If the predicted response and the real human feedback differ, the critic generates failure information, which is subsequently incorporated into the agent’s next action.
Similarly, in DEPS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib33" title="">33</a>]</cite>, the agent first designs a plan to accomplish a given task.
In the plan execution process, if an action fails, the explainer generates specific details explaining the cause of the failure. This information is then incorporated by the agent to redesign the plan.
In RoCo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib92" title="">92</a>]</cite>, the agent first proposes a sub-task plan and a path of 3D waypoints for each robot in a multi-robot collaboration task.
The plan and waypoints are then validated by a set of environment checks, such as collision detection and inverse kinematics. If any of the checks fail, the feedback is appended to each agent’s prompt and another round of dialog begins. The agents use LLMs to discuss and improve their plan and waypoints until they pass all validations.
PREFER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib93" title="">93</a>]</cite> extends this idea by leveraging LLMs to generate detailed feedback when the agent underperforms, enabling iterative refinement and performance improvement.</p>
</div>
<div class="ltx_para" id="S2.SS2.p10">
<p class="ltx_p" id="S2.SS2.p10.1">(2) <span class="ltx_text ltx_font_italic" id="S2.SS2.p10.1.1">Crowd-sourcing.</span> In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib94" title="">94</a>]</cite>, the authors design a debating mechanism that leverages the wisdom of crowds to enhance agent capabilities.
To begin with, different agents provide separate responses to a given question.
If their responses are not consistent, they will be prompted to incorporate the solutions from other agents and provide an updated response.
This iterative process continues until reaching a final consensus answer.
In this method, the capability of each agent is enhanced by understanding and incorporating the other agents’ opinions.</p>
</div>
<div class="ltx_para" id="S2.SS2.p11">
<p class="ltx_p" id="S2.SS2.p11.1">(3) <span class="ltx_text ltx_font_italic" id="S2.SS2.p11.1.1">Experience Accumulation.</span>
In GITM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib16" title="">16</a>]</cite>, the agent does not know how to solve a task in the beginning.
Then, it makes explorations, and once it has successfully accomplished a task, the actions used in this task are stored into the agent memory.
In the future, if the agent encounters a similar task, then the relevant memories are extracted to complete the current task.
In this process, the improved agent capability comes from the specially designed memory accumulation and utilization mechanisms.
Voyager <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib38" title="">38</a>]</cite> introduces a skill library, where executable codes for specific skills are refined through interactions with the environment, enabling efficient task execution over time.
In AppAgent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib95" title="">95</a>]</cite>, the agent is designed to interact with apps in a manner akin to human users, learning through both autonomous exploration and observation of human demonstrations. Throughout this process, it constructs a knowledge base, which serves as a reference for performing intricate tasks across various applications on a mobile phone.
In MemPrompt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib96" title="">96</a>]</cite>, the users are requested to provide feedback in natural language regarding the problem-solving intentions of the agent, and this feedback is stored in memory. When the agent encounters similar tasks, it attempts to retrieve related memories to generate more suitable responses.</p>
</div>
<div class="ltx_para" id="S2.SS2.p12">
<p class="ltx_p" id="S2.SS2.p12.1">(4) <span class="ltx_text ltx_font_italic" id="S2.SS2.p12.1.1">Self-driven Evolution.</span>
This method allows agents to autonomously improve through self-directed learning and feedback mechanisms.
LMA3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib97" title="">97</a>]</cite> enables the agent to autonomously set goals for itself, and gradually improve its capability by exploring the environment and receiving feedback from a reward function.
Following this mechanism, the agent can acquire knowledge and develop capabilities according to its own preferences.
SALLM-MS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib98" title="">98</a>]</cite> integrates advanced LLMs like GPT-4 into a multi-agent system, agents can adapt and perform complex tasks, showcasing advanced communication capabilities, thereby realizing self-driven evolution in their interactions with the environment.
In CLMTWA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib99" title="">99</a>]</cite>, by using a large language model as a teacher and a weaker language model as a student, the teacher can generate and communicate natural language explanations to improve the student’s reasoning skills via theory of mind. The teacher can also personalize its explanations for the student and intervene only when necessary, based on the expected utility of intervention.
Meanwhile, NLSOM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib100" title="">100</a>]</cite>,leverages natural language collaboration between agents, dynamically adjusting roles, tasks, and relationships based on feedback to solve problems beyond the scope of a single agent.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremarkx5">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic" id="Thmremarkx5.1.1.1">Remark</span></span><span class="ltx_text ltx_font_italic" id="Thmremarkx5.2.2">.</span>
</h6>
<div class="ltx_para" id="Thmremarkx5.p1">
<p class="ltx_p" id="Thmremarkx5.p1.1">Upon comparing the aforementioned strategies for agent capability acquisition, we can find that the fine-tuning method improves the agent capability by adjusting model parameters, which can incorporate a large amount of task-specific knowledge, but is only suitable for open-source LLMs. The method without fine-tuning usually enhances the agent capability based on delicate prompting strategies or mechanism engineering. They can be used for both open- and closed-source LLMs. However, due to the limitation of the input context window of LLMs, they cannot incorporate too much task information.
In addition, the designing spaces of the prompts and mechanisms are extremely large, which makes it not easy to find optimal solutions.</p>
</div>
</div>
<div class="ltx_para" id="S2.SS2.p13">
<p class="ltx_p" id="S2.SS2.p13.1">In the above sections, we have detailed the construction of LLM-based agents, where we focus on two aspects including the architecture design and capability acquisition. We present the correspondence between existing work and the above taxonomy in Table <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S2.T1" title="Table 1 ‣ 2.1.4 Action Module ‣ 2.1 Agent Architecture Design ‣ 2 LLM-based Autonomous Agent Construction ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_tag">1</span></a>.
It should be noted that, for the sake of integrity, we have also incorporated several studies, which do not explicitly mention LLM-based agents but are highly related to this area.</p>
</div>
<figure class="ltx_figure" id="S2.F5">
<p class="ltx_p ltx_align_center" id="S2.F5.1"><span class="ltx_text ltx_framed ltx_framed_rectangle" id="S2.F5.1.1" style="border-color: #000000;padding:0.0pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="246" id="S2.F5.1.1.g1" src="x5.png" width="830"/>
</span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The applications (left) and evaluation strategies (right) of LLM-based agents.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>LLM-based Autonomous Agent Application</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Owing to the strong language comprehension, complex task reasoning, and common sense understanding capabilities, LLM-based autonomous agents have shown significant potential to influence multiple domains.
This section provides a succinct summary of previous studies, categorizing them according to their applications in three distinct areas: social science, natural science, and engineering (see the left part of Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S2.F5" title="Figure 5 ‣ 2.2 Agent Capability Acquisition ‣ 2 LLM-based Autonomous Agent Construction ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_tag">5</span></a> for a global overview).</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Social Science</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Social science is one of the branches of science, devoted to the study of societies and the relationships among individuals within those societies.
LLM-based autonomous agents can promote this domain by leveraging their impressive human-like understanding, thinking and task solving capabilities.
In the following, we discuss several key areas that can be affected by LLM-based autonomous agents.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">Psychology</span>:
For the domain of psychology, LLM-based agents can be leveraged for conducting simulation experiments, providing mental health support and so on <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib101" title="">101</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib102" title="">102</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib103" title="">103</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib104" title="">104</a>]</cite>. For example, in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib101" title="">101</a>]</cite>, the authors assign LLMs with different profiles, and let them complete psychology experiments.
From the results, the authors find that LLMs are capable of generating results that align with those from studies involving human participants.
Additionally, it was observed that larger models tend to deliver more accurate simulation results compared to their smaller counterparts.
An interesting discovery is that, in many experiments, models like ChatGPT and GPT-4 can provide too perfect estimates (called “hyper-accuracy distortion”), which may influence the downstream applications.
In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib103" title="">103</a>]</cite>, the authors systematically analyze the effectiveness of LLM-based conversation agents for mental well-being support.
They collect 120 posts from Reddit, and find that such agents can help users cope with anxieties, social isolation and depression on demand.
At the same time, they also find that the agents may produce harmful contents sometimes.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">Political Science and Economy</span>:
LLM-based agents can also be utilized to study political science and economy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib104" title="">104</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib105" title="">105</a>]</cite>.
In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib29" title="">29</a>]</cite>, LLM-based agents are utilized for ideology detection and predicting voting patterns.
In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib104" title="">104</a>]</cite>, the authors focuses on understanding the discourse structure and persuasive elements of political speech through the assistance of LLM-based agents.
In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib105" title="">105</a>]</cite>, LLM-based agents are provided with specific traits such as talents, preferences, and personalities to explore human economic behaviors in simulated scenarios.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">Social Simulation</span>:
Previously, conducting experiments with human societies is often expensive, unethical, or even infeasible.
With the ever prospering of LLMs, many people explore to build virtual environment with LLM-based agents to simulate social phenomena, such as the propagation of harmful information, and so on <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib80" title="">80</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib106" title="">106</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib107" title="">107</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib108" title="">108</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib78" title="">78</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib109" title="">109</a>]</cite>.
For example, Social Simulacra <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib80" title="">80</a>]</cite> simulates an online social community and explores the potential of utilizing agent-based simulations to aid decision-makers to improve community regulations.
 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib106" title="">106</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib107" title="">107</a>]</cite> investigates the potential impacts of different behavioral characteristics of LLM-based agents in social networks.
Generative Agents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib20" title="">20</a>]</cite> and AgentSims<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib34" title="">34</a>]</cite> construct multiple agents in a virtual town to simulate the human daily life.
SocialAI School <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib108" title="">108</a>]</cite> employs LLM-based agents to simulate and investigate the fundamental social cognitive skills during the course of child development.
S<sup class="ltx_sup" id="S3.SS1.p4.1.2">3</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib78" title="">78</a>]</cite> builds a social network simulator, focusing on the propagation of information, emotion and attitude.
CGMI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib110" title="">110</a>]</cite> is a framework for multi-agent simulation. CGMI maintains the personality of the agents through a tree structure and constructs a cognitive model. The authors simulated a classroom scenario using CGMI.</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p5.1.1">Jurisprudence</span>: LLM-based agents can serve as aids in legal decision-making processes, facilitating more informed judgements <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib111" title="">111</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib112" title="">112</a>]</cite>.
Blind Judgement <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib112" title="">112</a>]</cite> employs several language models to simulate the decision-making processes of multiple judges. It gathers diverse opinions and consolidates the outcomes through a voting mechanism. ChatLaw <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib111" title="">111</a>]</cite> is a prominent Chinese legal model based on LLM. It adeptly supports both database and keyword search strategies, specifically designed to mitigate the hallucination issue prevalent in such models.
In addition, this model also employs self-attention mechanism to enhance the LLM’s capability via mitigating the impact of reference inaccuracies.</p>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p6.1.1">Research Assistant</span>:
Beyond their application in specialized domains, LLM-based agents are increasingly adopted as versatile assistants in the broad field of social science research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib113" title="">113</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib104" title="">104</a>]</cite>. In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib104" title="">104</a>]</cite>, LLM-based agents offer multifaceted assistance, ranging from generating concise article abstracts and extracting pivotal keywords to crafting detailed scripts for studies, showcasing their ability to enrich and streamline the research process. Meanwhile, in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib113" title="">113</a>]</cite>, LLM-based agents serve as a writing assistant, demonstrating their capability to identify novel research inquiries for social scientists, thereby opening new avenues for exploration and innovation in the field. These examples highlight the potential of LLM-based agents in enhancing the efficiency, creativity, and breadth of social science research.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Natural Science</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Natural science is one of the branches of science concerned with the description, understanding and prediction of natural phenomena, based on empirical evidence from observation and experimentation.
With the ever prospering of LLMs, the application of LLM-based agents in natural sciences becomes more and more popular.
In the following, we present many representative areas, where LLM-based agents can play important roles.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Documentation and Data Management</span>:
Natural scientific research often involves the collection, organization, and synthesis of substantial amounts of literature, which requires a significant dedication of time and human resources.
LLM-based agents have shown strong capabilities on language understanding and employing tools such as the internet and databases for text processing.
These capabilities empower the agent to excel in tasks related to documentation and data management.
In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib114" title="">114</a>]</cite>, the agent can efficiently query and utilize internet information to complete tasks such as question answering and experiment planning.
ChatMOF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib115" title="">115</a>]</cite> utilizes LLMs to extract important information from text descriptions written by humans.
It then formulates a plan to apply relevant tools for predicting the properties and structures of metal-organic frameworks.
ChemCrow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib76" title="">76</a>]</cite> utilizes chemistry-related databases to both validate the precision of compound representations and identify potentially dangerous substances. This functionality enhances the reliability and comprehensiveness of scientific inquiries by ensuring the accuracy of the data involved.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.1">Experiment Assistant</span>:
LLM-based agents have the ability to independently conduct experiments, making them valuable tools for supporting scientists in their research projects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib114" title="">114</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib76" title="">76</a>]</cite>.
For instance,  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib114" title="">114</a>]</cite> introduces an innovative agent system that utilizes LLMs for automating the design, planning, and execution of scientific experiments.
This system, when provided with the experimental objectives as input, accesses the Internet and retrieves relevant documents to gather the necessary information.
It subsequently utilizes Python code to conduct essential calculations and carry out the following experiments.
ChemCrow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib76" title="">76</a>]</cite> incorporates 17 carefully developed tools that are specifically designed to assist researchers in their chemical research. Once the input objectives are received, ChemCrow provides valuable recommendations for experimental procedures, while also emphasizing any potential safety risks associated with the proposed experiments.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p4.1.1">Natural Science Education</span>:
LLM-based agents can communicate with humans fluently, often being utilized to develop agent-based educational tools.
For example, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib114" title="">114</a>]</cite> develops agent-based education systems to facilitate students learning of experimental design, methodologies, and analysis.
The objective of these systems is to enhance students’ critical thinking and problem-solving skills, while also fostering a deeper comprehension of scientific principles.
Math Agents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib116" title="">116</a>]</cite> can assist researchers in exploring, discovering, solving and proving mathematical problems.
Additionally, it can communicate with humans and aids them in understanding and using mathematics.
 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib117" title="">117</a>]</cite> utilize the capabilities of CodeX <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib118" title="">118</a>]</cite> to automatically solve and explain university-level mathematical problems, which can be used as education tools to teach students and researchers.
CodeHelp <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib119" title="">119</a>]</cite> is an education agent for programming. It offers many useful features, such as setting course-specific keywords, monitoring student queries, and providing feedback to the system.
EduChat <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib87" title="">87</a>]</cite> is an LLM-based agent designed specifically for the education domain.
It provides personalized, equitable, and empathetic educational support to teachers, students, and parents through dialogue.
FreeText <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib120" title="">120</a>]</cite> is an agent that utilizes LLMs to automatically assess students’ responses to open-ended questions and offer feedback.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Representative applications of LLM-based autonomous agents.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T2.1" style="width:341.5pt;height:8650pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-19.0pt,480.6pt) scale(0.9,0.9) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.1.1.2.1">
<th class="ltx_td ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T2.1.1.2.1.1" style="width:113.8pt;padding-top:3pt;padding-bottom:3pt;"></th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T2.1.1.2.1.2" style="width:142.3pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.2.1.2.1">
<span class="ltx_p" id="S3.T2.1.1.2.1.2.1.1">Domain</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.2.1.3" style="width:250.4pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.2.1.3.1">
<span class="ltx_p" id="S3.T2.1.1.2.1.3.1.1">Work</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.1.3.1">
<td class="ltx_td ltx_align_middle ltx_border_r ltx_border_t" id="S3.T2.1.1.3.1.1" style="width:113.8pt;padding-top:3pt;padding-bottom:3pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T2.1.1.3.1.2" style="width:142.3pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.3.1.2.1">
<span class="ltx_p" id="S3.T2.1.1.3.1.2.1.1">Psychology</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T2.1.1.3.1.3" style="width:250.4pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.3.1.3.1">
<span class="ltx_p" id="S3.T2.1.1.3.1.3.1.1">TE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib101" title="">101</a>]</cite>, Akata et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib102" title="">102</a>]</cite>, Ziems et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib104" title="">104</a>]</cite>, Ma et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib103" title="">103</a>]</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.4.2">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T2.1.1.4.2.1" rowspan="5" style="width:113.8pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.4.2.1.1">
<span class="ltx_p" id="S3.T2.1.1.4.2.1.1.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" id="S3.T2.1.1.4.2.1.1.1.1" style="width:56.9pt;">
<span class="ltx_p" id="S3.T2.1.1.4.2.1.1.1.1.1">Social Science</span>
</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T2.1.1.4.2.2" style="width:142.3pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.4.2.2.1">
<span class="ltx_p" id="S3.T2.1.1.4.2.2.1.1">Political Science and Economy</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T2.1.1.4.2.3" style="width:250.4pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.4.2.3.1">
<span class="ltx_p" id="S3.T2.1.1.4.2.3.1.1">Argyle et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib29" title="">29</a>]</cite>, Horton <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib105" title="">105</a>]</cite>, Ziems et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib104" title="">104</a>]</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T2.1.1.1.2" style="width:142.3pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.2.1">
<span class="ltx_p" id="S3.T2.1.1.1.2.1.1">Social Simulation</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T2.1.1.1.1" style="width:250.4pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.1.1">
<span class="ltx_p" id="S3.T2.1.1.1.1.1.1">Social Simulacra <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib80" title="">80</a>]</cite>, Generative Agents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib20" title="">20</a>]</cite>, SocialAI School <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib108" title="">108</a>]</cite>, AgentSims <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib34" title="">34</a>]</cite>, S<sup class="ltx_sup" id="S3.T2.1.1.1.1.1.1.1">3</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib78" title="">78</a>]</cite>, Williams et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib109" title="">109</a>]</cite>, Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib106" title="">106</a>]</cite>, Chao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib107" title="">107</a>]</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.5.3">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T2.1.1.5.3.1" style="width:142.3pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.5.3.1.1">
<span class="ltx_p" id="S3.T2.1.1.5.3.1.1.1">Jurisprudence</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T2.1.1.5.3.2" style="width:250.4pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.5.3.2.1">
<span class="ltx_p" id="S3.T2.1.1.5.3.2.1.1">ChatLaw <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib111" title="">111</a>]</cite>, Blind Judgement <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib112" title="">112</a>]</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.6.4">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T2.1.1.6.4.1" style="width:142.3pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.6.4.1.1">
<span class="ltx_p" id="S3.T2.1.1.6.4.1.1.1">Research Assistant</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T2.1.1.6.4.2" style="width:250.4pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.6.4.2.1">
<span class="ltx_p" id="S3.T2.1.1.6.4.2.1.1">Ziems et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib104" title="">104</a>]</cite>, Bail et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib113" title="">113</a>]</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.7.5">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T2.1.1.7.5.1" style="width:142.3pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.7.5.1.1">
<span class="ltx_block ltx_parbox ltx_align_middle" id="S3.T2.1.1.7.5.1.1.1" style="width:113.8pt;">
<span class="ltx_p" id="S3.T2.1.1.7.5.1.1.1.1">Documentation and</span>
<span class="ltx_p" id="S3.T2.1.1.7.5.1.1.1.2">Data Management</span>
</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T2.1.1.7.5.2" style="width:250.4pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.7.5.2.1">
<span class="ltx_p" id="S3.T2.1.1.7.5.2.1.1">ChemCrow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib76" title="">76</a>]</cite>, ChatMOF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib115" title="">115</a>]</cite>, Boiko et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib114" title="">114</a>]</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.8.6">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T2.1.1.8.6.1" rowspan="2" style="width:113.8pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.8.6.1.1">
<span class="ltx_p" id="S3.T2.1.1.8.6.1.1.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" id="S3.T2.1.1.8.6.1.1.1.1" style="width:85.4pt;">
<span class="ltx_p" id="S3.T2.1.1.8.6.1.1.1.1.1">Natural Science</span>
</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T2.1.1.8.6.2" style="width:142.3pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.8.6.2.1">
<span class="ltx_p" id="S3.T2.1.1.8.6.2.1.1">Experiment Assistant</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T2.1.1.8.6.3" style="width:250.4pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.8.6.3.1">
<span class="ltx_p" id="S3.T2.1.1.8.6.3.1.1">ChemCrow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib76" title="">76</a>]</cite>, Boiko et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib114" title="">114</a>]</cite>, Grossmann et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib121" title="">121</a>]</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.9.7">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T2.1.1.9.7.1" style="width:142.3pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.9.7.1.1">
<span class="ltx_p" id="S3.T2.1.1.9.7.1.1.1">Natural Science Education</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T2.1.1.9.7.2" style="width:250.4pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.9.7.2.1">
<span class="ltx_p" id="S3.T2.1.1.9.7.2.1.1">ChemCrow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib76" title="">76</a>]</cite>, CodeHelp <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib119" title="">119</a>]</cite>, Boiko et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib114" title="">114</a>]</cite>, MathAgent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib116" title="">116</a>]</cite>, Drori et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib117" title="">117</a>]</cite>, EduChat <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib87" title="">87</a>]</cite>, FreeText <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib120" title="">120</a>]</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.10.8">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_r ltx_border_t" id="S3.T2.1.1.10.8.1" rowspan="3" style="width:113.8pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.10.8.1.1">
<span class="ltx_p" id="S3.T2.1.1.10.8.1.1.1">Engineering</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T2.1.1.10.8.2" style="width:142.3pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.10.8.2.1">
<span class="ltx_p" id="S3.T2.1.1.10.8.2.1.1">CS &amp; SE</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T2.1.1.10.8.3" style="width:250.4pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.10.8.3.1">
<span class="ltx_p" id="S3.T2.1.1.10.8.3.1.1">RestGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib71" title="">71</a>]</cite>, Self-collaboration <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib24" title="">24</a>]</cite>, SQL-PALM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib89" title="">89</a>]</cite>, RAH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib91" title="">91</a>]</cite>, D-Bot <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib122" title="">122</a>]</cite>, RecMind <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib53" title="">53</a>]</cite>, ChatEDA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib123" title="">123</a>]</cite>, InteRecAgent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib124" title="">124</a>]</cite>, PentestGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib125" title="">125</a>]</cite>, CodeHelp <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib119" title="">119</a>]</cite>, SmolModels <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib126" title="">126</a>]</cite>, DemoGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib127" title="">127</a>]</cite>, GPTEngineer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib128" title="">128</a>]</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.11.9">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T2.1.1.11.9.1" style="width:142.3pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.11.9.1.1">
<span class="ltx_p" id="S3.T2.1.1.11.9.1.1.1">Industrial Automation</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T2.1.1.11.9.2" style="width:250.4pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.11.9.2.1">
<span class="ltx_p" id="S3.T2.1.1.11.9.2.1.1">GPT4IA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib129" title="">129</a>]</cite>, IELLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib130" title="">130</a>]</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.12.10">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_r ltx_border_t" id="S3.T2.1.1.12.10.1" style="width:142.3pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.12.10.1.1">
<span class="ltx_p" id="S3.T2.1.1.12.10.1.1.1">Robotics &amp; Embodied AI</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="S3.T2.1.1.12.10.2" style="width:250.4pt;padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.12.10.2.1">
<span class="ltx_p" id="S3.T2.1.1.12.10.2.1.1">ProAgent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib131" title="">131</a>]</cite>, LLM4RL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib132" title="">132</a>]</cite>, PET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib133" title="">133</a>]</cite>, REMEMBERER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib134" title="">134</a>]</cite>, DEPS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib33" title="">33</a>]</cite>, Unified Agent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib135" title="">135</a>]</cite>, SayCan <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib79" title="">79</a>]</cite>, TidyBot <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib136" title="">136</a>]</cite>, RoCo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib92" title="">92</a>]</cite>, SayPlan <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib31" title="">31</a>]</cite>, TaPA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib137" title="">137</a>]</cite>,
Dasgupta et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib138" title="">138</a>]</cite>, DECKARD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib139" title="">139</a>]</cite>, Dialogue shaping <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib140" title="">140</a>]</cite></span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Engineering</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">LLM-based autonomous agents have shown great potential in assisting and enhancing engineering research and applications. In this section, we review and summarize the applications of LLM-based agents in several major engineering domains.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p2.1.1">Computer Science &amp; Software Engineering</span>:
In the field of computer science and software engineering, LLM-based agents offer potential for automating coding, testing, debugging, and documentation generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib128" title="">128</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib126" title="">126</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib127" title="">127</a>]</cite>. ChatDev <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib18" title="">18</a>]</cite> proposes an end-to-end framework, where multiple agent roles communicate and collaborate through natural language conversations to complete the software development life cycle. This framework demonstrates efficient and cost-effective generation of executable software systems.
MetaGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib23" title="">23</a>]</cite> abstracts multiple roles, such as product managers, architects, project managers, and engineers, to supervise code generation process and enhance the quality of the final output code. This enables low-cost software development.
 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib24" title="">24</a>]</cite> presents a self-collaboration framework for code generation using LLMs.
In this framework, multiple LLMs are assumed to be distinct "experts" for specific sub-tasks.
They collaborate and interact according to specified instructions, forming a virtual team that facilitates each other’s work.
Ultimately, the virtual team collaboratively addresses code generation tasks without requiring human intervention.
LLIFT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib141" title="">141</a>]</cite> employs LLMs to assist in conducting static analysis, specifically for identifying potential code vulnerabilities. This approach effectively manages the trade-off between accuracy and scalability.
ChatEDA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib123" title="">123</a>]</cite> is an agent developed for electronic design automation (EDA) to streamline the design process by integrating task planning, script generation, and execution.
CodeHelp <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib119" title="">119</a>]</cite> is an agent designed to assist students and developers in debugging and testing their code. Its features include providing detailed explanations of error messages, suggesting potential fixes, and ensuring the accuracy of the code.
Pentest <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib125" title="">125</a>]</cite> is a penetration testing tool based on LLMs, which can effectively identify common vulnerabilities, and interpret source code to develop exploits.
D-Bot <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib122" title="">122</a>]</cite> utilizes the capabilities of LLMs to systematically assess potential root causes of anomalies in databases.
Through the implementation of a tree of thought approach, D-Bot enables LLMs to backtrack to previous steps in case the current step proves unsuccessful, thus enhancing the accuracy of the diagnosis process.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p3.1.1">Industrial Automation</span>: In the field of industrial automation, LLM-based agents can be used to achieve intelligent planning and control of production processes. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib129" title="">129</a>]</cite> proposes a novel framework that integrates LLMs with digital twin systems to accommodate flexible production needs. The framework leverages prompt engineering techniques to create LLM agents that can adapt to specific tasks based on the information provided by digital twins. These agents can coordinate a series of atomic functionalities and skills to complete production tasks at different levels. This research demonstrates the potential of integrating LLMs into industrial automation systems, providing innovative solutions for more agile, flexible and adaptive production processes.
IELLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib130" title="">130</a>]</cite> showcases a case study on LLMs’ role in the oil and gas industry, covering applications like factory automation and PLC programming.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p4.1.1">Robotics &amp; Embodied Artificial Intelligence</span>:
Recent works have advanced the development of more efficient reinforcement learning agents for robotics and embodied artificial intelligence <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib138" title="">138</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib140" title="">140</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib139" title="">139</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib137" title="">137</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib132" title="">132</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib133" title="">133</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib134" title="">134</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib135" title="">135</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib79" title="">79</a>]</cite>. These efforts focus on enhancing autonomous agents’ capabilities in planning, reasoning, and collaboration within embodied environments.
For instance, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib138" title="">138</a>]</cite> proposes the Planner-Actor-Reporter paradigm for embodied reasoning and task planning.
DECKARD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib139" title="">139</a>]</cite> introduces the Planner-Actor-Reporter paradigm, which facilitates embodied reasoning and task planning by decoupling the agent’s planning, execution, and reporting processes.
TaPA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib137" title="">137</a>]</cite> constructs a multimodal dataset comprising multi-view RGB images of indoor scenes, human instructions, and corresponding plans to fine-tune LLMs. The fine-tuned models align visual perception with task planning, enabling them to generate more executable plans and significantly improving their performance in visually grounded tasks.</p>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1">To overcome the physical constraints, the agents can generate executable plans and accomplish long-term tasks by leveraging multiple skills.
In terms of control policies, SayCan <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib79" title="">79</a>]</cite> focuses on investigating a wide range of manipulation and navigation skills utilizing a mobile manipulator robot. Taking inspiration from typical tasks encountered in a kitchen environment, it presents a comprehensive set of 551 skills that cover seven skill families and 17 objects. These skills encompass various actions such as picking, placing, grasping, and manipulating objects, among others.
TidyBot <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib136" title="">136</a>]</cite> is an embodied agent designed to personalize household cleanup tasks.
It can learn users’ preferences on object placement and manipulation methods through textual examples.</p>
</div>
<div class="ltx_para" id="S3.SS3.p6">
<p class="ltx_p" id="S3.SS3.p6.1">To promote the application of LLM-based autonomous agents, researchers have also introduced many open-source libraries, based on which the developers can quickly implement and evaluate agents according to their customized requirements <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib82" title="">82</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib142" title="">142</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib143" title="">143</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib144" title="">144</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib145" title="">145</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib146" title="">146</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib147" title="">147</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib148" title="">148</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib149" title="">149</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib150" title="">150</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib151" title="">151</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib152" title="">152</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib127" title="">127</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib153" title="">153</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib154" title="">154</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib155" title="">155</a>]</cite>.
For example,
LangChain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib147" title="">147</a>]</cite> is an open-source framework that automates coding, testing, debugging, and documentation generation tasks. By integrating language models with data sources and facilitating interaction with the environment, LangChain enables efficient and cost-effective software development through natural language communication and collaboration among multiple agent roles.
Based on LangChain, XLang <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib145" title="">145</a>]</cite> provides a comprehensive set of tools and a fully integrated user interface. It focuses on executable language grounding, enabling the conversion of natural language instructions into code or action sequences that interact seamlessly with various environments, including databases, web applications, and physical robots.
AutoGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib82" title="">82</a>]</cite> is an agent that is fully automated. It sets one or more goals, breaks them down into corresponding tasks, and cycles through the tasks until the goal is achieved.
WorkGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib148" title="">148</a>]</cite> is an agent framework similar to AutoGPT and LangChain. By providing it with an instruction and a set of APIs, it engages in back-and-forth conversations with AI until the instruction is completed.
GPT-Engineer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib128" title="">128</a>]</cite> and DemoGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib127" title="">127</a>]</cite> are open-source projects that focus on automating code generation through prompts to complete development tasks.
SmolModels <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib126" title="">126</a>]</cite> offers a family of compact language models suitable for various tasks.
AGiXT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib144" title="">144</a>]</cite> is a dynamic AI automation platform that efficiently manages instructions and executes complex tasks across various AI providers, integrating adaptive memory, smart features, and a versatile plugin system.
AgentVerse <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib156" title="">156</a>]</cite> is a versatile framework that facilitates researchers in creating customized LLM-based agent simulations efficiently.
GPT Researcher <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib150" title="">150</a>]</cite> is an experimental application that leverages LLMs to efficiently develop research questions, trigger web crawls to gather information, summarize sources, and aggregate summaries.
BMTools <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib151" title="">151</a>]</cite> provides a platform for community-driven tool building and sharing. It supports various types of tools, enables simultaneous task execution using multiple tools, and offers a simple interface for loading plugins via URLs, fostering easy development and contribution to the BMTools ecosystem.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremarkx6">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic" id="Thmremarkx6.1.1.1">Remark</span></span><span class="ltx_text ltx_font_italic" id="Thmremarkx6.2.2">.</span>
</h6>
<div class="ltx_para" id="Thmremarkx6.p1">
<p class="ltx_p" id="Thmremarkx6.p1.1">Utilization of LLM-based agents in supporting above applications may also entail risks and challenges.
On one hand, LLMs themselves may be susceptible to illusions and other issues, occasionally providing erroneous answers, leading to incorrect conclusions, experimental failures, or even posing risks to human safety in hazardous experiments. Therefore, during experimentation, users must possess the necessary expertise and knowledge to exercise appropriate caution. On the other hand, LLM-based agents could potentially be exploited for malicious purposes, such as the development of chemical weapons, necessitating the implementation of security measures, such as human alignment, to ensure responsible and ethical use.</p>
</div>
<div class="ltx_para" id="Thmremarkx6.p2">
<p class="ltx_p" id="Thmremarkx6.p2.1">In summary, in the above sections, we introduce the typical applications of LLM-based autonomous agents in three important domains. To facilitate a clearer understanding, we have summarized the relationship between previous studies and their respective applications in Table <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S3.T2" title="Table 2 ‣ 3.2 Natural Science ‣ 3 LLM-based Autonomous Agent Application ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>LLM-based Autonomous Agent Evaluation</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Similar to LLMs themselves, evaluating the effectiveness of LLM-based autonomous agents is a challenging task.
This section outlines two prevalent approaches to evaluation: subjective and objective methods. For a comprehensive overview, please refer to the right portion of Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S2.F5" title="Figure 5 ‣ 2.2 Agent Capability Acquisition ‣ 2 LLM-based Autonomous Agent Construction ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Subjective Evaluation</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Subjective evaluation measures the agent capabilities based on human judgements <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib157" title="">157</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib80" title="">80</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib22" title="">22</a>]</cite>.
It is suitable for the scenarios where there are no evaluation datasets or it is very hard to design quantitative metrics, for example, evaluating the agent’s intelligence or user-friendliness.
In the following, we present two commonly used strategies for subjective evaluation.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Human Annotation</span>:
This evaluation method involves human evaluators directly scoring or ranking the outputs generated by various agents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib104" title="">104</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib22" title="">22</a>]</cite>.
For example, in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib20" title="">20</a>]</cite>, the authors engage numerous annotators by asking 25 questions that explore their abilities across five key areas directly related to agent capabilities.
In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib80" title="">80</a>]</cite>, annotators are asked to determine whether the specifically designed models can significantly enhance the development of rules within online communities.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">Turing Test</span>:
This evaluation strategy necessitates that human evaluators differentiate between outputs produced by agents and those created by humans.
If, in a given task, the evaluators cannot separate the agent and human results, it demonstrates that the agent can achieve human-like performance on this task.
For instance, researchers in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib29" title="">29</a>]</cite> conduct experiments on free-form Partisan text, and the human evaluators are asked to guess whether the responses are from human or LLM-based agent.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremarkx7">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic" id="Thmremarkx7.1.1.1">Remark</span></span><span class="ltx_text ltx_font_italic" id="Thmremarkx7.2.2">.</span>
</h6>
<div class="ltx_para" id="Thmremarkx7.p1">
<p class="ltx_p" id="Thmremarkx7.p1.1">LLM-based agents are usually designed to serve humans.
Thus, subjective agent evaluation plays a critical role, since it reflects human criterion.
However, this strategy also faces issues such as high costs, inefficiency, and population bias.
To address these issues, a growing number of researchers are investigating the use of LLMs themselves as intermediaries for carrying out these subjective assessments.
For example, in ChemCrow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib76" title="">76</a>]</cite>, researchers assess the experimental results using GPT. They consider both the completion of tasks and the accuracy of the underlying processes.
Similarly, ChatEval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib158" title="">158</a>]</cite> introduces a novel approach by employing multiple agents to critique and assess the results generated by various candidate models in a structured debate format.
This innovative use of LLMs for evaluation purposes holds promise for enhancing both the credibility and applicability of subjective assessments in the future. As LLM technology continues to evolve, it is anticipated that these methods will become increasingly reliable and find broader applications, thereby overcoming the current limitations of direct human evaluation.</p>
</div>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>
For subjective evaluation, we use ① and ② to represent human annotation and the Turing test, respectively.
For objective evaluation, we use ①, ②, ③, and ④ to represent real-world simulation, social evaluation, multi-task evaluation, and software testing, respectively. “<math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T3.2.m1.1"><semantics id="S4.T3.2.m1.1b"><mi id="S4.T3.2.m1.1.1" mathvariant="normal" xref="S4.T3.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T3.2.m1.1c"><ci id="S4.T3.2.m1.1.1.cmml" xref="S4.T3.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.m1.1d">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.m1.1e">✓</annotation></semantics></math>” indicates that the evaluations are based on benchmarks.</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T3.19" style="width:495.5pt;height:509.6pt;vertical-align:-7.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-18.6pt,18.9pt) scale(0.93,0.93) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.19.17">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.19.17.18.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T3.19.17.18.1.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.18.1.1.1">
<span class="ltx_p" id="S4.T3.19.17.18.1.1.1.1" style="width:116.7pt;">Model</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T3.19.17.18.1.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.18.1.2.1">
<span class="ltx_p" id="S4.T3.19.17.18.1.2.1.1" style="width:88.2pt;">Subjective</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T3.19.17.18.1.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.18.1.3.1">
<span class="ltx_p" id="S4.T3.19.17.18.1.3.1.1" style="width:88.2pt;">Objective</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T3.19.17.18.1.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.18.1.4.1">
<span class="ltx_p" id="S4.T3.19.17.18.1.4.1.1" style="width:88.2pt;">Benchmark</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T3.19.17.18.1.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.18.1.5.1">
<span class="ltx_p" id="S4.T3.19.17.18.1.5.1.1" style="width:88.2pt;">Time</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.3.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.3.1.1.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.3.1.1.2.1">
<span class="ltx_p" id="S4.T3.3.1.1.2.1.1" style="width:116.7pt;">WebShop <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib86" title="">86</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.3.1.1.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.3.1.1.3.1">
<span class="ltx_p" id="S4.T3.3.1.1.3.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.3.1.1.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.3.1.1.4.1">
<span class="ltx_p" id="S4.T3.3.1.1.4.1.1" style="width:88.2pt;">① ③</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.3.1.1.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.3.1.1.1.1">
<span class="ltx_p" id="S4.T3.3.1.1.1.1.1" style="width:88.2pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T3.3.1.1.1.1.1.m1.1"><semantics id="S4.T3.3.1.1.1.1.1.m1.1a"><mi id="S4.T3.3.1.1.1.1.1.m1.1.1" mathvariant="normal" xref="S4.T3.3.1.1.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T3.3.1.1.1.1.1.m1.1b"><ci id="S4.T3.3.1.1.1.1.1.m1.1.1.cmml" xref="S4.T3.3.1.1.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.1.1.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.1.1.1.1.1.m1.1d">✓</annotation></semantics></math></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.3.1.1.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.3.1.1.5.1">
<span class="ltx_p" id="S4.T3.3.1.1.5.1.1" style="width:88.2pt;">07/2022</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.19.17.19.1">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.19.1.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.19.1.1.1">
<span class="ltx_p" id="S4.T3.19.17.19.1.1.1.1" style="width:116.7pt;">Social Simulacra <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib80" title="">80</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.19.1.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.19.1.2.1">
<span class="ltx_p" id="S4.T3.19.17.19.1.2.1.1" style="width:88.2pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.19.1.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.19.1.3.1">
<span class="ltx_p" id="S4.T3.19.17.19.1.3.1.1" style="width:88.2pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.19.1.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.19.1.4.1">
<span class="ltx_p" id="S4.T3.19.17.19.1.4.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.19.1.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.19.1.5.1">
<span class="ltx_p" id="S4.T3.19.17.19.1.5.1.1" style="width:88.2pt;">08/2022</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.19.17.20.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.20.2.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.20.2.1.1">
<span class="ltx_p" id="S4.T3.19.17.20.2.1.1.1" style="width:116.7pt;">TE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib101" title="">101</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.20.2.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.20.2.2.1">
<span class="ltx_p" id="S4.T3.19.17.20.2.2.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.20.2.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.20.2.3.1">
<span class="ltx_p" id="S4.T3.19.17.20.2.3.1.1" style="width:88.2pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.20.2.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.20.2.4.1">
<span class="ltx_p" id="S4.T3.19.17.20.2.4.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.20.2.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.20.2.5.1">
<span class="ltx_p" id="S4.T3.19.17.20.2.5.1.1" style="width:88.2pt;">08/2022</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.19.17.21.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.21.3.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.21.3.1.1">
<span class="ltx_p" id="S4.T3.19.17.21.3.1.1.1" style="width:116.7pt;">LIBRO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib159" title="">159</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.21.3.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.21.3.2.1">
<span class="ltx_p" id="S4.T3.19.17.21.3.2.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.21.3.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.21.3.3.1">
<span class="ltx_p" id="S4.T3.19.17.21.3.3.1.1" style="width:88.2pt;">④</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.21.3.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.21.3.4.1">
<span class="ltx_p" id="S4.T3.19.17.21.3.4.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.21.3.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.21.3.5.1">
<span class="ltx_p" id="S4.T3.19.17.21.3.5.1.1" style="width:88.2pt;">09/2022</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.4.2.2.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.2.2.2.1">
<span class="ltx_p" id="S4.T3.4.2.2.2.1.1" style="width:116.7pt;">ReAct <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib60" title="">60</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.4.2.2.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.2.2.3.1">
<span class="ltx_p" id="S4.T3.4.2.2.3.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.4.2.2.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.2.2.4.1">
<span class="ltx_p" id="S4.T3.4.2.2.4.1.1" style="width:88.2pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.4.2.2.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.2.2.1.1">
<span class="ltx_p" id="S4.T3.4.2.2.1.1.1" style="width:88.2pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T3.4.2.2.1.1.1.m1.1"><semantics id="S4.T3.4.2.2.1.1.1.m1.1a"><mi id="S4.T3.4.2.2.1.1.1.m1.1.1" mathvariant="normal" xref="S4.T3.4.2.2.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T3.4.2.2.1.1.1.m1.1b"><ci id="S4.T3.4.2.2.1.1.1.m1.1.1.cmml" xref="S4.T3.4.2.2.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.2.2.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.2.2.1.1.1.m1.1d">✓</annotation></semantics></math></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.4.2.2.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.2.2.5.1">
<span class="ltx_p" id="S4.T3.4.2.2.5.1.1" style="width:88.2pt;">10/2022</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.19.17.22.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.22.4.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.22.4.1.1">
<span class="ltx_p" id="S4.T3.19.17.22.4.1.1.1" style="width:116.7pt;">Argyle et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib29" title="">29</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.22.4.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.22.4.2.1">
<span class="ltx_p" id="S4.T3.19.17.22.4.2.1.1" style="width:88.2pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.22.4.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.22.4.3.1">
<span class="ltx_p" id="S4.T3.19.17.22.4.3.1.1" style="width:88.2pt;">② ③</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.22.4.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.22.4.4.1">
<span class="ltx_p" id="S4.T3.19.17.22.4.4.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.22.4.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.22.4.5.1">
<span class="ltx_p" id="S4.T3.19.17.22.4.5.1.1" style="width:88.2pt;">02/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.5.3.3.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.5.3.3.2.1">
<span class="ltx_p" id="S4.T3.5.3.3.2.1.1" style="width:116.7pt;">DEPS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib33" title="">33</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.5.3.3.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.5.3.3.3.1">
<span class="ltx_p" id="S4.T3.5.3.3.3.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.5.3.3.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.5.3.3.4.1">
<span class="ltx_p" id="S4.T3.5.3.3.4.1.1" style="width:88.2pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.5.3.3.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.5.3.3.1.1">
<span class="ltx_p" id="S4.T3.5.3.3.1.1.1" style="width:88.2pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T3.5.3.3.1.1.1.m1.1"><semantics id="S4.T3.5.3.3.1.1.1.m1.1a"><mi id="S4.T3.5.3.3.1.1.1.m1.1.1" mathvariant="normal" xref="S4.T3.5.3.3.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T3.5.3.3.1.1.1.m1.1b"><ci id="S4.T3.5.3.3.1.1.1.m1.1.1.cmml" xref="S4.T3.5.3.3.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.3.3.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.5.3.3.1.1.1.m1.1d">✓</annotation></semantics></math></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.5.3.3.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.5.3.3.5.1">
<span class="ltx_p" id="S4.T3.5.3.3.5.1.1" style="width:88.2pt;">02/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.19.17.23.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.23.5.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.23.5.1.1">
<span class="ltx_p" id="S4.T3.19.17.23.5.1.1.1" style="width:116.7pt;">Jalil et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib160" title="">160</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.23.5.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.23.5.2.1">
<span class="ltx_p" id="S4.T3.19.17.23.5.2.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.23.5.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.23.5.3.1">
<span class="ltx_p" id="S4.T3.19.17.23.5.3.1.1" style="width:88.2pt;">④</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.23.5.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.23.5.4.1">
<span class="ltx_p" id="S4.T3.19.17.23.5.4.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.23.5.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.23.5.5.1">
<span class="ltx_p" id="S4.T3.19.17.23.5.5.1.1" style="width:88.2pt;">02/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.19.17.24.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.24.6.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.24.6.1.1">
<span class="ltx_p" id="S4.T3.19.17.24.6.1.1.1" style="width:116.7pt;">Reflexion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib12" title="">12</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.24.6.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.24.6.2.1">
<span class="ltx_p" id="S4.T3.19.17.24.6.2.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.24.6.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.24.6.3.1">
<span class="ltx_p" id="S4.T3.19.17.24.6.3.1.1" style="width:88.2pt;">① ③</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.24.6.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.24.6.4.1">
<span class="ltx_p" id="S4.T3.19.17.24.6.4.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.24.6.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.24.6.5.1">
<span class="ltx_p" id="S4.T3.19.17.24.6.5.1.1" style="width:88.2pt;">03/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.6.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.6.4.4.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.6.4.4.2.1">
<span class="ltx_p" id="S4.T3.6.4.4.2.1.1" style="width:116.7pt;">IGLU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib161" title="">161</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.6.4.4.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.6.4.4.3.1">
<span class="ltx_p" id="S4.T3.6.4.4.3.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.6.4.4.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.6.4.4.4.1">
<span class="ltx_p" id="S4.T3.6.4.4.4.1.1" style="width:88.2pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.6.4.4.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.6.4.4.1.1">
<span class="ltx_p" id="S4.T3.6.4.4.1.1.1" style="width:88.2pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T3.6.4.4.1.1.1.m1.1"><semantics id="S4.T3.6.4.4.1.1.1.m1.1a"><mi id="S4.T3.6.4.4.1.1.1.m1.1.1" mathvariant="normal" xref="S4.T3.6.4.4.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T3.6.4.4.1.1.1.m1.1b"><ci id="S4.T3.6.4.4.1.1.1.m1.1.1.cmml" xref="S4.T3.6.4.4.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.4.4.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.6.4.4.1.1.1.m1.1d">✓</annotation></semantics></math></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.6.4.4.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.6.4.4.5.1">
<span class="ltx_p" id="S4.T3.6.4.4.5.1.1" style="width:88.2pt;">04/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.19.17.25.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.25.7.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.25.7.1.1">
<span class="ltx_p" id="S4.T3.19.17.25.7.1.1.1" style="width:116.7pt;">Generative Agents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib20" title="">20</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.25.7.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.25.7.2.1">
<span class="ltx_p" id="S4.T3.19.17.25.7.2.1.1" style="width:88.2pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.25.7.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.25.7.3.1">
<span class="ltx_p" id="S4.T3.19.17.25.7.3.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.25.7.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.25.7.4.1">
<span class="ltx_p" id="S4.T3.19.17.25.7.4.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.25.7.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.25.7.5.1">
<span class="ltx_p" id="S4.T3.19.17.25.7.5.1.1" style="width:88.2pt;">04/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.7.5.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.7.5.5.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.7.5.5.2.1">
<span class="ltx_p" id="S4.T3.7.5.5.2.1.1" style="width:116.7pt;">ToolBench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib151" title="">151</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.7.5.5.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.7.5.5.3.1">
<span class="ltx_p" id="S4.T3.7.5.5.3.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.7.5.5.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.7.5.5.4.1">
<span class="ltx_p" id="S4.T3.7.5.5.4.1.1" style="width:88.2pt;">③</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.7.5.5.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.7.5.5.1.1">
<span class="ltx_p" id="S4.T3.7.5.5.1.1.1" style="width:88.2pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T3.7.5.5.1.1.1.m1.1"><semantics id="S4.T3.7.5.5.1.1.1.m1.1a"><mi id="S4.T3.7.5.5.1.1.1.m1.1.1" mathvariant="normal" xref="S4.T3.7.5.5.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T3.7.5.5.1.1.1.m1.1b"><ci id="S4.T3.7.5.5.1.1.1.m1.1.1.cmml" xref="S4.T3.7.5.5.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.5.5.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.7.5.5.1.1.1.m1.1d">✓</annotation></semantics></math></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.7.5.5.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.7.5.5.5.1">
<span class="ltx_p" id="S4.T3.7.5.5.5.1.1" style="width:88.2pt;">04/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.8.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.8.6.6.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.8.6.6.2.1">
<span class="ltx_p" id="S4.T3.8.6.6.2.1.1" style="width:116.7pt;">GITM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib16" title="">16</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.8.6.6.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.8.6.6.3.1">
<span class="ltx_p" id="S4.T3.8.6.6.3.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.8.6.6.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.8.6.6.4.1">
<span class="ltx_p" id="S4.T3.8.6.6.4.1.1" style="width:88.2pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.8.6.6.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.8.6.6.1.1">
<span class="ltx_p" id="S4.T3.8.6.6.1.1.1" style="width:88.2pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T3.8.6.6.1.1.1.m1.1"><semantics id="S4.T3.8.6.6.1.1.1.m1.1a"><mi id="S4.T3.8.6.6.1.1.1.m1.1.1" mathvariant="normal" xref="S4.T3.8.6.6.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T3.8.6.6.1.1.1.m1.1b"><ci id="S4.T3.8.6.6.1.1.1.m1.1.1.cmml" xref="S4.T3.8.6.6.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.6.6.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.8.6.6.1.1.1.m1.1d">✓</annotation></semantics></math></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.8.6.6.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.8.6.6.5.1">
<span class="ltx_p" id="S4.T3.8.6.6.5.1.1" style="width:88.2pt;">05/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.19.17.26.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.26.8.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.26.8.1.1">
<span class="ltx_p" id="S4.T3.19.17.26.8.1.1.1" style="width:116.7pt;">Two-Failures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib162" title="">162</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.26.8.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.26.8.2.1">
<span class="ltx_p" id="S4.T3.19.17.26.8.2.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.26.8.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.26.8.3.1">
<span class="ltx_p" id="S4.T3.19.17.26.8.3.1.1" style="width:88.2pt;">③</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.26.8.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.26.8.4.1">
<span class="ltx_p" id="S4.T3.19.17.26.8.4.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.26.8.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.26.8.5.1">
<span class="ltx_p" id="S4.T3.19.17.26.8.5.1.1" style="width:88.2pt;">05/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.9.7.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.9.7.7.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.9.7.7.2.1">
<span class="ltx_p" id="S4.T3.9.7.7.2.1.1" style="width:116.7pt;">Voyager <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib38" title="">38</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.9.7.7.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.9.7.7.3.1">
<span class="ltx_p" id="S4.T3.9.7.7.3.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.9.7.7.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.9.7.7.4.1">
<span class="ltx_p" id="S4.T3.9.7.7.4.1.1" style="width:88.2pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.9.7.7.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.9.7.7.1.1">
<span class="ltx_p" id="S4.T3.9.7.7.1.1.1" style="width:88.2pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T3.9.7.7.1.1.1.m1.1"><semantics id="S4.T3.9.7.7.1.1.1.m1.1a"><mi id="S4.T3.9.7.7.1.1.1.m1.1.1" mathvariant="normal" xref="S4.T3.9.7.7.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T3.9.7.7.1.1.1.m1.1b"><ci id="S4.T3.9.7.7.1.1.1.m1.1.1.cmml" xref="S4.T3.9.7.7.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.9.7.7.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.9.7.7.1.1.1.m1.1d">✓</annotation></semantics></math></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.9.7.7.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.9.7.7.5.1">
<span class="ltx_p" id="S4.T3.9.7.7.5.1.1" style="width:88.2pt;">05/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.10.8.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.10.8.8.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.10.8.8.2.1">
<span class="ltx_p" id="S4.T3.10.8.8.2.1.1" style="width:116.7pt;">SocKET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib163" title="">163</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.10.8.8.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.10.8.8.3.1">
<span class="ltx_p" id="S4.T3.10.8.8.3.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.10.8.8.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.10.8.8.4.1">
<span class="ltx_p" id="S4.T3.10.8.8.4.1.1" style="width:88.2pt;">② ③</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.10.8.8.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.10.8.8.1.1">
<span class="ltx_p" id="S4.T3.10.8.8.1.1.1" style="width:88.2pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T3.10.8.8.1.1.1.m1.1"><semantics id="S4.T3.10.8.8.1.1.1.m1.1a"><mi id="S4.T3.10.8.8.1.1.1.m1.1.1" mathvariant="normal" xref="S4.T3.10.8.8.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T3.10.8.8.1.1.1.m1.1b"><ci id="S4.T3.10.8.8.1.1.1.m1.1.1.cmml" xref="S4.T3.10.8.8.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.10.8.8.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.10.8.8.1.1.1.m1.1d">✓</annotation></semantics></math></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.10.8.8.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.10.8.8.5.1">
<span class="ltx_p" id="S4.T3.10.8.8.5.1.1" style="width:88.2pt;">05/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.9.9">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.11.9.9.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.11.9.9.2.1">
<span class="ltx_p" id="S4.T3.11.9.9.2.1.1" style="width:116.7pt;">MobileEnv <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib164" title="">164</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.11.9.9.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.11.9.9.3.1">
<span class="ltx_p" id="S4.T3.11.9.9.3.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.11.9.9.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.11.9.9.4.1">
<span class="ltx_p" id="S4.T3.11.9.9.4.1.1" style="width:88.2pt;">① ③</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.11.9.9.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.11.9.9.1.1">
<span class="ltx_p" id="S4.T3.11.9.9.1.1.1" style="width:88.2pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T3.11.9.9.1.1.1.m1.1"><semantics id="S4.T3.11.9.9.1.1.1.m1.1a"><mi id="S4.T3.11.9.9.1.1.1.m1.1.1" mathvariant="normal" xref="S4.T3.11.9.9.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T3.11.9.9.1.1.1.m1.1b"><ci id="S4.T3.11.9.9.1.1.1.m1.1.1.cmml" xref="S4.T3.11.9.9.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.11.9.9.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.11.9.9.1.1.1.m1.1d">✓</annotation></semantics></math></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.11.9.9.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.11.9.9.5.1">
<span class="ltx_p" id="S4.T3.11.9.9.5.1.1" style="width:88.2pt;">05/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.12.10.10">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.12.10.10.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.12.10.10.2.1">
<span class="ltx_p" id="S4.T3.12.10.10.2.1.1" style="width:116.7pt;">Clembench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib165" title="">165</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.12.10.10.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.12.10.10.3.1">
<span class="ltx_p" id="S4.T3.12.10.10.3.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.12.10.10.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.12.10.10.4.1">
<span class="ltx_p" id="S4.T3.12.10.10.4.1.1" style="width:88.2pt;">① ③</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.12.10.10.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.12.10.10.1.1">
<span class="ltx_p" id="S4.T3.12.10.10.1.1.1" style="width:88.2pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T3.12.10.10.1.1.1.m1.1"><semantics id="S4.T3.12.10.10.1.1.1.m1.1a"><mi id="S4.T3.12.10.10.1.1.1.m1.1.1" mathvariant="normal" xref="S4.T3.12.10.10.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T3.12.10.10.1.1.1.m1.1b"><ci id="S4.T3.12.10.10.1.1.1.m1.1.1.cmml" xref="S4.T3.12.10.10.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.12.10.10.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.12.10.10.1.1.1.m1.1d">✓</annotation></semantics></math></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.12.10.10.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.12.10.10.5.1">
<span class="ltx_p" id="S4.T3.12.10.10.5.1.1" style="width:88.2pt;">05/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.13.11.11">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.13.11.11.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.13.11.11.2.1">
<span class="ltx_p" id="S4.T3.13.11.11.2.1.1" style="width:116.7pt;">Dialop <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib166" title="">166</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.13.11.11.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.13.11.11.3.1">
<span class="ltx_p" id="S4.T3.13.11.11.3.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.13.11.11.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.13.11.11.4.1">
<span class="ltx_p" id="S4.T3.13.11.11.4.1.1" style="width:88.2pt;">③</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.13.11.11.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.13.11.11.1.1">
<span class="ltx_p" id="S4.T3.13.11.11.1.1.1" style="width:88.2pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T3.13.11.11.1.1.1.m1.1"><semantics id="S4.T3.13.11.11.1.1.1.m1.1a"><mi id="S4.T3.13.11.11.1.1.1.m1.1.1" mathvariant="normal" xref="S4.T3.13.11.11.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T3.13.11.11.1.1.1.m1.1b"><ci id="S4.T3.13.11.11.1.1.1.m1.1.1.cmml" xref="S4.T3.13.11.11.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.13.11.11.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.13.11.11.1.1.1.m1.1d">✓</annotation></semantics></math></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.13.11.11.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.13.11.11.5.1">
<span class="ltx_p" id="S4.T3.13.11.11.5.1.1" style="width:88.2pt;">06/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.19.17.27.9">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.27.9.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.27.9.1.1">
<span class="ltx_p" id="S4.T3.19.17.27.9.1.1.1" style="width:116.7pt;">Feldt et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib167" title="">167</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.27.9.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.27.9.2.1">
<span class="ltx_p" id="S4.T3.19.17.27.9.2.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.27.9.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.27.9.3.1">
<span class="ltx_p" id="S4.T3.19.17.27.9.3.1.1" style="width:88.2pt;">④</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.27.9.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.27.9.4.1">
<span class="ltx_p" id="S4.T3.19.17.27.9.4.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.27.9.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.27.9.5.1">
<span class="ltx_p" id="S4.T3.19.17.27.9.5.1.1" style="width:88.2pt;">06/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.19.17.28.10">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.28.10.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.28.10.1.1">
<span class="ltx_p" id="S4.T3.19.17.28.10.1.1.1" style="width:116.7pt;">CO-LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib22" title="">22</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.28.10.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.28.10.2.1">
<span class="ltx_p" id="S4.T3.19.17.28.10.2.1.1" style="width:88.2pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.28.10.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.28.10.3.1">
<span class="ltx_p" id="S4.T3.19.17.28.10.3.1.1" style="width:88.2pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.28.10.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.28.10.4.1">
<span class="ltx_p" id="S4.T3.19.17.28.10.4.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.28.10.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.28.10.5.1">
<span class="ltx_p" id="S4.T3.19.17.28.10.5.1.1" style="width:88.2pt;">07/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.14.12.12">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.14.12.12.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.14.12.12.2.1">
<span class="ltx_p" id="S4.T3.14.12.12.2.1.1" style="width:116.7pt;">Tachikuma <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib168" title="">168</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.14.12.12.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.14.12.12.3.1">
<span class="ltx_p" id="S4.T3.14.12.12.3.1.1" style="width:88.2pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.14.12.12.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.14.12.12.4.1">
<span class="ltx_p" id="S4.T3.14.12.12.4.1.1" style="width:88.2pt;">① ③</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.14.12.12.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.14.12.12.1.1">
<span class="ltx_p" id="S4.T3.14.12.12.1.1.1" style="width:88.2pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T3.14.12.12.1.1.1.m1.1"><semantics id="S4.T3.14.12.12.1.1.1.m1.1a"><mi id="S4.T3.14.12.12.1.1.1.m1.1.1" mathvariant="normal" xref="S4.T3.14.12.12.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T3.14.12.12.1.1.1.m1.1b"><ci id="S4.T3.14.12.12.1.1.1.m1.1.1.cmml" xref="S4.T3.14.12.12.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.14.12.12.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.14.12.12.1.1.1.m1.1d">✓</annotation></semantics></math></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.14.12.12.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.14.12.12.5.1">
<span class="ltx_p" id="S4.T3.14.12.12.5.1.1" style="width:88.2pt;">07/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.15.13.13">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.15.13.13.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.15.13.13.2.1">
<span class="ltx_p" id="S4.T3.15.13.13.2.1.1" style="width:116.7pt;">RocoBench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib92" title="">92</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.15.13.13.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.15.13.13.3.1">
<span class="ltx_p" id="S4.T3.15.13.13.3.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.15.13.13.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.15.13.13.4.1">
<span class="ltx_p" id="S4.T3.15.13.13.4.1.1" style="width:88.2pt;">① ③</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.15.13.13.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.15.13.13.1.1">
<span class="ltx_p" id="S4.T3.15.13.13.1.1.1" style="width:88.2pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T3.15.13.13.1.1.1.m1.1"><semantics id="S4.T3.15.13.13.1.1.1.m1.1a"><mi id="S4.T3.15.13.13.1.1.1.m1.1.1" mathvariant="normal" xref="S4.T3.15.13.13.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T3.15.13.13.1.1.1.m1.1b"><ci id="S4.T3.15.13.13.1.1.1.m1.1.1.cmml" xref="S4.T3.15.13.13.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.15.13.13.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.15.13.13.1.1.1.m1.1d">✓</annotation></semantics></math></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.15.13.13.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.15.13.13.5.1">
<span class="ltx_p" id="S4.T3.15.13.13.5.1.1" style="width:88.2pt;">07/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.19.17.29.11">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.29.11.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.29.11.1.1">
<span class="ltx_p" id="S4.T3.19.17.29.11.1.1.1" style="width:116.7pt;">AgentSims <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib34" title="">34</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.29.11.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.29.11.2.1">
<span class="ltx_p" id="S4.T3.19.17.29.11.2.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.29.11.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.29.11.3.1">
<span class="ltx_p" id="S4.T3.19.17.29.11.3.1.1" style="width:88.2pt;">②</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.29.11.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.29.11.4.1">
<span class="ltx_p" id="S4.T3.19.17.29.11.4.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.29.11.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.29.11.5.1">
<span class="ltx_p" id="S4.T3.19.17.29.11.5.1.1" style="width:88.2pt;">08/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.16.14.14">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.16.14.14.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.16.14.14.2.1">
<span class="ltx_p" id="S4.T3.16.14.14.2.1.1" style="width:116.7pt;">AgentBench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib169" title="">169</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.16.14.14.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.16.14.14.3.1">
<span class="ltx_p" id="S4.T3.16.14.14.3.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.16.14.14.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.16.14.14.4.1">
<span class="ltx_p" id="S4.T3.16.14.14.4.1.1" style="width:88.2pt;">③</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.16.14.14.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.16.14.14.1.1">
<span class="ltx_p" id="S4.T3.16.14.14.1.1.1" style="width:88.2pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T3.16.14.14.1.1.1.m1.1"><semantics id="S4.T3.16.14.14.1.1.1.m1.1a"><mi id="S4.T3.16.14.14.1.1.1.m1.1.1" mathvariant="normal" xref="S4.T3.16.14.14.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T3.16.14.14.1.1.1.m1.1b"><ci id="S4.T3.16.14.14.1.1.1.m1.1.1.cmml" xref="S4.T3.16.14.14.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.16.14.14.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.16.14.14.1.1.1.m1.1d">✓</annotation></semantics></math></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.16.14.14.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.16.14.14.5.1">
<span class="ltx_p" id="S4.T3.16.14.14.5.1.1" style="width:88.2pt;">08/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.17.15.15">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.17.15.15.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.17.15.15.2.1">
<span class="ltx_p" id="S4.T3.17.15.15.2.1.1" style="width:116.7pt;">BOLAA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib170" title="">170</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.17.15.15.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.17.15.15.3.1">
<span class="ltx_p" id="S4.T3.17.15.15.3.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.17.15.15.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.17.15.15.4.1">
<span class="ltx_p" id="S4.T3.17.15.15.4.1.1" style="width:88.2pt;">③</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.17.15.15.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.17.15.15.1.1">
<span class="ltx_p" id="S4.T3.17.15.15.1.1.1" style="width:88.2pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T3.17.15.15.1.1.1.m1.1"><semantics id="S4.T3.17.15.15.1.1.1.m1.1a"><mi id="S4.T3.17.15.15.1.1.1.m1.1.1" mathvariant="normal" xref="S4.T3.17.15.15.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T3.17.15.15.1.1.1.m1.1b"><ci id="S4.T3.17.15.15.1.1.1.m1.1.1.cmml" xref="S4.T3.17.15.15.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.17.15.15.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.17.15.15.1.1.1.m1.1d">✓</annotation></semantics></math></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.17.15.15.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.17.15.15.5.1">
<span class="ltx_p" id="S4.T3.17.15.15.5.1.1" style="width:88.2pt;">08/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.18.16.16">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.18.16.16.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.18.16.16.2.1">
<span class="ltx_p" id="S4.T3.18.16.16.2.1.1" style="width:116.7pt;">Gentopia <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib171" title="">171</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.18.16.16.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.18.16.16.3.1">
<span class="ltx_p" id="S4.T3.18.16.16.3.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.18.16.16.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.18.16.16.4.1">
<span class="ltx_p" id="S4.T3.18.16.16.4.1.1" style="width:88.2pt;">③</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.18.16.16.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.18.16.16.1.1">
<span class="ltx_p" id="S4.T3.18.16.16.1.1.1" style="width:88.2pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T3.18.16.16.1.1.1.m1.1"><semantics id="S4.T3.18.16.16.1.1.1.m1.1a"><mi id="S4.T3.18.16.16.1.1.1.m1.1.1" mathvariant="normal" xref="S4.T3.18.16.16.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T3.18.16.16.1.1.1.m1.1b"><ci id="S4.T3.18.16.16.1.1.1.m1.1.1.cmml" xref="S4.T3.18.16.16.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.18.16.16.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.18.16.16.1.1.1.m1.1d">✓</annotation></semantics></math></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.18.16.16.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.18.16.16.5.1">
<span class="ltx_p" id="S4.T3.18.16.16.5.1.1" style="width:88.2pt;">08/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.19.17.17">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.17.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.17.2.1">
<span class="ltx_p" id="S4.T3.19.17.17.2.1.1" style="width:116.7pt;">EmotionBench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib172" title="">172</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.17.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.17.3.1">
<span class="ltx_p" id="S4.T3.19.17.17.3.1.1" style="width:88.2pt;">①</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.17.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.17.4.1">
<span class="ltx_p" id="S4.T3.19.17.17.4.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.17.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.17.1.1">
<span class="ltx_p" id="S4.T3.19.17.17.1.1.1" style="width:88.2pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T3.19.17.17.1.1.1.m1.1"><semantics id="S4.T3.19.17.17.1.1.1.m1.1a"><mi id="S4.T3.19.17.17.1.1.1.m1.1.1" mathvariant="normal" xref="S4.T3.19.17.17.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T3.19.17.17.1.1.1.m1.1b"><ci id="S4.T3.19.17.17.1.1.1.m1.1.1.cmml" xref="S4.T3.19.17.17.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.19.17.17.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.19.17.17.1.1.1.m1.1d">✓</annotation></semantics></math></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.19.17.17.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.17.5.1">
<span class="ltx_p" id="S4.T3.19.17.17.5.1.1" style="width:88.2pt;">08/2023</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.19.17.30.12">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T3.19.17.30.12.1" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.30.12.1.1">
<span class="ltx_p" id="S4.T3.19.17.30.12.1.1.1" style="width:116.7pt;">PTB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib125" title="">125</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T3.19.17.30.12.2" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.30.12.2.1">
<span class="ltx_p" id="S4.T3.19.17.30.12.2.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T3.19.17.30.12.3" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.30.12.3.1">
<span class="ltx_p" id="S4.T3.19.17.30.12.3.1.1" style="width:88.2pt;">④</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T3.19.17.30.12.4" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.30.12.4.1">
<span class="ltx_p" id="S4.T3.19.17.30.12.4.1.1" style="width:88.2pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T3.19.17.30.12.5" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.19.17.30.12.5.1">
<span class="ltx_p" id="S4.T3.19.17.30.12.5.1.1" style="width:88.2pt;">08/2023</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Objective Evaluation</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Objective evaluation refers to assessing the capabilities of LLM-based autonomous agents using quantitative metrics that can be computed, compared and tracked over time.
In contrast to subjective evaluation, objective metrics aim to provide concrete, measurable insights into the agent performance.
For conducting objective evaluation, there are three important aspects, that is, the evaluation metrics, protocols and benchmarks.
In the following, we introduce these aspects more in detail.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">Metrics</span>:
In order to objectively evaluate the effectiveness of the agents, designing proper metrics is significant, which may influence the evaluation accuracy and comprehensiveness.
Ideal evaluation metrics should precisely reflect the quality of the agents, and align with the human feelings when using them in real-world scenarios.
In existing work, we can conclude the following representative evaluation metrics.
(1) <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.2">Task success metrics:</span> These metrics measure how well an agent can complete tasks and achieve goals. Common metrics include success rate <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib58" title="">58</a>]</cite>, reward/score <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib161" title="">161</a>]</cite>, coverage <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib16" title="">16</a>]</cite>, and accuracy/error rate <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib80" title="">80</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib101" title="">101</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib40" title="">40</a>]</cite>.
Depending on the scenario, accuracy may reflect aspects such as program executability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib18" title="">18</a>]</cite> or task validity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib101" title="">101</a>]</cite>.
Higher values across these task success metrics indicate greater task completion ability.
(2) <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.3">Human similarity metrics:</span>
These metrics quantify the degree to which the agent behaviors closely resembles those of humans by emphasizing various aspects related to human traits, such as coherent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib104" title="">104</a>]</cite>, fluent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib104" title="">104</a>]</cite>, dialogue similarities with human <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib80" title="">80</a>]</cite> and human acceptance rate <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib101" title="">101</a>]</cite>.
Higher similarity suggests better human simulation performance.
(3) <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.4">Efficiency metrics:</span>
In contrast to the aforementioned metrics used to evaluate the agent effectiveness, these metrics aim to assess the efficiency of agent.
Commonly considered metrics encompass the cost associated with development <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib18" title="">18</a>]</cite> and training efficiency <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib38" title="">38</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.1">Protocols</span>:
In addition to the evaluation metrics, another important aspect for objective evaluation is how to leverage these metrics.
In the previous work, we can identify the following commonly used evaluation protocols:
(1) <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.2">Real-world simulation:</span>
In this method, the agents are evaluated within immersive environments like games and interactive simulators.
The agents are required to perform tasks autonomously, and then metrics like task success rate and human similarity are leveraged to evaluate the capability of the agents based on their trajectories and completed objectives <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib161" title="">161</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib168" title="">168</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib86" title="">86</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib164" title="">164</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib12" title="">12</a>]</cite>.
By simulating real-world scenarios, this approach aims to provide a comprehensive evaluation of the agents’ practical capabilities.
(2) <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.3">Social evaluation</span>:
This method utilizes metrics to assess social intelligence based on the agent interactions in simulated societies.
Various approaches have been adopted, such as collaborative tasks to evaluate teamwork skills, debates to analyze argumentative reasoning, and human studies to measure social aptitude <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib80" title="">80</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib101" title="">101</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib163" title="">163</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib34" title="">34</a>]</cite>. These approaches analyze qualities such as coherence, theory of mind, and social IQ to assess agents’ capabilities in areas including cooperation, communication, empathy, and mimicking human social behavior. By subjecting agents to complex interactive settings, social evaluation provides valuable insights into agents’ higher-level social cognition.
(3) <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.4">Multi-task evaluation:</span> In this method, people use a set of diverse tasks from different domains to evaluate the agent, which can effectively measure the agent generalization capability in open-domain environments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib163" title="">163</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib151" title="">151</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib169" title="">169</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib170" title="">170</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib86" title="">86</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib164" title="">164</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib162" title="">162</a>]</cite>.
(4) <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.5">Software testing:</span> In this method, researchers evaluate the agents by letting them conduct tasks such as software testing tasks, such as generating test cases, reproducing bugs, debugging code, and interacting with developers and external tools <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib160" title="">160</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib159" title="">159</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib167" title="">167</a>]</cite>. Then, one can use metrics like test coverage and bug detection rate to measure the effectiveness of LLM-based agents.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p4.1.1">Benchmarks</span>:
Given the metrics and protocols, a crucial aspect of evaluation is the selection of appropriate benchmarks. Over time, various benchmarks have been introduced to assess the capabilities of LLM-based agents across diverse domains and scenarios.
Many studies employ environments such as ALFWorld <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib60" title="">60</a>]</cite>, IGLU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib161" title="">161</a>]</cite>, and Minecraft <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib33" title="">33</a>]</cite> to evaluate agent capabilities in interactive and task-oriented simulations.
Tachikuma <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib168" title="">168</a>]</cite> evaluates LLMs’ ability to infer and understand complex interactions involving multiple characters and novel objects through TRPG game logs.
AgentBench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib169" title="">169</a>]</cite> provides a comprehensive framework for evaluating LLMs as autonomous agents across diverse environments. It represents the first systematic assessment of LLMs as agents on real-world challenges across diverse domains.
SocKET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib163" title="">163</a>]</cite> is a comprehensive benchmark for evaluating the social capabilities of LLMs across 58 tasks covering five categories of social information such as humor and sarcasm, emotions and feelings, credibility, etc.
AgentSims <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib34" title="">34</a>]</cite> is a versatile framework for evaluating LLM-based agents, where one can flexibly design the agent planning, memory and action strategies, and measure the effectiveness of different agent modules in interactive environments.
ToolBench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib151" title="">151</a>]</cite> focuses on assessing and enhancing language models’ ability to use tools, featuring 16,464 real-world RESTful APIs and diverse instructions tailored for single- and multi-tool scenarios.
WebShop <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib86" title="">86</a>]</cite> develops a benchmark for evaluating LLM-based agents in terms of their capabilities for product search and retrieval, which is constructed using a collection of 1.18 million real-world items.
Mobile-Env <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib164" title="">164</a>]</cite> serves as an extendable interactive platform designed to evaluate the multi-step interaction capabilities of LLM-based agents.
WebArena <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib173" title="">173</a>]</cite> offers a comprehensive website environment that spans multiple domains. Its purpose is to evaluate agents in an end-to-end fashion and determine the accuracy of their completed tasks.
GentBench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib171" title="">171</a>]</cite> is crafted to evaluate the agent capabilities, including their reasoning, safety, and efficiency, when utilizing tools to complete complex tasks.
RocoBench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib92" title="">92</a>]</cite> comprises six tasks that evaluate multi-agent collaboration across diverse scenarios, emphasizing communication and coordination strategies to assess adaptability and generalization in cooperative robotics.
EmotionBench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib172" title="">172</a>]</cite> evaluates the emotion appraisal ability of LLMs, i.e., how their feelings change when presented with specific situations. It collects over 400 situations that elicit eight negative emotions and measures the emotional states of LLMs and human subjects using self-report scales.
PEB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib125" title="">125</a>]</cite> is tailored for assessing LLM-based agents in penetration testing scenarios, comprising 13 diverse targets from leading platforms. It offers a structured evaluation across varying difficulty levels, reflecting real-world challenges for agents.
ClemBench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib165" title="">165</a>]</cite> contains five Dialogue Games to assess LLMs’ ability as a player.
E2E <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib174" title="">174</a>]</cite> serves as an end-to-end benchmark for testing the accuracy and usefulness of chatbots.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremarkx8">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic" id="Thmremarkx8.1.1.1">Remark</span></span><span class="ltx_text ltx_font_italic" id="Thmremarkx8.2.2">.</span>
</h6>
<div class="ltx_para" id="Thmremarkx8.p1">
<p class="ltx_p" id="Thmremarkx8.p1.1">Objective evaluation facilitates the quantitative analysis of capabilities in LLM-based agents through a variety of metrics.
While current techniques can not perfectly measure all types of agent capabilities, objective evaluation provides essential insights that complement subjective assessment.
Continued advancements in benchmarks and methodologies for objective evaluation will enhance the development and understanding of LLM-based autonomous agents further.</p>
</div>
<div class="ltx_para" id="Thmremarkx8.p2">
<p class="ltx_p" id="Thmremarkx8.p2.1">In the above sections, we introduce both subjective and objective strategies for LLM-based autonomous agent evaluation.
The evaluation of the agents play significant roles in this domain. However, both subjective and objective evaluation have their own strengths and weakness.
Maybe, in practice, they should be combined to comprehensively evaluate the agents.
We summarize the correspondence between the previous work and these evaluation strategies in Table <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#S4.T3" title="Table 3 ‣ 4.1 Subjective Evaluation ‣ 4 LLM-based Autonomous Agent Evaluation ‣ A Survey on Large Language Model based Autonomous Agents"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Surveys</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">With the vigorous development of large language models, a variety of comprehensive surveys have emerged, providing detailed insights into various aspects.  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib175" title="">175</a>]</cite> extensively introduces the background, main findings, and mainstream technologies of LLMs, encompassing a vast array of existing works. On the other hand,  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib176" title="">176</a>]</cite> primarily focus on the applications of LLMs in various downstream tasks and the challenges associated with their deployment. Aligning LLMs with human intelligence is an active area of research to address concerns such as biases and illusions.  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib177" title="">177</a>]</cite> have compiled existing techniques for human alignment, including data collection and model training methodologies. Reasoning is a crucial aspect of intelligence, influencing decision-making, problem-solving, and other cognitive abilities.
 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib178" title="">178</a>]</cite> presents the current state of research on LLMs’ reasoning abilities, exploring approaches to improve and evaluate their reasoning skills.  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib179" title="">179</a>]</cite> propose that language models can be enhanced with reasoning capabilities and the ability to utilize tools, termed Augmented Language Models (ALMs). They conduct a comprehensive review of the latest advancements in ALMs. As the utilization of large-scale models becomes more prevalent, evaluating their performance is increasingly critical.
 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib180" title="">180</a>]</cite> shed light on evaluating LLMs, addressing what to evaluate, where to evaluate, and how to assess their performance in downstream tasks and societal impact.
 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib181" title="">181</a>]</cite> also discusses the capabilities and limitations of LLMs in various downstream tasks.
The aforementioned research encompasses various aspects of large models, including training, application, and evaluation. However, prior to this paper, no work has specifically focused on the rapidly emerging and highly promising field of LLM-based Agents. In this study, we have compiled 100 relevant works on LLM-based Agents, covering their construction, applications, and evaluation processes.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Challenges</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">While previous work on LLM-based autonomous agent has obtained many remarkable successes, this field is still at its initial stage, and there are several significant challenges that need to be addressed in its development. In the following, we present many representative challenges.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Role-playing Capability</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">Different from traditional LLMs, autonomous agent usually has to play as specific roles (<em class="ltx_emph ltx_font_italic" id="S6.SS1.p1.1.1">e.g.</em>, program coder, researcher and chemist) for accomplishing different tasks.
Thus, the capability of the agent for role-playing is very important.
Although LLMs can effectively simulate many common roles such as movie reviewers, there are still various roles and aspects that they struggle to capture accurately.
To begin with, LLMs are usually trained based on web-corpus, thus for the roles which are seldom discussed on the web or the newly emerging roles, LLMs may not simulate them well.
In addition, previous research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib30" title="">30</a>]</cite> has shown that existing LLMs may not well model the human cognitive psychology characters, leading to the lack of self-awareness in conversation scenarios.
Potential solution to these problems may include fine-tuning LLMs or carefully designing the agent prompts/architectures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib182" title="">182</a>]</cite>.
For example, one can firstly collect real-human data for uncommon roles or psychology characters, and then leverage it to fine-tune LLMs.
However, how to ensure that fine-tuned model still perform well for the common roles may pose further challenges.
Beyond fine-tuning, one can also design tailored agent prompts/architectures to enhance the capability of LLM on role-playing.
However, finding the optimal prompts/architectures is not easy, since their designing spaces are too large.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Generalized Human Alignment</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">Human alignment has been discussed a lot for traditional LLMs.
In the field of LLM-based autonomous agent, especially when the agents are leveraged for simulation, we believe this concept should be discussed more in depth.
In order to better serve human-beings, traditional LLMs are usually fine-tuned to be aligned with correct human values, for example, the agent should not plan to make a bomb for avenging society.
However, when the agents are leveraged for real-world simulation, an ideal simulator should be able to honestly depict diverse human traits, including the ones with incorrect values.
Actually, simulating the human negative aspects can be even more important, since an important goal of simulation is to discover and solve problems, and without negative aspects means no problem to be solved.
For example, to simulate the real-world society, we may have to allow the agent to plan for making a bomb, and observe how it will act to implement the plan as well as the influence of its behaviors.
Based on these observations, people can make better actions to stop similar behaviors in real-world society.
Inspired by the above case, maybe an important problem for agent-based simulation is how to conduct generalized human alignment, that is, for different purposes and applications, the agent should be able to align with diverse human values.
However, existing powerful LLMs including ChatGPT and GPT-4 are mostly aligned with unified human values.
Thus, an interesting direction is how to “realign” these models by designing proper prompting strategies.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Prompt Robustness</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">To ensure rational behavior in agents, it’s a common practice for designers to embed supplementary modules, such as memory and planning modules, into LLMs. However, the inclusion of these modules necessitates the development of more complex prompts in order to facilitate consistent operation and effective communication. Previous research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib183" title="">183</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib184" title="">184</a>]</cite> has highlighted the lack of robustness in prompts for LLMs, as even minor alterations can yield substantially different outcomes. This issue becomes more pronounced when constructing autonomous agents, as they encompass not a single prompt but a prompt framework that considers all modules, wherein the prompt for one module has the potential to influence others.
Moreover, the prompt frameworks can vary significantly across different LLMs.
The development of a unified and resilient prompt framework applicable across diverse LLMs remains a critical and unresolved challenge.
There are two potential solutions to the aforementioned problems: (1) manually crafting the essential prompt elements through trial and error, or (2) automatically generating prompts using GPT.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Hallucination</h3>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">Hallucination poses a fundamental challenge for LLMs, characterized by the models’ tendency to produce false information with a high level of confidence. This challenge is not limited to LLMs alone but is also a significant concern in the domain of autonomous agents. For instance, in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib185" title="">185</a>]</cite>, it was observed that when confronted with simplistic instructions during code generation tasks, the agent may exhibit hallucinatory behavior. Hallucination can lead to serious consequences such as incorrect or misleading code, security risks, and ethical issues <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib185" title="">185</a>]</cite>. To mitigate this issue, incorporating human correction feedback directly into the iterative process of human-agent interaction presents a viable approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib23" title="">23</a>]</cite>.
More discussions on the hallucination problem can be seen in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib175" title="">175</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5 </span>Knowledge Boundary</h3>
<div class="ltx_para" id="S6.SS5.p1">
<p class="ltx_p" id="S6.SS5.p1.1">A pivotal application of LLM-based autonomous agents lies in simulating diverse real-world human behaviors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2308.11432v7#bib.bib20" title="">20</a>]</cite>.
The study of human simulation has a long history, and the recent surge in interest can be attributed to the remarkable advancements made by LLMs, which have demonstrated significant capabilities in simulating human behavior.
However, it is important to recognize that the power of LLMs may not always be advantageous. Specifically, an ideal simulation should accurately replicate human knowledge. In this context, LLMs may display overwhelming capabilities, being trained on a vast corpus of web knowledge that far exceeds what an average individual might know.
The immense capabilities of LLMs can significantly impact the effectiveness of simulations. For instance, when attempting to simulate user selection behaviors for various movies, it is crucial to ensure that LLMs assume a position of having no prior knowledge of these movies. However, there is a possibility that LLMs have already acquired information about these movies. Without implementing appropriate strategies, LLMs may make decisions based on their extensive knowledge, even though real-world users would not have access to the contents of these movies beforehand.
Based on the above example, we may conclude that for building believable agent simulation environment, an important problem is how to constrain the utilization of user-unknown knowledge of LLM.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.6 </span>Efficiency</h3>
<div class="ltx_para" id="S6.SS6.p1">
<p class="ltx_p" id="S6.SS6.p1.1">Due to their autoregressive architecture, LLMs typically have slow inference speeds.
However, the agent may need to query LLMs for each action multiple times, such as extracting information from memory, make plans before taking actions and so on.
Consequently, the efficiency of agent actions is greatly affected by the speed of LLM inference.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this survey, we systematically summarize existing research in the field of LLM-based autonomous agents.
We present and review these studies from three aspects
including the construction, application, and evaluation of the agents.
For each of these aspects, we provide a detailed taxonomy to draw connections among the existing research, summarizing the major techniques and their development histories.
In addition to reviewing the previous work, we also propose several challenges in this field, which are expected to guide potential future directions.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work is supported in part by National Natural Science Foundation of China (No. 62102420), Beijing Outstanding Young Scientist Program NO. BJJWZYJH012019100020098, Intelligent Social Governance Platform, Major Innovation &amp; Planning Interdisciplinary Platform for the "Double-First Class" Initiative, Renmin University of China, Public Computing Cloud, Renmin University of China, fund for building world-class universities (disciplines) of Renmin University of China, Intelligent Social Governance Platform.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Mnih V, Kavukcuoglu K, Silver D, Rusu A A, Veness J, Bellemare M G, Graves A,
Riedmiller M, Fidjeland A K, Ostrovski G, others .

</span>
<span class="ltx_bibblock">Human-level control through deep reinforcement learning.

</span>
<span class="ltx_bibblock">nature, 2015, 518(7540): 529–533

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Lillicrap T P, Hunt J J, Pritzel A, Heess N, Erez T, Tassa Y, Silver D,
Wierstra D.

</span>
<span class="ltx_bibblock">Continuous control with deep reinforcement learning.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1509.02971, 2015

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Schulman J, Wolski F, Dhariwal P, Radford A, Klimov O.

</span>
<span class="ltx_bibblock">Proximal policy optimization algorithms.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1707.06347, 2017

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Haarnoja T, Zhou A, Abbeel P, Levine S.

</span>
<span class="ltx_bibblock">Soft actor-critic: Off-policy maximum entropy deep reinforcement
learning with a stochastic actor.

</span>
<span class="ltx_bibblock">In: International conference on machine learning.

</span>
<span class="ltx_bibblock">2018, 1861–1870

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Brown T, Mann B, Ryder N, Subbiah M, Kaplan J D, Dhariwal P, Neelakantan A,
Shyam P, Sastry G, Askell A, others .

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock">Advances in neural information processing systems, 2020, 33:
1877–1901

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I, others .

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock">OpenAI blog, 2019, 1(8): 9

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Achiam J, Adler S, Agarwal S, Ahmad L, Akkaya I, Aleman F L, Almeida D,
Altenschmidt J, Altman S, Anadkat S, others .

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2303.08774, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Anthropic .

</span>
<span class="ltx_bibblock">Model card and evaluations for claude models.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf?ref=maginative.com" title="">https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf?ref=maginative.com</a>,
2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Touvron H, Lavril T, Izacard G, Martinet X, Lachaux M A, Lacroix T, Rozière
B, Goyal N, Hambro E, Azhar F, others .

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2302.13971, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Touvron H, Martin L, Stone K, Albert P, Almahairi A, Babaei Y, Bashlykov N,
Batra S, Bhargava P, Bhosale S, others .

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2307.09288, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Chen X, Li S, Li H, Jiang S, Qi Y, Song L.

</span>
<span class="ltx_bibblock">Generative adversarial user model for reinforcement learning based
recommendation system.

</span>
<span class="ltx_bibblock">In: International Conference on Machine Learning.

</span>
<span class="ltx_bibblock">2019, 1052–1061

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Shinn N, Cassano F, Gopinath A, Narasimhan K, Yao S.

</span>
<span class="ltx_bibblock">Reflexion: Language agents with verbal reinforcement learning.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems, 2024, 36

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Shen Y, Song K, Tan X, Li D, Lu W, Zhuang Y.

</span>
<span class="ltx_bibblock">Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging
face.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems, 2024, 36

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Qin Y, Liang S, Ye Y, Zhu K, Yan L, Lu Y, Lin Y, Cong X, Tang X, Qian B, others
.

</span>
<span class="ltx_bibblock">Toolllm: Facilitating large language models to master 16000+
real-world apis.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2307.16789, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Schick T, Dwivedi-Yu J, Dessì R, Raileanu R, Lomeli M, Hambro E,
Zettlemoyer L, Cancedda N, Scialom T.

</span>
<span class="ltx_bibblock">Toolformer: Language models can teach themselves to use tools.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems, 2024, 36

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Zhu X, Chen Y, Tian H, Tao C, Su W, Yang C, Huang G, Li B, Lu L, Wang X, others
.

</span>
<span class="ltx_bibblock">Ghost in the minecraft: Generally capable agents for open-world
enviroments via large language models with text-based knowledge and memory.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.17144, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Sclar M, Kumar S, West P, Suhr A, Choi Y, Tsvetkov Y.

</span>
<span class="ltx_bibblock">Minding language models’(lack of) theory of mind: A plug-and-play
multi-character belief tracker.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2306.00924, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Qian C, Cong X, Yang C, Chen W, Su Y, Xu J, Liu Z, Sun M.

</span>
<span class="ltx_bibblock">Communicative agents for software development.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2307.07924, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
al. e C.

</span>
<span class="ltx_bibblock">Agentverse.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/OpenBMB/AgentVerse" title="">https://github.com/OpenBMB/AgentVerse</a>, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Park J S, O’Brien J, Cai C J, Morris M R, Liang P, Bernstein M S.

</span>
<span class="ltx_bibblock">Generative agents: Interactive simulacra of human behavior.

</span>
<span class="ltx_bibblock">In: Proceedings of the 36th Annual ACM Symposium on User Interface
Software and Technology.

</span>
<span class="ltx_bibblock">2023, 1–22

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Wang L, Zhang J, Chen X, Lin Y, Song R, Zhao W X, Wen J R.

</span>
<span class="ltx_bibblock">Recagent: A novel simulation paradigm for recommender systems.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2306.02552, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Zhang H, Du W, Shan J, Zhou Q, Du Y, Tenenbaum J B, Shu T, Gan C.

</span>
<span class="ltx_bibblock">Building cooperative embodied agents modularly with large language
models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2307.02485, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Hong S, Zheng X, Chen J, Cheng Y, Wang J, Zhang C, Wang Z, Yau S K S, Lin Z,
Zhou L, others .

</span>
<span class="ltx_bibblock">Metagpt: Meta programming for multi-agent collaborative framework.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.00352, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Dong Y, Jiang X, Jin Z, Li G.

</span>
<span class="ltx_bibblock">Self-collaboration code generation via chatgpt.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2304.07590, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Safdari M, Serapio-García G, Crepy C, Fitz S, Romero P, Sun L, Abdulhai M,
Faust A, Matarić M.

</span>
<span class="ltx_bibblock">Personality traits in large language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2307.00184, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Johnson J A.

</span>
<span class="ltx_bibblock">Measuring thirty facets of the five factor model with a 120-item
public domain inventory: Development of the ipip-neo-120.

</span>
<span class="ltx_bibblock">Journal of research in personality, 2014, 51: 78–89

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
John O P, Donahue E M, Kentle R L.

</span>
<span class="ltx_bibblock">Big five inventory.

</span>
<span class="ltx_bibblock">Journal of Personality and Social Psychology, 1991

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Deshpande A, Murahari V, Rajpurohit T, Kalyan A, Narasimhan K.

</span>
<span class="ltx_bibblock">Toxicity in chatgpt: Analyzing persona-assigned language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2304.05335, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Argyle L P, Busby E C, Fulda N, Gubler J R, Rytting C, Wingate D.

</span>
<span class="ltx_bibblock">Out of one, many: Using language models to simulate human samples.

</span>
<span class="ltx_bibblock">Political Analysis, 2023, 31(3): 337–351

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Fischer K A.

</span>
<span class="ltx_bibblock">Reflective linguistic programming (rlp): A stepping stone in
socially-aware agi (socialagi).

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.12647, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Rana K, Haviland J, Garg S, Abou-Chakra J, Reid I, Suenderhauf N.

</span>
<span class="ltx_bibblock">Sayplan: Grounding large language models using 3d scene graphs for
scalable robot task planning.

</span>
<span class="ltx_bibblock">In: 7th Annual Conference on Robot Learning.

</span>
<span class="ltx_bibblock">2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Zhu A, Martin L, Head A, Callison-Burch C.

</span>
<span class="ltx_bibblock">Calypso: Llms as dungeon master’s assistants.

</span>
<span class="ltx_bibblock">In: Proceedings of the AAAI Conference on Artificial Intelligence and
Interactive Digital Entertainment.

</span>
<span class="ltx_bibblock">2023, 380–390

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Wang Z, Cai S, Chen G, Liu A, Ma X, Liang Y.

</span>
<span class="ltx_bibblock">Describe, explain, plan and select: Interactive planning with large
language models enables open-world multi-task agents.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2302.01560, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Lin J, Zhao H, Zhang A, Wu Y, Ping H, Chen Q.

</span>
<span class="ltx_bibblock">Agentsims: An open-source sandbox for large language model
evaluation.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.04026, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Liang X, Wang B, Huang H, Wu S, Wu P, Lu L, Ma Z, Li Z.

</span>
<span class="ltx_bibblock">Unleashing infinite-length input capacity for large-scale language
models with self-controlled memory system.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2304.13343, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Ng Y, Miyashita D, Hoshi Y, Morioka Y, Torii O, Kodama T, Deguchi J.

</span>
<span class="ltx_bibblock">Simplyretrieve: A private and lightweight retrieval-centric
generative ai tool.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.03983, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Huang Z, Gutierrez S, Kamana H, MacNeil S.

</span>
<span class="ltx_bibblock">Memory sandbox: Transparent and interactive memory management for
conversational agents.

</span>
<span class="ltx_bibblock">In: Adjunct Proceedings of the 36th Annual ACM Symposium on User
Interface Software and Technology.

</span>
<span class="ltx_bibblock">2023, 1–3

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Wang G, Xie Y, Jiang Y, Mandlekar A, Xiao C, Zhu Y, Fan L, Anandkumar A.

</span>
<span class="ltx_bibblock">Voyager: An open-ended embodied agent with large language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.16291, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Zhong W, Guo L, Gao Q, Wang Y.

</span>
<span class="ltx_bibblock">Memorybank: Enhancing large language models with long-term memory.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.10250, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Hu C, Fu J, Du C, Luo S, Zhao J, Zhao H.

</span>
<span class="ltx_bibblock">Chatdb: Augmenting llms with databases as their symbolic memory.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2306.03901, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Modarressi A, Imani A, Fayyaz M, Schütze H.

</span>
<span class="ltx_bibblock">Ret-llm: Towards a general read-write memory for large language
models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.14322, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Schuurmans D.

</span>
<span class="ltx_bibblock">Memory augmented large language models are computationally universal.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2301.04589, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Zhao A, Huang D, Xu Q, Lin M, Liu Y J, Huang G.

</span>
<span class="ltx_bibblock">Expel: Llm agents are experiential learners.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.10144, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Huang W, Abbeel P, Pathak D, Mordatch I.

</span>
<span class="ltx_bibblock">Language models as zero-shot planners: Extracting actionable
knowledge for embodied agents.

</span>
<span class="ltx_bibblock">In: International Conference on Machine Learning.

</span>
<span class="ltx_bibblock">2022, 9118–9147

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Wei J, Wang X, Schuurmans D, Bosma M, Xia F, Chi E, Le Q V, Zhou D, others .

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language
models.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems, 2022, 35:
24824–24837

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Kojima T, Gu S S, Reid M, Matsuo Y, Iwasawa Y.

</span>
<span class="ltx_bibblock">Large language models are zero-shot reasoners.

</span>
<span class="ltx_bibblock">Advances in neural information processing systems, 2022, 35:
22199–22213

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Raman S S, Cohen V, Rosen E, Idrees I, Paulius D, Tellex S.

</span>
<span class="ltx_bibblock">Planning with large language models via corrective re-prompting.

</span>
<span class="ltx_bibblock">In: NeurIPS 2022 Foundation Models for Decision Making Workshop.

</span>
<span class="ltx_bibblock">2022

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Xu B, Peng Z, Lei B, Mukherjee S, Liu Y, Xu D.

</span>
<span class="ltx_bibblock">Rewoo: Decoupling reasoning from observations for efficient augmented
language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.18323, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Lin B Y, Fu Y, Yang K, Brahman F, Huang S, Bhagavatula C, Ammanabrolu P, Choi
Y, Ren X.

</span>
<span class="ltx_bibblock">Swiftsage: A generative agent with fast and slow thinking for complex
interactive tasks.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems, 2024, 36

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Evans J S B, Stanovich K E.

</span>
<span class="ltx_bibblock">Dual-process theories of higher cognition: Advancing the debate.

</span>
<span class="ltx_bibblock">Perspectives on psychological science, 2013, 8(3): 223–241

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Wang X, Wei J, Schuurmans D, Le Q, Chi E, Narang S, Chowdhery A, Zhou D.

</span>
<span class="ltx_bibblock">Self-consistency improves chain of thought reasoning in language
models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2203.11171, 2022

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Yao S, Yu D, Zhao J, Shafran I, Griffiths T, Cao Y, Narasimhan K.

</span>
<span class="ltx_bibblock">Tree of thoughts: Deliberate problem solving with large language
models.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems, 2024, 36

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Wang Y, Jiang Z, Chen Z, Yang F, Zhou Y, Cho E, Fan X, Huang X, Lu Y, Yang Y.

</span>
<span class="ltx_bibblock">Recmind: Large language model powered agent for recommendation.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.14296, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Besta M, Blach N, Kubicek A, Gerstenberger R, Gianinazzi L, Gajda J, Lehmann T,
Podstawski M, Niewiadomski H, Nyczyk P, others .

</span>
<span class="ltx_bibblock">Graph of thoughts: Solving elaborate problems with large language
models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.09687, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Sel B, Al-Tawaha A, Khattar V, Wang L, Jia R, Jin M.

</span>
<span class="ltx_bibblock">Algorithm of thoughts: Enhancing exploration of ideas in large
language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.10379, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Gramopadhye M, Szafir D.

</span>
<span class="ltx_bibblock">Generating executable action plans with environmentally-aware
language models.

</span>
<span class="ltx_bibblock">In: 2023 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS).

</span>
<span class="ltx_bibblock">2023, 3568–3575

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Hao S, Gu Y, Ma H, Hong J J, Wang Z, Wang D Z, Hu Z.

</span>
<span class="ltx_bibblock">Reasoning with language model is planning with world model.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.14992, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Liu B, Jiang Y, Zhang X, Liu Q, Zhang S, Biswas J, Stone P.

</span>
<span class="ltx_bibblock">LLM+P: Empowering large language models with optimal planning
proficiency.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2304.11477, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Dagan G, Keller F, Lascarides A.

</span>
<span class="ltx_bibblock">Dynamic planning with a llm.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.06391, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Yao S, Zhao J, Yu D, Du N, Shafran I, Narasimhan K, Cao Y.

</span>
<span class="ltx_bibblock">React: Synergizing reasoning and acting in language models.

</span>
<span class="ltx_bibblock">In: The Twelfth International Conference on Learning Representations.

</span>
<span class="ltx_bibblock">2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Song C H, Wu J, Washington C, Sadler B M, Chao W L, Su Y.

</span>
<span class="ltx_bibblock">Llm-planner: Few-shot grounded planning for embodied agents with
large language models.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE/CVF International Conference on Computer
Vision.

</span>
<span class="ltx_bibblock">2023, 2998–3009

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Huang W, Xia F, Xiao T, Chan H, Liang J, Florence P, Zeng A, Tompson J,
Mordatch I, Chebotar Y, others .

</span>
<span class="ltx_bibblock">Inner monologue: Embodied reasoning through planning with language
models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2207.05608, 2022

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Madaan A, Tandon N, Gupta P, Hallinan S, Gao L, Wiegreffe S, Alon U, Dziri N,
Prabhumoye S, Yang Y, others .

</span>
<span class="ltx_bibblock">Self-refine: Iterative refinement with self-feedback.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems, 2024, 36

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Miao N, Teh Y W, Rainforth T.

</span>
<span class="ltx_bibblock">Selfcheck: Using llms to zero-shot check their own step-by-step
reasoning.

</span>
<span class="ltx_bibblock">In: The Twelfth International Conference on Learning Representations.

</span>
<span class="ltx_bibblock">2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Chen P L, Chang C S.

</span>
<span class="ltx_bibblock">Interact: Exploring the potentials of chatgpt as a cooperative agent.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.01552, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Chen Z, Zhou K, Zhang B, Gong Z, Zhao W X, Wen J R.

</span>
<span class="ltx_bibblock">Chatcot: Tool-augmented chain-of-thought reasoning
on<math alttext="\backslash" class="ltx_Math" display="inline" id="bib.bib66.1.m1.1"><semantics id="bib.bib66.1.m1.1a"><mo id="bib.bib66.1.m1.1.1" xref="bib.bib66.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="bib.bib66.1.m1.1b"><ci id="bib.bib66.1.m1.1.1.cmml" xref="bib.bib66.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib66.1.m1.1c">\backslash</annotation><annotation encoding="application/x-llamapun" id="bib.bib66.1.m1.1d">\</annotation></semantics></math><math alttext="\backslash" class="ltx_Math" display="inline" id="bib.bib66.2.m2.1"><semantics id="bib.bib66.2.m2.1a"><mo id="bib.bib66.2.m2.1.1" xref="bib.bib66.2.m2.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="bib.bib66.2.m2.1b"><ci id="bib.bib66.2.m2.1.1.cmml" xref="bib.bib66.2.m2.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib66.2.m2.1c">\backslash</annotation><annotation encoding="application/x-llamapun" id="bib.bib66.2.m2.1d">\</annotation></semantics></math>chat-based large language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.14323, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Nakano R, Hilton J, Balaji S, Wu J, Ouyang L, Kim C, Hesse C, Jain S, Kosaraju
V, Saunders W, others .

</span>
<span class="ltx_bibblock">Webgpt: Browser-assisted question-answering with human feedback.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2112.09332, 2021

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Ruan J, Chen Y, Zhang B, Xu Z, Bao T, Du G, Shi S, Mao H, Zeng X, Zhao R.

</span>
<span class="ltx_bibblock">TPTU: Task planning and tool usage of large language model-based
AI agents.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.03427, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Patil S G, Zhang T, Wang X, Gonzalez J E.

</span>
<span class="ltx_bibblock">Gorilla: Large language model connected with massive apis.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.15334, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Li M, Song F, Yu B, Yu H, Li Z, Huang F, Li Y.

</span>
<span class="ltx_bibblock">Api-bank: A benchmark for tool-augmented llms.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2304.08244, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Song Y, Xiong W, Zhu D, Li C, Wang K, Tian Y, Li S.

</span>
<span class="ltx_bibblock">Restgpt: Connecting large language models with real-world
applications via restful apis.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2306.06624, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Liang Y, Wu C, Song T, Wu W, Xia Y, Liu Y, Ou Y, Lu S, Ji L, Mao S, others .

</span>
<span class="ltx_bibblock">Taskmatrix. ai: Completing tasks by connecting foundation models with
millions of apis.

</span>
<span class="ltx_bibblock">Intelligent Computing, 2024, 3: 0063

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Karpas E, Abend O, Belinkov Y, Lenz B, Lieber O, Ratner N, Shoham Y, Bata H,
Levine Y, Leyton-Brown K, others .

</span>
<span class="ltx_bibblock">Mrkl systems: A modular, neuro-symbolic architecture that combines
large language models, external knowledge sources and discrete reasoning.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2205.00445, 2022

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Ge Y, Hua W, Mei K, Tan J, Xu S, Li Z, Zhang Y, others .

</span>
<span class="ltx_bibblock">Openagi: When llm meets domain experts.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems, 2024, 36

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Surís D, Menon S, Vondrick C.

</span>
<span class="ltx_bibblock">Vipergpt: Visual inference via python execution for reasoning.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2303.08128, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Bran A M, Cox S, White A D, Schwaller P.

</span>
<span class="ltx_bibblock">Chemcrow: Augmenting large-language models with chemistry tools.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2304.05376, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Yang Z, Li L, Wang J, Lin K, Azarnasab E, Ahmed F, Liu Z, Liu C, Zeng M, Wang
L.

</span>
<span class="ltx_bibblock">Mm-react: Prompting chatgpt for multimodal reasoning and action.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2303.11381, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Gao C, Lan X, Lu Z, Mao J, Piao J, Wang H, Jin D, Li Y.

</span>
<span class="ltx_bibblock">S3: Social-network simulation system with large language
model-empowered agents.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2307.14984, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Ahn M, Brohan A, Brown N, Chebotar Y, Cortes O, David B, Finn C, Fu C,
Gopalakrishnan K, Hausman K, others .

</span>
<span class="ltx_bibblock">Do as i can, not as i say: Grounding language in robotic affordances.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2204.01691, 2022

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Park J S, Popowski L, Cai C, Morris M R, Liang P, Bernstein M S.

</span>
<span class="ltx_bibblock">Social simulacra: Creating populated prototypes for social computing
systems.

</span>
<span class="ltx_bibblock">In: Proceedings of the 35th Annual ACM Symposium on User Interface
Software and Technology.

</span>
<span class="ltx_bibblock">2022, 1–18

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Li G, Hammoud H A A K, Itani H, Khizbullin D, Ghanem B.

</span>
<span class="ltx_bibblock">Camel: Communicative agents for" mind" exploration of large scale
language model society.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2303.17760, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
al. e T.

</span>
<span class="ltx_bibblock">Auto-GPT.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Significant-Gravitas/Auto-GPT" title="">https://github.com/Significant-Gravitas/Auto-GPT</a>, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Liu R, Yang R, Jia C, Zhang G, Zhou D, Dai A M, Yang D, Vosoughi S.

</span>
<span class="ltx_bibblock">Training socially aligned language models in simulated human society.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.16960, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Chen L, Wang L, Dong H, Du Y, Yan J, Yang F, Li S, Zhao P, Qin S, Rajmohan S,
others .

</span>
<span class="ltx_bibblock">Introspective tips: Large language model for in-context decision
making.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.11598, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Liu H, Sferrazza C, Abbeel P.

</span>
<span class="ltx_bibblock">Chain of hindsight aligns language models with feedback.

</span>
<span class="ltx_bibblock">In: The Twelfth International Conference on Learning Representations.

</span>
<span class="ltx_bibblock">2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Yao S, Chen H, Yang J, Narasimhan K.

</span>
<span class="ltx_bibblock">Webshop: Towards scalable real-world web interaction with grounded
language agents.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems, 2022, 35:
20744–20757

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Dan Y, Lei Z, Gu Y, Li Y, Yin J, Lin J, Ye L, Tie Z, Zhou Y, Wang Y, others .

</span>
<span class="ltx_bibblock">Educhat: A large-scale language model-based chatbot system for
intelligent education.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.02773, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Deng X, Gu Y, Zheng B, Chen S, Stevens S, Wang B, Sun H, Su Y.

</span>
<span class="ltx_bibblock">Mind2web: Towards a generalist agent for the web.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems, 2024, 36

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Sun R, Arik S O, Nakhost H, Dai H, Sinha R, Yin P, Pfister T.

</span>
<span class="ltx_bibblock">Sql-palm: Improved large language modeladaptation for text-to-sql.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2306.00739, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
Yao W, Heinecke S, Niebles J C, Liu Z, Feng Y, Xue L, Murthy R, Chen Z, Zhang
J, Arpit D, Xu R, Mui P, Wang H, Xiong C, Savarese S.

</span>
<span class="ltx_bibblock">Retroformer: Retrospective large language agents with policy gradient
optimization, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
Shu Y, Gu H, Zhang P, Zhang H, Lu T, Li D, Gu N.

</span>
<span class="ltx_bibblock">Rah! recsys-assistant-human: A human-central recommendation framework
with large language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.09904, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Mandi Z, Jain S, Song S.

</span>
<span class="ltx_bibblock">Roco: Dialectic multi-robot collaboration with large language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2307.04738, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Zhang C, Liu L, Wang J, Wang C, Sun X, Wang H, Cai M.

</span>
<span class="ltx_bibblock">Prefer: Prompt ensemble learning via feedback-reflect-refine.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.12033, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Du Y, Li S, Torralba A, Tenenbaum J B, Mordatch I.

</span>
<span class="ltx_bibblock">Improving factuality and reasoning in language models through
multiagent debate.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.14325, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
Yang Z, Liu J, Han Y, Chen X, Huang Z, Fu B, Yu G.

</span>
<span class="ltx_bibblock">Appagent: Multimodal agents as smartphone users.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2312.13771, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
Madaan A, Tandon N, Clark P, Yang Y.

</span>
<span class="ltx_bibblock">Memory-assisted prompt editing to improve GPT-3 after deployment.

</span>
<span class="ltx_bibblock">In: Proceedings of the 2023 Conference on Empirical Methods in
Natural Language Processing.

</span>
<span class="ltx_bibblock">2022

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
Colas C, Teodorescu L, Oudeyer P Y, Yuan X, Côté M A.

</span>
<span class="ltx_bibblock">Augmenting autotelic agents with large language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.12487, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
Nascimento N, Alencar P, Cowan D.

</span>
<span class="ltx_bibblock">Self-adaptive large language model (llm)-based multiagent systems.

</span>
<span class="ltx_bibblock">In: 2023 IEEE International Conference on Autonomic Computing and
Self-Organizing Systems Companion (ACSOS-C).

</span>
<span class="ltx_bibblock">2023, 104–109

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
Saha S, Hase P, Bansal M.

</span>
<span class="ltx_bibblock">Can language models teach weaker agents? teacher explanations improve
students via theory of mind.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2306.09299, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
Zhuge M, Liu H, Faccio F, Ashley D R, Csordás R, Gopalakrishnan A, Hamdi A,
Hammoud H A A K, Herrmann V, Irie K, others .

</span>
<span class="ltx_bibblock">Mindstorms in natural language-based societies of mind.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.17066, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
Aher G V, Arriaga R I, Kalai A T.

</span>
<span class="ltx_bibblock">Using large language models to simulate multiple humans and replicate
human subject studies.

</span>
<span class="ltx_bibblock">In: International Conference on Machine Learning.

</span>
<span class="ltx_bibblock">2023, 337–371

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
Akata E, Schulz L, Coda-Forno J, Oh S J, Bethge M, Schulz E.

</span>
<span class="ltx_bibblock">Playing repeated games with large language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.16867, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
Ma Z, Mei Y, Su Z.

</span>
<span class="ltx_bibblock">Understanding the benefits and challenges of using large language
model-based conversational agents for mental well-being support.

</span>
<span class="ltx_bibblock">In: AMIA Annual Symposium Proceedings.

</span>
<span class="ltx_bibblock">2023, 1105

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
Ziems C, Held W, Shaikh O, Chen J, Zhang Z, Yang D.

</span>
<span class="ltx_bibblock">Can large language models transform computational social science?

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.03514, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
Horton J J.

</span>
<span class="ltx_bibblock">Large language models as simulated economic agents: What can we learn
from homo silicus?

</span>
<span class="ltx_bibblock">Technical report, National Bureau of Economic Research, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
Li S, Yang J, Zhao K.

</span>
<span class="ltx_bibblock">Are you in a masquerade? exploring the behavior and impact of large
language model driven social bots in online social networks.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2307.10337, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
Li C, Su X, Fan C, Han H, Xue C, Zheng C.

</span>
<span class="ltx_bibblock">Quantifying the impact of large language models on collective opinion
dynamics.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.03313, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
Kovač G, Portelas R, Dominey P F, Oudeyer P Y.

</span>
<span class="ltx_bibblock">The socialai school: Insights from developmental psychology towards
artificial socio-cultural agents.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2307.07871, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
Williams R, Hosseinichimeh N, Majumdar A, Ghaffarzadegan N.

</span>
<span class="ltx_bibblock">Epidemic modeling with generative agents.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2307.04986, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
Jinxin S, Jiabao Z, Yilei W, Xingjiao W, Jiawen L, Liang H.

</span>
<span class="ltx_bibblock">Cgmi: Configurable general multi-agent interaction framework.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.12503, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
Cui J, Li Z, Yan Y, Chen B, Yuan L.

</span>
<span class="ltx_bibblock">Chatlaw: Open-source legal large language model with integrated
external knowledge bases.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2306.16092, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
Hamilton S.

</span>
<span class="ltx_bibblock">Blind judgement: Agent-based supreme court modelling with gpt.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2301.05327, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
Bail C A.

</span>
<span class="ltx_bibblock">Can generative ai improve social science?

</span>
<span class="ltx_bibblock">2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
Boiko D A, MacKnight R, Gomes G.

</span>
<span class="ltx_bibblock">Emergent autonomous scientific research capabilities of large
language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2304.05332, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
Kang Y, Kim J.

</span>
<span class="ltx_bibblock">Chatmof: An autonomous ai system for predicting and generating
metal-organic frameworks.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.01423, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
Swan M, Kido T, Roland E, Santos R P d.

</span>
<span class="ltx_bibblock">Math agents: Computational infrastructure, mathematical embedding,
and genomics.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2307.02502, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
Drori I, Zhang S, Shuttleworth R, Tang L, Lu A, Ke E, Liu K, Chen L, Tran S,
Cheng N, others .

</span>
<span class="ltx_bibblock">A neural network solves, explains, and generates university math
problems by program synthesis and few-shot learning at human level.

</span>
<span class="ltx_bibblock">Proceedings of the National Academy of Sciences, 2022, 119(32):
e2123433119

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
Chen M, Tworek J, Jun H, Yuan Q, Pinto H P d O, Kaplan J, Edwards H, Burda Y,
Joseph N, Brockman G, others .

</span>
<span class="ltx_bibblock">Evaluating large language models trained on code.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2107.03374, 2021

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
Liffiton M, Sheese B E, Savelka J, Denny P.

</span>
<span class="ltx_bibblock">Codehelp: Using large language models with guardrails for scalable
support in programming classes.

</span>
<span class="ltx_bibblock">In: Proceedings of the 23rd Koli Calling International Conference on
Computing Education Research.

</span>
<span class="ltx_bibblock">2023, 1–11

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
Matelsky J K, Parodi F, Liu T, Lange R D, Kording K P.

</span>
<span class="ltx_bibblock">A large language model-assisted education tool to provide feedback on
open-ended responses.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.02439, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
Grossmann I, Feinberg M, Parker D C, Christakis N A, Tetlock P E, Cunningham
W A.

</span>
<span class="ltx_bibblock">Ai and the transformation of social science research.

</span>
<span class="ltx_bibblock">Science, 2023, 380(6650): 1108–1109

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
Zhou X, Li G, Liu Z.

</span>
<span class="ltx_bibblock">Llm as dba.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.05481, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
He Z, Wu H, Zhang X, Yao X, Zheng S, Zheng H, Yu B.

</span>
<span class="ltx_bibblock">Chateda: A large language model powered autonomous agent for eda.

</span>
<span class="ltx_bibblock">In: 2023 ACM/IEEE 5th Workshop on Machine Learning for CAD (MLCAD).

</span>
<span class="ltx_bibblock">2023, 1–6

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
Huang X, Lian J, Lei Y, Yao J, Lian D, Xie X.

</span>
<span class="ltx_bibblock">Recommender ai agent: Integrating large language models for
interactive recommendations.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.16505, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
Deng G, Liu Y, Mayoral-Vilches V, Liu P, Li Y, Xu Y, Zhang T, Liu Y, Pinzger M,
Rass S.

</span>
<span class="ltx_bibblock">Pentestgpt: An llm-empowered automatic penetration testing tool.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.06782, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
al. e S.

</span>
<span class="ltx_bibblock">Smolmodels.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/smol-ai/developer" title="">https://github.com/smol-ai/developer</a>, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
al. e M U.

</span>
<span class="ltx_bibblock">DemoGPT.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/melih-unsal/DemoGPT" title="">https://github.com/melih-unsal/DemoGPT</a>, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
al. e A O.

</span>
<span class="ltx_bibblock">GPT engineer.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/AntonOsika/gpt-engineer" title="">https://github.com/AntonOsika/gpt-engineer</a>, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
Xia Y, Shenoy M, Jazdi N, Weyrich M.

</span>
<span class="ltx_bibblock">Towards autonomous system: flexible modular production system
enhanced with large language model agents.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2304.14721, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
Ogundare O, Madasu S, Wiggins N.

</span>
<span class="ltx_bibblock">Industrial engineering with large language models: A case study of
chatgpt’s performance on oil &amp; gas problems.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2304.14354, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
Zhang C, Yang K, Hu S, Wang Z, Li G, Sun Y, Zhang C, Zhang Z, Liu A, Zhu S C,
others .

</span>
<span class="ltx_bibblock">Proagent: Building proactive cooperative ai with large language
models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.11339, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
Hu B, Zhao C, others .

</span>
<span class="ltx_bibblock">Enabling intelligent interactions between an agent and an llm: A
reinforcement learning approach.

</span>
<span class="ltx_bibblock">arXiv:2306.03604, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
Wu Y, Min S Y, Bisk Y, Salakhutdinov R, Azaria A, Li Y, Mitchell T, Prabhumoye
S.

</span>
<span class="ltx_bibblock">Plan, eliminate, and track–language models are good teachers for
embodied agents.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.02412, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
Zhang D, Chen L, Zhang S, Xu H, Zhao Z, Yu K.

</span>
<span class="ltx_bibblock">Large language models are semi-parametric reinforcement learning
agents.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems, 2024, 36

</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
Di Palo N, Byravan A, Hasenclever L, Wulfmeier M, Heess N, Riedmiller M.

</span>
<span class="ltx_bibblock">Towards a unified agent with foundation models.

</span>
<span class="ltx_bibblock">In: Workshop on Reincarnating Reinforcement Learning at ICLR 2023.

</span>
<span class="ltx_bibblock">2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
Wu J, Antonova R, Kan A, Lepert M, Zeng A, Song S, Bohg J, Rusinkiewicz S,
Funkhouser T.

</span>
<span class="ltx_bibblock">Tidybot: Personalized robot assistance with large language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.05658, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
Wu Z, Wang Z, Xu X, Lu J, Yan H.

</span>
<span class="ltx_bibblock">Embodied task planning with large language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2307.01848, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
Dasgupta I, Kaeser-Chen C, Marino K, Ahuja A, Babayan S, Hill F, Fergus R.

</span>
<span class="ltx_bibblock">Collaborating with language models for embodied reasoning.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2302.00763, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
Nottingham K, Ammanabrolu P, Suhr A, Choi Y, Hajishirzi H, Singh S, Fox R.

</span>
<span class="ltx_bibblock">Do embodied agents dream of pixelated sheep?: Embodied decision
making using language guided world modelling.

</span>
<span class="ltx_bibblock">In: Workshop on Reincarnating Reinforcement Learning at ICLR 2023.

</span>
<span class="ltx_bibblock">2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
Zhou W, Peng X, Riedl M.

</span>
<span class="ltx_bibblock">Dialogue shaping: Empowering agents through npc interaction.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2307.15833, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
Li H, Hao Y, Zhai Y, Qian Z.

</span>
<span class="ltx_bibblock">The hitchhiker’s guide to program analysis: A journey with large
language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.00245, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
al. e R.

</span>
<span class="ltx_bibblock">AgentGPT.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/reworkd/AgentGPT" title="">https://github.com/reworkd/AgentGPT</a>, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
al. e E.

</span>
<span class="ltx_bibblock">Ai-legion.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/eumemic/ai-legion" title="">https://github.com/eumemic/ai-legion</a>, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
al. e J X.

</span>
<span class="ltx_bibblock">Agixt.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Josh-XT/AGiXT" title="">https://github.com/Josh-XT/AGiXT</a>, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
al. e C.

</span>
<span class="ltx_bibblock">Xlang.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/xlang-ai/xlang" title="">https://github.com/xlang-ai/xlang</a>, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
al. e N.

</span>
<span class="ltx_bibblock">Babyagi.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/yoheinakajima" title="">https://github.com/yoheinakajima</a>, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
Chase H.

</span>
<span class="ltx_bibblock">langchain.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://docs.langchain.com/docs/" title="">https://docs.langchain.com/docs/</a>, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
al. e A M.

</span>
<span class="ltx_bibblock">WorkGPT.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/team-openpm/workgpt" title="">https://github.com/team-openpm/workgpt</a>, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
al. e F R.

</span>
<span class="ltx_bibblock">LoopGPT.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/farizrahman4u/loopgpt" title="">https://github.com/farizrahman4u/loopgpt</a>, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
al. e A E.

</span>
<span class="ltx_bibblock">GPT-researcher.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/assafelovic/gpt-researcher" title="">https://github.com/assafelovic/gpt-researcher</a>, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
Qin Y, Hu S, Lin Y, Chen W, Ding N, Cui G, Zeng Z, Huang Y, Xiao C, Han C,
others .

</span>
<span class="ltx_bibblock">Tool learning with foundation models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2304.08354, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
Face H.

</span>
<span class="ltx_bibblock">transformers-agent.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/docs/transformers/transformers_agents" title="">https://huggingface.co/docs/transformers/transformers_agents</a>,
2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
al. e E.

</span>
<span class="ltx_bibblock">Miniagi.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/muellerberndt/mini-agi" title="">https://github.com/muellerberndt/mini-agi</a>, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
al. e T.

</span>
<span class="ltx_bibblock">Superagi.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/TransformerOptimus/SuperAGI" title="">https://github.com/TransformerOptimus/SuperAGI</a>, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
Wu Q, Bansal G, Zhang J, Wu Y, Zhang S, Zhu E, Li B, Jiang L, Zhang X, Wang C.

</span>
<span class="ltx_bibblock">Autogen: Enabling next-gen llm applications via multi-agent
conversation framework.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.08155, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
Chen W, Su Y, Zuo J, Yang C, Yuan C, Qian C, Chan C M, Qin Y, Lu Y, Xie R,
others .

</span>
<span class="ltx_bibblock">Agentverse: Facilitating multi-agent collaboration and exploring
emergent behaviors in agents.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.10848, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
Lee M, Srivastava M, Hardy A, Thickstun J, Durmus E, Paranjape A, Gerard-Ursin
I, Li X L, Ladhak F, Rong F, others .

</span>
<span class="ltx_bibblock">Evaluating human-language model interaction.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2212.09746, 2022

</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
Chan C M, Chen W, Su Y, Yu J, Xue W, Zhang S, Fu J, Liu Z.

</span>
<span class="ltx_bibblock">Chateval: Towards better llm-based evaluators through multi-agent
debate.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.07201, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
Kang S, Yoon J, Yoo S.

</span>
<span class="ltx_bibblock">Large language models are few-shot testers: Exploring llm-based
general bug reproduction.

</span>
<span class="ltx_bibblock">In: 2023 IEEE/ACM 45th International Conference on Software
Engineering (ICSE).

</span>
<span class="ltx_bibblock">2023, 2312–2323

</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
Jalil S, Rafi S, LaToza T D, Moran K, Lam W.

</span>
<span class="ltx_bibblock">Chatgpt and software testing education: Promises &amp; perils.

</span>
<span class="ltx_bibblock">In: 2023 IEEE International Conference on Software Testing,
Verification and Validation Workshops (ICSTW).

</span>
<span class="ltx_bibblock">2023, 4130–4137

</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock">
Mehta N, Teruel M, Sanz P F, Deng X, Awadallah A H, Kiseleva J.

</span>
<span class="ltx_bibblock">Improving grounded language understanding in a collaborative
environment by interacting with agents through help feedback.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2304.10750, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock">
Chen A, Phang J, Parrish A, Padmakumar V, Zhao C, Bowman S R, Cho K.

</span>
<span class="ltx_bibblock">Two failures of self-consistency in the multi-step reasoning of llms.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.14279, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock">
Choi M, Pei J, Kumar S, Shu C, Jurgens D.

</span>
<span class="ltx_bibblock">Do llms understand social knowledge? evaluating the sociability of
large language models with socket benchmark.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.14938, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_tag_bibitem">[164]</span>
<span class="ltx_bibblock">
Zhang D, Chen L, Zhao Z, Cao R, Yu K.

</span>
<span class="ltx_bibblock">Mobile-env: An evaluation platform and benchmark for interactive
agents in llm era.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.08144, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_tag_bibitem">[165]</span>
<span class="ltx_bibblock">
Chalamalasetti K, Götze J, Hakimov S, Madureira B, Sadler P, Schlangen D.

</span>
<span class="ltx_bibblock">clembench: Using game play to evaluate chat-optimized language models
as conversational agents.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.13455, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_tag_bibitem">[166]</span>
<span class="ltx_bibblock">
Lin J, Tomlin N, Andreas J, Eisner J.

</span>
<span class="ltx_bibblock">Decision-oriented dialogue for human-ai collaboration.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2305.20076, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_tag_bibitem">[167]</span>
<span class="ltx_bibblock">
Feldt R, Kang S, Yoon J, Yoo S.

</span>
<span class="ltx_bibblock">Towards autonomous testing agents via conversational large language
models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2306.05152, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_tag_bibitem">[168]</span>
<span class="ltx_bibblock">
Liang Y, Zhu L, Yang Y.

</span>
<span class="ltx_bibblock">Tachikuma: Understading complex interactions with multi-character and
novel objects by large language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2307.12573, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_tag_bibitem">[169]</span>
<span class="ltx_bibblock">
Liu X, Yu H, Zhang H, Xu Y, Lei X, Lai H, Gu Y, Ding H, Men K, Yang K, others .

</span>
<span class="ltx_bibblock">Agentbench: Evaluating llms as agents.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.03688, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_tag_bibitem">[170]</span>
<span class="ltx_bibblock">
Liu Z, Yao W, Zhang J, Xue L, Heinecke S, Murthy R, Feng Y, Chen Z, Niebles
J C, Arpit D, others .

</span>
<span class="ltx_bibblock">Bolaa: Benchmarking and orchestrating llm-augmented autonomous
agents.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.05960, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_tag_bibitem">[171]</span>
<span class="ltx_bibblock">
Xu B, Liu X, Shen H, Han Z, Li Y, Yue M, Peng Z, Liu Y, Yao Z, Xu D.

</span>
<span class="ltx_bibblock">Gentopia. ai: A collaborative platform for tool-augmented llms.

</span>
<span class="ltx_bibblock">In: Proceedings of the 2023 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations.

</span>
<span class="ltx_bibblock">2023, 237–245

</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_tag_bibitem">[172]</span>
<span class="ltx_bibblock">
Huang J t, Lam M H, Li E J, Ren S, Wang W, Jiao W, Tu Z, Lyu M R.

</span>
<span class="ltx_bibblock">Emotionally numb or empathetic? evaluating how llms feel using
emotionbench.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.03656, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_tag_bibitem">[173]</span>
<span class="ltx_bibblock">
Zhou S, Xu F F, Zhu H, Zhou X, Lo R, Sridhar A, Cheng X, Bisk Y, Fried D, Alon
U, others .

</span>
<span class="ltx_bibblock">Webarena: A realistic web environment for building autonomous agents.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2307.13854, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_tag_bibitem">[174]</span>
<span class="ltx_bibblock">
Banerjee D, Singh P, Avadhanam A, Srivastava S.

</span>
<span class="ltx_bibblock">Benchmarking llm powered chatbots: methods and metrics.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2308.04624, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_tag_bibitem">[175]</span>
<span class="ltx_bibblock">
Zhao W X, Zhou K, Li J, Tang T, Wang X, Hou Y, Min Y, Zhang B, Zhang J, Dong Z,
others .

</span>
<span class="ltx_bibblock">A survey of large language models.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2303.18223, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_tag_bibitem">[176]</span>
<span class="ltx_bibblock">
Yang J, Jin H, Tang R, Han X, Feng Q, Jiang H, Zhong S, Yin B, Hu X.

</span>
<span class="ltx_bibblock">Harnessing the power of llms in practice: A survey on chatgpt and
beyond.

</span>
<span class="ltx_bibblock">ACM Transactions on Knowledge Discovery from Data, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_tag_bibitem">[177]</span>
<span class="ltx_bibblock">
Wang Y, Zhong W, Li L, Mi F, Zeng X, Huang W, Shang L, Jiang X, Liu Q.

</span>
<span class="ltx_bibblock">Aligning large language models with human: A survey.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2307.12966, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_tag_bibitem">[178]</span>
<span class="ltx_bibblock">
Huang J, Chang K C C.

</span>
<span class="ltx_bibblock">Towards reasoning in large language models: A survey.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2212.10403, 2022

</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_tag_bibitem">[179]</span>
<span class="ltx_bibblock">
Mialon G, Dessì R, Lomeli M, Nalmpantis C, Pasunuru R, Raileanu R,
Rozière B, Schick T, Dwivedi-Yu J, Celikyilmaz A, others .

</span>
<span class="ltx_bibblock">Augmented language models: a survey.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2302.07842, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_tag_bibitem">[180]</span>
<span class="ltx_bibblock">
Chang Y, Wang X, Wang J, Wu Y, Yang L, Zhu K, Chen H, Yi X, Wang C, Wang Y,
others .

</span>
<span class="ltx_bibblock">A survey on evaluation of large language models.

</span>
<span class="ltx_bibblock">ACM Transactions on Intelligent Systems and Technology, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_tag_bibitem">[181]</span>
<span class="ltx_bibblock">
Chang T A, Bergen B K.

</span>
<span class="ltx_bibblock">Language model behavior: A comprehensive survey.

</span>
<span class="ltx_bibblock">Computational Linguistics, 2024, 1–58

</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_tag_bibitem">[182]</span>
<span class="ltx_bibblock">
Li C, Wang J, Zhu K, Zhang Y, Hou W, Lian J, Xie X.

</span>
<span class="ltx_bibblock">Emotionprompt: Leveraging psychology for large language models
enhancement via emotional stimulus.

</span>
<span class="ltx_bibblock">arXiv e-prints, 2023, arXiv–2307

</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_tag_bibitem">[183]</span>
<span class="ltx_bibblock">
Zhuo T Y, Li Z, Huang Y, Shiri F, Wang W, Haffari G, Li Y F.

</span>
<span class="ltx_bibblock">On robustness of prompt-based semantic parsing with large pre-trained
language model: An empirical study on codex.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2301.12868, 2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_tag_bibitem">[184]</span>
<span class="ltx_bibblock">
Gekhman Z, Oved N, Keller O, Szpektor I, Reichart R.

</span>
<span class="ltx_bibblock">On the robustness of dialogue history representation in
conversational question answering: a comprehensive study and a new
prompt-based method.

</span>
<span class="ltx_bibblock">Transactions of the Association for Computational Linguistics, 2023,
11: 351–366

</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_tag_bibitem">[185]</span>
<span class="ltx_bibblock">
Ji Z, Lee N, Frieske R, Yu T, Su D, Xu Y, Ishii E, Bang Y J, Madotto A, Fung P.

</span>
<span class="ltx_bibblock">Survey of hallucination in natural language generation.

</span>
<span class="ltx_bibblock">ACM Computing Surveys, 2023, 55(12): 1–38

</span>
</li>
</ul>
</section>
<div class="ltx_para" id="p2">
<span class="ltx_ERROR undefined" id="p2.1">{biography}</span>
<p class="ltx_p" id="p2.2">authors/1.png
Lei Wang is a Ph.D. candidate at Renmin University of China, Beijing. His research focuses on recommender systems and agent-based large language models.</p>
</div>
<div class="ltx_para" id="p3">
<span class="ltx_ERROR undefined" id="p3.1">{biography}</span>
<p class="ltx_p" id="p3.2">authors/2.png
Chen Ma
is currently pursuing a Master’s degree at Renmin University of
China, Beijing, China. His research interests include recommender system, agent based on large language model.

<span class="ltx_ERROR undefined" id="p3.2.1">{biography}</span>authors/3.png
Xueyang Feng
is currently studying for a Ph.D. degree at Renmin University of
China, Beijing, China. His research interests include recommender system, agent based on large language model.

<span class="ltx_ERROR undefined" id="p3.2.2">{biography}</span>authors/4.png
Zeyu Zhang
is currently pursuing a Master’s degree at Renmin University of
China, Beijing, China. His research interests include recommender system, causal inference, agent based on large language model.

<span class="ltx_ERROR undefined" id="p3.2.3">{biography}</span>authors/5.png
Hao Yang
is currently studying for a Ph.D. degree at Renmin University of
China, Beijing, China. His research interests include recommender system, causal inference.

<span class="ltx_ERROR undefined" id="p3.2.4">{biography}</span>authors/6.png
Jingsen Zhang
is currently studying for a Ph.D. degree at Renmin University of
China, Beijing, China. His research interests include recommender system.

<span class="ltx_ERROR undefined" id="p3.2.5">{biography}</span>authors/7.png
Zhi-Yuan Chen
is pursuing his Ph.D. in Gaoling school of Artificial Intelligence, Renmin University of China. His research mainly focuses on language model reasoning and agent based on large language model.

<span class="ltx_ERROR undefined" id="p3.2.6">{biography}</span>authors/8.png
Jiakai Tang
is currently pursuing a Master’s degree at Renmin University of
China, Beijing, China. His research interests include recommender system.</p>
</div>
<div class="ltx_para" id="p4">
<span class="ltx_ERROR undefined" id="p4.1">{biography}</span>
<p class="ltx_p" id="p4.2">authors/9.png
Xu Chen obtained his PhD degree from Tsinghua University, China. Before joining Renmin University of China, he was a postdoc researcher at University College London, UK. In the period from March to September of 2017, he was studying at Georgia Institute of Technology, USA, as a visiting scholar. His research mainly focuses on the recommender system, reinforcement learning and causal inference.</p>
</div>
<div class="ltx_para" id="p5">
<span class="ltx_ERROR undefined" id="p5.1">{biography}</span>
<p class="ltx_p" id="p5.2">authors/10.png
Yankai Lin received his B.E. and Ph.D. degrees from Tsinghua University in 2014 and 2019. After that, he worked as a senior researcher in Tencent WeChat, and joined Renmin University of China in 2022 as a tenure-track assistant professor. His main research interests are pretrained models and natural language processing.</p>
</div>
<div class="ltx_para" id="p6">
<span class="ltx_ERROR undefined" id="p6.1">{biography}</span>
<p class="ltx_p" id="p6.2">authors/11.png
Wayne Xin Zhao received his Ph.D. in Computer Science from Peking University in 2014. His research interests include data mining, natural language processing and information retrieval in general. The main goal is to study how to organize, analyze and mine user generated data for improving the service of real-world applications.</p>
</div>
<div class="ltx_para" id="p7">
<span class="ltx_ERROR undefined" id="p7.1">{biography}</span>
<p class="ltx_p" id="p7.2">authors/12.png
Zhewei Wei received his Ph.D. of Computer Science and Engineering from Hong Kong University of Science and Technology. He did postdoctoral research in Aarhus University from 2012 to 2014, and joined Renmin University of China in 2014.</p>
</div>
<div class="ltx_para" id="p8">
<span class="ltx_ERROR undefined" id="p8.1">{biography}</span>
<p class="ltx_p" id="p8.2">authors/13.png
Ji-Rong Wen is a full professor, the executive dean of Gaoling School of Artificial Intelligence, and the dean of School of Information at Renmin University of China. He has been working in the big data and AI areas for many years, and publishing extensively on prestigious international conferences and journals.</p>
</div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Mar  2 04:03:11 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
