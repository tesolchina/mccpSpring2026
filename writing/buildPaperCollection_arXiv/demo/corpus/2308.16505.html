<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations</title>
<!--Generated on Tue Jan 30 03:17:23 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2308.16505v3/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S1" title="1 Introduction ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S2" title="2 Related Work ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S2.SS1" title="2.1 Conversational Recommender System ‣ 2 Related Work ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Conversational Recommender System</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S2.SS2" title="2.2 Enhancing LLMs ‣ 2 Related Work ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Enhancing LLMs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S3" title="3 Methodologies ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodologies</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S3.SS1" title="3.1 The Overall Framework ‣ 3 Methodologies ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>The Overall Framework</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S3.SS2" title="3.2 Memory Mechanism ‣ 3 Methodologies ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Memory Mechanism</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S3.SS2.SSSx1" title="Candidate Bus ‣ 3.2 Memory Mechanism ‣ 3 Methodologies ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title">Candidate Bus</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S3.SS2.SSSx2" title="User Profile ‣ 3.2 Memory Mechanism ‣ 3 Methodologies ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title">User Profile</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S3.SS3" title="3.3 Plan-first Execution with Dynamic Demonstrations ‣ 3 Methodologies ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Plan-first Execution with Dynamic Demonstrations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S3.SS4" title="3.4 Reflection ‣ 3 Methodologies ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Reflection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S3.SS5" title="3.5 Tool Learning with Small Language Models ‣ 3 Methodologies ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Tool Learning with Small Language Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4" title="4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.SS1" title="4.1 Experimental Setup ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.SS1.SSSx1" title="Evaluation Strategies. ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title">Evaluation Strategies.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.SS1.SSSx2" title="Dataset. ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title">Dataset.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.SS1.SSSx3" title="Baselines. ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title">Baselines.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.SS1.SSSx4" title="Metrics. ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title">Metrics.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.SS1.SSSx5" title="Implementation Details. ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title">Implementation Details.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.SS2" title="4.2 Evaluation with User Simulator ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Evaluation with User Simulator</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.SS2.SSSx1" title="Session-wise setting. ‣ 4.2 Evaluation with User Simulator ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title">Session-wise setting.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.SS2.SSSx2" title="Lifelong conversation setting. ‣ 4.2 Evaluation with User Simulator ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title">Lifelong conversation setting.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.SS3" title="4.3 Evaluation with One-Turn Recommendation ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Evaluation with One-Turn Recommendation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.SS4" title="4.4 Comparions of Different LLMs as the Brain ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Comparions of Different LLMs as the Brain</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.SS5" title="4.5 Ablation Study ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Ablation Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.SS6" title="4.6 Case Study ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Case Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S5" title="5 Conclusion ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#A1" title="Appendix A Dataset ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#A2" title="Appendix B Prompts ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Prompts</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#A2.SS1" title="B.1 Task Descriptions ‣ Appendix B Prompts ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Task Descriptions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#A2.SS2" title="B.2 Tool Descriptions ‣ Appendix B Prompts ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Tool Descriptions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#A2.SS3" title="B.3 Reflection ‣ Appendix B Prompts ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.3 </span>Reflection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#A2.SS4" title="B.4 Demonstration Generation ‣ Appendix B Prompts ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.4 </span>Demonstration Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#A2.SS5" title="B.5 User Simulator ‣ Appendix B Prompts ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.5 </span>User Simulator</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#A2.SS6" title="B.6 One-Turn Conversation Generation ‣ Appendix B Prompts ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.6 </span>One-Turn Conversation Generation</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewbox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: bibentry</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p>
</div><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2308.16505v3 [cs.IR] 30 Jan 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Xu Huang<sup class="ltx_sup" id="id1.1.id1">1</sup>,
Jianxun Lian<sup class="ltx_sup" id="id2.2.id2">2</sup><span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Corresponding authors.</span></span></span>,
Yuxuan Lei<sup class="ltx_sup" id="id3.3.id3">1</sup>,
Jing Yao<sup class="ltx_sup" id="id4.4.id4">2</sup>,
Defu Lian<sup class="ltx_sup" id="id5.5.id5">1</sup><span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>,
Xing Xie<sup class="ltx_sup" id="id6.6.id6">2</sup>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id7.id1">Recommender models excel at providing domain-specific item recommendations by leveraging extensive user behavior data. Despite their ability to act as lightweight domain experts, they struggle to perform versatile tasks such as providing explanations and engaging in conversations. On the other hand, large language models (LLMs) represent a significant step towards artificial general intelligence, showcasing remarkable capabilities in instruction comprehension, commonsense reasoning, and human interaction. However, LLMs lack the knowledge of domain-specific item catalogs and behavioral patterns, particularly in areas that diverge from general world knowledge, such as online e-commerce. Finetuning LLMs for each domain is neither economic nor efficient.</p>
<p class="ltx_p" id="id8.id2">In this paper, we bridge the gap between recommender models and LLMs, combining their respective strengths to create a versatile and interactive recommender system. We introduce an efficient framework called <span class="ltx_text ltx_font_bold" id="id8.id2.1">InteRecAgent</span>, which employs LLMs as the brain and recommender models as tools. We first outline a minimal set of essential tools required to transform LLMs into InteRecAgent. We then propose an efficient workflow within InteRecAgent for task execution, incorporating key components such as memory components, dynamic demonstration-augmented task planning, and reflection. InteRecAgent enables traditional recommender systems, such as those ID-based matrix factorization models, to become interactive systems with a natural language interface through the integration of LLMs. Experimental results on several public datasets show that InteRecAgent achieves satisfying performance as a conversational recommender system, outperforming general-purpose LLMs.
The source code of InteRecAgent is released at <a class="ltx_ref ltx_url" href="https://aka.ms/recagent" title="">https://aka.ms/recagent</a>.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recommender systems (RSs) have become an essential component of the digital landscape, playing a significant role in helping users navigate the vast array of choices available across various domains such as e-commerce and entertainment. By analyzing user preferences, historical data, and contextual information, these systems can deliver personalized recommendations that cater to individual tastes. Over the years, recommender systems have evolved from simple collaborative filtering algorithms to more advanced hybrid approaches that integrate deep learning techniques. However, as users increasingly rely on conversational interfaces for discovering and exploring products, there is a growing need to develop more sophisticated and interactive recommendation systems that can understand and respond effectively to diverse user inquiries and intents in an conversational manner.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Large language models (LLMs), such as GPT-3 <cite class="ltx_cite ltx_citemacro_citep">(Brown et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib2" title="">2020</a>)</cite> and PaLM <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib6" title="">2022</a>)</cite>, have made significant strides in recent years, demonstrating remarkable capabilities in artificial general intelligence and revolutionizing the field of natural language processing. A variety of practical tasks can be accomplished in the manner of users conversing with AI agents such as ChatGPT <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url" href="https://chat.openai.com/" title="">https://chat.openai.com/</a></span></span></span> and Claude <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url" href="https://claude.ai/" title="">https://claude.ai/</a></span></span></span>.
With their ability to understand context, generate human-like text, and perform complex reasoning tasks, LLMs can facilitate more engaging and intuitive interactions between users and RSs, thus offering promising prospects for the next generation of RSs. By integrating LLMs into RSs, it becomes possible to provide a more natural and seamless user experience that goes beyond traditional recommendation techniques, fostering a more timely understanding of user preferences and delivering more comprehensive and persuasive suggestions.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Despite their potential, leveraging LLMs for recommender systems is not without its challenges and limitations. Firstly, while LLMs are pretrained on vast amounts of textual data from the internet, covering various domains and demonstrating impressive general world knowledge, they may fail to capture fine-grained, domain-specific behavior patterns, especially in domains with massive training data. Secondly, LLMs may struggle to understand a domain well if the domain data is private and less openly accessible on the internet. Thirdly, LLMs lack knowledge of new items released after the collection of pretraining data, and fine-tuning with up-to-date data can be prohibitively expensive. In contrast, in-domain models can naturally address these challenges. A common paradigm to overcome these limitations is to combine LLMs with in-domain models, thereby filling the gaps and producing more powerful intelligence. Notable examples include AutoGPT <span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url" href="https://github.com/Significant-Gravitas/Auto-GPT" title="">https://github.com/Significant-Gravitas/Auto-GPT</a></span></span></span>, HuggingGPT<cite class="ltx_cite ltx_citemacro_citep">(Shen et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib30" title="">2023</a>)</cite>, and Visual ChatGPT<cite class="ltx_cite ltx_citemacro_citep">(Wu et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib45" title="">2023</a>)</cite>. The core idea is to utilize LLMs as the “brains” and in-domain models as “tools” that extend LLMs’ capabilities when handling domain-specific tasks.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we connect LLMs with traditional recommendation models for interactive recommender systems. We propose InteRecAgent (<span class="ltx_text ltx_font_bold" id="S1.p4.1.1">Inte</span>ractive <span class="ltx_text ltx_font_bold" id="S1.p4.1.2">Rec</span>ommender <span class="ltx_text ltx_font_bold" id="S1.p4.1.3">Agent</span>), a framework explicitly designed to cater to the specific requirements and nuances of recommender systems, thereby establishing a more effective connection between the LLM’s general capabilities and the specialized needs of the recommendation domain.
This framework consists of three distinct sets of tools, including querying, retrieval, and ranking, which are designed to cater to the diverse needs of users’ daily inquiries.
Given the typically large number of item candidates, storing item names in the tools’ input and output as observations with prompts is impractical. Therefore, we introduce a “shared candidate bus” to store intermediate states and facilitate communication between tools.
To enhance the capabilities of dealing with long conversations and even lifelong conversations, we introduce a “long-term and short-term user profile” module to track the preferences and history of the user, leveraged as the input of the ranking tool to improve personalization. The “shared candidate bus” along with the “long-term and short-term user profile” constitute the advanced memory mechanisms within the InteRecAgent framework.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Regarding task planning, we employ a “plan-first execution” strategy as opposed to a <span class="ltx_text ltx_font_slanted" id="S1.p5.1.1">step-by-step</span> approach. This strategy not only lowers the inference costs of LLMs but can also be seamlessly integrated with the dynamic demonstration strategy to enhance the quality of plan generation. Specifically, InteRecAgent generates all the steps of tool-calling at once and strictly follows the execution plan to accomplish the task. During the conversation, InteRecAgent parses the user’s intent and retrieves a few demonstrations that are most similar to the current intent. These dynamically retrieved demonstrations help LLMs formulate a correct task execution plan.
In addition, we implement a reflection strategy, wherein another LLM acts as a critic to evaluate the quality of the results and identify any errors during the task execution. If the results are unsatisfactory or errors are detected, InteRecAgent reverts to the initial state and repeats the plan-then-tool-execution process.
</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Employing GPT-4 as the LLM within InteRecAgent has yielded impressive results in our experiments. This naturally leads to the attractive question: is it possible to harness a smaller language model to act as the brain? To explore this, we have developed an imitation dataset featuring tool plan generations derived from interactions between InteRecAgent and a user simulator, both powered by GPT-4. Through fine-tuning the LlaMA 2 <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib35" title="">2023b</a>)</cite> model with this dataset, we have created RecLlama. Remarkably, RecLlama surpasses several larger models in its effectiveness as the core of a recommender agent.
Our main contributions are summarized as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose InteRecAgent, a compact LLM-based agent framework that democratizes interactive recommender systems by connecting LLMs with three distinct sets of traditional recommendation tools.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">In response to the challenges posed by the application of LLM-based agents in recommendation systems, we introduce a suite of advanced modules, including shared candidate bus, long-term and short-term user profile, dynamic demonstration-augmented plan-first strategy, and a reflection strategy.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">To enable small language models to serve as the brain for recommender agents, we create an imitation dataset derived from GPT-4. Leveraging this dataset, we have successfully fine-tuned a 7-billion-parameter model, which we refer to as RecLlama.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">Experimental results from three public datasets demonstrate the effectiveness of InteRecAgent, with particularly significant advantages in domains that are less covered by world knowledge.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Conversational Recommender System</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Existing researches in conversational recommender systems (CRS) can be primarily categorized into two main areas <cite class="ltx_cite ltx_citemacro_citep">(Gao et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib9" title="">2021</a>)</cite>: attribute-based question-answering<cite class="ltx_cite ltx_citemacro_citep">(Zou and Kanoulas <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib56" title="">2019</a>; Zou, Chen, and Kanoulas <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib55" title="">2020</a>; Xu et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib47" title="">2021</a>)</cite> and open-ended conversation <cite class="ltx_cite ltx_citemacro_citep">(Li et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib15" title="">2018</a>; Wang et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib43" title="">2022b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib36" title="">2021</a>)</cite>.
In attribute-based question-answering CRS, the system aims to recommend suitable items to users within as few rounds as possible. The interaction between the system and users primarily revolves around question-answering concerning desired item attributes, iteratively refining user interests. Key research challenges in this area include developing strategies for selecting queried attributes<cite class="ltx_cite ltx_citemacro_citep">(Mirzadeh, Ricci, and Bansal <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib21" title="">2005</a>; Zhang et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib51" title="">2018</a>)</cite> and addressing the exploration-exploitation trade-off<cite class="ltx_cite ltx_citemacro_citep">(Christakopoulou, Radlinski, and Hofmann <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib7" title="">2016</a>; Xie et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib46" title="">2021</a>)</cite>.
In open-ended conversation CRS, the system manages free-format conversational data. Initial research efforts in this area focused on leveraging pretrained language models for conversation understanding and response generation<cite class="ltx_cite ltx_citemacro_citep">(Li et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib15" title="">2018</a>; Penha and Hauff <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib25" title="">2020</a>)</cite>. Subsequent studies incorporated external knowledge to enhance the performance of open-ended CRS<cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib3" title="">2019</a>; Wang, Su, and Chen <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib40" title="">2022</a>; Wang et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib43" title="">2022b</a>)</cite>. Nevertheless, these approaches struggle to reason with complex user inquiries and maintain seamless communication with users. The emergence of LLMs presents an opportunity to revolutionize the construction of conversational recommender systems, potentially addressing the limitations of existing approaches and enhancing the overall user experience.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Enhancing LLMs</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The scaling-up of parameters and data has led to significant advancements in the capabilities of LLMs, including in-context learning <cite class="ltx_cite ltx_citemacro_citep">(Brown et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib2" title="">2020</a>; Liu et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib18" title="">2021</a>; Rubin, Herzig, and Berant <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib28" title="">2021</a>)</cite>, instruction following <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib24" title="">2022</a>; Touvron et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib34" title="">2023a</a>; OpenAI <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib23" title="">2023</a>)</cite>, planning and reasoning <cite class="ltx_cite ltx_citemacro_citep">(Wei et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib44" title="">2022</a>; Wang et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib42" title="">2022a</a>; Yao et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib50" title="">2022</a>; Yang et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib48" title="">2023</a>; Wang et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib39" title="">2023b</a>)</cite>. In recommender systems, the application of LLMs is becoming a rapidly growing trend <cite class="ltx_cite ltx_citemacro_citep">(Liu et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib17" title="">2023a</a>; Dai et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib8" title="">2023</a>; Kang et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib13" title="">2023</a>; Wang and Lim <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib37" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">As models show emergent intelligence, researchers have started exploring the potential to leverage LLMs as autonomous agents <cite class="ltx_cite ltx_citemacro_citep">(Wang et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib38" title="">2023a</a>; Zhao, Jin, and Cheng <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib52" title="">2023</a>)</cite>, augmented with memory modules, planning ability, and tool-using capabilities. For example, <cite class="ltx_cite ltx_citemacro_citep">(Wang et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib41" title="">2023c</a>; Zhong et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib54" title="">2023</a>; Liu et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib19" title="">2023b</a>)</cite> have equipped LLMs with an external memory, empowering LLMs with growth potential. Regarding the planning, CoT <cite class="ltx_cite ltx_citemacro_citep">(Wei et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib44" title="">2022</a>; Kojima et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib14" title="">2022</a>)</cite> and ReAct <cite class="ltx_cite ltx_citemacro_citep">(Yao et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib50" title="">2022</a>)</cite> propose to enhance planning by step-wise reasoning; ToT <cite class="ltx_cite ltx_citemacro_citep">(Yao et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib49" title="">2023</a>)</cite> and GoT <cite class="ltx_cite ltx_citemacro_citep">(Besta et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib1" title="">2023</a>)</cite> introduce multi-path reasoning to ensure consistency and correctness; Self-Refine <cite class="ltx_cite ltx_citemacro_citep">(Madaan et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib20" title="">2023</a>)</cite> and Reflexion <cite class="ltx_cite ltx_citemacro_citep">(Shinn et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib31" title="">2023</a>)</cite> lead the LLMs to reflect on errors, with the ultimate goal of improving their subsequent problem-solving success rates. To possess domain-specific skills, some works <cite class="ltx_cite ltx_citemacro_citep">(Qin et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib26" title="">2023a</a>)</cite> study guiding LLMs to use external tools, such as a web search engine <cite class="ltx_cite ltx_citemacro_citep">(Nakano et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib22" title="">2021</a>; Shuster et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib32" title="">2022</a>)</cite>, mathematical tools <cite class="ltx_cite ltx_citemacro_citep">(Schick et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib29" title="">2023</a>; Thoppilan et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib33" title="">2022</a>)</cite>, code interpreters <cite class="ltx_cite ltx_citemacro_citep">(Gao et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib10" title="">2023a</a>; Chen et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib4" title="">2022</a>)</cite> and visual models <cite class="ltx_cite ltx_citemacro_citep">(Wu et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib45" title="">2023</a>; Shen et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib30" title="">2023</a>)</cite>. To the best of our knowledge, this paper is the first to explore the LLM + tools paradigm in the field of recommender systems.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodologies</h2>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="546" id="S3.F1.g1" src="x1.png" width="1660"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>InteRecAgent Framework. (a) The overall pipeline of InteRecAgent; (b) The memory module, consisting of a candidate memory bus, a long-term and a short-term user profile; (c) Tool module, consisting of various tools, the plan-first execution strategy and the fine-tuning of RecLlama; (d) Planning module, involving the dynamic demonstrations and the reflection strategy; (e) Sources of fine-tuning data for RecLlama.</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>The Overall Framework</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The comprehensive framework of InteRecAgent is depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S3.F1" title="Figure 1 ‣ 3 Methodologies ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">1</span></a>. Fundamentally, LLMs function as the brain, while recommendation models serve as tools that supply domain-specific knowledge. Users engage with an LLM using natural language. The LLM interprets users’ intentions and determines whether the current conversation necessitates the assistance of tools. For instance, in a casual chit-chat, the LLM will respond based on its own knowledge; whereas for in-domain recommendations, the LLM initiates a chain of tool calls and subsequently generates a response by observing the execution results of the tools. Consequently, the quality of recommendations relies heavily on the tools, making the composition of tools a critical factor in overall performance. To ensure seamless communication between users and InteRecAgent, covering both casual conversation and item recommendations, we propose a minimum set of tools that encompass the following aspects:</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">(1) Information Query.</span>
During conversations, the InteRecAgent not only handles item recommendation tasks but also frequently addresses users’ inquiries. For example, within a gaming platform, users may ask questions like, <span class="ltx_text ltx_font_slanted" id="S3.SS1.p2.1.2">“What is the release date of this game and how much does it cost?”</span> To accommodate such queries, we include an item information query module. This module can retrieve detailed item information from the backend database using Structured Query Language (SQL) expressions.
</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">(2) Item Retrieval.</span>
Retrieval tools aim to propose a list of item candidates that satisfy a user’s demand from the entire item pool. These tools can be compared to the retrieval stage of a recommender system, which narrows down relevant candidates to a smaller list for large-scale serving. In InteRecAgent, we consider two types of demands that a user may express in their intent: hard conditions and soft conditions. Hard conditions refer to explicit demands on items, such as <span class="ltx_text ltx_font_slanted" id="S3.SS1.p3.1.2">“I want some popular sports games”</span> or “<span class="ltx_text ltx_font_slanted" id="S3.SS1.p3.1.3">Recommend me some RPG games under $100”</span>. Soft conditions pertain to demands that cannot be explicitly expressed with discrete attributes and require the use of semantic matching models, like <span class="ltx_text ltx_font_slanted" id="S3.SS1.p3.1.4">“I want some games similar to Call of Duty and Fortnite”</span>. It is essential to incorporate multiple tools to address both conditions. Consequently, we utilize an SQL tool to handle hard conditions, finding candidates from the item database. For soft conditions, we employ an item-to-item tool that matches similar items based on latent embeddings.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">(3) Item Ranking.</span>
Ranking tools execute a more sophisticated prediction of user preferences on the chosen candidates by leveraging user profiles. Similar to the rankers in conventional recommender systems, these tools typically employ a one-tower architecture. The selection of candidates could emerge from the output of item retrieval tools or be directly supplied by users, as in queries like “<span class="ltx_text ltx_font_slanted" id="S3.SS1.p4.1.2">Which one is more suitable for me, item A or item B?</span>”. Ranking tools guarantee that the recommended items are not only pertinent to the user’s immediate intent but also consonant with their broader preferences.
</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.2">LLMs have the potential to handle various user inquiries when supplemented with these diverse tools. For instance, a user may ask, “<span class="ltx_text ltx_font_slanted" id="S3.SS1.p5.2.1">I’ve played Fortnite and Call of Duty before. Now, I want to play some puzzle games with a release date after Fortnite’s. Do you have any recommendations?</span>” In this scenario, the tool execution sequence would be “SQL Query Tool <math alttext="\to" class="ltx_Math" display="inline" id="S3.SS1.p5.1.m1.1"><semantics id="S3.SS1.p5.1.m1.1a"><mo id="S3.SS1.p5.1.m1.1.1" stretchy="false" xref="S3.SS1.p5.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><ci id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.1.m1.1d">→</annotation></semantics></math> SQL Retrieval Tool <math alttext="\to" class="ltx_Math" display="inline" id="S3.SS1.p5.2.m2.1"><semantics id="S3.SS1.p5.2.m2.1a"><mo id="S3.SS1.p5.2.m2.1.1" stretchy="false" xref="S3.SS1.p5.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.1b"><ci id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.2.m2.1d">→</annotation></semantics></math> Ranker Tool.” First, the release date of Fortnite is queried, then the release date and puzzle genre are interpreted as hard conditions for the SQL retrieval. Finally, <span class="ltx_text ltx_font_slanted" id="S3.SS1.p5.2.2">Fortnite</span> and <span class="ltx_text ltx_font_slanted" id="S3.SS1.p5.2.3">Call of Duty</span> are considered as the user profile for the ranking model.</p>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1">Typically, the tool augmentation is implemented via ReAct <cite class="ltx_cite ltx_citemacro_citep">(Yao et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib50" title="">2022</a>)</cite>, where LLMs generate reasoning traces, actions, and observations in an interleaved manner. We refer to this style of execution as <span class="ltx_text ltx_font_italic" id="S3.SS1.p6.1.1">step-by-step</span>. Our initial implementation also employed the step-by-step approach. However, we soon observed some limitations due to various challenges. Firstly, retrieval tools may return a large number of items, resulting in an excessively long observation prompt for LLMs. Additionally, including numerous entity names in the prompt can degrade LLMs performance. Secondly, despite their powerful intelligence, LLMs may use tools incorrectly to complete tasks, such as selecting the wrong tool to call or omitting key execution steps. To tackle these challenges, we enhance the three critical components of a typical LLM-based agent, namely memory (Section <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S3.SS2" title="3.2 Memory Mechanism ‣ 3 Methodologies ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">3.2</span></a>), task planning (Section <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S3.SS3" title="3.3 Plan-first Execution with Dynamic Demonstrations ‣ 3 Methodologies ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">3.3</span></a> and  <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S3.SS4" title="3.4 Reflection ‣ 3 Methodologies ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">3.4</span></a>), and tool learning abilities (Section <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S3.SS5" title="3.5 Tool Learning with Small Language Models ‣ 3 Methodologies ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">3.5</span></a>).</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Memory Mechanism</h3>
<section class="ltx_subsubsection" id="S3.SS2.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Candidate Bus</h4>
<div class="ltx_para" id="S3.SS2.SSSx1.p1">
<p class="ltx_p" id="S3.SS2.SSSx1.p1.1">The large number of items can pose a challenge when attempting to include items generated by tools in prompts as observations for the LLM, due to input context length limitations. Meanwhile, the input of a subsequent tool often depends on the output of preceding tools, necessitating effective communication between tools. Thus, we propose Candidate Bus, which is a separate memory to store the current item candidates, eliminating the need to append them to prompt inputs. The Candidate Bus, accessible by all tools, comprises two parts: a data bus for storing candidate items, and a tracker for recording each tool’s output.
</p>
</div>
<div class="ltx_para" id="S3.SS2.SSSx1.p2">
<p class="ltx_p" id="S3.SS2.SSSx1.p2.1">The candidate items in the data bus are initialized to include all items at the beginning of each conversation turn by default. At the start of each tool execution, candidate items are read from the data bus, and the data bus is then refreshed with the filtered items at the end of each tool execution. This mechanism allows candidate items to flow sequentially through the various tools in a streaming manner. Notably, users may explicitly specify a set of candidate items in the conversation, such as “<span class="ltx_text ltx_font_slanted" id="S3.SS2.SSSx1.p2.1.1">Which of these movies do you think is most suitable for me: [Movie List]?</span>” In this case, the LLM will call a special tool—the memory initialization tool—to set the user-specified items as the initial candidate items.
</p>
</div>
<div class="ltx_para" id="S3.SS2.SSSx1.p3">
<p class="ltx_p" id="S3.SS2.SSSx1.p3.6">The tracker within the memory serves to record tool execution. Each tool call record is represented as a triplet <math alttext="(f_{k},i_{k},o_{k})" class="ltx_Math" display="inline" id="S3.SS2.SSSx1.p3.1.m1.3"><semantics id="S3.SS2.SSSx1.p3.1.m1.3a"><mrow id="S3.SS2.SSSx1.p3.1.m1.3.3.3" xref="S3.SS2.SSSx1.p3.1.m1.3.3.4.cmml"><mo id="S3.SS2.SSSx1.p3.1.m1.3.3.3.4" stretchy="false" xref="S3.SS2.SSSx1.p3.1.m1.3.3.4.cmml">(</mo><msub id="S3.SS2.SSSx1.p3.1.m1.1.1.1.1" xref="S3.SS2.SSSx1.p3.1.m1.1.1.1.1.cmml"><mi id="S3.SS2.SSSx1.p3.1.m1.1.1.1.1.2" xref="S3.SS2.SSSx1.p3.1.m1.1.1.1.1.2.cmml">f</mi><mi id="S3.SS2.SSSx1.p3.1.m1.1.1.1.1.3" xref="S3.SS2.SSSx1.p3.1.m1.1.1.1.1.3.cmml">k</mi></msub><mo id="S3.SS2.SSSx1.p3.1.m1.3.3.3.5" xref="S3.SS2.SSSx1.p3.1.m1.3.3.4.cmml">,</mo><msub id="S3.SS2.SSSx1.p3.1.m1.2.2.2.2" xref="S3.SS2.SSSx1.p3.1.m1.2.2.2.2.cmml"><mi id="S3.SS2.SSSx1.p3.1.m1.2.2.2.2.2" xref="S3.SS2.SSSx1.p3.1.m1.2.2.2.2.2.cmml">i</mi><mi id="S3.SS2.SSSx1.p3.1.m1.2.2.2.2.3" xref="S3.SS2.SSSx1.p3.1.m1.2.2.2.2.3.cmml">k</mi></msub><mo id="S3.SS2.SSSx1.p3.1.m1.3.3.3.6" xref="S3.SS2.SSSx1.p3.1.m1.3.3.4.cmml">,</mo><msub id="S3.SS2.SSSx1.p3.1.m1.3.3.3.3" xref="S3.SS2.SSSx1.p3.1.m1.3.3.3.3.cmml"><mi id="S3.SS2.SSSx1.p3.1.m1.3.3.3.3.2" xref="S3.SS2.SSSx1.p3.1.m1.3.3.3.3.2.cmml">o</mi><mi id="S3.SS2.SSSx1.p3.1.m1.3.3.3.3.3" xref="S3.SS2.SSSx1.p3.1.m1.3.3.3.3.3.cmml">k</mi></msub><mo id="S3.SS2.SSSx1.p3.1.m1.3.3.3.7" stretchy="false" xref="S3.SS2.SSSx1.p3.1.m1.3.3.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx1.p3.1.m1.3b"><vector id="S3.SS2.SSSx1.p3.1.m1.3.3.4.cmml" xref="S3.SS2.SSSx1.p3.1.m1.3.3.3"><apply id="S3.SS2.SSSx1.p3.1.m1.1.1.1.1.cmml" xref="S3.SS2.SSSx1.p3.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSSx1.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.SSSx1.p3.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSSx1.p3.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.SSSx1.p3.1.m1.1.1.1.1.2">𝑓</ci><ci id="S3.SS2.SSSx1.p3.1.m1.1.1.1.1.3.cmml" xref="S3.SS2.SSSx1.p3.1.m1.1.1.1.1.3">𝑘</ci></apply><apply id="S3.SS2.SSSx1.p3.1.m1.2.2.2.2.cmml" xref="S3.SS2.SSSx1.p3.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSSx1.p3.1.m1.2.2.2.2.1.cmml" xref="S3.SS2.SSSx1.p3.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS2.SSSx1.p3.1.m1.2.2.2.2.2.cmml" xref="S3.SS2.SSSx1.p3.1.m1.2.2.2.2.2">𝑖</ci><ci id="S3.SS2.SSSx1.p3.1.m1.2.2.2.2.3.cmml" xref="S3.SS2.SSSx1.p3.1.m1.2.2.2.2.3">𝑘</ci></apply><apply id="S3.SS2.SSSx1.p3.1.m1.3.3.3.3.cmml" xref="S3.SS2.SSSx1.p3.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSSx1.p3.1.m1.3.3.3.3.1.cmml" xref="S3.SS2.SSSx1.p3.1.m1.3.3.3.3">subscript</csymbol><ci id="S3.SS2.SSSx1.p3.1.m1.3.3.3.3.2.cmml" xref="S3.SS2.SSSx1.p3.1.m1.3.3.3.3.2">𝑜</ci><ci id="S3.SS2.SSSx1.p3.1.m1.3.3.3.3.3.cmml" xref="S3.SS2.SSSx1.p3.1.m1.3.3.3.3.3">𝑘</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx1.p3.1.m1.3c">(f_{k},i_{k},o_{k})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSSx1.p3.1.m1.3d">( italic_f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_i start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_o start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )</annotation></semantics></math>, where <math alttext="f_{k}" class="ltx_Math" display="inline" id="S3.SS2.SSSx1.p3.2.m2.1"><semantics id="S3.SS2.SSSx1.p3.2.m2.1a"><msub id="S3.SS2.SSSx1.p3.2.m2.1.1" xref="S3.SS2.SSSx1.p3.2.m2.1.1.cmml"><mi id="S3.SS2.SSSx1.p3.2.m2.1.1.2" xref="S3.SS2.SSSx1.p3.2.m2.1.1.2.cmml">f</mi><mi id="S3.SS2.SSSx1.p3.2.m2.1.1.3" xref="S3.SS2.SSSx1.p3.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx1.p3.2.m2.1b"><apply id="S3.SS2.SSSx1.p3.2.m2.1.1.cmml" xref="S3.SS2.SSSx1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSSx1.p3.2.m2.1.1.1.cmml" xref="S3.SS2.SSSx1.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSSx1.p3.2.m2.1.1.2.cmml" xref="S3.SS2.SSSx1.p3.2.m2.1.1.2">𝑓</ci><ci id="S3.SS2.SSSx1.p3.2.m2.1.1.3.cmml" xref="S3.SS2.SSSx1.p3.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx1.p3.2.m2.1c">f_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSSx1.p3.2.m2.1d">italic_f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> denotes the name of the <math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.SSSx1.p3.3.m3.1"><semantics id="S3.SS2.SSSx1.p3.3.m3.1a"><mi id="S3.SS2.SSSx1.p3.3.m3.1.1" xref="S3.SS2.SSSx1.p3.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx1.p3.3.m3.1b"><ci id="S3.SS2.SSSx1.p3.3.m3.1.1.cmml" xref="S3.SS2.SSSx1.p3.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx1.p3.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSSx1.p3.3.m3.1d">italic_k</annotation></semantics></math>-th tool, and <math alttext="i_{k},o_{k}" class="ltx_Math" display="inline" id="S3.SS2.SSSx1.p3.4.m4.2"><semantics id="S3.SS2.SSSx1.p3.4.m4.2a"><mrow id="S3.SS2.SSSx1.p3.4.m4.2.2.2" xref="S3.SS2.SSSx1.p3.4.m4.2.2.3.cmml"><msub id="S3.SS2.SSSx1.p3.4.m4.1.1.1.1" xref="S3.SS2.SSSx1.p3.4.m4.1.1.1.1.cmml"><mi id="S3.SS2.SSSx1.p3.4.m4.1.1.1.1.2" xref="S3.SS2.SSSx1.p3.4.m4.1.1.1.1.2.cmml">i</mi><mi id="S3.SS2.SSSx1.p3.4.m4.1.1.1.1.3" xref="S3.SS2.SSSx1.p3.4.m4.1.1.1.1.3.cmml">k</mi></msub><mo id="S3.SS2.SSSx1.p3.4.m4.2.2.2.3" xref="S3.SS2.SSSx1.p3.4.m4.2.2.3.cmml">,</mo><msub id="S3.SS2.SSSx1.p3.4.m4.2.2.2.2" xref="S3.SS2.SSSx1.p3.4.m4.2.2.2.2.cmml"><mi id="S3.SS2.SSSx1.p3.4.m4.2.2.2.2.2" xref="S3.SS2.SSSx1.p3.4.m4.2.2.2.2.2.cmml">o</mi><mi id="S3.SS2.SSSx1.p3.4.m4.2.2.2.2.3" xref="S3.SS2.SSSx1.p3.4.m4.2.2.2.2.3.cmml">k</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx1.p3.4.m4.2b"><list id="S3.SS2.SSSx1.p3.4.m4.2.2.3.cmml" xref="S3.SS2.SSSx1.p3.4.m4.2.2.2"><apply id="S3.SS2.SSSx1.p3.4.m4.1.1.1.1.cmml" xref="S3.SS2.SSSx1.p3.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSSx1.p3.4.m4.1.1.1.1.1.cmml" xref="S3.SS2.SSSx1.p3.4.m4.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSSx1.p3.4.m4.1.1.1.1.2.cmml" xref="S3.SS2.SSSx1.p3.4.m4.1.1.1.1.2">𝑖</ci><ci id="S3.SS2.SSSx1.p3.4.m4.1.1.1.1.3.cmml" xref="S3.SS2.SSSx1.p3.4.m4.1.1.1.1.3">𝑘</ci></apply><apply id="S3.SS2.SSSx1.p3.4.m4.2.2.2.2.cmml" xref="S3.SS2.SSSx1.p3.4.m4.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSSx1.p3.4.m4.2.2.2.2.1.cmml" xref="S3.SS2.SSSx1.p3.4.m4.2.2.2.2">subscript</csymbol><ci id="S3.SS2.SSSx1.p3.4.m4.2.2.2.2.2.cmml" xref="S3.SS2.SSSx1.p3.4.m4.2.2.2.2.2">𝑜</ci><ci id="S3.SS2.SSSx1.p3.4.m4.2.2.2.2.3.cmml" xref="S3.SS2.SSSx1.p3.4.m4.2.2.2.2.3">𝑘</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx1.p3.4.m4.2c">i_{k},o_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSSx1.p3.4.m4.2d">italic_i start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_o start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> are the input and output of the tool’s execution, such as the number of remaining candidates, runtime errors. The tracker’s main function is to aid the critic in making judgments within the reflection mechanism, acting as the <math alttext="\boldsymbol{o}^{t}" class="ltx_Math" display="inline" id="S3.SS2.SSSx1.p3.5.m5.1"><semantics id="S3.SS2.SSSx1.p3.5.m5.1a"><msup id="S3.SS2.SSSx1.p3.5.m5.1.1" xref="S3.SS2.SSSx1.p3.5.m5.1.1.cmml"><mi id="S3.SS2.SSSx1.p3.5.m5.1.1.2" xref="S3.SS2.SSSx1.p3.5.m5.1.1.2.cmml">𝒐</mi><mi id="S3.SS2.SSSx1.p3.5.m5.1.1.3" xref="S3.SS2.SSSx1.p3.5.m5.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx1.p3.5.m5.1b"><apply id="S3.SS2.SSSx1.p3.5.m5.1.1.cmml" xref="S3.SS2.SSSx1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSSx1.p3.5.m5.1.1.1.cmml" xref="S3.SS2.SSSx1.p3.5.m5.1.1">superscript</csymbol><ci id="S3.SS2.SSSx1.p3.5.m5.1.1.2.cmml" xref="S3.SS2.SSSx1.p3.5.m5.1.1.2">𝒐</ci><ci id="S3.SS2.SSSx1.p3.5.m5.1.1.3.cmml" xref="S3.SS2.SSSx1.p3.5.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx1.p3.5.m5.1c">\boldsymbol{o}^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSSx1.p3.5.m5.1d">bold_italic_o start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> in <math alttext="\operatorname{reflect}(\cdot)" class="ltx_Math" display="inline" id="S3.SS2.SSSx1.p3.6.m6.2"><semantics id="S3.SS2.SSSx1.p3.6.m6.2a"><mrow id="S3.SS2.SSSx1.p3.6.m6.2.3.2" xref="S3.SS2.SSSx1.p3.6.m6.2.3.1.cmml"><mi id="S3.SS2.SSSx1.p3.6.m6.1.1" xref="S3.SS2.SSSx1.p3.6.m6.1.1.cmml">reflect</mi><mo id="S3.SS2.SSSx1.p3.6.m6.2.3.2a" xref="S3.SS2.SSSx1.p3.6.m6.2.3.1.cmml">⁡</mo><mrow id="S3.SS2.SSSx1.p3.6.m6.2.3.2.1" xref="S3.SS2.SSSx1.p3.6.m6.2.3.1.cmml"><mo id="S3.SS2.SSSx1.p3.6.m6.2.3.2.1.1" stretchy="false" xref="S3.SS2.SSSx1.p3.6.m6.2.3.1.cmml">(</mo><mo id="S3.SS2.SSSx1.p3.6.m6.2.2" lspace="0em" rspace="0em" xref="S3.SS2.SSSx1.p3.6.m6.2.2.cmml">⋅</mo><mo id="S3.SS2.SSSx1.p3.6.m6.2.3.2.1.2" stretchy="false" xref="S3.SS2.SSSx1.p3.6.m6.2.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx1.p3.6.m6.2b"><apply id="S3.SS2.SSSx1.p3.6.m6.2.3.1.cmml" xref="S3.SS2.SSSx1.p3.6.m6.2.3.2"><ci id="S3.SS2.SSSx1.p3.6.m6.1.1.cmml" xref="S3.SS2.SSSx1.p3.6.m6.1.1">reflect</ci><ci id="S3.SS2.SSSx1.p3.6.m6.2.2.cmml" xref="S3.SS2.SSSx1.p3.6.m6.2.2">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx1.p3.6.m6.2c">\operatorname{reflect}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSSx1.p3.6.m6.2d">roman_reflect ( ⋅ )</annotation></semantics></math>, as described in Section <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S3.SS4" title="3.4 Reflection ‣ 3 Methodologies ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">3.4</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSSx1.p4">
<p class="ltx_p" id="S3.SS2.SSSx1.p4.1">With the help of the Candidate Bus component, items can be transmitted in a streaming manner between various tools and continuously filtered according to conditions, presenting a funnel-like structure for the recommendation. The tracker’s records can be considered as short-term memory for further reflection. We depict an example of the memory bus in the upper of Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S3.F3" title="Figure 3 ‣ 3.4 Reflection ‣ 3 Methodologies ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">User Profile</h4>
<div class="ltx_para" id="S3.SS2.SSSx2.p1">
<p class="ltx_p" id="S3.SS2.SSSx2.p1.1">To facilitate the invocation of tools, we explicitly maintain a user profile in memory. This profile is structured as a dictionary that encapsulates three facets of user preference: “like”, “dislike”, and “expect”. The “like” and “dislike” facets reflect the user’s favorable and unfavorable tastes, respectively, whereas “expect” monitors the user’s immediate requests during the current dialogue, such as conducting a search, which is not necessarily indicative of the user’s inherent preferences. Each facet may contain content that includes item names or categories.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSSx2.p2">
<p class="ltx_p" id="S3.SS2.SSSx2.p2.1">User profiles are synthesized by LLMs based on conversation history. To address situations where the conversation history grows excessively long, such as in lifelong learning scenarios where conversations from all days may be stored for ongoing interactions, we devise two distinct user profiles: one representing long-term memory and another for short-term memory. Should the current dialogue exceed the LLM’s input window size, we partition the dialogue, retrieve the user profile from the preceding segment, and merge it with the existing long-term memory to update the memory state. The short-term memory is consistently derived from the most recent conversations within the current prompt. When it comes to tool invocation, a comprehensive user profile is formed by the combination of both long-term and short-term memories.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Plan-first Execution with Dynamic Demonstrations</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Rather than using the <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.1">step-by-step</span> approach, we adopt a two-phase method. In the first phase, we prompt the LLM to generate a complete tool execution plan based on the user’s intention derived from the dialogue. In the second phase, the LLM strictly adheres to the plan, calling tools in sequence while allowing them to communicate via the Candidate Bus.
Concretely, the plan-first execution consists of the following two phases.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.8"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.8.1">Plan</span>: LLM accepts the user’s current input <math alttext="x^{t}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.1.m1.1"><semantics id="S3.I1.i1.p1.1.m1.1a"><msup id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml"><mi id="S3.I1.i1.p1.1.m1.1.1.2" xref="S3.I1.i1.p1.1.m1.1.1.2.cmml">x</mi><mi id="S3.I1.i1.p1.1.m1.1.1.3" xref="S3.I1.i1.p1.1.m1.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><apply id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.1.m1.1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1">superscript</csymbol><ci id="S3.I1.i1.p1.1.m1.1.1.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1.2">𝑥</ci><ci id="S3.I1.i1.p1.1.m1.1.1.3.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">x^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.1.m1.1d">italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math>, dialogue context <math alttext="C^{t-1}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.2.m2.1"><semantics id="S3.I1.i1.p1.2.m2.1a"><msup id="S3.I1.i1.p1.2.m2.1.1" xref="S3.I1.i1.p1.2.m2.1.1.cmml"><mi id="S3.I1.i1.p1.2.m2.1.1.2" xref="S3.I1.i1.p1.2.m2.1.1.2.cmml">C</mi><mrow id="S3.I1.i1.p1.2.m2.1.1.3" xref="S3.I1.i1.p1.2.m2.1.1.3.cmml"><mi id="S3.I1.i1.p1.2.m2.1.1.3.2" xref="S3.I1.i1.p1.2.m2.1.1.3.2.cmml">t</mi><mo id="S3.I1.i1.p1.2.m2.1.1.3.1" xref="S3.I1.i1.p1.2.m2.1.1.3.1.cmml">−</mo><mn id="S3.I1.i1.p1.2.m2.1.1.3.3" xref="S3.I1.i1.p1.2.m2.1.1.3.3.cmml">1</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.2.m2.1b"><apply id="S3.I1.i1.p1.2.m2.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.2.m2.1.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1">superscript</csymbol><ci id="S3.I1.i1.p1.2.m2.1.1.2.cmml" xref="S3.I1.i1.p1.2.m2.1.1.2">𝐶</ci><apply id="S3.I1.i1.p1.2.m2.1.1.3.cmml" xref="S3.I1.i1.p1.2.m2.1.1.3"><minus id="S3.I1.i1.p1.2.m2.1.1.3.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1.3.1"></minus><ci id="S3.I1.i1.p1.2.m2.1.1.3.2.cmml" xref="S3.I1.i1.p1.2.m2.1.1.3.2">𝑡</ci><cn id="S3.I1.i1.p1.2.m2.1.1.3.3.cmml" type="integer" xref="S3.I1.i1.p1.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.2.m2.1c">C^{t-1}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.2.m2.1d">italic_C start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT</annotation></semantics></math>, descriptions of various tools <math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.3.m3.1"><semantics id="S3.I1.i1.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i1.p1.3.m3.1.1" xref="S3.I1.i1.p1.3.m3.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.3.m3.1b"><ci id="S3.I1.i1.p1.3.m3.1.1.cmml" xref="S3.I1.i1.p1.3.m3.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.3.m3.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.3.m3.1d">caligraphic_F</annotation></semantics></math>, and demonstration <math alttext="\mathcal{D}_{x^{t}}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.4.m4.1"><semantics id="S3.I1.i1.p1.4.m4.1a"><msub id="S3.I1.i1.p1.4.m4.1.1" xref="S3.I1.i1.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i1.p1.4.m4.1.1.2" xref="S3.I1.i1.p1.4.m4.1.1.2.cmml">𝒟</mi><msup id="S3.I1.i1.p1.4.m4.1.1.3" xref="S3.I1.i1.p1.4.m4.1.1.3.cmml"><mi id="S3.I1.i1.p1.4.m4.1.1.3.2" xref="S3.I1.i1.p1.4.m4.1.1.3.2.cmml">x</mi><mi id="S3.I1.i1.p1.4.m4.1.1.3.3" xref="S3.I1.i1.p1.4.m4.1.1.3.3.cmml">t</mi></msup></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.4.m4.1b"><apply id="S3.I1.i1.p1.4.m4.1.1.cmml" xref="S3.I1.i1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.4.m4.1.1.1.cmml" xref="S3.I1.i1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.I1.i1.p1.4.m4.1.1.2.cmml" xref="S3.I1.i1.p1.4.m4.1.1.2">𝒟</ci><apply id="S3.I1.i1.p1.4.m4.1.1.3.cmml" xref="S3.I1.i1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.I1.i1.p1.4.m4.1.1.3.1.cmml" xref="S3.I1.i1.p1.4.m4.1.1.3">superscript</csymbol><ci id="S3.I1.i1.p1.4.m4.1.1.3.2.cmml" xref="S3.I1.i1.p1.4.m4.1.1.3.2">𝑥</ci><ci id="S3.I1.i1.p1.4.m4.1.1.3.3.cmml" xref="S3.I1.i1.p1.4.m4.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.4.m4.1c">\mathcal{D}_{x^{t}}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.4.m4.1d">caligraphic_D start_POSTSUBSCRIPT italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> for in-context learning. LLM formulates a tool usage plan based on user intent and preferences, providing inputs for each tool, i.e., <math alttext="\boldsymbol{p}^{t}=\{p^{t}_{1},\cdots,p^{t}_{n}\}=\operatorname{plan}\left(x^{%
t},C^{t-1},\mathcal{F},\mathcal{D}_{x^{t}}\right)" class="ltx_Math" display="inline" id="S3.I1.i1.p1.5.m5.8"><semantics id="S3.I1.i1.p1.5.m5.8a"><mrow id="S3.I1.i1.p1.5.m5.8.8" xref="S3.I1.i1.p1.5.m5.8.8.cmml"><msup id="S3.I1.i1.p1.5.m5.8.8.7" xref="S3.I1.i1.p1.5.m5.8.8.7.cmml"><mi id="S3.I1.i1.p1.5.m5.8.8.7.2" xref="S3.I1.i1.p1.5.m5.8.8.7.2.cmml">𝒑</mi><mi id="S3.I1.i1.p1.5.m5.8.8.7.3" xref="S3.I1.i1.p1.5.m5.8.8.7.3.cmml">t</mi></msup><mo id="S3.I1.i1.p1.5.m5.8.8.8" xref="S3.I1.i1.p1.5.m5.8.8.8.cmml">=</mo><mrow id="S3.I1.i1.p1.5.m5.5.5.2.2" xref="S3.I1.i1.p1.5.m5.5.5.2.3.cmml"><mo id="S3.I1.i1.p1.5.m5.5.5.2.2.3" stretchy="false" xref="S3.I1.i1.p1.5.m5.5.5.2.3.cmml">{</mo><msubsup id="S3.I1.i1.p1.5.m5.4.4.1.1.1" xref="S3.I1.i1.p1.5.m5.4.4.1.1.1.cmml"><mi id="S3.I1.i1.p1.5.m5.4.4.1.1.1.2.2" xref="S3.I1.i1.p1.5.m5.4.4.1.1.1.2.2.cmml">p</mi><mn id="S3.I1.i1.p1.5.m5.4.4.1.1.1.3" xref="S3.I1.i1.p1.5.m5.4.4.1.1.1.3.cmml">1</mn><mi id="S3.I1.i1.p1.5.m5.4.4.1.1.1.2.3" xref="S3.I1.i1.p1.5.m5.4.4.1.1.1.2.3.cmml">t</mi></msubsup><mo id="S3.I1.i1.p1.5.m5.5.5.2.2.4" xref="S3.I1.i1.p1.5.m5.5.5.2.3.cmml">,</mo><mi id="S3.I1.i1.p1.5.m5.1.1" mathvariant="normal" xref="S3.I1.i1.p1.5.m5.1.1.cmml">⋯</mi><mo id="S3.I1.i1.p1.5.m5.5.5.2.2.5" xref="S3.I1.i1.p1.5.m5.5.5.2.3.cmml">,</mo><msubsup id="S3.I1.i1.p1.5.m5.5.5.2.2.2" xref="S3.I1.i1.p1.5.m5.5.5.2.2.2.cmml"><mi id="S3.I1.i1.p1.5.m5.5.5.2.2.2.2.2" xref="S3.I1.i1.p1.5.m5.5.5.2.2.2.2.2.cmml">p</mi><mi id="S3.I1.i1.p1.5.m5.5.5.2.2.2.3" xref="S3.I1.i1.p1.5.m5.5.5.2.2.2.3.cmml">n</mi><mi id="S3.I1.i1.p1.5.m5.5.5.2.2.2.2.3" xref="S3.I1.i1.p1.5.m5.5.5.2.2.2.2.3.cmml">t</mi></msubsup><mo id="S3.I1.i1.p1.5.m5.5.5.2.2.6" stretchy="false" xref="S3.I1.i1.p1.5.m5.5.5.2.3.cmml">}</mo></mrow><mo id="S3.I1.i1.p1.5.m5.8.8.9" xref="S3.I1.i1.p1.5.m5.8.8.9.cmml">=</mo><mrow id="S3.I1.i1.p1.5.m5.8.8.5.3" xref="S3.I1.i1.p1.5.m5.8.8.5.4.cmml"><mi id="S3.I1.i1.p1.5.m5.2.2" xref="S3.I1.i1.p1.5.m5.2.2.cmml">plan</mi><mo id="S3.I1.i1.p1.5.m5.8.8.5.3a" xref="S3.I1.i1.p1.5.m5.8.8.5.4.cmml">⁡</mo><mrow id="S3.I1.i1.p1.5.m5.8.8.5.3.3" xref="S3.I1.i1.p1.5.m5.8.8.5.4.cmml"><mo id="S3.I1.i1.p1.5.m5.8.8.5.3.3.4" xref="S3.I1.i1.p1.5.m5.8.8.5.4.cmml">(</mo><msup id="S3.I1.i1.p1.5.m5.6.6.3.1.1.1" xref="S3.I1.i1.p1.5.m5.6.6.3.1.1.1.cmml"><mi id="S3.I1.i1.p1.5.m5.6.6.3.1.1.1.2" xref="S3.I1.i1.p1.5.m5.6.6.3.1.1.1.2.cmml">x</mi><mi id="S3.I1.i1.p1.5.m5.6.6.3.1.1.1.3" xref="S3.I1.i1.p1.5.m5.6.6.3.1.1.1.3.cmml">t</mi></msup><mo id="S3.I1.i1.p1.5.m5.8.8.5.3.3.5" xref="S3.I1.i1.p1.5.m5.8.8.5.4.cmml">,</mo><msup id="S3.I1.i1.p1.5.m5.7.7.4.2.2.2" xref="S3.I1.i1.p1.5.m5.7.7.4.2.2.2.cmml"><mi id="S3.I1.i1.p1.5.m5.7.7.4.2.2.2.2" xref="S3.I1.i1.p1.5.m5.7.7.4.2.2.2.2.cmml">C</mi><mrow id="S3.I1.i1.p1.5.m5.7.7.4.2.2.2.3" xref="S3.I1.i1.p1.5.m5.7.7.4.2.2.2.3.cmml"><mi id="S3.I1.i1.p1.5.m5.7.7.4.2.2.2.3.2" xref="S3.I1.i1.p1.5.m5.7.7.4.2.2.2.3.2.cmml">t</mi><mo id="S3.I1.i1.p1.5.m5.7.7.4.2.2.2.3.1" xref="S3.I1.i1.p1.5.m5.7.7.4.2.2.2.3.1.cmml">−</mo><mn id="S3.I1.i1.p1.5.m5.7.7.4.2.2.2.3.3" xref="S3.I1.i1.p1.5.m5.7.7.4.2.2.2.3.3.cmml">1</mn></mrow></msup><mo id="S3.I1.i1.p1.5.m5.8.8.5.3.3.6" xref="S3.I1.i1.p1.5.m5.8.8.5.4.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.I1.i1.p1.5.m5.3.3" xref="S3.I1.i1.p1.5.m5.3.3.cmml">ℱ</mi><mo id="S3.I1.i1.p1.5.m5.8.8.5.3.3.7" xref="S3.I1.i1.p1.5.m5.8.8.5.4.cmml">,</mo><msub id="S3.I1.i1.p1.5.m5.8.8.5.3.3.3" xref="S3.I1.i1.p1.5.m5.8.8.5.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i1.p1.5.m5.8.8.5.3.3.3.2" xref="S3.I1.i1.p1.5.m5.8.8.5.3.3.3.2.cmml">𝒟</mi><msup id="S3.I1.i1.p1.5.m5.8.8.5.3.3.3.3" xref="S3.I1.i1.p1.5.m5.8.8.5.3.3.3.3.cmml"><mi id="S3.I1.i1.p1.5.m5.8.8.5.3.3.3.3.2" xref="S3.I1.i1.p1.5.m5.8.8.5.3.3.3.3.2.cmml">x</mi><mi id="S3.I1.i1.p1.5.m5.8.8.5.3.3.3.3.3" xref="S3.I1.i1.p1.5.m5.8.8.5.3.3.3.3.3.cmml">t</mi></msup></msub><mo id="S3.I1.i1.p1.5.m5.8.8.5.3.3.8" xref="S3.I1.i1.p1.5.m5.8.8.5.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.5.m5.8b"><apply id="S3.I1.i1.p1.5.m5.8.8.cmml" xref="S3.I1.i1.p1.5.m5.8.8"><and id="S3.I1.i1.p1.5.m5.8.8a.cmml" xref="S3.I1.i1.p1.5.m5.8.8"></and><apply id="S3.I1.i1.p1.5.m5.8.8b.cmml" xref="S3.I1.i1.p1.5.m5.8.8"><eq id="S3.I1.i1.p1.5.m5.8.8.8.cmml" xref="S3.I1.i1.p1.5.m5.8.8.8"></eq><apply id="S3.I1.i1.p1.5.m5.8.8.7.cmml" xref="S3.I1.i1.p1.5.m5.8.8.7"><csymbol cd="ambiguous" id="S3.I1.i1.p1.5.m5.8.8.7.1.cmml" xref="S3.I1.i1.p1.5.m5.8.8.7">superscript</csymbol><ci id="S3.I1.i1.p1.5.m5.8.8.7.2.cmml" xref="S3.I1.i1.p1.5.m5.8.8.7.2">𝒑</ci><ci id="S3.I1.i1.p1.5.m5.8.8.7.3.cmml" xref="S3.I1.i1.p1.5.m5.8.8.7.3">𝑡</ci></apply><set id="S3.I1.i1.p1.5.m5.5.5.2.3.cmml" xref="S3.I1.i1.p1.5.m5.5.5.2.2"><apply id="S3.I1.i1.p1.5.m5.4.4.1.1.1.cmml" xref="S3.I1.i1.p1.5.m5.4.4.1.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.5.m5.4.4.1.1.1.1.cmml" xref="S3.I1.i1.p1.5.m5.4.4.1.1.1">subscript</csymbol><apply id="S3.I1.i1.p1.5.m5.4.4.1.1.1.2.cmml" xref="S3.I1.i1.p1.5.m5.4.4.1.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.5.m5.4.4.1.1.1.2.1.cmml" xref="S3.I1.i1.p1.5.m5.4.4.1.1.1">superscript</csymbol><ci id="S3.I1.i1.p1.5.m5.4.4.1.1.1.2.2.cmml" xref="S3.I1.i1.p1.5.m5.4.4.1.1.1.2.2">𝑝</ci><ci id="S3.I1.i1.p1.5.m5.4.4.1.1.1.2.3.cmml" xref="S3.I1.i1.p1.5.m5.4.4.1.1.1.2.3">𝑡</ci></apply><cn id="S3.I1.i1.p1.5.m5.4.4.1.1.1.3.cmml" type="integer" xref="S3.I1.i1.p1.5.m5.4.4.1.1.1.3">1</cn></apply><ci id="S3.I1.i1.p1.5.m5.1.1.cmml" xref="S3.I1.i1.p1.5.m5.1.1">⋯</ci><apply id="S3.I1.i1.p1.5.m5.5.5.2.2.2.cmml" xref="S3.I1.i1.p1.5.m5.5.5.2.2.2"><csymbol cd="ambiguous" id="S3.I1.i1.p1.5.m5.5.5.2.2.2.1.cmml" xref="S3.I1.i1.p1.5.m5.5.5.2.2.2">subscript</csymbol><apply id="S3.I1.i1.p1.5.m5.5.5.2.2.2.2.cmml" xref="S3.I1.i1.p1.5.m5.5.5.2.2.2"><csymbol cd="ambiguous" id="S3.I1.i1.p1.5.m5.5.5.2.2.2.2.1.cmml" xref="S3.I1.i1.p1.5.m5.5.5.2.2.2">superscript</csymbol><ci id="S3.I1.i1.p1.5.m5.5.5.2.2.2.2.2.cmml" xref="S3.I1.i1.p1.5.m5.5.5.2.2.2.2.2">𝑝</ci><ci id="S3.I1.i1.p1.5.m5.5.5.2.2.2.2.3.cmml" xref="S3.I1.i1.p1.5.m5.5.5.2.2.2.2.3">𝑡</ci></apply><ci id="S3.I1.i1.p1.5.m5.5.5.2.2.2.3.cmml" xref="S3.I1.i1.p1.5.m5.5.5.2.2.2.3">𝑛</ci></apply></set></apply><apply id="S3.I1.i1.p1.5.m5.8.8c.cmml" xref="S3.I1.i1.p1.5.m5.8.8"><eq id="S3.I1.i1.p1.5.m5.8.8.9.cmml" xref="S3.I1.i1.p1.5.m5.8.8.9"></eq><share href="#S3.I1.i1.p1.5.m5.5.5.2.cmml" id="S3.I1.i1.p1.5.m5.8.8d.cmml" xref="S3.I1.i1.p1.5.m5.8.8"></share><apply id="S3.I1.i1.p1.5.m5.8.8.5.4.cmml" xref="S3.I1.i1.p1.5.m5.8.8.5.3"><ci id="S3.I1.i1.p1.5.m5.2.2.cmml" xref="S3.I1.i1.p1.5.m5.2.2">plan</ci><apply id="S3.I1.i1.p1.5.m5.6.6.3.1.1.1.cmml" xref="S3.I1.i1.p1.5.m5.6.6.3.1.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.5.m5.6.6.3.1.1.1.1.cmml" xref="S3.I1.i1.p1.5.m5.6.6.3.1.1.1">superscript</csymbol><ci id="S3.I1.i1.p1.5.m5.6.6.3.1.1.1.2.cmml" xref="S3.I1.i1.p1.5.m5.6.6.3.1.1.1.2">𝑥</ci><ci id="S3.I1.i1.p1.5.m5.6.6.3.1.1.1.3.cmml" xref="S3.I1.i1.p1.5.m5.6.6.3.1.1.1.3">𝑡</ci></apply><apply id="S3.I1.i1.p1.5.m5.7.7.4.2.2.2.cmml" xref="S3.I1.i1.p1.5.m5.7.7.4.2.2.2"><csymbol cd="ambiguous" id="S3.I1.i1.p1.5.m5.7.7.4.2.2.2.1.cmml" xref="S3.I1.i1.p1.5.m5.7.7.4.2.2.2">superscript</csymbol><ci id="S3.I1.i1.p1.5.m5.7.7.4.2.2.2.2.cmml" xref="S3.I1.i1.p1.5.m5.7.7.4.2.2.2.2">𝐶</ci><apply id="S3.I1.i1.p1.5.m5.7.7.4.2.2.2.3.cmml" xref="S3.I1.i1.p1.5.m5.7.7.4.2.2.2.3"><minus id="S3.I1.i1.p1.5.m5.7.7.4.2.2.2.3.1.cmml" xref="S3.I1.i1.p1.5.m5.7.7.4.2.2.2.3.1"></minus><ci id="S3.I1.i1.p1.5.m5.7.7.4.2.2.2.3.2.cmml" xref="S3.I1.i1.p1.5.m5.7.7.4.2.2.2.3.2">𝑡</ci><cn id="S3.I1.i1.p1.5.m5.7.7.4.2.2.2.3.3.cmml" type="integer" xref="S3.I1.i1.p1.5.m5.7.7.4.2.2.2.3.3">1</cn></apply></apply><ci id="S3.I1.i1.p1.5.m5.3.3.cmml" xref="S3.I1.i1.p1.5.m5.3.3">ℱ</ci><apply id="S3.I1.i1.p1.5.m5.8.8.5.3.3.3.cmml" xref="S3.I1.i1.p1.5.m5.8.8.5.3.3.3"><csymbol cd="ambiguous" id="S3.I1.i1.p1.5.m5.8.8.5.3.3.3.1.cmml" xref="S3.I1.i1.p1.5.m5.8.8.5.3.3.3">subscript</csymbol><ci id="S3.I1.i1.p1.5.m5.8.8.5.3.3.3.2.cmml" xref="S3.I1.i1.p1.5.m5.8.8.5.3.3.3.2">𝒟</ci><apply id="S3.I1.i1.p1.5.m5.8.8.5.3.3.3.3.cmml" xref="S3.I1.i1.p1.5.m5.8.8.5.3.3.3.3"><csymbol cd="ambiguous" id="S3.I1.i1.p1.5.m5.8.8.5.3.3.3.3.1.cmml" xref="S3.I1.i1.p1.5.m5.8.8.5.3.3.3.3">superscript</csymbol><ci id="S3.I1.i1.p1.5.m5.8.8.5.3.3.3.3.2.cmml" xref="S3.I1.i1.p1.5.m5.8.8.5.3.3.3.3.2">𝑥</ci><ci id="S3.I1.i1.p1.5.m5.8.8.5.3.3.3.3.3.cmml" xref="S3.I1.i1.p1.5.m5.8.8.5.3.3.3.3.3">𝑡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.5.m5.8c">\boldsymbol{p}^{t}=\{p^{t}_{1},\cdots,p^{t}_{n}\}=\operatorname{plan}\left(x^{%
t},C^{t-1},\mathcal{F},\mathcal{D}_{x^{t}}\right)</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.5.m5.8d">bold_italic_p start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT = { italic_p start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ⋯ , italic_p start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } = roman_plan ( italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , italic_C start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT , caligraphic_F , caligraphic_D start_POSTSUBSCRIPT italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT end_POSTSUBSCRIPT )</annotation></semantics></math>, where <math alttext="p^{t}_{k}=(f_{k},i_{k})" class="ltx_Math" display="inline" id="S3.I1.i1.p1.6.m6.2"><semantics id="S3.I1.i1.p1.6.m6.2a"><mrow id="S3.I1.i1.p1.6.m6.2.2" xref="S3.I1.i1.p1.6.m6.2.2.cmml"><msubsup id="S3.I1.i1.p1.6.m6.2.2.4" xref="S3.I1.i1.p1.6.m6.2.2.4.cmml"><mi id="S3.I1.i1.p1.6.m6.2.2.4.2.2" xref="S3.I1.i1.p1.6.m6.2.2.4.2.2.cmml">p</mi><mi id="S3.I1.i1.p1.6.m6.2.2.4.3" xref="S3.I1.i1.p1.6.m6.2.2.4.3.cmml">k</mi><mi id="S3.I1.i1.p1.6.m6.2.2.4.2.3" xref="S3.I1.i1.p1.6.m6.2.2.4.2.3.cmml">t</mi></msubsup><mo id="S3.I1.i1.p1.6.m6.2.2.3" xref="S3.I1.i1.p1.6.m6.2.2.3.cmml">=</mo><mrow id="S3.I1.i1.p1.6.m6.2.2.2.2" xref="S3.I1.i1.p1.6.m6.2.2.2.3.cmml"><mo id="S3.I1.i1.p1.6.m6.2.2.2.2.3" stretchy="false" xref="S3.I1.i1.p1.6.m6.2.2.2.3.cmml">(</mo><msub id="S3.I1.i1.p1.6.m6.1.1.1.1.1" xref="S3.I1.i1.p1.6.m6.1.1.1.1.1.cmml"><mi id="S3.I1.i1.p1.6.m6.1.1.1.1.1.2" xref="S3.I1.i1.p1.6.m6.1.1.1.1.1.2.cmml">f</mi><mi id="S3.I1.i1.p1.6.m6.1.1.1.1.1.3" xref="S3.I1.i1.p1.6.m6.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S3.I1.i1.p1.6.m6.2.2.2.2.4" xref="S3.I1.i1.p1.6.m6.2.2.2.3.cmml">,</mo><msub id="S3.I1.i1.p1.6.m6.2.2.2.2.2" xref="S3.I1.i1.p1.6.m6.2.2.2.2.2.cmml"><mi id="S3.I1.i1.p1.6.m6.2.2.2.2.2.2" xref="S3.I1.i1.p1.6.m6.2.2.2.2.2.2.cmml">i</mi><mi id="S3.I1.i1.p1.6.m6.2.2.2.2.2.3" xref="S3.I1.i1.p1.6.m6.2.2.2.2.2.3.cmml">k</mi></msub><mo id="S3.I1.i1.p1.6.m6.2.2.2.2.5" stretchy="false" xref="S3.I1.i1.p1.6.m6.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.6.m6.2b"><apply id="S3.I1.i1.p1.6.m6.2.2.cmml" xref="S3.I1.i1.p1.6.m6.2.2"><eq id="S3.I1.i1.p1.6.m6.2.2.3.cmml" xref="S3.I1.i1.p1.6.m6.2.2.3"></eq><apply id="S3.I1.i1.p1.6.m6.2.2.4.cmml" xref="S3.I1.i1.p1.6.m6.2.2.4"><csymbol cd="ambiguous" id="S3.I1.i1.p1.6.m6.2.2.4.1.cmml" xref="S3.I1.i1.p1.6.m6.2.2.4">subscript</csymbol><apply id="S3.I1.i1.p1.6.m6.2.2.4.2.cmml" xref="S3.I1.i1.p1.6.m6.2.2.4"><csymbol cd="ambiguous" id="S3.I1.i1.p1.6.m6.2.2.4.2.1.cmml" xref="S3.I1.i1.p1.6.m6.2.2.4">superscript</csymbol><ci id="S3.I1.i1.p1.6.m6.2.2.4.2.2.cmml" xref="S3.I1.i1.p1.6.m6.2.2.4.2.2">𝑝</ci><ci id="S3.I1.i1.p1.6.m6.2.2.4.2.3.cmml" xref="S3.I1.i1.p1.6.m6.2.2.4.2.3">𝑡</ci></apply><ci id="S3.I1.i1.p1.6.m6.2.2.4.3.cmml" xref="S3.I1.i1.p1.6.m6.2.2.4.3">𝑘</ci></apply><interval closure="open" id="S3.I1.i1.p1.6.m6.2.2.2.3.cmml" xref="S3.I1.i1.p1.6.m6.2.2.2.2"><apply id="S3.I1.i1.p1.6.m6.1.1.1.1.1.cmml" xref="S3.I1.i1.p1.6.m6.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.6.m6.1.1.1.1.1.1.cmml" xref="S3.I1.i1.p1.6.m6.1.1.1.1.1">subscript</csymbol><ci id="S3.I1.i1.p1.6.m6.1.1.1.1.1.2.cmml" xref="S3.I1.i1.p1.6.m6.1.1.1.1.1.2">𝑓</ci><ci id="S3.I1.i1.p1.6.m6.1.1.1.1.1.3.cmml" xref="S3.I1.i1.p1.6.m6.1.1.1.1.1.3">𝑘</ci></apply><apply id="S3.I1.i1.p1.6.m6.2.2.2.2.2.cmml" xref="S3.I1.i1.p1.6.m6.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.I1.i1.p1.6.m6.2.2.2.2.2.1.cmml" xref="S3.I1.i1.p1.6.m6.2.2.2.2.2">subscript</csymbol><ci id="S3.I1.i1.p1.6.m6.2.2.2.2.2.2.cmml" xref="S3.I1.i1.p1.6.m6.2.2.2.2.2.2">𝑖</ci><ci id="S3.I1.i1.p1.6.m6.2.2.2.2.2.3.cmml" xref="S3.I1.i1.p1.6.m6.2.2.2.2.2.3">𝑘</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.6.m6.2c">p^{t}_{k}=(f_{k},i_{k})</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.6.m6.2d">italic_p start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = ( italic_f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_i start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )</annotation></semantics></math> consists of the tool <math alttext="f_{k}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.7.m7.1"><semantics id="S3.I1.i1.p1.7.m7.1a"><msub id="S3.I1.i1.p1.7.m7.1.1" xref="S3.I1.i1.p1.7.m7.1.1.cmml"><mi id="S3.I1.i1.p1.7.m7.1.1.2" xref="S3.I1.i1.p1.7.m7.1.1.2.cmml">f</mi><mi id="S3.I1.i1.p1.7.m7.1.1.3" xref="S3.I1.i1.p1.7.m7.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.7.m7.1b"><apply id="S3.I1.i1.p1.7.m7.1.1.cmml" xref="S3.I1.i1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.7.m7.1.1.1.cmml" xref="S3.I1.i1.p1.7.m7.1.1">subscript</csymbol><ci id="S3.I1.i1.p1.7.m7.1.1.2.cmml" xref="S3.I1.i1.p1.7.m7.1.1.2">𝑓</ci><ci id="S3.I1.i1.p1.7.m7.1.1.3.cmml" xref="S3.I1.i1.p1.7.m7.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.7.m7.1c">f_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.7.m7.1d">italic_f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> and its input <math alttext="i_{k}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.8.m8.1"><semantics id="S3.I1.i1.p1.8.m8.1a"><msub id="S3.I1.i1.p1.8.m8.1.1" xref="S3.I1.i1.p1.8.m8.1.1.cmml"><mi id="S3.I1.i1.p1.8.m8.1.1.2" xref="S3.I1.i1.p1.8.m8.1.1.2.cmml">i</mi><mi id="S3.I1.i1.p1.8.m8.1.1.3" xref="S3.I1.i1.p1.8.m8.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.8.m8.1b"><apply id="S3.I1.i1.p1.8.m8.1.1.cmml" xref="S3.I1.i1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.8.m8.1.1.1.cmml" xref="S3.I1.i1.p1.8.m8.1.1">subscript</csymbol><ci id="S3.I1.i1.p1.8.m8.1.1.2.cmml" xref="S3.I1.i1.p1.8.m8.1.1.2">𝑖</ci><ci id="S3.I1.i1.p1.8.m8.1.1.3.cmml" xref="S3.I1.i1.p1.8.m8.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.8.m8.1c">i_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.8.m8.1d">italic_i start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.6"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.6.1">Execution</span>: The tool executor invokes the tools step-by-step according to the plan <math alttext="\boldsymbol{p}_{t}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.1.m1.1"><semantics id="S3.I1.i2.p1.1.m1.1a"><msub id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml"><mi id="S3.I1.i2.p1.1.m1.1.1.2" xref="S3.I1.i2.p1.1.m1.1.1.2.cmml">𝒑</mi><mi id="S3.I1.i2.p1.1.m1.1.1.3" xref="S3.I1.i2.p1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><apply id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.1.m1.1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I1.i2.p1.1.m1.1.1.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2">𝒑</ci><ci id="S3.I1.i2.p1.1.m1.1.1.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">\boldsymbol{p}_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.1.m1.1d">bold_italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and obtains outputs from each tool, i.e., <math alttext="\boldsymbol{o}^{t}=\{o^{t}_{1},\cdots,o^{t}_{n}\}=\operatorname{exec}(%
\boldsymbol{p}^{t},\mathcal{F})" class="ltx_Math" display="inline" id="S3.I1.i2.p1.2.m2.6"><semantics id="S3.I1.i2.p1.2.m2.6a"><mrow id="S3.I1.i2.p1.2.m2.6.6" xref="S3.I1.i2.p1.2.m2.6.6.cmml"><msup id="S3.I1.i2.p1.2.m2.6.6.5" xref="S3.I1.i2.p1.2.m2.6.6.5.cmml"><mi id="S3.I1.i2.p1.2.m2.6.6.5.2" xref="S3.I1.i2.p1.2.m2.6.6.5.2.cmml">𝒐</mi><mi id="S3.I1.i2.p1.2.m2.6.6.5.3" xref="S3.I1.i2.p1.2.m2.6.6.5.3.cmml">t</mi></msup><mo id="S3.I1.i2.p1.2.m2.6.6.6" xref="S3.I1.i2.p1.2.m2.6.6.6.cmml">=</mo><mrow id="S3.I1.i2.p1.2.m2.5.5.2.2" xref="S3.I1.i2.p1.2.m2.5.5.2.3.cmml"><mo id="S3.I1.i2.p1.2.m2.5.5.2.2.3" stretchy="false" xref="S3.I1.i2.p1.2.m2.5.5.2.3.cmml">{</mo><msubsup id="S3.I1.i2.p1.2.m2.4.4.1.1.1" xref="S3.I1.i2.p1.2.m2.4.4.1.1.1.cmml"><mi id="S3.I1.i2.p1.2.m2.4.4.1.1.1.2.2" xref="S3.I1.i2.p1.2.m2.4.4.1.1.1.2.2.cmml">o</mi><mn id="S3.I1.i2.p1.2.m2.4.4.1.1.1.3" xref="S3.I1.i2.p1.2.m2.4.4.1.1.1.3.cmml">1</mn><mi id="S3.I1.i2.p1.2.m2.4.4.1.1.1.2.3" xref="S3.I1.i2.p1.2.m2.4.4.1.1.1.2.3.cmml">t</mi></msubsup><mo id="S3.I1.i2.p1.2.m2.5.5.2.2.4" xref="S3.I1.i2.p1.2.m2.5.5.2.3.cmml">,</mo><mi id="S3.I1.i2.p1.2.m2.1.1" mathvariant="normal" xref="S3.I1.i2.p1.2.m2.1.1.cmml">⋯</mi><mo id="S3.I1.i2.p1.2.m2.5.5.2.2.5" xref="S3.I1.i2.p1.2.m2.5.5.2.3.cmml">,</mo><msubsup id="S3.I1.i2.p1.2.m2.5.5.2.2.2" xref="S3.I1.i2.p1.2.m2.5.5.2.2.2.cmml"><mi id="S3.I1.i2.p1.2.m2.5.5.2.2.2.2.2" xref="S3.I1.i2.p1.2.m2.5.5.2.2.2.2.2.cmml">o</mi><mi id="S3.I1.i2.p1.2.m2.5.5.2.2.2.3" xref="S3.I1.i2.p1.2.m2.5.5.2.2.2.3.cmml">n</mi><mi id="S3.I1.i2.p1.2.m2.5.5.2.2.2.2.3" xref="S3.I1.i2.p1.2.m2.5.5.2.2.2.2.3.cmml">t</mi></msubsup><mo id="S3.I1.i2.p1.2.m2.5.5.2.2.6" stretchy="false" xref="S3.I1.i2.p1.2.m2.5.5.2.3.cmml">}</mo></mrow><mo id="S3.I1.i2.p1.2.m2.6.6.7" xref="S3.I1.i2.p1.2.m2.6.6.7.cmml">=</mo><mrow id="S3.I1.i2.p1.2.m2.6.6.3.1" xref="S3.I1.i2.p1.2.m2.6.6.3.2.cmml"><mi id="S3.I1.i2.p1.2.m2.2.2" xref="S3.I1.i2.p1.2.m2.2.2.cmml">exec</mi><mo id="S3.I1.i2.p1.2.m2.6.6.3.1a" xref="S3.I1.i2.p1.2.m2.6.6.3.2.cmml">⁡</mo><mrow id="S3.I1.i2.p1.2.m2.6.6.3.1.1" xref="S3.I1.i2.p1.2.m2.6.6.3.2.cmml"><mo id="S3.I1.i2.p1.2.m2.6.6.3.1.1.2" stretchy="false" xref="S3.I1.i2.p1.2.m2.6.6.3.2.cmml">(</mo><msup id="S3.I1.i2.p1.2.m2.6.6.3.1.1.1" xref="S3.I1.i2.p1.2.m2.6.6.3.1.1.1.cmml"><mi id="S3.I1.i2.p1.2.m2.6.6.3.1.1.1.2" xref="S3.I1.i2.p1.2.m2.6.6.3.1.1.1.2.cmml">𝒑</mi><mi id="S3.I1.i2.p1.2.m2.6.6.3.1.1.1.3" xref="S3.I1.i2.p1.2.m2.6.6.3.1.1.1.3.cmml">t</mi></msup><mo id="S3.I1.i2.p1.2.m2.6.6.3.1.1.3" xref="S3.I1.i2.p1.2.m2.6.6.3.2.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.I1.i2.p1.2.m2.3.3" xref="S3.I1.i2.p1.2.m2.3.3.cmml">ℱ</mi><mo id="S3.I1.i2.p1.2.m2.6.6.3.1.1.4" stretchy="false" xref="S3.I1.i2.p1.2.m2.6.6.3.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.2.m2.6b"><apply id="S3.I1.i2.p1.2.m2.6.6.cmml" xref="S3.I1.i2.p1.2.m2.6.6"><and id="S3.I1.i2.p1.2.m2.6.6a.cmml" xref="S3.I1.i2.p1.2.m2.6.6"></and><apply id="S3.I1.i2.p1.2.m2.6.6b.cmml" xref="S3.I1.i2.p1.2.m2.6.6"><eq id="S3.I1.i2.p1.2.m2.6.6.6.cmml" xref="S3.I1.i2.p1.2.m2.6.6.6"></eq><apply id="S3.I1.i2.p1.2.m2.6.6.5.cmml" xref="S3.I1.i2.p1.2.m2.6.6.5"><csymbol cd="ambiguous" id="S3.I1.i2.p1.2.m2.6.6.5.1.cmml" xref="S3.I1.i2.p1.2.m2.6.6.5">superscript</csymbol><ci id="S3.I1.i2.p1.2.m2.6.6.5.2.cmml" xref="S3.I1.i2.p1.2.m2.6.6.5.2">𝒐</ci><ci id="S3.I1.i2.p1.2.m2.6.6.5.3.cmml" xref="S3.I1.i2.p1.2.m2.6.6.5.3">𝑡</ci></apply><set id="S3.I1.i2.p1.2.m2.5.5.2.3.cmml" xref="S3.I1.i2.p1.2.m2.5.5.2.2"><apply id="S3.I1.i2.p1.2.m2.4.4.1.1.1.cmml" xref="S3.I1.i2.p1.2.m2.4.4.1.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.2.m2.4.4.1.1.1.1.cmml" xref="S3.I1.i2.p1.2.m2.4.4.1.1.1">subscript</csymbol><apply id="S3.I1.i2.p1.2.m2.4.4.1.1.1.2.cmml" xref="S3.I1.i2.p1.2.m2.4.4.1.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.2.m2.4.4.1.1.1.2.1.cmml" xref="S3.I1.i2.p1.2.m2.4.4.1.1.1">superscript</csymbol><ci id="S3.I1.i2.p1.2.m2.4.4.1.1.1.2.2.cmml" xref="S3.I1.i2.p1.2.m2.4.4.1.1.1.2.2">𝑜</ci><ci id="S3.I1.i2.p1.2.m2.4.4.1.1.1.2.3.cmml" xref="S3.I1.i2.p1.2.m2.4.4.1.1.1.2.3">𝑡</ci></apply><cn id="S3.I1.i2.p1.2.m2.4.4.1.1.1.3.cmml" type="integer" xref="S3.I1.i2.p1.2.m2.4.4.1.1.1.3">1</cn></apply><ci id="S3.I1.i2.p1.2.m2.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1">⋯</ci><apply id="S3.I1.i2.p1.2.m2.5.5.2.2.2.cmml" xref="S3.I1.i2.p1.2.m2.5.5.2.2.2"><csymbol cd="ambiguous" id="S3.I1.i2.p1.2.m2.5.5.2.2.2.1.cmml" xref="S3.I1.i2.p1.2.m2.5.5.2.2.2">subscript</csymbol><apply id="S3.I1.i2.p1.2.m2.5.5.2.2.2.2.cmml" xref="S3.I1.i2.p1.2.m2.5.5.2.2.2"><csymbol cd="ambiguous" id="S3.I1.i2.p1.2.m2.5.5.2.2.2.2.1.cmml" xref="S3.I1.i2.p1.2.m2.5.5.2.2.2">superscript</csymbol><ci id="S3.I1.i2.p1.2.m2.5.5.2.2.2.2.2.cmml" xref="S3.I1.i2.p1.2.m2.5.5.2.2.2.2.2">𝑜</ci><ci id="S3.I1.i2.p1.2.m2.5.5.2.2.2.2.3.cmml" xref="S3.I1.i2.p1.2.m2.5.5.2.2.2.2.3">𝑡</ci></apply><ci id="S3.I1.i2.p1.2.m2.5.5.2.2.2.3.cmml" xref="S3.I1.i2.p1.2.m2.5.5.2.2.2.3">𝑛</ci></apply></set></apply><apply id="S3.I1.i2.p1.2.m2.6.6c.cmml" xref="S3.I1.i2.p1.2.m2.6.6"><eq id="S3.I1.i2.p1.2.m2.6.6.7.cmml" xref="S3.I1.i2.p1.2.m2.6.6.7"></eq><share href="#S3.I1.i2.p1.2.m2.5.5.2.cmml" id="S3.I1.i2.p1.2.m2.6.6d.cmml" xref="S3.I1.i2.p1.2.m2.6.6"></share><apply id="S3.I1.i2.p1.2.m2.6.6.3.2.cmml" xref="S3.I1.i2.p1.2.m2.6.6.3.1"><ci id="S3.I1.i2.p1.2.m2.2.2.cmml" xref="S3.I1.i2.p1.2.m2.2.2">exec</ci><apply id="S3.I1.i2.p1.2.m2.6.6.3.1.1.1.cmml" xref="S3.I1.i2.p1.2.m2.6.6.3.1.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.2.m2.6.6.3.1.1.1.1.cmml" xref="S3.I1.i2.p1.2.m2.6.6.3.1.1.1">superscript</csymbol><ci id="S3.I1.i2.p1.2.m2.6.6.3.1.1.1.2.cmml" xref="S3.I1.i2.p1.2.m2.6.6.3.1.1.1.2">𝒑</ci><ci id="S3.I1.i2.p1.2.m2.6.6.3.1.1.1.3.cmml" xref="S3.I1.i2.p1.2.m2.6.6.3.1.1.1.3">𝑡</ci></apply><ci id="S3.I1.i2.p1.2.m2.3.3.cmml" xref="S3.I1.i2.p1.2.m2.3.3">ℱ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.2.m2.6c">\boldsymbol{o}^{t}=\{o^{t}_{1},\cdots,o^{t}_{n}\}=\operatorname{exec}(%
\boldsymbol{p}^{t},\mathcal{F})</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.2.m2.6d">bold_italic_o start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT = { italic_o start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ⋯ , italic_o start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } = roman_exec ( bold_italic_p start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , caligraphic_F )</annotation></semantics></math>. The output feedback of each tool <math alttext="f_{k}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.3.m3.1"><semantics id="S3.I1.i2.p1.3.m3.1a"><msub id="S3.I1.i2.p1.3.m3.1.1" xref="S3.I1.i2.p1.3.m3.1.1.cmml"><mi id="S3.I1.i2.p1.3.m3.1.1.2" xref="S3.I1.i2.p1.3.m3.1.1.2.cmml">f</mi><mi id="S3.I1.i2.p1.3.m3.1.1.3" xref="S3.I1.i2.p1.3.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.3.m3.1b"><apply id="S3.I1.i2.p1.3.m3.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.3.m3.1.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.I1.i2.p1.3.m3.1.1.2.cmml" xref="S3.I1.i2.p1.3.m3.1.1.2">𝑓</ci><ci id="S3.I1.i2.p1.3.m3.1.1.3.cmml" xref="S3.I1.i2.p1.3.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.3.m3.1c">f_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.3.m3.1d">italic_f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> is defined as <math alttext="o^{t}_{k}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.4.m4.1"><semantics id="S3.I1.i2.p1.4.m4.1a"><msubsup id="S3.I1.i2.p1.4.m4.1.1" xref="S3.I1.i2.p1.4.m4.1.1.cmml"><mi id="S3.I1.i2.p1.4.m4.1.1.2.2" xref="S3.I1.i2.p1.4.m4.1.1.2.2.cmml">o</mi><mi id="S3.I1.i2.p1.4.m4.1.1.3" xref="S3.I1.i2.p1.4.m4.1.1.3.cmml">k</mi><mi id="S3.I1.i2.p1.4.m4.1.1.2.3" xref="S3.I1.i2.p1.4.m4.1.1.2.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.4.m4.1b"><apply id="S3.I1.i2.p1.4.m4.1.1.cmml" xref="S3.I1.i2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.4.m4.1.1.1.cmml" xref="S3.I1.i2.p1.4.m4.1.1">subscript</csymbol><apply id="S3.I1.i2.p1.4.m4.1.1.2.cmml" xref="S3.I1.i2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.4.m4.1.1.2.1.cmml" xref="S3.I1.i2.p1.4.m4.1.1">superscript</csymbol><ci id="S3.I1.i2.p1.4.m4.1.1.2.2.cmml" xref="S3.I1.i2.p1.4.m4.1.1.2.2">𝑜</ci><ci id="S3.I1.i2.p1.4.m4.1.1.2.3.cmml" xref="S3.I1.i2.p1.4.m4.1.1.2.3">𝑡</ci></apply><ci id="S3.I1.i2.p1.4.m4.1.1.3.cmml" xref="S3.I1.i2.p1.4.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.4.m4.1c">o^{t}_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.4.m4.1d">italic_o start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>, where only the item information <math alttext="o^{t}_{n}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.5.m5.1"><semantics id="S3.I1.i2.p1.5.m5.1a"><msubsup id="S3.I1.i2.p1.5.m5.1.1" xref="S3.I1.i2.p1.5.m5.1.1.cmml"><mi id="S3.I1.i2.p1.5.m5.1.1.2.2" xref="S3.I1.i2.p1.5.m5.1.1.2.2.cmml">o</mi><mi id="S3.I1.i2.p1.5.m5.1.1.3" xref="S3.I1.i2.p1.5.m5.1.1.3.cmml">n</mi><mi id="S3.I1.i2.p1.5.m5.1.1.2.3" xref="S3.I1.i2.p1.5.m5.1.1.2.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.5.m5.1b"><apply id="S3.I1.i2.p1.5.m5.1.1.cmml" xref="S3.I1.i2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.5.m5.1.1.1.cmml" xref="S3.I1.i2.p1.5.m5.1.1">subscript</csymbol><apply id="S3.I1.i2.p1.5.m5.1.1.2.cmml" xref="S3.I1.i2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.5.m5.1.1.2.1.cmml" xref="S3.I1.i2.p1.5.m5.1.1">superscript</csymbol><ci id="S3.I1.i2.p1.5.m5.1.1.2.2.cmml" xref="S3.I1.i2.p1.5.m5.1.1.2.2">𝑜</ci><ci id="S3.I1.i2.p1.5.m5.1.1.2.3.cmml" xref="S3.I1.i2.p1.5.m5.1.1.2.3">𝑡</ci></apply><ci id="S3.I1.i2.p1.5.m5.1.1.3.cmml" xref="S3.I1.i2.p1.5.m5.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.5.m5.1c">o^{t}_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.5.m5.1d">italic_o start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> from the last tool’s output serves as LLM’s observation for generating the response <math alttext="y^{t}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.6.m6.1"><semantics id="S3.I1.i2.p1.6.m6.1a"><msup id="S3.I1.i2.p1.6.m6.1.1" xref="S3.I1.i2.p1.6.m6.1.1.cmml"><mi id="S3.I1.i2.p1.6.m6.1.1.2" xref="S3.I1.i2.p1.6.m6.1.1.2.cmml">y</mi><mi id="S3.I1.i2.p1.6.m6.1.1.3" xref="S3.I1.i2.p1.6.m6.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.6.m6.1b"><apply id="S3.I1.i2.p1.6.m6.1.1.cmml" xref="S3.I1.i2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.6.m6.1.1.1.cmml" xref="S3.I1.i2.p1.6.m6.1.1">superscript</csymbol><ci id="S3.I1.i2.p1.6.m6.1.1.2.cmml" xref="S3.I1.i2.p1.6.m6.1.1.2">𝑦</ci><ci id="S3.I1.i2.p1.6.m6.1.1.3.cmml" xref="S3.I1.i2.p1.6.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.6.m6.1c">y^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.6.m6.1d">italic_y start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math>. The remaining information is tracked by the candidate memory bus for further reflection (see Section <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S3.SS4" title="3.4 Reflection ‣ 3 Methodologies ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">3.4</span></a>).</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">We summarize the differences between our plan-first execution strategy and <span class="ltx_text ltx_font_slanted" id="S3.SS3.p3.1.1">step-by-step</span> strategy in Table <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S3.T1" title="Table 1 ‣ 3.3 Plan-first Execution with Dynamic Demonstrations ‣ 3 Methodologies ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">1</span></a> from six aspects. Fundamentally, <span class="ltx_text ltx_font_slanted" id="S3.SS3.p3.1.2">step-by-step</span> strategy executes reasoning and action execution alternately, while our plan-first execution is a two-phase strategy, where a series of executions is conducted followed by one-time planning.
In <span class="ltx_text ltx_font_slanted" id="S3.SS3.p3.1.3">step-by-step</span> strategy, the LLMs are responsible for thinking and reasoning at each step. The task entails reasoning for individual observation, resulting in-context learning being challenging due to the difficulty in crafting demonstrations comprising dynamic observations. Differently, the primary task of LLM in our plan-first execution is to make a tool utilizing plan, which could be easily guided by <math alttext="\langle\text{query},\text{plan}\rangle" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.2"><semantics id="S3.SS3.p3.1.m1.2a"><mrow id="S3.SS3.p3.1.m1.2.3.2" xref="S3.SS3.p3.1.m1.2.3.1.cmml"><mo id="S3.SS3.p3.1.m1.2.3.2.1" stretchy="false" xref="S3.SS3.p3.1.m1.2.3.1.cmml">⟨</mo><mtext id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1a.cmml">query</mtext><mo id="S3.SS3.p3.1.m1.2.3.2.2" xref="S3.SS3.p3.1.m1.2.3.1.cmml">,</mo><mtext id="S3.SS3.p3.1.m1.2.2" xref="S3.SS3.p3.1.m1.2.2a.cmml">plan</mtext><mo id="S3.SS3.p3.1.m1.2.3.2.3" stretchy="false" xref="S3.SS3.p3.1.m1.2.3.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.2b"><list id="S3.SS3.p3.1.m1.2.3.1.cmml" xref="S3.SS3.p3.1.m1.2.3.2"><ci id="S3.SS3.p3.1.m1.1.1a.cmml" xref="S3.SS3.p3.1.m1.1.1"><mtext id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">query</mtext></ci><ci id="S3.SS3.p3.1.m1.2.2a.cmml" xref="S3.SS3.p3.1.m1.2.2"><mtext id="S3.SS3.p3.1.m1.2.2.cmml" xref="S3.SS3.p3.1.m1.2.2">plan</mtext></ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.2c">\langle\text{query},\text{plan}\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.2d">⟨ query , plan ⟩</annotation></semantics></math> pairs.
The foremost advantage of our plan-first execution resides in the reduction of API calls. When employing N steps to address a task, our strategy necessitates merely 2 API calls, as opposed to N+1 calls in ReAct. This leads to a decrease in latency, which is of particular importance in conversational settings.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Property Comparisons between ReAct and Plan-first Execution. ICL is the abbreviation of In-Context Learning. </figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.2">
<tr class="ltx_tr" id="S3.T1.2.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.2.3.1">Property</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.2.3.2">ReAct</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.2.3.3">Plan-first Exe</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.4.1">Basic Idea</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.4.2">step-wise reason</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.4.3">task-wise plan</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.5.1">ICL</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.5.2">hard</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.5.3">easy</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.6.1">Reflection</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.6.2">internal</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.6.3">external</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.7.1"># API Call</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.7.2">N+1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.7.3">2</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.2">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.2.2.3">Latency</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.1.1.1"><math alttext="(N+1)\Delta t_{api}+\Delta t_{exe}" class="ltx_Math" display="inline" id="S3.T1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.m1.1a"><mrow id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml"><mrow id="S3.T1.1.1.1.m1.1.1.1" xref="S3.T1.1.1.1.m1.1.1.1.cmml"><mrow id="S3.T1.1.1.1.m1.1.1.1.1.1" xref="S3.T1.1.1.1.m1.1.1.1.1.1.1.cmml"><mo id="S3.T1.1.1.1.m1.1.1.1.1.1.2" stretchy="false" xref="S3.T1.1.1.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.T1.1.1.1.m1.1.1.1.1.1.1" xref="S3.T1.1.1.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.T1.1.1.1.m1.1.1.1.1.1.1.2" xref="S3.T1.1.1.1.m1.1.1.1.1.1.1.2.cmml">N</mi><mo id="S3.T1.1.1.1.m1.1.1.1.1.1.1.1" xref="S3.T1.1.1.1.m1.1.1.1.1.1.1.1.cmml">+</mo><mn id="S3.T1.1.1.1.m1.1.1.1.1.1.1.3" xref="S3.T1.1.1.1.m1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.T1.1.1.1.m1.1.1.1.1.1.3" stretchy="false" xref="S3.T1.1.1.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.T1.1.1.1.m1.1.1.1.2" xref="S3.T1.1.1.1.m1.1.1.1.2.cmml">⁢</mo><mi id="S3.T1.1.1.1.m1.1.1.1.3" mathvariant="normal" xref="S3.T1.1.1.1.m1.1.1.1.3.cmml">Δ</mi><mo id="S3.T1.1.1.1.m1.1.1.1.2a" xref="S3.T1.1.1.1.m1.1.1.1.2.cmml">⁢</mo><msub id="S3.T1.1.1.1.m1.1.1.1.4" xref="S3.T1.1.1.1.m1.1.1.1.4.cmml"><mi id="S3.T1.1.1.1.m1.1.1.1.4.2" xref="S3.T1.1.1.1.m1.1.1.1.4.2.cmml">t</mi><mrow id="S3.T1.1.1.1.m1.1.1.1.4.3" xref="S3.T1.1.1.1.m1.1.1.1.4.3.cmml"><mi id="S3.T1.1.1.1.m1.1.1.1.4.3.2" xref="S3.T1.1.1.1.m1.1.1.1.4.3.2.cmml">a</mi><mo id="S3.T1.1.1.1.m1.1.1.1.4.3.1" xref="S3.T1.1.1.1.m1.1.1.1.4.3.1.cmml">⁢</mo><mi id="S3.T1.1.1.1.m1.1.1.1.4.3.3" xref="S3.T1.1.1.1.m1.1.1.1.4.3.3.cmml">p</mi><mo id="S3.T1.1.1.1.m1.1.1.1.4.3.1a" xref="S3.T1.1.1.1.m1.1.1.1.4.3.1.cmml">⁢</mo><mi id="S3.T1.1.1.1.m1.1.1.1.4.3.4" xref="S3.T1.1.1.1.m1.1.1.1.4.3.4.cmml">i</mi></mrow></msub></mrow><mo id="S3.T1.1.1.1.m1.1.1.2" xref="S3.T1.1.1.1.m1.1.1.2.cmml">+</mo><mrow id="S3.T1.1.1.1.m1.1.1.3" xref="S3.T1.1.1.1.m1.1.1.3.cmml"><mi id="S3.T1.1.1.1.m1.1.1.3.2" mathvariant="normal" xref="S3.T1.1.1.1.m1.1.1.3.2.cmml">Δ</mi><mo id="S3.T1.1.1.1.m1.1.1.3.1" xref="S3.T1.1.1.1.m1.1.1.3.1.cmml">⁢</mo><msub id="S3.T1.1.1.1.m1.1.1.3.3" xref="S3.T1.1.1.1.m1.1.1.3.3.cmml"><mi id="S3.T1.1.1.1.m1.1.1.3.3.2" xref="S3.T1.1.1.1.m1.1.1.3.3.2.cmml">t</mi><mrow id="S3.T1.1.1.1.m1.1.1.3.3.3" xref="S3.T1.1.1.1.m1.1.1.3.3.3.cmml"><mi id="S3.T1.1.1.1.m1.1.1.3.3.3.2" xref="S3.T1.1.1.1.m1.1.1.3.3.3.2.cmml">e</mi><mo id="S3.T1.1.1.1.m1.1.1.3.3.3.1" xref="S3.T1.1.1.1.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.T1.1.1.1.m1.1.1.3.3.3.3" xref="S3.T1.1.1.1.m1.1.1.3.3.3.3.cmml">x</mi><mo id="S3.T1.1.1.1.m1.1.1.3.3.3.1a" xref="S3.T1.1.1.1.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.T1.1.1.1.m1.1.1.3.3.3.4" xref="S3.T1.1.1.1.m1.1.1.3.3.3.4.cmml">e</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><apply id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1"><plus id="S3.T1.1.1.1.m1.1.1.2.cmml" xref="S3.T1.1.1.1.m1.1.1.2"></plus><apply id="S3.T1.1.1.1.m1.1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1.1"><times id="S3.T1.1.1.1.m1.1.1.1.2.cmml" xref="S3.T1.1.1.1.m1.1.1.1.2"></times><apply id="S3.T1.1.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1.1.1.1"><plus id="S3.T1.1.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1.1.1.1.1.1"></plus><ci id="S3.T1.1.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.T1.1.1.1.m1.1.1.1.1.1.1.2">𝑁</ci><cn id="S3.T1.1.1.1.m1.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.T1.1.1.1.m1.1.1.1.1.1.1.3">1</cn></apply><ci id="S3.T1.1.1.1.m1.1.1.1.3.cmml" xref="S3.T1.1.1.1.m1.1.1.1.3">Δ</ci><apply id="S3.T1.1.1.1.m1.1.1.1.4.cmml" xref="S3.T1.1.1.1.m1.1.1.1.4"><csymbol cd="ambiguous" id="S3.T1.1.1.1.m1.1.1.1.4.1.cmml" xref="S3.T1.1.1.1.m1.1.1.1.4">subscript</csymbol><ci id="S3.T1.1.1.1.m1.1.1.1.4.2.cmml" xref="S3.T1.1.1.1.m1.1.1.1.4.2">𝑡</ci><apply id="S3.T1.1.1.1.m1.1.1.1.4.3.cmml" xref="S3.T1.1.1.1.m1.1.1.1.4.3"><times id="S3.T1.1.1.1.m1.1.1.1.4.3.1.cmml" xref="S3.T1.1.1.1.m1.1.1.1.4.3.1"></times><ci id="S3.T1.1.1.1.m1.1.1.1.4.3.2.cmml" xref="S3.T1.1.1.1.m1.1.1.1.4.3.2">𝑎</ci><ci id="S3.T1.1.1.1.m1.1.1.1.4.3.3.cmml" xref="S3.T1.1.1.1.m1.1.1.1.4.3.3">𝑝</ci><ci id="S3.T1.1.1.1.m1.1.1.1.4.3.4.cmml" xref="S3.T1.1.1.1.m1.1.1.1.4.3.4">𝑖</ci></apply></apply></apply><apply id="S3.T1.1.1.1.m1.1.1.3.cmml" xref="S3.T1.1.1.1.m1.1.1.3"><times id="S3.T1.1.1.1.m1.1.1.3.1.cmml" xref="S3.T1.1.1.1.m1.1.1.3.1"></times><ci id="S3.T1.1.1.1.m1.1.1.3.2.cmml" xref="S3.T1.1.1.1.m1.1.1.3.2">Δ</ci><apply id="S3.T1.1.1.1.m1.1.1.3.3.cmml" xref="S3.T1.1.1.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.T1.1.1.1.m1.1.1.3.3.1.cmml" xref="S3.T1.1.1.1.m1.1.1.3.3">subscript</csymbol><ci id="S3.T1.1.1.1.m1.1.1.3.3.2.cmml" xref="S3.T1.1.1.1.m1.1.1.3.3.2">𝑡</ci><apply id="S3.T1.1.1.1.m1.1.1.3.3.3.cmml" xref="S3.T1.1.1.1.m1.1.1.3.3.3"><times id="S3.T1.1.1.1.m1.1.1.3.3.3.1.cmml" xref="S3.T1.1.1.1.m1.1.1.3.3.3.1"></times><ci id="S3.T1.1.1.1.m1.1.1.3.3.3.2.cmml" xref="S3.T1.1.1.1.m1.1.1.3.3.3.2">𝑒</ci><ci id="S3.T1.1.1.1.m1.1.1.3.3.3.3.cmml" xref="S3.T1.1.1.1.m1.1.1.3.3.3.3">𝑥</ci><ci id="S3.T1.1.1.1.m1.1.1.3.3.3.4.cmml" xref="S3.T1.1.1.1.m1.1.1.3.3.3.4">𝑒</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">(N+1)\Delta t_{api}+\Delta t_{exe}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.m1.1d">( italic_N + 1 ) roman_Δ italic_t start_POSTSUBSCRIPT italic_a italic_p italic_i end_POSTSUBSCRIPT + roman_Δ italic_t start_POSTSUBSCRIPT italic_e italic_x italic_e end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.2.2.2"><math alttext="2\Delta t_{api}+\Delta t_{exe}" class="ltx_Math" display="inline" id="S3.T1.2.2.2.m1.1"><semantics id="S3.T1.2.2.2.m1.1a"><mrow id="S3.T1.2.2.2.m1.1.1" xref="S3.T1.2.2.2.m1.1.1.cmml"><mrow id="S3.T1.2.2.2.m1.1.1.2" xref="S3.T1.2.2.2.m1.1.1.2.cmml"><mn id="S3.T1.2.2.2.m1.1.1.2.2" xref="S3.T1.2.2.2.m1.1.1.2.2.cmml">2</mn><mo id="S3.T1.2.2.2.m1.1.1.2.1" xref="S3.T1.2.2.2.m1.1.1.2.1.cmml">⁢</mo><mi id="S3.T1.2.2.2.m1.1.1.2.3" mathvariant="normal" xref="S3.T1.2.2.2.m1.1.1.2.3.cmml">Δ</mi><mo id="S3.T1.2.2.2.m1.1.1.2.1a" xref="S3.T1.2.2.2.m1.1.1.2.1.cmml">⁢</mo><msub id="S3.T1.2.2.2.m1.1.1.2.4" xref="S3.T1.2.2.2.m1.1.1.2.4.cmml"><mi id="S3.T1.2.2.2.m1.1.1.2.4.2" xref="S3.T1.2.2.2.m1.1.1.2.4.2.cmml">t</mi><mrow id="S3.T1.2.2.2.m1.1.1.2.4.3" xref="S3.T1.2.2.2.m1.1.1.2.4.3.cmml"><mi id="S3.T1.2.2.2.m1.1.1.2.4.3.2" xref="S3.T1.2.2.2.m1.1.1.2.4.3.2.cmml">a</mi><mo id="S3.T1.2.2.2.m1.1.1.2.4.3.1" xref="S3.T1.2.2.2.m1.1.1.2.4.3.1.cmml">⁢</mo><mi id="S3.T1.2.2.2.m1.1.1.2.4.3.3" xref="S3.T1.2.2.2.m1.1.1.2.4.3.3.cmml">p</mi><mo id="S3.T1.2.2.2.m1.1.1.2.4.3.1a" xref="S3.T1.2.2.2.m1.1.1.2.4.3.1.cmml">⁢</mo><mi id="S3.T1.2.2.2.m1.1.1.2.4.3.4" xref="S3.T1.2.2.2.m1.1.1.2.4.3.4.cmml">i</mi></mrow></msub></mrow><mo id="S3.T1.2.2.2.m1.1.1.1" xref="S3.T1.2.2.2.m1.1.1.1.cmml">+</mo><mrow id="S3.T1.2.2.2.m1.1.1.3" xref="S3.T1.2.2.2.m1.1.1.3.cmml"><mi id="S3.T1.2.2.2.m1.1.1.3.2" mathvariant="normal" xref="S3.T1.2.2.2.m1.1.1.3.2.cmml">Δ</mi><mo id="S3.T1.2.2.2.m1.1.1.3.1" xref="S3.T1.2.2.2.m1.1.1.3.1.cmml">⁢</mo><msub id="S3.T1.2.2.2.m1.1.1.3.3" xref="S3.T1.2.2.2.m1.1.1.3.3.cmml"><mi id="S3.T1.2.2.2.m1.1.1.3.3.2" xref="S3.T1.2.2.2.m1.1.1.3.3.2.cmml">t</mi><mrow id="S3.T1.2.2.2.m1.1.1.3.3.3" xref="S3.T1.2.2.2.m1.1.1.3.3.3.cmml"><mi id="S3.T1.2.2.2.m1.1.1.3.3.3.2" xref="S3.T1.2.2.2.m1.1.1.3.3.3.2.cmml">e</mi><mo id="S3.T1.2.2.2.m1.1.1.3.3.3.1" xref="S3.T1.2.2.2.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.T1.2.2.2.m1.1.1.3.3.3.3" xref="S3.T1.2.2.2.m1.1.1.3.3.3.3.cmml">x</mi><mo id="S3.T1.2.2.2.m1.1.1.3.3.3.1a" xref="S3.T1.2.2.2.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.T1.2.2.2.m1.1.1.3.3.3.4" xref="S3.T1.2.2.2.m1.1.1.3.3.3.4.cmml">e</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.m1.1b"><apply id="S3.T1.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.m1.1.1"><plus id="S3.T1.2.2.2.m1.1.1.1.cmml" xref="S3.T1.2.2.2.m1.1.1.1"></plus><apply id="S3.T1.2.2.2.m1.1.1.2.cmml" xref="S3.T1.2.2.2.m1.1.1.2"><times id="S3.T1.2.2.2.m1.1.1.2.1.cmml" xref="S3.T1.2.2.2.m1.1.1.2.1"></times><cn id="S3.T1.2.2.2.m1.1.1.2.2.cmml" type="integer" xref="S3.T1.2.2.2.m1.1.1.2.2">2</cn><ci id="S3.T1.2.2.2.m1.1.1.2.3.cmml" xref="S3.T1.2.2.2.m1.1.1.2.3">Δ</ci><apply id="S3.T1.2.2.2.m1.1.1.2.4.cmml" xref="S3.T1.2.2.2.m1.1.1.2.4"><csymbol cd="ambiguous" id="S3.T1.2.2.2.m1.1.1.2.4.1.cmml" xref="S3.T1.2.2.2.m1.1.1.2.4">subscript</csymbol><ci id="S3.T1.2.2.2.m1.1.1.2.4.2.cmml" xref="S3.T1.2.2.2.m1.1.1.2.4.2">𝑡</ci><apply id="S3.T1.2.2.2.m1.1.1.2.4.3.cmml" xref="S3.T1.2.2.2.m1.1.1.2.4.3"><times id="S3.T1.2.2.2.m1.1.1.2.4.3.1.cmml" xref="S3.T1.2.2.2.m1.1.1.2.4.3.1"></times><ci id="S3.T1.2.2.2.m1.1.1.2.4.3.2.cmml" xref="S3.T1.2.2.2.m1.1.1.2.4.3.2">𝑎</ci><ci id="S3.T1.2.2.2.m1.1.1.2.4.3.3.cmml" xref="S3.T1.2.2.2.m1.1.1.2.4.3.3">𝑝</ci><ci id="S3.T1.2.2.2.m1.1.1.2.4.3.4.cmml" xref="S3.T1.2.2.2.m1.1.1.2.4.3.4">𝑖</ci></apply></apply></apply><apply id="S3.T1.2.2.2.m1.1.1.3.cmml" xref="S3.T1.2.2.2.m1.1.1.3"><times id="S3.T1.2.2.2.m1.1.1.3.1.cmml" xref="S3.T1.2.2.2.m1.1.1.3.1"></times><ci id="S3.T1.2.2.2.m1.1.1.3.2.cmml" xref="S3.T1.2.2.2.m1.1.1.3.2">Δ</ci><apply id="S3.T1.2.2.2.m1.1.1.3.3.cmml" xref="S3.T1.2.2.2.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.T1.2.2.2.m1.1.1.3.3.1.cmml" xref="S3.T1.2.2.2.m1.1.1.3.3">subscript</csymbol><ci id="S3.T1.2.2.2.m1.1.1.3.3.2.cmml" xref="S3.T1.2.2.2.m1.1.1.3.3.2">𝑡</ci><apply id="S3.T1.2.2.2.m1.1.1.3.3.3.cmml" xref="S3.T1.2.2.2.m1.1.1.3.3.3"><times id="S3.T1.2.2.2.m1.1.1.3.3.3.1.cmml" xref="S3.T1.2.2.2.m1.1.1.3.3.3.1"></times><ci id="S3.T1.2.2.2.m1.1.1.3.3.3.2.cmml" xref="S3.T1.2.2.2.m1.1.1.3.3.3.2">𝑒</ci><ci id="S3.T1.2.2.2.m1.1.1.3.3.3.3.cmml" xref="S3.T1.2.2.2.m1.1.1.3.3.3.3">𝑥</ci><ci id="S3.T1.2.2.2.m1.1.1.3.3.3.4.cmml" xref="S3.T1.2.2.2.m1.1.1.3.3.3.4">𝑒</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.m1.1c">2\Delta t_{api}+\Delta t_{exe}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.2.m1.1d">2 roman_Δ italic_t start_POSTSUBSCRIPT italic_a italic_p italic_i end_POSTSUBSCRIPT + roman_Δ italic_t start_POSTSUBSCRIPT italic_e italic_x italic_e end_POSTSUBSCRIPT</annotation></semantics></math></td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.3">In order to improve the planning capability of LLM, demonstrations <math alttext="\mathcal{D}_{x^{t}}" class="ltx_Math" display="inline" id="S3.SS3.p4.1.m1.1"><semantics id="S3.SS3.p4.1.m1.1a"><msub id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p4.1.m1.1.1.2" xref="S3.SS3.p4.1.m1.1.1.2.cmml">𝒟</mi><msup id="S3.SS3.p4.1.m1.1.1.3" xref="S3.SS3.p4.1.m1.1.1.3.cmml"><mi id="S3.SS3.p4.1.m1.1.1.3.2" xref="S3.SS3.p4.1.m1.1.1.3.2.cmml">x</mi><mi id="S3.SS3.p4.1.m1.1.1.3.3" xref="S3.SS3.p4.1.m1.1.1.3.3.cmml">t</mi></msup></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><apply id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.1.m1.1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p4.1.m1.1.1.2.cmml" xref="S3.SS3.p4.1.m1.1.1.2">𝒟</ci><apply id="S3.SS3.p4.1.m1.1.1.3.cmml" xref="S3.SS3.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p4.1.m1.1.1.3.1.cmml" xref="S3.SS3.p4.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS3.p4.1.m1.1.1.3.2.cmml" xref="S3.SS3.p4.1.m1.1.1.3.2">𝑥</ci><ci id="S3.SS3.p4.1.m1.1.1.3.3.cmml" xref="S3.SS3.p4.1.m1.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">\mathcal{D}_{x^{t}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.1.m1.1d">caligraphic_D start_POSTSUBSCRIPT italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> are injected into prompts for in-context learning in the <span class="ltx_text ltx_font_bold" id="S3.SS3.p4.3.1">Plan</span> phase. Each demonstration consists of a user intent <math alttext="x" class="ltx_Math" display="inline" id="S3.SS3.p4.2.m2.1"><semantics id="S3.SS3.p4.2.m2.1a"><mi id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><ci id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.2.m2.1d">italic_x</annotation></semantics></math> and tool execution path <math alttext="\boldsymbol{p}" class="ltx_Math" display="inline" id="S3.SS3.p4.3.m3.1"><semantics id="S3.SS3.p4.3.m3.1a"><mi id="S3.SS3.p4.3.m3.1.1" xref="S3.SS3.p4.3.m3.1.1.cmml">𝒑</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m3.1b"><ci id="S3.SS3.p4.3.m3.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1">𝒑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m3.1c">\boldsymbol{p}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.3.m3.1d">bold_italic_p</annotation></semantics></math>. However, the number of demonstrations is strictly limited by the contextual length that LLM can process, which makes the quality of demonstrations of paramount importance. To address the challenge, we introduce a dynamic demonstration strategy, where only a few demonstrations that are most similar to current user intent are incorporated into the prompt. For example, if the current user input is “<span class="ltx_text ltx_font_slanted" id="S3.SS3.p4.3.2">My game history is Call of Duty and Fortnite, please give me some recommendations</span>”, then demonstration with user intent “<span class="ltx_text ltx_font_slanted" id="S3.SS3.p4.3.3">I enjoyed</span> <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p4.3.4">ITEM1</span><span class="ltx_text ltx_font_slanted" id="S3.SS3.p4.3.5">,</span> <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p4.3.6">ITEM2</span> <span class="ltx_text ltx_font_slanted" id="S3.SS3.p4.3.7">in the past, give me some suggestions</span>” may be retrieved as a high-quality demonstration.</p>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.8">Inspired by Self-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Madaan et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib20" title="">2023</a>)</cite>, we use LLM to generate demonstrations of tool-using plans in the form of <math alttext="(x,\boldsymbol{p})" class="ltx_Math" display="inline" id="S3.SS3.p5.1.m1.2"><semantics id="S3.SS3.p5.1.m1.2a"><mrow id="S3.SS3.p5.1.m1.2.3.2" xref="S3.SS3.p5.1.m1.2.3.1.cmml"><mo id="S3.SS3.p5.1.m1.2.3.2.1" stretchy="false" xref="S3.SS3.p5.1.m1.2.3.1.cmml">(</mo><mi id="S3.SS3.p5.1.m1.1.1" xref="S3.SS3.p5.1.m1.1.1.cmml">x</mi><mo id="S3.SS3.p5.1.m1.2.3.2.2" xref="S3.SS3.p5.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS3.p5.1.m1.2.2" xref="S3.SS3.p5.1.m1.2.2.cmml">𝒑</mi><mo id="S3.SS3.p5.1.m1.2.3.2.3" stretchy="false" xref="S3.SS3.p5.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.2b"><interval closure="open" id="S3.SS3.p5.1.m1.2.3.1.cmml" xref="S3.SS3.p5.1.m1.2.3.2"><ci id="S3.SS3.p5.1.m1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1">𝑥</ci><ci id="S3.SS3.p5.1.m1.2.2.cmml" xref="S3.SS3.p5.1.m1.2.2">𝒑</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.2c">(x,\boldsymbol{p})</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.1.m1.2d">( italic_x , bold_italic_p )</annotation></semantics></math>. First, we manually write some (~20) typical user intents and the corresponding execution as seed demonstrations; then, we use the input-first and output-first strategies to generate more demonstrations using LLM. In the input-first strategy, there are two stages: first, the LLM generates <math alttext="x" class="ltx_Math" display="inline" id="S3.SS3.p5.2.m2.1"><semantics id="S3.SS3.p5.2.m2.1a"><mi id="S3.SS3.p5.2.m2.1.1" xref="S3.SS3.p5.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.2.m2.1b"><ci id="S3.SS3.p5.2.m2.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.2.m2.1d">italic_x</annotation></semantics></math> by emulating the intents in seed demonstrations, and then the LLM makes plans <math alttext="\boldsymbol{p}" class="ltx_Math" display="inline" id="S3.SS3.p5.3.m3.1"><semantics id="S3.SS3.p5.3.m3.1a"><mi id="S3.SS3.p5.3.m3.1.1" xref="S3.SS3.p5.3.m3.1.1.cmml">𝒑</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.3.m3.1b"><ci id="S3.SS3.p5.3.m3.1.1.cmml" xref="S3.SS3.p5.3.m3.1.1">𝒑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.3.m3.1c">\boldsymbol{p}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.3.m3.1d">bold_italic_p</annotation></semantics></math> for these intents. The output-first method consists of three stages: first, we provide the LLM with a plan <math alttext="\boldsymbol{p}" class="ltx_Math" display="inline" id="S3.SS3.p5.4.m4.1"><semantics id="S3.SS3.p5.4.m4.1a"><mi id="S3.SS3.p5.4.m4.1.1" xref="S3.SS3.p5.4.m4.1.1.cmml">𝒑</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.4.m4.1b"><ci id="S3.SS3.p5.4.m4.1.1.cmml" xref="S3.SS3.p5.4.m4.1.1">𝒑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.4.m4.1c">\boldsymbol{p}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.4.m4.1d">bold_italic_p</annotation></semantics></math> and generate corresponding user intent <math alttext="x" class="ltx_Math" display="inline" id="S3.SS3.p5.5.m5.1"><semantics id="S3.SS3.p5.5.m5.1a"><mi id="S3.SS3.p5.5.m5.1.1" xref="S3.SS3.p5.5.m5.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.5.m5.1b"><ci id="S3.SS3.p5.5.m5.1.1.cmml" xref="S3.SS3.p5.5.m5.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.5.m5.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.5.m5.1d">italic_x</annotation></semantics></math>. Then, we use LLM to make plans <math alttext="\boldsymbol{\tilde{p}}" class="ltx_Math" display="inline" id="S3.SS3.p5.6.m6.1"><semantics id="S3.SS3.p5.6.m6.1a"><mover accent="true" id="S3.SS3.p5.6.m6.1.1" xref="S3.SS3.p5.6.m6.1.1.cmml"><mi id="S3.SS3.p5.6.m6.1.1.2" xref="S3.SS3.p5.6.m6.1.1.2.cmml">𝒑</mi><mo id="S3.SS3.p5.6.m6.1.1.1" mathvariant="bold" xref="S3.SS3.p5.6.m6.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.6.m6.1b"><apply id="S3.SS3.p5.6.m6.1.1.cmml" xref="S3.SS3.p5.6.m6.1.1"><ci id="S3.SS3.p5.6.m6.1.1.1.cmml" xref="S3.SS3.p5.6.m6.1.1.1">bold-~</ci><ci id="S3.SS3.p5.6.m6.1.1.2.cmml" xref="S3.SS3.p5.6.m6.1.1.2">𝒑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.6.m6.1c">\boldsymbol{\tilde{p}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.6.m6.1d">overbold_~ start_ARG bold_italic_p end_ARG</annotation></semantics></math> for the intent, and finally, we verify whether the generated plan <math alttext="\boldsymbol{\tilde{p}}" class="ltx_Math" display="inline" id="S3.SS3.p5.7.m7.1"><semantics id="S3.SS3.p5.7.m7.1a"><mover accent="true" id="S3.SS3.p5.7.m7.1.1" xref="S3.SS3.p5.7.m7.1.1.cmml"><mi id="S3.SS3.p5.7.m7.1.1.2" xref="S3.SS3.p5.7.m7.1.1.2.cmml">𝒑</mi><mo id="S3.SS3.p5.7.m7.1.1.1" mathvariant="bold" xref="S3.SS3.p5.7.m7.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.7.m7.1b"><apply id="S3.SS3.p5.7.m7.1.1.cmml" xref="S3.SS3.p5.7.m7.1.1"><ci id="S3.SS3.p5.7.m7.1.1.1.cmml" xref="S3.SS3.p5.7.m7.1.1.1">bold-~</ci><ci id="S3.SS3.p5.7.m7.1.1.2.cmml" xref="S3.SS3.p5.7.m7.1.1.2">𝒑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.7.m7.1c">\boldsymbol{\tilde{p}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.7.m7.1d">overbold_~ start_ARG bold_italic_p end_ARG</annotation></semantics></math> is consistent with the given plan <math alttext="\boldsymbol{p}" class="ltx_Math" display="inline" id="S3.SS3.p5.8.m8.1"><semantics id="S3.SS3.p5.8.m8.1a"><mi id="S3.SS3.p5.8.m8.1.1" xref="S3.SS3.p5.8.m8.1.1.cmml">𝒑</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.8.m8.1b"><ci id="S3.SS3.p5.8.m8.1.1.cmml" xref="S3.SS3.p5.8.m8.1.1">𝒑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.8.m8.1c">\boldsymbol{p}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.8.m8.1d">bold_italic_p</annotation></semantics></math>. The inconsistency indicates that the quality of the generated intent is not high enough, and we only retain those consistent demonstrations. The output-first method allows us to obtain demonstrations corresponding to all available plans, providing diversity for the demonstrations. Examples generated by input-first and output-first are illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S3.F2" title="Figure 2 ‣ 3.4 Reflection ‣ 3 Methodologies ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Reflection</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Despite LLM’s strong intelligence, it still exhibits occasional errors in reasoning and tool utilization <cite class="ltx_cite ltx_citemacro_citep">(Madaan et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib20" title="">2023</a>; Shinn et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib31" title="">2023</a>)</cite>.
For example, it may violate instructions in the prompt by selecting a non-existent tool, omit or overuse some tools, or fail to prepare tool inputs in the proper format, resulting in errors in tool execution.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">To reduce the occurrence of such errors, some studies have employed self-reflection <cite class="ltx_cite ltx_citemacro_citep">(Shinn et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib31" title="">2023</a>)</cite> mechanisms to enable LLM to have some error-correcting capabilities during decision-making. In InteRecAgent, we utilize an <span class="ltx_text ltx_font_bold" id="S3.SS4.p2.1.1">actor-critic</span> reflection mechanism to enhance the agent’s robustness and the error-correcting ability. In the following part, we will formalize this self-reflection mechanism.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.6">Assume that in the <math alttext="t" class="ltx_Math" display="inline" id="S3.SS4.p3.1.m1.1"><semantics id="S3.SS4.p3.1.m1.1a"><mi id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><ci id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.1.m1.1d">italic_t</annotation></semantics></math>-th round, the dialogue context is <math alttext="C^{t-1}" class="ltx_Math" display="inline" id="S3.SS4.p3.2.m2.1"><semantics id="S3.SS4.p3.2.m2.1a"><msup id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml"><mi id="S3.SS4.p3.2.m2.1.1.2" xref="S3.SS4.p3.2.m2.1.1.2.cmml">C</mi><mrow id="S3.SS4.p3.2.m2.1.1.3" xref="S3.SS4.p3.2.m2.1.1.3.cmml"><mi id="S3.SS4.p3.2.m2.1.1.3.2" xref="S3.SS4.p3.2.m2.1.1.3.2.cmml">t</mi><mo id="S3.SS4.p3.2.m2.1.1.3.1" xref="S3.SS4.p3.2.m2.1.1.3.1.cmml">−</mo><mn id="S3.SS4.p3.2.m2.1.1.3.3" xref="S3.SS4.p3.2.m2.1.1.3.3.cmml">1</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><apply id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.2.m2.1.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">superscript</csymbol><ci id="S3.SS4.p3.2.m2.1.1.2.cmml" xref="S3.SS4.p3.2.m2.1.1.2">𝐶</ci><apply id="S3.SS4.p3.2.m2.1.1.3.cmml" xref="S3.SS4.p3.2.m2.1.1.3"><minus id="S3.SS4.p3.2.m2.1.1.3.1.cmml" xref="S3.SS4.p3.2.m2.1.1.3.1"></minus><ci id="S3.SS4.p3.2.m2.1.1.3.2.cmml" xref="S3.SS4.p3.2.m2.1.1.3.2">𝑡</ci><cn id="S3.SS4.p3.2.m2.1.1.3.3.cmml" type="integer" xref="S3.SS4.p3.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">C^{t-1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.2.m2.1d">italic_C start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT</annotation></semantics></math> and the current user input is <math alttext="x^{t}" class="ltx_Math" display="inline" id="S3.SS4.p3.3.m3.1"><semantics id="S3.SS4.p3.3.m3.1a"><msup id="S3.SS4.p3.3.m3.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml"><mi id="S3.SS4.p3.3.m3.1.1.2" xref="S3.SS4.p3.3.m3.1.1.2.cmml">x</mi><mi id="S3.SS4.p3.3.m3.1.1.3" xref="S3.SS4.p3.3.m3.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.1b"><apply id="S3.SS4.p3.3.m3.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.3.m3.1.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1">superscript</csymbol><ci id="S3.SS4.p3.3.m3.1.1.2.cmml" xref="S3.SS4.p3.3.m3.1.1.2">𝑥</ci><ci id="S3.SS4.p3.3.m3.1.1.3.cmml" xref="S3.SS4.p3.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.1c">x^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.3.m3.1d">italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math>. The actor is an LLM equipped with tools and inspired by the dynamic demonstration-augmented plan-first execution mechanism. For the user input, the actor would make a plan <math alttext="\boldsymbol{p}^{t}" class="ltx_Math" display="inline" id="S3.SS4.p3.4.m4.1"><semantics id="S3.SS4.p3.4.m4.1a"><msup id="S3.SS4.p3.4.m4.1.1" xref="S3.SS4.p3.4.m4.1.1.cmml"><mi id="S3.SS4.p3.4.m4.1.1.2" xref="S3.SS4.p3.4.m4.1.1.2.cmml">𝒑</mi><mi id="S3.SS4.p3.4.m4.1.1.3" xref="S3.SS4.p3.4.m4.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.4.m4.1b"><apply id="S3.SS4.p3.4.m4.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.4.m4.1.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1">superscript</csymbol><ci id="S3.SS4.p3.4.m4.1.1.2.cmml" xref="S3.SS4.p3.4.m4.1.1.2">𝒑</ci><ci id="S3.SS4.p3.4.m4.1.1.3.cmml" xref="S3.SS4.p3.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.4.m4.1c">\boldsymbol{p}^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.4.m4.1d">bold_italic_p start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math>, obtain the tools’ output <math alttext="\boldsymbol{o}^{t}" class="ltx_Math" display="inline" id="S3.SS4.p3.5.m5.1"><semantics id="S3.SS4.p3.5.m5.1a"><msup id="S3.SS4.p3.5.m5.1.1" xref="S3.SS4.p3.5.m5.1.1.cmml"><mi id="S3.SS4.p3.5.m5.1.1.2" xref="S3.SS4.p3.5.m5.1.1.2.cmml">𝒐</mi><mi id="S3.SS4.p3.5.m5.1.1.3" xref="S3.SS4.p3.5.m5.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.5.m5.1b"><apply id="S3.SS4.p3.5.m5.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.5.m5.1.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1">superscript</csymbol><ci id="S3.SS4.p3.5.m5.1.1.2.cmml" xref="S3.SS4.p3.5.m5.1.1.2">𝒐</ci><ci id="S3.SS4.p3.5.m5.1.1.3.cmml" xref="S3.SS4.p3.5.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.5.m5.1c">\boldsymbol{o}^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.5.m5.1d">bold_italic_o start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> and generate the response <math alttext="y^{t}" class="ltx_Math" display="inline" id="S3.SS4.p3.6.m6.1"><semantics id="S3.SS4.p3.6.m6.1a"><msup id="S3.SS4.p3.6.m6.1.1" xref="S3.SS4.p3.6.m6.1.1.cmml"><mi id="S3.SS4.p3.6.m6.1.1.2" xref="S3.SS4.p3.6.m6.1.1.2.cmml">y</mi><mi id="S3.SS4.p3.6.m6.1.1.3" xref="S3.SS4.p3.6.m6.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.6.m6.1b"><apply id="S3.SS4.p3.6.m6.1.1.cmml" xref="S3.SS4.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.6.m6.1.1.1.cmml" xref="S3.SS4.p3.6.m6.1.1">superscript</csymbol><ci id="S3.SS4.p3.6.m6.1.1.2.cmml" xref="S3.SS4.p3.6.m6.1.1.2">𝑦</ci><ci id="S3.SS4.p3.6.m6.1.1.3.cmml" xref="S3.SS4.p3.6.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.6.m6.1c">y^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.6.m6.1d">italic_y start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math>.
The critic evaluates the behavioral decisions of the actor. The execution steps of the reflection mechanism are listed as follows:
</p>
</div>
<figure class="ltx_figure" id="S3.F2">
<span class="ltx_inline-block ltx_align_center ltx_framed_rectangle" id="S3.F2.1" style="border-color: black;">
<span class="ltx_p" id="S3.F2.1.1"><span class="ltx_text ltx_font_bold" id="S3.F2.1.1.1">Intent(by GPT-4)</span>: Can you suggest some TYPE1 and TYPE2 items based on my preferences: ITEM1, ITEM2, and ITEM3? 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S3.F2.1.1.2">Plan(by GPT-4)</span>: 1. SQL Retrieval Tool (TYPE1 and TYPE2); 2. Ranking Tool (by preference using ITEM1, ITEM2, and ITEM3); 3. Candidate Fetching Tool. 
<br class="ltx_break"/></span>
<span class="ltx_p" id="S3.F2.1.2"><span class="ltx_text ltx_font_bold" id="S3.F2.1.2.1">Plan</span>: 1. Candidates Storing Tool (ITEM1, ITEM2, ITEM3); 2. SQL Retrieval Tool (TYPE); 3. ItemCF Retrieval Tool (ITEM); 4. Ranking Tool (by preference); 5. Candidate Fetching Tool.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S3.F2.1.2.2">Intent(by GPT-4)</span>: I have a list of items: ITEM1, ITEM2, ITEM3. I want a TYPE item that is similar to ITEM, and please rank them based on my preferences.</span>
</span>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Examples of generated demonstrations in game domain.</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_flex_size_1 ltx_align_center" id="S3.F3.1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="202" id="S3.F3.1.g1" src="x2.png" width="822"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_flex_size_1 ltx_align_center" id="S3.F3.2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="137" id="S3.F3.2.g1" src="x3.png" width="822"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example of memory bus (upper) and reflection (lower).</figcaption>
</figure>
<div class="ltx_para" id="S3.SS4.p4">
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.4"><span class="ltx_text ltx_font_bold" id="S3.I2.i1.p1.4.1">Step1</span>: The critic evaluates the actor’s output <math alttext="\boldsymbol{p}^{t}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.1.m1.1"><semantics id="S3.I2.i1.p1.1.m1.1a"><msup id="S3.I2.i1.p1.1.m1.1.1" xref="S3.I2.i1.p1.1.m1.1.1.cmml"><mi id="S3.I2.i1.p1.1.m1.1.1.2" xref="S3.I2.i1.p1.1.m1.1.1.2.cmml">𝒑</mi><mi id="S3.I2.i1.p1.1.m1.1.1.3" xref="S3.I2.i1.p1.1.m1.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.1.m1.1b"><apply id="S3.I2.i1.p1.1.m1.1.1.cmml" xref="S3.I2.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.1.m1.1.1.1.cmml" xref="S3.I2.i1.p1.1.m1.1.1">superscript</csymbol><ci id="S3.I2.i1.p1.1.m1.1.1.2.cmml" xref="S3.I2.i1.p1.1.m1.1.1.2">𝒑</ci><ci id="S3.I2.i1.p1.1.m1.1.1.3.cmml" xref="S3.I2.i1.p1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.1.m1.1c">\boldsymbol{p}^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.1.m1.1d">bold_italic_p start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="\boldsymbol{o}^{t}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.2.m2.1"><semantics id="S3.I2.i1.p1.2.m2.1a"><msup id="S3.I2.i1.p1.2.m2.1.1" xref="S3.I2.i1.p1.2.m2.1.1.cmml"><mi id="S3.I2.i1.p1.2.m2.1.1.2" xref="S3.I2.i1.p1.2.m2.1.1.2.cmml">𝒐</mi><mi id="S3.I2.i1.p1.2.m2.1.1.3" xref="S3.I2.i1.p1.2.m2.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.2.m2.1b"><apply id="S3.I2.i1.p1.2.m2.1.1.cmml" xref="S3.I2.i1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.2.m2.1.1.1.cmml" xref="S3.I2.i1.p1.2.m2.1.1">superscript</csymbol><ci id="S3.I2.i1.p1.2.m2.1.1.2.cmml" xref="S3.I2.i1.p1.2.m2.1.1.2">𝒐</ci><ci id="S3.I2.i1.p1.2.m2.1.1.3.cmml" xref="S3.I2.i1.p1.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.2.m2.1c">\boldsymbol{o}^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.2.m2.1d">bold_italic_o start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="y^{t}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.3.m3.1"><semantics id="S3.I2.i1.p1.3.m3.1a"><msup id="S3.I2.i1.p1.3.m3.1.1" xref="S3.I2.i1.p1.3.m3.1.1.cmml"><mi id="S3.I2.i1.p1.3.m3.1.1.2" xref="S3.I2.i1.p1.3.m3.1.1.2.cmml">y</mi><mi id="S3.I2.i1.p1.3.m3.1.1.3" xref="S3.I2.i1.p1.3.m3.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.3.m3.1b"><apply id="S3.I2.i1.p1.3.m3.1.1.cmml" xref="S3.I2.i1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.3.m3.1.1.1.cmml" xref="S3.I2.i1.p1.3.m3.1.1">superscript</csymbol><ci id="S3.I2.i1.p1.3.m3.1.1.2.cmml" xref="S3.I2.i1.p1.3.m3.1.1.2">𝑦</ci><ci id="S3.I2.i1.p1.3.m3.1.1.3.cmml" xref="S3.I2.i1.p1.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.3.m3.1c">y^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.3.m3.1d">italic_y start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> under the current dialogue context and obtains the judgment <math alttext="\gamma=\operatorname{reflect}(x^{t},C^{t-1},\boldsymbol{p}^{t},\boldsymbol{o}^%
{t},y^{t})" class="ltx_Math" display="inline" id="S3.I2.i1.p1.4.m4.6"><semantics id="S3.I2.i1.p1.4.m4.6a"><mrow id="S3.I2.i1.p1.4.m4.6.6" xref="S3.I2.i1.p1.4.m4.6.6.cmml"><mi id="S3.I2.i1.p1.4.m4.6.6.7" xref="S3.I2.i1.p1.4.m4.6.6.7.cmml">γ</mi><mo id="S3.I2.i1.p1.4.m4.6.6.6" xref="S3.I2.i1.p1.4.m4.6.6.6.cmml">=</mo><mrow id="S3.I2.i1.p1.4.m4.6.6.5.5" xref="S3.I2.i1.p1.4.m4.6.6.5.6.cmml"><mi id="S3.I2.i1.p1.4.m4.1.1" xref="S3.I2.i1.p1.4.m4.1.1.cmml">reflect</mi><mo id="S3.I2.i1.p1.4.m4.6.6.5.5a" xref="S3.I2.i1.p1.4.m4.6.6.5.6.cmml">⁡</mo><mrow id="S3.I2.i1.p1.4.m4.6.6.5.5.5" xref="S3.I2.i1.p1.4.m4.6.6.5.6.cmml"><mo id="S3.I2.i1.p1.4.m4.6.6.5.5.5.6" stretchy="false" xref="S3.I2.i1.p1.4.m4.6.6.5.6.cmml">(</mo><msup id="S3.I2.i1.p1.4.m4.2.2.1.1.1.1" xref="S3.I2.i1.p1.4.m4.2.2.1.1.1.1.cmml"><mi id="S3.I2.i1.p1.4.m4.2.2.1.1.1.1.2" xref="S3.I2.i1.p1.4.m4.2.2.1.1.1.1.2.cmml">x</mi><mi id="S3.I2.i1.p1.4.m4.2.2.1.1.1.1.3" xref="S3.I2.i1.p1.4.m4.2.2.1.1.1.1.3.cmml">t</mi></msup><mo id="S3.I2.i1.p1.4.m4.6.6.5.5.5.7" xref="S3.I2.i1.p1.4.m4.6.6.5.6.cmml">,</mo><msup id="S3.I2.i1.p1.4.m4.3.3.2.2.2.2" xref="S3.I2.i1.p1.4.m4.3.3.2.2.2.2.cmml"><mi id="S3.I2.i1.p1.4.m4.3.3.2.2.2.2.2" xref="S3.I2.i1.p1.4.m4.3.3.2.2.2.2.2.cmml">C</mi><mrow id="S3.I2.i1.p1.4.m4.3.3.2.2.2.2.3" xref="S3.I2.i1.p1.4.m4.3.3.2.2.2.2.3.cmml"><mi id="S3.I2.i1.p1.4.m4.3.3.2.2.2.2.3.2" xref="S3.I2.i1.p1.4.m4.3.3.2.2.2.2.3.2.cmml">t</mi><mo id="S3.I2.i1.p1.4.m4.3.3.2.2.2.2.3.1" xref="S3.I2.i1.p1.4.m4.3.3.2.2.2.2.3.1.cmml">−</mo><mn id="S3.I2.i1.p1.4.m4.3.3.2.2.2.2.3.3" xref="S3.I2.i1.p1.4.m4.3.3.2.2.2.2.3.3.cmml">1</mn></mrow></msup><mo id="S3.I2.i1.p1.4.m4.6.6.5.5.5.8" xref="S3.I2.i1.p1.4.m4.6.6.5.6.cmml">,</mo><msup id="S3.I2.i1.p1.4.m4.4.4.3.3.3.3" xref="S3.I2.i1.p1.4.m4.4.4.3.3.3.3.cmml"><mi id="S3.I2.i1.p1.4.m4.4.4.3.3.3.3.2" xref="S3.I2.i1.p1.4.m4.4.4.3.3.3.3.2.cmml">𝒑</mi><mi id="S3.I2.i1.p1.4.m4.4.4.3.3.3.3.3" xref="S3.I2.i1.p1.4.m4.4.4.3.3.3.3.3.cmml">t</mi></msup><mo id="S3.I2.i1.p1.4.m4.6.6.5.5.5.9" xref="S3.I2.i1.p1.4.m4.6.6.5.6.cmml">,</mo><msup id="S3.I2.i1.p1.4.m4.5.5.4.4.4.4" xref="S3.I2.i1.p1.4.m4.5.5.4.4.4.4.cmml"><mi id="S3.I2.i1.p1.4.m4.5.5.4.4.4.4.2" xref="S3.I2.i1.p1.4.m4.5.5.4.4.4.4.2.cmml">𝒐</mi><mi id="S3.I2.i1.p1.4.m4.5.5.4.4.4.4.3" xref="S3.I2.i1.p1.4.m4.5.5.4.4.4.4.3.cmml">t</mi></msup><mo id="S3.I2.i1.p1.4.m4.6.6.5.5.5.10" xref="S3.I2.i1.p1.4.m4.6.6.5.6.cmml">,</mo><msup id="S3.I2.i1.p1.4.m4.6.6.5.5.5.5" xref="S3.I2.i1.p1.4.m4.6.6.5.5.5.5.cmml"><mi id="S3.I2.i1.p1.4.m4.6.6.5.5.5.5.2" xref="S3.I2.i1.p1.4.m4.6.6.5.5.5.5.2.cmml">y</mi><mi id="S3.I2.i1.p1.4.m4.6.6.5.5.5.5.3" xref="S3.I2.i1.p1.4.m4.6.6.5.5.5.5.3.cmml">t</mi></msup><mo id="S3.I2.i1.p1.4.m4.6.6.5.5.5.11" stretchy="false" xref="S3.I2.i1.p1.4.m4.6.6.5.6.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.4.m4.6b"><apply id="S3.I2.i1.p1.4.m4.6.6.cmml" xref="S3.I2.i1.p1.4.m4.6.6"><eq id="S3.I2.i1.p1.4.m4.6.6.6.cmml" xref="S3.I2.i1.p1.4.m4.6.6.6"></eq><ci id="S3.I2.i1.p1.4.m4.6.6.7.cmml" xref="S3.I2.i1.p1.4.m4.6.6.7">𝛾</ci><apply id="S3.I2.i1.p1.4.m4.6.6.5.6.cmml" xref="S3.I2.i1.p1.4.m4.6.6.5.5"><ci id="S3.I2.i1.p1.4.m4.1.1.cmml" xref="S3.I2.i1.p1.4.m4.1.1">reflect</ci><apply id="S3.I2.i1.p1.4.m4.2.2.1.1.1.1.cmml" xref="S3.I2.i1.p1.4.m4.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.4.m4.2.2.1.1.1.1.1.cmml" xref="S3.I2.i1.p1.4.m4.2.2.1.1.1.1">superscript</csymbol><ci id="S3.I2.i1.p1.4.m4.2.2.1.1.1.1.2.cmml" xref="S3.I2.i1.p1.4.m4.2.2.1.1.1.1.2">𝑥</ci><ci id="S3.I2.i1.p1.4.m4.2.2.1.1.1.1.3.cmml" xref="S3.I2.i1.p1.4.m4.2.2.1.1.1.1.3">𝑡</ci></apply><apply id="S3.I2.i1.p1.4.m4.3.3.2.2.2.2.cmml" xref="S3.I2.i1.p1.4.m4.3.3.2.2.2.2"><csymbol cd="ambiguous" id="S3.I2.i1.p1.4.m4.3.3.2.2.2.2.1.cmml" xref="S3.I2.i1.p1.4.m4.3.3.2.2.2.2">superscript</csymbol><ci id="S3.I2.i1.p1.4.m4.3.3.2.2.2.2.2.cmml" xref="S3.I2.i1.p1.4.m4.3.3.2.2.2.2.2">𝐶</ci><apply id="S3.I2.i1.p1.4.m4.3.3.2.2.2.2.3.cmml" xref="S3.I2.i1.p1.4.m4.3.3.2.2.2.2.3"><minus id="S3.I2.i1.p1.4.m4.3.3.2.2.2.2.3.1.cmml" xref="S3.I2.i1.p1.4.m4.3.3.2.2.2.2.3.1"></minus><ci id="S3.I2.i1.p1.4.m4.3.3.2.2.2.2.3.2.cmml" xref="S3.I2.i1.p1.4.m4.3.3.2.2.2.2.3.2">𝑡</ci><cn id="S3.I2.i1.p1.4.m4.3.3.2.2.2.2.3.3.cmml" type="integer" xref="S3.I2.i1.p1.4.m4.3.3.2.2.2.2.3.3">1</cn></apply></apply><apply id="S3.I2.i1.p1.4.m4.4.4.3.3.3.3.cmml" xref="S3.I2.i1.p1.4.m4.4.4.3.3.3.3"><csymbol cd="ambiguous" id="S3.I2.i1.p1.4.m4.4.4.3.3.3.3.1.cmml" xref="S3.I2.i1.p1.4.m4.4.4.3.3.3.3">superscript</csymbol><ci id="S3.I2.i1.p1.4.m4.4.4.3.3.3.3.2.cmml" xref="S3.I2.i1.p1.4.m4.4.4.3.3.3.3.2">𝒑</ci><ci id="S3.I2.i1.p1.4.m4.4.4.3.3.3.3.3.cmml" xref="S3.I2.i1.p1.4.m4.4.4.3.3.3.3.3">𝑡</ci></apply><apply id="S3.I2.i1.p1.4.m4.5.5.4.4.4.4.cmml" xref="S3.I2.i1.p1.4.m4.5.5.4.4.4.4"><csymbol cd="ambiguous" id="S3.I2.i1.p1.4.m4.5.5.4.4.4.4.1.cmml" xref="S3.I2.i1.p1.4.m4.5.5.4.4.4.4">superscript</csymbol><ci id="S3.I2.i1.p1.4.m4.5.5.4.4.4.4.2.cmml" xref="S3.I2.i1.p1.4.m4.5.5.4.4.4.4.2">𝒐</ci><ci id="S3.I2.i1.p1.4.m4.5.5.4.4.4.4.3.cmml" xref="S3.I2.i1.p1.4.m4.5.5.4.4.4.4.3">𝑡</ci></apply><apply id="S3.I2.i1.p1.4.m4.6.6.5.5.5.5.cmml" xref="S3.I2.i1.p1.4.m4.6.6.5.5.5.5"><csymbol cd="ambiguous" id="S3.I2.i1.p1.4.m4.6.6.5.5.5.5.1.cmml" xref="S3.I2.i1.p1.4.m4.6.6.5.5.5.5">superscript</csymbol><ci id="S3.I2.i1.p1.4.m4.6.6.5.5.5.5.2.cmml" xref="S3.I2.i1.p1.4.m4.6.6.5.5.5.5.2">𝑦</ci><ci id="S3.I2.i1.p1.4.m4.6.6.5.5.5.5.3.cmml" xref="S3.I2.i1.p1.4.m4.6.6.5.5.5.5.3">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.4.m4.6c">\gamma=\operatorname{reflect}(x^{t},C^{t-1},\boldsymbol{p}^{t},\boldsymbol{o}^%
{t},y^{t})</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.4.m4.6d">italic_γ = roman_reflect ( italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , italic_C start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT , bold_italic_p start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , bold_italic_o start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , italic_y start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT )</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.5"><span class="ltx_text ltx_font_bold" id="S3.I2.i2.p1.5.1">Step2</span>: When the judgment <math alttext="\gamma" class="ltx_Math" display="inline" id="S3.I2.i2.p1.1.m1.1"><semantics id="S3.I2.i2.p1.1.m1.1a"><mi id="S3.I2.i2.p1.1.m1.1.1" xref="S3.I2.i2.p1.1.m1.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.1.m1.1b"><ci id="S3.I2.i2.p1.1.m1.1.1.cmml" xref="S3.I2.i2.p1.1.m1.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.1.m1.1c">\gamma</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.p1.1.m1.1d">italic_γ</annotation></semantics></math> is positive, it indicates that the actor’s execution and response are reasonable, and the response <math alttext="y^{t}" class="ltx_Math" display="inline" id="S3.I2.i2.p1.2.m2.1"><semantics id="S3.I2.i2.p1.2.m2.1a"><msup id="S3.I2.i2.p1.2.m2.1.1" xref="S3.I2.i2.p1.2.m2.1.1.cmml"><mi id="S3.I2.i2.p1.2.m2.1.1.2" xref="S3.I2.i2.p1.2.m2.1.1.2.cmml">y</mi><mi id="S3.I2.i2.p1.2.m2.1.1.3" xref="S3.I2.i2.p1.2.m2.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.2.m2.1b"><apply id="S3.I2.i2.p1.2.m2.1.1.cmml" xref="S3.I2.i2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.I2.i2.p1.2.m2.1.1.1.cmml" xref="S3.I2.i2.p1.2.m2.1.1">superscript</csymbol><ci id="S3.I2.i2.p1.2.m2.1.1.2.cmml" xref="S3.I2.i2.p1.2.m2.1.1.2">𝑦</ci><ci id="S3.I2.i2.p1.2.m2.1.1.3.cmml" xref="S3.I2.i2.p1.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.2.m2.1c">y^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.p1.2.m2.1d">italic_y start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> is directly provided to the user, ending the reflection phase. When the judgment <math alttext="\gamma" class="ltx_Math" display="inline" id="S3.I2.i2.p1.3.m3.1"><semantics id="S3.I2.i2.p1.3.m3.1a"><mi id="S3.I2.i2.p1.3.m3.1.1" xref="S3.I2.i2.p1.3.m3.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.3.m3.1b"><ci id="S3.I2.i2.p1.3.m3.1.1.cmml" xref="S3.I2.i2.p1.3.m3.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.3.m3.1c">\gamma</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.p1.3.m3.1d">italic_γ</annotation></semantics></math> is negative, it indicates that the actor’s execution or response is unreasonable. The feedback <math alttext="\gamma" class="ltx_Math" display="inline" id="S3.I2.i2.p1.4.m4.1"><semantics id="S3.I2.i2.p1.4.m4.1a"><mi id="S3.I2.i2.p1.4.m4.1.1" xref="S3.I2.i2.p1.4.m4.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.4.m4.1b"><ci id="S3.I2.i2.p1.4.m4.1.1.cmml" xref="S3.I2.i2.p1.4.m4.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.4.m4.1c">\gamma</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.p1.4.m4.1d">italic_γ</annotation></semantics></math> is used as a signal to instruct the actor to rechain, which is used as the input of <math alttext="\operatorname{plan}(\cdot)" class="ltx_Math" display="inline" id="S3.I2.i2.p1.5.m5.2"><semantics id="S3.I2.i2.p1.5.m5.2a"><mrow id="S3.I2.i2.p1.5.m5.2.3.2" xref="S3.I2.i2.p1.5.m5.2.3.1.cmml"><mi id="S3.I2.i2.p1.5.m5.1.1" xref="S3.I2.i2.p1.5.m5.1.1.cmml">plan</mi><mo id="S3.I2.i2.p1.5.m5.2.3.2a" xref="S3.I2.i2.p1.5.m5.2.3.1.cmml">⁡</mo><mrow id="S3.I2.i2.p1.5.m5.2.3.2.1" xref="S3.I2.i2.p1.5.m5.2.3.1.cmml"><mo id="S3.I2.i2.p1.5.m5.2.3.2.1.1" stretchy="false" xref="S3.I2.i2.p1.5.m5.2.3.1.cmml">(</mo><mo id="S3.I2.i2.p1.5.m5.2.2" lspace="0em" rspace="0em" xref="S3.I2.i2.p1.5.m5.2.2.cmml">⋅</mo><mo id="S3.I2.i2.p1.5.m5.2.3.2.1.2" stretchy="false" xref="S3.I2.i2.p1.5.m5.2.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.5.m5.2b"><apply id="S3.I2.i2.p1.5.m5.2.3.1.cmml" xref="S3.I2.i2.p1.5.m5.2.3.2"><ci id="S3.I2.i2.p1.5.m5.1.1.cmml" xref="S3.I2.i2.p1.5.m5.1.1">plan</ci><ci id="S3.I2.i2.p1.5.m5.2.2.cmml" xref="S3.I2.i2.p1.5.m5.2.2">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.5.m5.2c">\operatorname{plan}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.p1.5.m5.2d">roman_plan ( ⋅ )</annotation></semantics></math>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS4.p5">
<p class="ltx_p" id="S3.SS4.p5.1">In the actor-critic reflection mechanism, the actor is responsible for the challenging plan-making task, while the critic is responsible for the relative simple evaluation task. The two agents cooperate on two different types of tasks and mutually reinforce each other through in-context interactions. This endows InteRecAgent with enhanced robustness to errors and improved error correction capabilities, culminating in more precise tool utilization and recommendations. An example of reflection is shown in the lower of Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S3.F3" title="Figure 3 ‣ 3.4 Reflection ‣ 3 Methodologies ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Tool Learning with Small Language Models</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">The default LLM served as the brain is GPT-4, chosen for its exceptional ability to follow instructions compared to other LLMs. We are intrigued by the possibility of distilling GPT-4’s proficiency in instruction-following to smaller language models (SLMs) such as the 7B-parameter Llama, aiming to reduce the costs associated with large-scale online services and to democratize our InteRecAgent framework to small and medium-sized business clients. To achieve this, we utilize GPT-4 to create a specialized dataset comprising pairs of [instructions, tool execution plans]. The “instruction” element encompasses both the system prompt and the user-agent conversation history, acting as the input to elicit a tool execution plan from the LLM; the “tool execution plan” is the output crafted by GPT-4, which serves as the target for fine-tuning Llama-7B. We denote the fine-tuned version of this model <span class="ltx_text ltx_font_bold" id="S3.SS5.p1.1.1">RecLlama</span>.</p>
</div>
<div class="ltx_para" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1">To ensure the high quality of the RecLlama dataset, we employ two methods to generate data samples. The first method gathers samples from dialogues between a user simulator and a recommender agent, which is powered by GPT-4. Note that during one conversation, each exchange of user-agent produces one data sample, capturing the full range of GPT-4’s responses to the evolving context of the conversation. However, this method might not encompass a sufficiently diverse array of tool execution scenarios due to the finite number of training samples we can manage. Therefore, we complement this with a second method wherein we initially craft 30 varied dialogues designed to span a wide range of tool execution combinations. Then, for each iteration, we select three of these dialogues at random and prompt GPT-4 to generate both a conversation history and a suitable tool execution plan. This approach significantly enhances the diversity of the RecLlama dataset.</p>
</div>
<div class="ltx_para" id="S3.SS5.p3">
<p class="ltx_p" id="S3.SS5.p3.1">To evaluate RecLlama’s capacity for domain generalization, we limit the generation of training data to the Steam and MovieLens datasets, excluding the Beauty dataset (the details of datasets will be elaborated in Section <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.SS1.SSSx2" title="Dataset. ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">4.1</span></a>). The final RecLlama dataset comprises 16,183 samples, with 13,525 derived from the first method and 2,658 from the second.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>
<section class="ltx_subsubsection" id="S4.SS1.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Evaluation Strategies.</h4>
<div class="ltx_para" id="S4.SS1.SSSx1.p1">
<p class="ltx_p" id="S4.SS1.SSSx1.p1.1">Evaluating conversational recommender systems presents a challenge, as the seeker communicates their preferences and the recommendation agent provides suggestions through natural, open-ended dialogues. To enable the quantitative assessment of InteRecAgent, we design the following two evaluation strategies:</p>
</div>
<div class="ltx_para" id="S4.SS1.SSSx1.p2">
<p class="ltx_p" id="S4.SS1.SSSx1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSSx1.p2.1.1">(1) User Simulator.</span> We manually tune a role-playing prompt to facilitate GPT-4 in emulating real-world users with varying preferences. A simulated user’s preference is ascertained by injecting their historical behaviors into the role-playing prompt, leaving out the last item in their history as the target of their next interest. Following this, the simulated user engages with the recommendation agent to discover content that fits their interest. In this way, GPT-4 operates from the standpoint of the user, swiftly reacting to the recommended outcomes, thereby crafting a more natural dialogue scenario. This approach is utilized to assess the efficacy of InteRecAgent within <span class="ltx_text ltx_font_bold" id="S4.SS1.SSSx1.p2.1.2">multi-turn dialogue</span> settings. An illustrative example of a user simulator prompt can be found in Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.F4" title="Figure 4 ‣ Evaluation Strategies. ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F4">
<span class="ltx_inline-block ltx_align_center ltx_framed_rectangle" id="S4.F4.1" style="border-color: black;">
<span class="ltx_p" id="S4.F4.1.1">You are a user chatting with a recommender for {item} recommendation in turn.
Your history is {history}. Your target items: {target}.
Here is the information about target you could use: {target_item_info}. 
<br class="ltx_break"/>You must follow the rules below during chat. 
<br class="ltx_break"/>If the recommender recommends {target}, you should accept.
If the recommender recommends other items, you should refuse them and provide the information about {target}.
If the recommender asks for your preference, you should provide the information about {target}. 
<br class="ltx_break"/>You could provide your history.
Your output is only allowed to be the words from the user you act.
If you think the conversation comes to an ending, output a <math alttext="\langle\text{END}\rangle" class="ltx_Math" display="inline" id="S4.F4.1.1.m1.1"><semantics id="S4.F4.1.1.m1.1a"><mrow id="S4.F4.1.1.m1.1.2.2" xref="S4.F4.1.1.m1.1.2.1.cmml"><mo id="S4.F4.1.1.m1.1.2.2.1" stretchy="false" xref="S4.F4.1.1.m1.1.2.1.1.cmml">⟨</mo><mtext id="S4.F4.1.1.m1.1.1" xref="S4.F4.1.1.m1.1.1a.cmml">END</mtext><mo id="S4.F4.1.1.m1.1.2.2.2" stretchy="false" xref="S4.F4.1.1.m1.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F4.1.1.m1.1b"><apply id="S4.F4.1.1.m1.1.2.1.cmml" xref="S4.F4.1.1.m1.1.2.2"><csymbol cd="latexml" id="S4.F4.1.1.m1.1.2.1.1.cmml" xref="S4.F4.1.1.m1.1.2.2.1">delimited-⟨⟩</csymbol><ci id="S4.F4.1.1.m1.1.1a.cmml" xref="S4.F4.1.1.m1.1.1"><mtext id="S4.F4.1.1.m1.1.1.cmml" xref="S4.F4.1.1.m1.1.1">END</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.1.1.m1.1c">\langle\text{END}\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.F4.1.1.m1.1d">⟨ END ⟩</annotation></semantics></math>.
You should never directly tell the target item.
Only use the provided information about the target.
Never give many details about the target items at one time. Less than 3 conditions is better. 
<br class="ltx_break"/>Now lets start, you first, act as a user.
Here are the previous conversation you have completed: {chat_history}.</span>
</span>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Prompt for user simulator.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSSx1.p3">
<p class="ltx_p" id="S4.SS1.SSSx1.p3.1">The default configuration for the user simulator is set to “<span class="ltx_text ltx_font_bold" id="S4.SS1.SSSx1.p3.1.1">session-wise</span>”. This implies that the agent will only access content within the current dialogue session, and its memory will be cleared once the user either successfully locates what they are seeking or fails to do so. The conversation turns in “session-wise” setting is usually limited, thus, the long-term memory module in InteRecAgent will not be activated. In order to assess the performance while handling “<span class="ltx_text ltx_font_bold" id="S4.SS1.SSSx1.p3.1.2">lifelong memory</span>” (refer to Section <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S3.SS2.SSSx2" title="User Profile ‣ 3.2 Memory Mechanism ‣ 3 Methodologies ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">3.2</span></a>), we have formulated two strategies for simulating extended dialogues. The first strategy, referred to as <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.SSSx1.p3.1.3">Long-Chat</span>, mandates extended conversations between the user and the recommendation agent. This is achieved by alternately incorporating three types of chat intents within the user simulator: sharing history, detailing the target item, and participating in casual conversation. The simulator alternates between providing information (either historical or target-related) and casual chat every five rounds. During this process, if the agent mentions the target item, the conversation can be terminated and labeled as a success. The second strategy, referred to as <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.SSSx1.p3.1.4">Long-Context</span>, initially synthesizes multi-day conversations utilizing user history. Subsequently, based on these extended dialogues, the user simulator interacts with the agent in a manner akin to the “session-wise” setting. For our method, the lengthy conversation history is loaded into the long-term memory module. However, for baseline methods, the extended conversation history will be truncated if it surpasses the maximum window size of the LLM.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSSx1.p4">
<p class="ltx_p" id="S4.SS1.SSSx1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSSx1.p4.1.1">(2) One-Turn Recommendation.</span>
Following the settings of traditional conversational recommender systems on ReDial <cite class="ltx_cite ltx_citemacro_citep">(Li et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib15" title="">2018</a>)</cite>, we also adopt the one-turn recommendation strategy. Given a user’s history, we design a prompt that enables GPT-4 to generate a dialogue, thereby emulating the interaction between a user and a recommendation agent. The objective is to ascertain whether the recommendation agent can accurately suggest the ground truth item in its next response. We assess both the item retrieval task (retrieval from the entire space) and the ranking task (ranking of provided candidates). Specifically, the dialogue context is presented to the recommendation agent, accompanied by the instruction <span class="ltx_text ltx_font_italic" id="S4.SS1.SSSx1.p4.1.2">Please give me k recommendations based on the chat history</span> for the retrieval task, and the instruction <span class="ltx_text ltx_font_italic" id="S4.SS1.SSSx1.p4.1.3">Please rank these candidate items based on the chat history</span> for the ranking task. To ensure a fair comparison with baseline LLMs, the <span class="ltx_text ltx_font_slanted" id="S4.SS1.SSSx1.p4.1.4">One-Turn Recommendation</span> evaluation protocol employs only the “session-wise” setting, and the long-term memory module in InteRecAgent remains deactivated.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">Dataset.</h4>
<div class="ltx_para" id="S4.SS1.SSSx2.p1">
<p class="ltx_p" id="S4.SS1.SSSx2.p1.1">To compare methods across different domains, we conduct experiments using three datasets: Steam<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url" href="https://github.com/kang205/SASRec" title="">https://github.com/kang205/SASRec</a></span></span></span>, MovieLens<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url" href="https://grouplens.org/datasets/movielens/10m" title="">https://grouplens.org/datasets/movielens/10m</a></span></span></span> and Amazon Beauty<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url" href="http://jmcauley.ucsd.edu/data/amazon/links.html" title="">http://jmcauley.ucsd.edu/data/amazon/links.html</a></span></span></span>. Each dataset comprises user-item interaction history data and item metadata. We apply the leave-one-out method to divide the interaction data into training, validation, and testing sets. The training of all utilized tools is performed on the training and validation sets. Due to budget constraints, we randomly sample 1000 and 500 instances from the testing set for user simulator and one-turn benchmarking respectively. For the lifelong simulator, due to the costly long conversation, we use 100 instances in evaluation.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSSx3">
<h4 class="ltx_title ltx_title_subsubsection">Baselines.</h4>
<div class="ltx_para" id="S4.SS1.SSSx3.p1">
<p class="ltx_p" id="S4.SS1.SSSx3.p1.1">As dialogue recommendation agents, we compare our methods with the following baselines:</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Random</span>: Sample k items uniformly from entire item set.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Popularity</span>: Sample k items with item popularity as the weight.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.1">LlaMA-2-7B-chat</span>, <span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.2">LlaMA-2-13B-chat</span> <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib35" title="">2023b</a>)</cite>: The second version of the LlaMA model released by Meta.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i4.p1.1.1">Vicuna-v1.5-7B</span>, <span class="ltx_text ltx_font_bold" id="S4.I1.i4.p1.1.2">Vicuna-v1.5-13B</span> <cite class="ltx_cite ltx_citemacro_citep">(Chiang et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib5" title="">2023</a>)</cite>: Open-source models fine-tuned with user-shared data from the ShareGPT<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url" href="https://sharegpt.com/" title="">https://sharegpt.com/</a></span></span></span> based on LlaMA-2 foundation models.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i5.p1.1.1">Chat-Rec</span> <cite class="ltx_cite ltx_citemacro_citep">(Gao et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib11" title="">2023b</a>)</cite>: A recently proposed conversational recommendation agent utilizes a text-embedding tool (OpenAI text-embedding-ada-002) to retrieve candidates. It then processes the content with an LLM before responding to users. We denote the use of GPT-3.5 as the LLM in the second stage with ”Chat-Rec (3.5)” and the use of GPT-4 with ”Chat-Rec (4)”.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i6.p1">
<p class="ltx_p" id="S4.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i6.p1.1.1">GPT-3.5</span>, <span class="ltx_text ltx_font_bold" id="S4.I1.i6.p1.1.2">GPT-4</span> <cite class="ltx_cite ltx_citemacro_citep">(OpenAI <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib23" title="">2023</a>)</cite>: We access these LLMs from OpenAI by API service. The GPT-3.5 version in use is gpt-3.5-turbo-0613 and GPT-4 version is gpt-4-0613<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url" href="https://platform.openai.com/docs/models/" title="">https://platform.openai.com/docs/models/</a></span></span></span>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.SSSx3.p2">
<p class="ltx_p" id="S4.SS1.SSSx3.p2.1">For the LlaMA and Vicuna models, we employ the FastChat <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib53" title="">2023</a>)</cite> package to establish local APIs, ensuring their usage is consistent with GPT-3.5 and GPT-4.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance comparisons with the user simulator strategy (session-wise). H@5 is an abbreviation for Hit@5.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.6">
<tr class="ltx_tr" id="S4.T2.6.7">
<td class="ltx_td ltx_border_r ltx_border_tt" id="S4.T2.6.7.1" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S4.T2.6.7.2" style="padding-left:4.0pt;padding-right:4.0pt;">Steam</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S4.T2.6.7.3" style="padding-left:4.0pt;padding-right:4.0pt;">MovieLens</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T2.6.7.4" style="padding-left:4.0pt;padding-right:4.0pt;">Beauty</td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.6">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.6.6.7" style="padding-left:4.0pt;padding-right:4.0pt;">Methods</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">H@5<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.m1.1.1" stretchy="false" xref="S4.T2.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.2.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">AT@5<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.2.2.2.m1.1"><semantics id="S4.T2.2.2.2.m1.1a"><mo id="S4.T2.2.2.2.m1.1.1" stretchy="false" xref="S4.T2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.3.3.3" style="padding-left:4.0pt;padding-right:4.0pt;">H@5<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.3.3.3.m1.1"><semantics id="S4.T2.3.3.3.m1.1a"><mo id="S4.T2.3.3.3.m1.1.1" stretchy="false" xref="S4.T2.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.m1.1b"><ci id="S4.T2.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.3.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.4.4.4" style="padding-left:4.0pt;padding-right:4.0pt;">AT@5<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.4.4.4.m1.1"><semantics id="S4.T2.4.4.4.m1.1a"><mo id="S4.T2.4.4.4.m1.1.1" stretchy="false" xref="S4.T2.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.m1.1b"><ci id="S4.T2.4.4.4.m1.1.1.cmml" xref="S4.T2.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.4.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.5.5.5" style="padding-left:4.0pt;padding-right:4.0pt;">H@5<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.5.5.5.m1.1"><semantics id="S4.T2.5.5.5.m1.1a"><mo id="S4.T2.5.5.5.m1.1.1" stretchy="false" xref="S4.T2.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.m1.1b"><ci id="S4.T2.5.5.5.m1.1.1.cmml" xref="S4.T2.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.5.5.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.6.6.6" style="padding-left:4.0pt;padding-right:4.0pt;">AT@5<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.6.6.6.m1.1"><semantics id="S4.T2.6.6.6.m1.1a"><mo id="S4.T2.6.6.6.m1.1.1" stretchy="false" xref="S4.T2.6.6.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.6.m1.1b"><ci id="S4.T2.6.6.6.m1.1.1.cmml" xref="S4.T2.6.6.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.6.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.6.6.m1.1d">↓</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.8">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.6.8.1" style="padding-left:4.0pt;padding-right:4.0pt;">LlaMA2-7B</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.6.8.2" style="padding-left:4.0pt;padding-right:4.0pt;">0.36</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.6.8.3" style="padding-left:4.0pt;padding-right:4.0pt;">4.76</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.6.8.4" style="padding-left:4.0pt;padding-right:4.0pt;">0.50</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.6.8.5" style="padding-left:4.0pt;padding-right:4.0pt;">4.71</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.6.8.6" style="padding-left:4.0pt;padding-right:4.0pt;">0.03</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.6.8.7" style="padding-left:4.0pt;padding-right:4.0pt;">5.91</td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.6.9.1" style="padding-left:4.0pt;padding-right:4.0pt;">LlaMA2-13B</td>
<td class="ltx_td ltx_align_left" id="S4.T2.6.9.2" style="padding-left:4.0pt;padding-right:4.0pt;">0.39</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.6.9.3" style="padding-left:4.0pt;padding-right:4.0pt;">4.56</td>
<td class="ltx_td ltx_align_left" id="S4.T2.6.9.4" style="padding-left:4.0pt;padding-right:4.0pt;">0.53</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.6.9.5" style="padding-left:4.0pt;padding-right:4.0pt;">4.52</td>
<td class="ltx_td ltx_align_left" id="S4.T2.6.9.6" style="padding-left:4.0pt;padding-right:4.0pt;">0.05</td>
<td class="ltx_td ltx_align_left" id="S4.T2.6.9.7" style="padding-left:4.0pt;padding-right:4.0pt;">5.87</td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.6.10.1" style="padding-left:4.0pt;padding-right:4.0pt;">Vicuna-7B</td>
<td class="ltx_td ltx_align_left" id="S4.T2.6.10.2" style="padding-left:4.0pt;padding-right:4.0pt;">0.38</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.6.10.3" style="padding-left:4.0pt;padding-right:4.0pt;">4.70</td>
<td class="ltx_td ltx_align_left" id="S4.T2.6.10.4" style="padding-left:4.0pt;padding-right:4.0pt;">0.51</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.6.10.5" style="padding-left:4.0pt;padding-right:4.0pt;">4.70</td>
<td class="ltx_td ltx_align_left" id="S4.T2.6.10.6" style="padding-left:4.0pt;padding-right:4.0pt;">0.03</td>
<td class="ltx_td ltx_align_left" id="S4.T2.6.10.7" style="padding-left:4.0pt;padding-right:4.0pt;">5.90</td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.6.11.1" style="padding-left:4.0pt;padding-right:4.0pt;">Vicuna-13B</td>
<td class="ltx_td ltx_align_left" id="S4.T2.6.11.2" style="padding-left:4.0pt;padding-right:4.0pt;">0.40</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.6.11.3" style="padding-left:4.0pt;padding-right:4.0pt;">4.60</td>
<td class="ltx_td ltx_align_left" id="S4.T2.6.11.4" style="padding-left:4.0pt;padding-right:4.0pt;">0.54</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.6.11.5" style="padding-left:4.0pt;padding-right:4.0pt;">4.56</td>
<td class="ltx_td ltx_align_left" id="S4.T2.6.11.6" style="padding-left:4.0pt;padding-right:4.0pt;">0.07</td>
<td class="ltx_td ltx_align_left" id="S4.T2.6.11.7" style="padding-left:4.0pt;padding-right:4.0pt;">5.85</td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.12">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.6.12.1" style="padding-left:4.0pt;padding-right:4.0pt;">Chat-Rec(3.5)</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.6.12.2" style="padding-left:4.0pt;padding-right:4.0pt;">0.74</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.6.12.3" style="padding-left:4.0pt;padding-right:4.0pt;">3.63</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.6.12.4" style="padding-left:4.0pt;padding-right:4.0pt;">0.76</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.6.12.5" style="padding-left:4.0pt;padding-right:4.0pt;">3.78</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.6.12.6" style="padding-left:4.0pt;padding-right:4.0pt;">0.39</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.6.12.7" style="padding-left:4.0pt;padding-right:4.0pt;">4.89</td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.13">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.6.13.1" style="padding-left:4.0pt;padding-right:4.0pt;">Chat-Rec(4)</td>
<td class="ltx_td ltx_align_left" id="S4.T2.6.13.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_framed_underline" id="S4.T2.6.13.2.1">0.83</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.6.13.3" style="padding-left:4.0pt;padding-right:4.0pt;">3.42</td>
<td class="ltx_td ltx_align_left" id="S4.T2.6.13.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_framed_underline" id="S4.T2.6.13.4.1">0.82</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.6.13.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_framed_underline" id="S4.T2.6.13.5.1">3.62</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.6.13.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_framed_underline" id="S4.T2.6.13.6.1">0.40</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.6.13.7" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_framed_underline" id="S4.T2.6.13.7.1">4.80</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.14">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.6.14.1" style="padding-left:4.0pt;padding-right:4.0pt;">GPT-3.5</td>
<td class="ltx_td ltx_align_left" id="S4.T2.6.14.2" style="padding-left:4.0pt;padding-right:4.0pt;">0.69</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.6.14.3" style="padding-left:4.0pt;padding-right:4.0pt;">3.68</td>
<td class="ltx_td ltx_align_left" id="S4.T2.6.14.4" style="padding-left:4.0pt;padding-right:4.0pt;">0.75</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.6.14.5" style="padding-left:4.0pt;padding-right:4.0pt;">3.75</td>
<td class="ltx_td ltx_align_left" id="S4.T2.6.14.6" style="padding-left:4.0pt;padding-right:4.0pt;">0.13</td>
<td class="ltx_td ltx_align_left" id="S4.T2.6.14.7" style="padding-left:4.0pt;padding-right:4.0pt;">5.68</td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.15">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.6.15.1" style="padding-left:4.0pt;padding-right:4.0pt;">GPT-4</td>
<td class="ltx_td ltx_align_left" id="S4.T2.6.15.2" style="padding-left:4.0pt;padding-right:4.0pt;">0.78</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.6.15.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_framed_underline" id="S4.T2.6.15.3.1">3.34</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.6.15.4" style="padding-left:4.0pt;padding-right:4.0pt;">0.79</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.6.15.5" style="padding-left:4.0pt;padding-right:4.0pt;">3.70</td>
<td class="ltx_td ltx_align_left" id="S4.T2.6.15.6" style="padding-left:4.0pt;padding-right:4.0pt;">0.15</td>
<td class="ltx_td ltx_align_left" id="S4.T2.6.15.7" style="padding-left:4.0pt;padding-right:4.0pt;">5.59</td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.16">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S4.T2.6.16.1" style="padding-left:4.0pt;padding-right:4.0pt;">Ours</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T2.6.16.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.6.16.2.1">0.87</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S4.T2.6.16.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.6.16.3.1">2.86</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T2.6.16.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.6.16.4.1">0.85</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S4.T2.6.16.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.6.16.5.1">3.15</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T2.6.16.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.6.16.6.1">0.54</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T2.6.16.7" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.6.16.7.1">3.99</span></td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSSx4">
<h4 class="ltx_title ltx_title_subsubsection">Metrics.</h4>
<div class="ltx_para" id="S4.SS1.SSSx4.p1">
<p class="ltx_p" id="S4.SS1.SSSx4.p1.11">Since both our method and baselines utilize LLMs to generate response, which exhibit state-of-the-art text generation capabilities, our experiments primarily compare recommendation performance of different methods.
For the <span class="ltx_text ltx_font_slanted" id="S4.SS1.SSSx4.p1.11.1">user simulator</span> strategy, we employ two metrics: Hit@<math alttext="k" class="ltx_Math" display="inline" id="S4.SS1.SSSx4.p1.1.m1.1"><semantics id="S4.SS1.SSSx4.p1.1.m1.1a"><mi id="S4.SS1.SSSx4.p1.1.m1.1.1" xref="S4.SS1.SSSx4.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSSx4.p1.1.m1.1b"><ci id="S4.SS1.SSSx4.p1.1.m1.1.1.cmml" xref="S4.SS1.SSSx4.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSSx4.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSSx4.p1.1.m1.1d">italic_k</annotation></semantics></math> and AT@<math alttext="k" class="ltx_Math" display="inline" id="S4.SS1.SSSx4.p1.2.m2.1"><semantics id="S4.SS1.SSSx4.p1.2.m2.1a"><mi id="S4.SS1.SSSx4.p1.2.m2.1.1" xref="S4.SS1.SSSx4.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSSx4.p1.2.m2.1b"><ci id="S4.SS1.SSSx4.p1.2.m2.1.1.cmml" xref="S4.SS1.SSSx4.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSSx4.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSSx4.p1.2.m2.1d">italic_k</annotation></semantics></math>, representing the success of recommending the target item within <math alttext="k" class="ltx_Math" display="inline" id="S4.SS1.SSSx4.p1.3.m3.1"><semantics id="S4.SS1.SSSx4.p1.3.m3.1a"><mi id="S4.SS1.SSSx4.p1.3.m3.1.1" xref="S4.SS1.SSSx4.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSSx4.p1.3.m3.1b"><ci id="S4.SS1.SSSx4.p1.3.m3.1.1.cmml" xref="S4.SS1.SSSx4.p1.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSSx4.p1.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSSx4.p1.3.m3.1d">italic_k</annotation></semantics></math> turns and the average turns (AT) required for a successful recommendation, respectively. Unsuccessful recommendations within k rounds are recorded as <math alttext="k+1" class="ltx_Math" display="inline" id="S4.SS1.SSSx4.p1.4.m4.1"><semantics id="S4.SS1.SSSx4.p1.4.m4.1a"><mrow id="S4.SS1.SSSx4.p1.4.m4.1.1" xref="S4.SS1.SSSx4.p1.4.m4.1.1.cmml"><mi id="S4.SS1.SSSx4.p1.4.m4.1.1.2" xref="S4.SS1.SSSx4.p1.4.m4.1.1.2.cmml">k</mi><mo id="S4.SS1.SSSx4.p1.4.m4.1.1.1" xref="S4.SS1.SSSx4.p1.4.m4.1.1.1.cmml">+</mo><mn id="S4.SS1.SSSx4.p1.4.m4.1.1.3" xref="S4.SS1.SSSx4.p1.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSSx4.p1.4.m4.1b"><apply id="S4.SS1.SSSx4.p1.4.m4.1.1.cmml" xref="S4.SS1.SSSx4.p1.4.m4.1.1"><plus id="S4.SS1.SSSx4.p1.4.m4.1.1.1.cmml" xref="S4.SS1.SSSx4.p1.4.m4.1.1.1"></plus><ci id="S4.SS1.SSSx4.p1.4.m4.1.1.2.cmml" xref="S4.SS1.SSSx4.p1.4.m4.1.1.2">𝑘</ci><cn id="S4.SS1.SSSx4.p1.4.m4.1.1.3.cmml" type="integer" xref="S4.SS1.SSSx4.p1.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSSx4.p1.4.m4.1c">k+1</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSSx4.p1.4.m4.1d">italic_k + 1</annotation></semantics></math> in calculating AT. In the <span class="ltx_text ltx_font_slanted" id="S4.SS1.SSSx4.p1.11.2">one-turn</span> strategy, we focus on the Recall@<math alttext="k" class="ltx_Math" display="inline" id="S4.SS1.SSSx4.p1.5.m5.1"><semantics id="S4.SS1.SSSx4.p1.5.m5.1a"><mi id="S4.SS1.SSSx4.p1.5.m5.1.1" xref="S4.SS1.SSSx4.p1.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSSx4.p1.5.m5.1b"><ci id="S4.SS1.SSSx4.p1.5.m5.1.1.cmml" xref="S4.SS1.SSSx4.p1.5.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSSx4.p1.5.m5.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSSx4.p1.5.m5.1d">italic_k</annotation></semantics></math> and NDCG@<math alttext="k" class="ltx_Math" display="inline" id="S4.SS1.SSSx4.p1.6.m6.1"><semantics id="S4.SS1.SSSx4.p1.6.m6.1a"><mi id="S4.SS1.SSSx4.p1.6.m6.1.1" xref="S4.SS1.SSSx4.p1.6.m6.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSSx4.p1.6.m6.1b"><ci id="S4.SS1.SSSx4.p1.6.m6.1.1.cmml" xref="S4.SS1.SSSx4.p1.6.m6.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSSx4.p1.6.m6.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSSx4.p1.6.m6.1d">italic_k</annotation></semantics></math> metric for retrieval and ranking task, respectively.
In Recall@<math alttext="k" class="ltx_Math" display="inline" id="S4.SS1.SSSx4.p1.7.m7.1"><semantics id="S4.SS1.SSSx4.p1.7.m7.1a"><mi id="S4.SS1.SSSx4.p1.7.m7.1.1" xref="S4.SS1.SSSx4.p1.7.m7.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSSx4.p1.7.m7.1b"><ci id="S4.SS1.SSSx4.p1.7.m7.1.1.cmml" xref="S4.SS1.SSSx4.p1.7.m7.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSSx4.p1.7.m7.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSSx4.p1.7.m7.1d">italic_k</annotation></semantics></math>, the <math alttext="k" class="ltx_Math" display="inline" id="S4.SS1.SSSx4.p1.8.m8.1"><semantics id="S4.SS1.SSSx4.p1.8.m8.1a"><mi id="S4.SS1.SSSx4.p1.8.m8.1.1" xref="S4.SS1.SSSx4.p1.8.m8.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSSx4.p1.8.m8.1b"><ci id="S4.SS1.SSSx4.p1.8.m8.1.1.cmml" xref="S4.SS1.SSSx4.p1.8.m8.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSSx4.p1.8.m8.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSSx4.p1.8.m8.1d">italic_k</annotation></semantics></math> represents the retrieval of <math alttext="k" class="ltx_Math" display="inline" id="S4.SS1.SSSx4.p1.9.m9.1"><semantics id="S4.SS1.SSSx4.p1.9.m9.1a"><mi id="S4.SS1.SSSx4.p1.9.m9.1.1" xref="S4.SS1.SSSx4.p1.9.m9.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSSx4.p1.9.m9.1b"><ci id="S4.SS1.SSSx4.p1.9.m9.1.1.cmml" xref="S4.SS1.SSSx4.p1.9.m9.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSSx4.p1.9.m9.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSSx4.p1.9.m9.1d">italic_k</annotation></semantics></math> items, whereas in NDCG@<math alttext="k" class="ltx_Math" display="inline" id="S4.SS1.SSSx4.p1.10.m10.1"><semantics id="S4.SS1.SSSx4.p1.10.m10.1a"><mi id="S4.SS1.SSSx4.p1.10.m10.1.1" xref="S4.SS1.SSSx4.p1.10.m10.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSSx4.p1.10.m10.1b"><ci id="S4.SS1.SSSx4.p1.10.m10.1.1.cmml" xref="S4.SS1.SSSx4.p1.10.m10.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSSx4.p1.10.m10.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSSx4.p1.10.m10.1d">italic_k</annotation></semantics></math>, the <math alttext="k" class="ltx_Math" display="inline" id="S4.SS1.SSSx4.p1.11.m11.1"><semantics id="S4.SS1.SSSx4.p1.11.m11.1a"><mi id="S4.SS1.SSSx4.p1.11.m11.1.1" xref="S4.SS1.SSSx4.p1.11.m11.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSSx4.p1.11.m11.1b"><ci id="S4.SS1.SSSx4.p1.11.m11.1.1.cmml" xref="S4.SS1.SSSx4.p1.11.m11.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSSx4.p1.11.m11.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSSx4.p1.11.m11.1d">italic_k</annotation></semantics></math> denotes the number of candidates to be ranked.
</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSSx5">
<h4 class="ltx_title ltx_title_subsubsection">Implementation Details.</h4>
<div class="ltx_para" id="S4.SS1.SSSx5.p1">
<p class="ltx_p" id="S4.SS1.SSSx5.p1.1">We employ GPT-4 as the brain of the InteRecAgent for user intent parsing and tool planing. Regarding tools, we use SQL as information query tool, SQL and ItemCF <cite class="ltx_cite ltx_citemacro_citep">(Linden, Smith, and York <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib16" title="">2003</a>)</cite> as hard condition and soft condition item retrieval tools, respectively, and SASRec <cite class="ltx_cite ltx_citemacro_citep">(Kang and McAuley <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib12" title="">2018</a>)</cite> without position embedding as the ranking tool. SQL is implemented with SQLite integrated in pandasql<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref ltx_url" href="https://github.com/yhat/pandasql/" title="">https://github.com/yhat/pandasql/</a></span></span></span> and retrieval and ranking models are implemented with PyTorch. The framework of InteRecAgent is implement with Python and LangChain<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a class="ltx_ref ltx_url" href="https://www.langchain.com/" title="">https://www.langchain.com/</a></span></span></span>. For dynamic demonstration selection, we employ sentence-transformers<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span><a class="ltx_ref ltx_url" href="https://huggingface.co/sentence-transformers" title="">https://huggingface.co/sentence-transformers</a></span></span></span> to encode demonstrations into vectors and store them using ChromaDB<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><a class="ltx_ref ltx_url" href="https://www.trychroma.com/" title="">https://www.trychroma.com/</a></span></span></span>, which facilitates ANN search during runtime.
Regarding hyperparameter settings, we set the number of dynamic demonstrations to 3, the maximum number of candidates for hard condition retrieval to 1000, and the threshold for soft condition retrieval cut to the top 5%.
</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation with User Simulator</h3>
<section class="ltx_subsubsection" id="S4.SS2.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Session-wise setting.</h4>
<div class="ltx_para" id="S4.SS2.SSSx1.p1">
<p class="ltx_p" id="S4.SS2.SSSx1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.T2" title="Table 2 ‣ Baselines. ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">2</span></a> presents the results of evaluations conducted using the user simulator strategy. Our method surpasses other LLMs in terms of both hit rate and average turns across the three datasets. These results suggest that our InteRecAgent is capable of delivering more accurate and efficient recommendations in conversations compared to general LLMs. Overall, LLMs with larger parameter sizes perform better. GPT-3.5 and GPT4, with parameter sizes exceeding 100B, significantly outperform LlaMA2 and Vicuna-v1.5 13B models from the same series almost always surpass 7B models, except for LlaMA2-7B and LlaMA2-13B, which both perform extremely poorly on the Beauty dataset.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSSx1.p2">
<p class="ltx_p" id="S4.SS2.SSSx1.p2.1">Another interesting observation is the more significant improvement in relatively private domains, such as Amazon Beauty. In comparison to gaming and movie domains, the beauty product domain is more private, featuring a larger number of items not well-covered by common world knowledge or being new. Table <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.T2" title="Table 2 ‣ Baselines. ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">2</span></a> reveals that GPT-3.5 and GPT-4 exhibit competitive performance in gaming and movie domains.
However, in the Amazon Beauty domain, most LLMs suffer severe hallucination issue due to the professional, long, and complex item names, resulting in a significant drop in performance. This phenomenon highlights the necessity of recommender agents in private domains. Leveraging the text embedding retrieval tool, Chat-Rec shows superior performance compared to GPT-3.5 and GPT-4, but still falling short of the performance achieved by InteRecAgent. Chat-Rec can be seen as a simplified version of InteRecAgent, incorporating just a single tool within the agent’s framework. Consequently, Chat-Rec lacks the capability to handle multifaceted queries, such as procuring detailed information about an item or searching for items based on intricate criteria.
</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Performance comparisons with the user simulator strategy(<span class="ltx_text ltx_font_smallcaps" id="S4.T3.2.1">Long-Chat</span>). ”+LT Mem.” means activating the long-term memory module in our InteRecAgent. The higher Hit@50 and the lower AT@50, the better performance.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.3">
<tr class="ltx_tr" id="S4.T3.3.1">
<td class="ltx_td ltx_border_r ltx_border_tt" id="S4.T3.3.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S4.T3.3.1.2" style="padding-left:4.0pt;padding-right:4.0pt;">Steam</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S4.T3.3.1.3" style="padding-left:4.0pt;padding-right:4.0pt;">MovieLens</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T3.3.1.4" style="padding-left:4.0pt;padding-right:4.0pt;">Beauty</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.3.2.1" style="padding-left:4.0pt;padding-right:4.0pt;">Methods</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.3.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">H@50</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.3.2.3" style="padding-left:4.0pt;padding-right:4.0pt;">AT@50</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.3.2.4" style="padding-left:4.0pt;padding-right:4.0pt;">H@50</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.3.2.5" style="padding-left:4.0pt;padding-right:4.0pt;">AT@50</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.3.2.6" style="padding-left:4.0pt;padding-right:4.0pt;">H@50</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.3.2.7" style="padding-left:4.0pt;padding-right:4.0pt;">AT@50</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.3.3.1" style="padding-left:4.0pt;padding-right:4.0pt;">GPT-4</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.3.3.2" style="padding-left:4.0pt;padding-right:4.0pt;">0.70</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.3.3.3" style="padding-left:4.0pt;padding-right:4.0pt;">20.56</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.3.3.4" style="padding-left:4.0pt;padding-right:4.0pt;">0.71</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.3.3.5" style="padding-left:4.0pt;padding-right:4.0pt;">24.06</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.3.3.6" style="padding-left:4.0pt;padding-right:4.0pt;">0.06</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.3.3.7" style="padding-left:4.0pt;padding-right:4.0pt;">49.42</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.4">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.3.4.1" style="padding-left:4.0pt;padding-right:4.0pt;">Ours</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.3.4.2" style="padding-left:4.0pt;padding-right:4.0pt;">0.83</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.3.4.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.3.4.3.1">16.85</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.3.4.4" style="padding-left:4.0pt;padding-right:4.0pt;">0.76</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.3.4.5" style="padding-left:4.0pt;padding-right:4.0pt;">20.13</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.3.4.6" style="padding-left:4.0pt;padding-right:4.0pt;">0.69</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.3.4.7" style="padding-left:4.0pt;padding-right:4.0pt;">27.14</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.5">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T3.3.5.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="S4.T3.3.5.1.1"></span><span class="ltx_text" id="S4.T3.3.5.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.3.5.1.2.1">
<span class="ltx_tr" id="S4.T3.3.5.1.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T3.3.5.1.2.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">+LT Mem.</span></span>
</span></span> <span class="ltx_text" id="S4.T3.3.5.1.3"></span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.3.5.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.3.5.2.1">0.86</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T3.3.5.3" style="padding-left:4.0pt;padding-right:4.0pt;">17.58</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.3.5.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.3.5.4.1">0.77</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T3.3.5.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.3.5.5.1">20.06</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.3.5.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.3.5.6.1">0.74</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.3.5.7" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.3.5.7.1">25.88</span></td>
</tr>
</table>
</figure>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Performance comparisons with the lifelong user simulator strategy(<span class="ltx_text ltx_font_smallcaps" id="S4.T4.8.1">Long-Context</span>). ”+LT Mem.” means activating the long-term memory module in our InteRecAgent. </figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T4.6">
<tr class="ltx_tr" id="S4.T4.6.7">
<td class="ltx_td ltx_border_r ltx_border_tt" id="S4.T4.6.7.1" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S4.T4.6.7.2" style="padding-left:4.0pt;padding-right:4.0pt;">Steam</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S4.T4.6.7.3" style="padding-left:4.0pt;padding-right:4.0pt;">MovieLens</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T4.6.7.4" style="padding-left:4.0pt;padding-right:4.0pt;">Beauty</td>
</tr>
<tr class="ltx_tr" id="S4.T4.6.6">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.6.6.7" style="padding-left:4.0pt;padding-right:4.0pt;">Methods</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">H@5<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.m1.1a"><mo id="S4.T4.1.1.1.m1.1.1" stretchy="false" xref="S4.T4.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.2.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">AT@5<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.2.2.2.m1.1"><semantics id="S4.T4.2.2.2.m1.1a"><mo id="S4.T4.2.2.2.m1.1.1" stretchy="false" xref="S4.T4.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.m1.1b"><ci id="S4.T4.2.2.2.m1.1.1.cmml" xref="S4.T4.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.2.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.3.3.3" style="padding-left:4.0pt;padding-right:4.0pt;">H@5<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.3.3.3.m1.1"><semantics id="S4.T4.3.3.3.m1.1a"><mo id="S4.T4.3.3.3.m1.1.1" stretchy="false" xref="S4.T4.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.m1.1b"><ci id="S4.T4.3.3.3.m1.1.1.cmml" xref="S4.T4.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.3.3.3.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.4.4.4" style="padding-left:4.0pt;padding-right:4.0pt;">AT@5<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.4.4.4.m1.1"><semantics id="S4.T4.4.4.4.m1.1a"><mo id="S4.T4.4.4.4.m1.1.1" stretchy="false" xref="S4.T4.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.4.m1.1b"><ci id="S4.T4.4.4.4.m1.1.1.cmml" xref="S4.T4.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.4.4.4.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.5.5.5" style="padding-left:4.0pt;padding-right:4.0pt;">H@5<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.5.5.5.m1.1"><semantics id="S4.T4.5.5.5.m1.1a"><mo id="S4.T4.5.5.5.m1.1.1" stretchy="false" xref="S4.T4.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.5.m1.1b"><ci id="S4.T4.5.5.5.m1.1.1.cmml" xref="S4.T4.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.5.5.5.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.6.6.6" style="padding-left:4.0pt;padding-right:4.0pt;">AT@5<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.6.6.6.m1.1"><semantics id="S4.T4.6.6.6.m1.1a"><mo id="S4.T4.6.6.6.m1.1.1" stretchy="false" xref="S4.T4.6.6.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.6.m1.1b"><ci id="S4.T4.6.6.6.m1.1.1.cmml" xref="S4.T4.6.6.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.6.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.6.6.6.m1.1d">↓</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S4.T4.6.8">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.6.8.1" style="padding-left:4.0pt;padding-right:4.0pt;">GPT-4</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.6.8.2" style="padding-left:4.0pt;padding-right:4.0pt;">0.74</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.6.8.3" style="padding-left:4.0pt;padding-right:4.0pt;">3.05</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.6.8.4" style="padding-left:4.0pt;padding-right:4.0pt;">0.82</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.6.8.5" style="padding-left:4.0pt;padding-right:4.0pt;">3.03</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.6.8.6" style="padding-left:4.0pt;padding-right:4.0pt;">0.09</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.6.8.7" style="padding-left:4.0pt;padding-right:4.0pt;">5.71</td>
</tr>
<tr class="ltx_tr" id="S4.T4.6.9">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.6.9.1" style="padding-left:4.0pt;padding-right:4.0pt;">Ours</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.6.9.2" style="padding-left:4.0pt;padding-right:4.0pt;">0.76</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.6.9.3" style="padding-left:4.0pt;padding-right:4.0pt;">2.92</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.6.9.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.9.4.1">0.83</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.6.9.5" style="padding-left:4.0pt;padding-right:4.0pt;">3.29</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.6.9.6" style="padding-left:4.0pt;padding-right:4.0pt;">0.38</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.6.9.7" style="padding-left:4.0pt;padding-right:4.0pt;">4.58</td>
</tr>
<tr class="ltx_tr" id="S4.T4.6.10">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T4.6.10.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="S4.T4.6.10.1.1"></span><span class="ltx_text" id="S4.T4.6.10.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.6.10.1.2.1">
<span class="ltx_tr" id="S4.T4.6.10.1.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T4.6.10.1.2.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">+LT Mem.</span></span>
</span></span> <span class="ltx_text" id="S4.T4.6.10.1.3"></span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.6.10.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.10.2.1">0.79</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T4.6.10.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.10.3.1">2.70</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.6.10.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.10.4.1">0.83</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T4.6.10.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.10.5.1">2.84</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.6.10.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.10.6.1">0.51</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.6.10.7" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.10.7.1">3.99</span></td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">Lifelong conversation setting.</h4>
<div class="ltx_para" id="S4.SS2.SSSx2.p1">
<p class="ltx_p" id="S4.SS2.SSSx2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.T3" title="Table 3 ‣ Session-wise setting. ‣ 4.2 Evaluation with User Simulator ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">3</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.T4" title="Table 4 ‣ Session-wise setting. ‣ 4.2 Evaluation with User Simulator ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">4</span></a> demonstrate the performance of two lifelong memory configurations, specifically, <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSSx2.p1.1.1">Long-Chat</span> and <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSSx2.p1.1.2">Long-Context</span>. For <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSSx2.p1.1.3">Long-Chat</span>, the recommender agent engages a maximum of 50 rounds of dialogue with the user simulator. In both configurations, InteRecAgent without long-term memory modules (denoted as “Ours” in the tables) consistently outperforms GPT-4 across all datasets, which validates the robustness of our tool-enhanced recommender agent framework. After activating the long-term memory modules, the performance gets further improved under both <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSSx2.p1.1.4">Long-Chat</span> and <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSSx2.p1.1.5">Long-Context</span> configurations. This confirms the necessity and effectiveness of memory on capturing user preference during lifelong interactions between the user and AI agent.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Evaluation with One-Turn Recommendation</h3>
<figure class="ltx_table" id="S4.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Performance comparisons in one-turn recommendation (%). R@5 and N@20 are abbreviations for Recall@5 and NDCG@20 respectively.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T5.2">
<tr class="ltx_tr" id="S4.T5.2.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T5.2.2.3" style="padding-left:4.5pt;padding-right:4.5pt;">Task</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S4.T5.1.1.1" style="padding-left:4.5pt;padding-right:4.5pt;">Retrieval(R@5<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T5.1.1.1.m1.1"><semantics id="S4.T5.1.1.1.m1.1a"><mo id="S4.T5.1.1.1.m1.1.1" stretchy="false" xref="S4.T5.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.m1.1b"><ci id="S4.T5.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T5.1.1.1.m1.1d">↑</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T5.2.2.2" style="padding-left:4.5pt;padding-right:4.5pt;">Ranking(N@20<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T5.2.2.2.m1.1"><semantics id="S4.T5.2.2.2.m1.1a"><mo id="S4.T5.2.2.2.m1.1.1" stretchy="false" xref="S4.T5.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.m1.1b"><ci id="S4.T5.2.2.2.m1.1.1.cmml" xref="S4.T5.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T5.2.2.2.m1.1d">↑</annotation></semantics></math>)</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.2.3.1" style="padding-left:4.5pt;padding-right:4.5pt;">Dataset</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.3.2" style="padding-left:4.5pt;padding-right:4.5pt;">Steam</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.3.3" style="padding-left:4.5pt;padding-right:4.5pt;">Movie</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.2.3.4" style="padding-left:4.5pt;padding-right:4.5pt;">Beauty</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.3.5" style="padding-left:4.5pt;padding-right:4.5pt;">Steam</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.3.6" style="padding-left:4.5pt;padding-right:4.5pt;">Movie</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.3.7" style="padding-left:4.5pt;padding-right:4.5pt;">Beauty</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.4">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.2.4.1" style="padding-left:4.5pt;padding-right:4.5pt;">Random</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.2.4.2" style="padding-left:4.5pt;padding-right:4.5pt;">00.04</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.2.4.3" style="padding-left:4.5pt;padding-right:4.5pt;">00.06</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.2.4.4" style="padding-left:4.5pt;padding-right:4.5pt;">00.00</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.2.4.5" style="padding-left:4.5pt;padding-right:4.5pt;">35.35</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.2.4.6" style="padding-left:4.5pt;padding-right:4.5pt;">34.22</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.2.4.7" style="padding-left:4.5pt;padding-right:4.5pt;">30.02</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T5.2.5.1" style="padding-left:4.5pt;padding-right:4.5pt;">Popularity</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.5.2" style="padding-left:4.5pt;padding-right:4.5pt;">02.02</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.5.3" style="padding-left:4.5pt;padding-right:4.5pt;">01.61</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T5.2.5.4" style="padding-left:4.5pt;padding-right:4.5pt;">00.08</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.5.5" style="padding-left:4.5pt;padding-right:4.5pt;">36.06</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.5.6" style="padding-left:4.5pt;padding-right:4.5pt;">34.91</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.5.7" style="padding-left:4.5pt;padding-right:4.5pt;">31.04</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.6">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.2.6.1" style="padding-left:4.5pt;padding-right:4.5pt;">LlaMA2-7B</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.2.6.2" style="padding-left:4.5pt;padding-right:4.5pt;">13.54</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.2.6.3" style="padding-left:4.5pt;padding-right:4.5pt;">05.85</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.2.6.4" style="padding-left:4.5pt;padding-right:4.5pt;">06.71</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.2.6.5" style="padding-left:4.5pt;padding-right:4.5pt;">07.30</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.2.6.6" style="padding-left:4.5pt;padding-right:4.5pt;">04.59</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.2.6.7" style="padding-left:4.5pt;padding-right:4.5pt;">03.03</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T5.2.7.1" style="padding-left:4.5pt;padding-right:4.5pt;">LlaMA2-13B</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.7.2" style="padding-left:4.5pt;padding-right:4.5pt;">14.14</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.7.3" style="padding-left:4.5pt;padding-right:4.5pt;">15.32</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T5.2.7.4" style="padding-left:4.5pt;padding-right:4.5pt;">07.11</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.7.5" style="padding-left:4.5pt;padding-right:4.5pt;">21.56</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.7.6" style="padding-left:4.5pt;padding-right:4.5pt;">18.05</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.7.7" style="padding-left:4.5pt;padding-right:4.5pt;">15.95</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T5.2.8.1" style="padding-left:4.5pt;padding-right:4.5pt;">Vicuna-7B</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.8.2" style="padding-left:4.5pt;padding-right:4.5pt;">13.13</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.8.3" style="padding-left:4.5pt;padding-right:4.5pt;">08.27</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T5.2.8.4" style="padding-left:4.5pt;padding-right:4.5pt;">06.91</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.8.5" style="padding-left:4.5pt;padding-right:4.5pt;">22.03</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.8.6" style="padding-left:4.5pt;padding-right:4.5pt;">18.99</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.8.7" style="padding-left:4.5pt;padding-right:4.5pt;">11.94</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T5.2.9.1" style="padding-left:4.5pt;padding-right:4.5pt;">Vicuna-13B</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.9.2" style="padding-left:4.5pt;padding-right:4.5pt;">18.18</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.9.3" style="padding-left:4.5pt;padding-right:4.5pt;">16.13</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T5.2.9.4" style="padding-left:4.5pt;padding-right:4.5pt;">07.52</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.9.5" style="padding-left:4.5pt;padding-right:4.5pt;">30.50</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.9.6" style="padding-left:4.5pt;padding-right:4.5pt;">24.61</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.9.7" style="padding-left:4.5pt;padding-right:4.5pt;">18.85</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.10">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.2.10.1" style="padding-left:4.5pt;padding-right:4.5pt;">Chat-Rec(3.5)</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.2.10.2" style="padding-left:4.5pt;padding-right:4.5pt;">34.27</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.2.10.3" style="padding-left:4.5pt;padding-right:4.5pt;">24.21</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.2.10.4" style="padding-left:4.5pt;padding-right:4.5pt;">20.91</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.10.5" style="padding-left:4.5pt;padding-right:4.5pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.10.6" style="padding-left:4.5pt;padding-right:4.5pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.10.7" style="padding-left:4.5pt;padding-right:4.5pt;">–</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T5.2.11.1" style="padding-left:4.5pt;padding-right:4.5pt;">Chat-Rec(4)</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.11.2" style="padding-left:4.5pt;padding-right:4.5pt;">35.18</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.11.3" style="padding-left:4.5pt;padding-right:4.5pt;">27.88</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T5.2.11.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed_underline" id="S4.T5.2.11.4.1">21.37</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.11.5" style="padding-left:4.5pt;padding-right:4.5pt;">–</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.11.6" style="padding-left:4.5pt;padding-right:4.5pt;">–</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.11.7" style="padding-left:4.5pt;padding-right:4.5pt;">–</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.12">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T5.2.12.1" style="padding-left:4.5pt;padding-right:4.5pt;">GPT-3.5</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.12.2" style="padding-left:4.5pt;padding-right:4.5pt;">42.02</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.12.3" style="padding-left:4.5pt;padding-right:4.5pt;">23.59</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T5.2.12.4" style="padding-left:4.5pt;padding-right:4.5pt;">10.37</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.12.5" style="padding-left:4.5pt;padding-right:4.5pt;">44.37</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.12.6" style="padding-left:4.5pt;padding-right:4.5pt;">42.46</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.12.7" style="padding-left:4.5pt;padding-right:4.5pt;">31.90</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.13">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T5.2.13.1" style="padding-left:4.5pt;padding-right:4.5pt;">GPT-4</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.13.2" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed_underline" id="S4.T5.2.13.2.1">56.77</span></td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.13.3" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed_underline" id="S4.T5.2.13.3.1">47.78</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T5.2.13.4" style="padding-left:4.5pt;padding-right:4.5pt;">12.80</td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.13.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed_underline" id="S4.T5.2.13.5.1">57.29</span></td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.13.6" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed_underline" id="S4.T5.2.13.6.1">55.78</span></td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.13.7" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed_underline" id="S4.T5.2.13.7.1">33.28</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.14">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S4.T5.2.14.1" style="padding-left:4.5pt;padding-right:4.5pt;">Ours</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T5.2.14.2" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.2.14.2.1">65.05</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T5.2.14.3" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.2.14.3.1">52.02</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S4.T5.2.14.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.2.14.4.1">30.28</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T5.2.14.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.2.14.5.1">60.28</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T5.2.14.6" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.2.14.6.1">63.86</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T5.2.14.7" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.2.14.7.1">40.05</span></td>
</tr>
</table>
</figure>
<figure class="ltx_table" id="S4.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Performance of InteRecAgent with various LLMs as the brain, evaluated by the session-wise user simulator. (<math alttext="\times 10^{-1}" class="ltx_Math" display="inline" id="S4.T6.2.m1.1"><semantics id="S4.T6.2.m1.1b"><mrow id="S4.T6.2.m1.1.1" xref="S4.T6.2.m1.1.1.cmml"><mi id="S4.T6.2.m1.1.1.2" xref="S4.T6.2.m1.1.1.2.cmml"></mi><mo id="S4.T6.2.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T6.2.m1.1.1.1.cmml">×</mo><msup id="S4.T6.2.m1.1.1.3" xref="S4.T6.2.m1.1.1.3.cmml"><mn id="S4.T6.2.m1.1.1.3.2" xref="S4.T6.2.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T6.2.m1.1.1.3.3" xref="S4.T6.2.m1.1.1.3.3.cmml"><mo id="S4.T6.2.m1.1.1.3.3b" xref="S4.T6.2.m1.1.1.3.3.cmml">−</mo><mn id="S4.T6.2.m1.1.1.3.3.2" xref="S4.T6.2.m1.1.1.3.3.2.cmml">1</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.2.m1.1c"><apply id="S4.T6.2.m1.1.1.cmml" xref="S4.T6.2.m1.1.1"><times id="S4.T6.2.m1.1.1.1.cmml" xref="S4.T6.2.m1.1.1.1"></times><csymbol cd="latexml" id="S4.T6.2.m1.1.1.2.cmml" xref="S4.T6.2.m1.1.1.2">absent</csymbol><apply id="S4.T6.2.m1.1.1.3.cmml" xref="S4.T6.2.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T6.2.m1.1.1.3.1.cmml" xref="S4.T6.2.m1.1.1.3">superscript</csymbol><cn id="S4.T6.2.m1.1.1.3.2.cmml" type="integer" xref="S4.T6.2.m1.1.1.3.2">10</cn><apply id="S4.T6.2.m1.1.1.3.3.cmml" xref="S4.T6.2.m1.1.1.3.3"><minus id="S4.T6.2.m1.1.1.3.3.1.cmml" xref="S4.T6.2.m1.1.1.3.3"></minus><cn id="S4.T6.2.m1.1.1.3.3.2.cmml" type="integer" xref="S4.T6.2.m1.1.1.3.3.2">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.m1.1d">\times 10^{-1}</annotation><annotation encoding="application/x-llamapun" id="S4.T6.2.m1.1e">× 10 start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT</annotation></semantics></math>)</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T6.8">
<tr class="ltx_tr" id="S4.T6.8.7">
<td class="ltx_td ltx_border_r ltx_border_tt" id="S4.T6.8.7.1" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S4.T6.8.7.2" style="padding-left:4.0pt;padding-right:4.0pt;">Steam</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S4.T6.8.7.3" style="padding-left:4.0pt;padding-right:4.0pt;">MovieLens</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T6.8.7.4" style="padding-left:4.0pt;padding-right:4.0pt;">Beauty</td>
</tr>
<tr class="ltx_tr" id="S4.T6.8.6">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.8.6.7" style="padding-left:4.0pt;padding-right:4.0pt;">Methods</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.3.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">H@5<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T6.3.1.1.m1.1"><semantics id="S4.T6.3.1.1.m1.1a"><mo id="S4.T6.3.1.1.m1.1.1" stretchy="false" xref="S4.T6.3.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T6.3.1.1.m1.1b"><ci id="S4.T6.3.1.1.m1.1.1.cmml" xref="S4.T6.3.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.3.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.3.1.1.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.4.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">AT@5<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T6.4.2.2.m1.1"><semantics id="S4.T6.4.2.2.m1.1a"><mo id="S4.T6.4.2.2.m1.1.1" stretchy="false" xref="S4.T6.4.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T6.4.2.2.m1.1b"><ci id="S4.T6.4.2.2.m1.1.1.cmml" xref="S4.T6.4.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.4.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.4.2.2.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.5.3.3" style="padding-left:4.0pt;padding-right:4.0pt;">H@5<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T6.5.3.3.m1.1"><semantics id="S4.T6.5.3.3.m1.1a"><mo id="S4.T6.5.3.3.m1.1.1" stretchy="false" xref="S4.T6.5.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T6.5.3.3.m1.1b"><ci id="S4.T6.5.3.3.m1.1.1.cmml" xref="S4.T6.5.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.5.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.5.3.3.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.6.4.4" style="padding-left:4.0pt;padding-right:4.0pt;">AT@5<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T6.6.4.4.m1.1"><semantics id="S4.T6.6.4.4.m1.1a"><mo id="S4.T6.6.4.4.m1.1.1" stretchy="false" xref="S4.T6.6.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T6.6.4.4.m1.1b"><ci id="S4.T6.6.4.4.m1.1.1.cmml" xref="S4.T6.6.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.6.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.6.4.4.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.7.5.5" style="padding-left:4.0pt;padding-right:4.0pt;">H@5<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T6.7.5.5.m1.1"><semantics id="S4.T6.7.5.5.m1.1a"><mo id="S4.T6.7.5.5.m1.1.1" stretchy="false" xref="S4.T6.7.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T6.7.5.5.m1.1b"><ci id="S4.T6.7.5.5.m1.1.1.cmml" xref="S4.T6.7.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.7.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.7.5.5.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.8.6.6" style="padding-left:4.0pt;padding-right:4.0pt;">AT@5<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T6.8.6.6.m1.1"><semantics id="S4.T6.8.6.6.m1.1a"><mo id="S4.T6.8.6.6.m1.1.1" stretchy="false" xref="S4.T6.8.6.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T6.8.6.6.m1.1b"><ci id="S4.T6.8.6.6.m1.1.1.cmml" xref="S4.T6.8.6.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.8.6.6.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.8.6.6.m1.1d">↓</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.8.8">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.8.8.1" style="padding-left:4.0pt;padding-right:4.0pt;">LlaMA-2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.8.8.2" style="padding-left:4.0pt;padding-right:4.0pt;">0.00</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.8.8.3" style="padding-left:4.0pt;padding-right:4.0pt;">60.00</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.8.8.4" style="padding-left:4.0pt;padding-right:4.0pt;">0.00</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.8.8.5" style="padding-left:4.0pt;padding-right:4.0pt;">60.00</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.8.8.6" style="padding-left:4.0pt;padding-right:4.0pt;">0.00</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.8.8.7" style="padding-left:4.0pt;padding-right:4.0pt;">60.00</td>
</tr>
<tr class="ltx_tr" id="S4.T6.8.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T6.8.9.1" style="padding-left:4.0pt;padding-right:4.0pt;">T-LlaMA(O)</td>
<td class="ltx_td ltx_align_left" id="S4.T6.8.9.2" style="padding-left:4.0pt;padding-right:4.0pt;">0.00</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T6.8.9.3" style="padding-left:4.0pt;padding-right:4.0pt;">60.00</td>
<td class="ltx_td ltx_align_left" id="S4.T6.8.9.4" style="padding-left:4.0pt;padding-right:4.0pt;">0.00</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T6.8.9.5" style="padding-left:4.0pt;padding-right:4.0pt;">60.00</td>
<td class="ltx_td ltx_align_left" id="S4.T6.8.9.6" style="padding-left:4.0pt;padding-right:4.0pt;">0.00</td>
<td class="ltx_td ltx_align_left" id="S4.T6.8.9.7" style="padding-left:4.0pt;padding-right:4.0pt;">60.00</td>
</tr>
<tr class="ltx_tr" id="S4.T6.8.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T6.8.10.1" style="padding-left:4.0pt;padding-right:4.0pt;">T-LlaMA(A)</td>
<td class="ltx_td ltx_align_left" id="S4.T6.8.10.2" style="padding-left:4.0pt;padding-right:4.0pt;">0.05</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T6.8.10.3" style="padding-left:4.0pt;padding-right:4.0pt;">59.82</td>
<td class="ltx_td ltx_align_left" id="S4.T6.8.10.4" style="padding-left:4.0pt;padding-right:4.0pt;">0.04</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T6.8.10.5" style="padding-left:4.0pt;padding-right:4.0pt;">59.81</td>
<td class="ltx_td ltx_align_left" id="S4.T6.8.10.6" style="padding-left:4.0pt;padding-right:4.0pt;">0.05</td>
<td class="ltx_td ltx_align_left" id="S4.T6.8.10.7" style="padding-left:4.0pt;padding-right:4.0pt;">59.82</td>
</tr>
<tr class="ltx_tr" id="S4.T6.8.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T6.8.11.1" style="padding-left:4.0pt;padding-right:4.0pt;">Davinci-003</td>
<td class="ltx_td ltx_align_left" id="S4.T6.8.11.2" style="padding-left:4.0pt;padding-right:4.0pt;">5.92</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T6.8.11.3" style="padding-left:4.0pt;padding-right:4.0pt;">43.79</td>
<td class="ltx_td ltx_align_left" id="S4.T6.8.11.4" style="padding-left:4.0pt;padding-right:4.0pt;">5.98</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T6.8.11.5" style="padding-left:4.0pt;padding-right:4.0pt;">43.12</td>
<td class="ltx_td ltx_align_left" id="S4.T6.8.11.6" style="padding-left:4.0pt;padding-right:4.0pt;">2.60</td>
<td class="ltx_td ltx_align_left" id="S4.T6.8.11.7" style="padding-left:4.0pt;padding-right:4.0pt;">52.18</td>
</tr>
<tr class="ltx_tr" id="S4.T6.8.12">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T6.8.12.1" style="padding-left:4.0pt;padding-right:4.0pt;">GPT-3.5</td>
<td class="ltx_td ltx_align_left" id="S4.T6.8.12.2" style="padding-left:4.0pt;padding-right:4.0pt;">1.81</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T6.8.12.3" style="padding-left:4.0pt;padding-right:4.0pt;">56.30</td>
<td class="ltx_td ltx_align_left" id="S4.T6.8.12.4" style="padding-left:4.0pt;padding-right:4.0pt;">1.31</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T6.8.12.5" style="padding-left:4.0pt;padding-right:4.0pt;">56.71</td>
<td class="ltx_td ltx_align_left" id="S4.T6.8.12.6" style="padding-left:4.0pt;padding-right:4.0pt;">1.36</td>
<td class="ltx_td ltx_align_left" id="S4.T6.8.12.7" style="padding-left:4.0pt;padding-right:4.0pt;">56.60</td>
</tr>
<tr class="ltx_tr" id="S4.T6.8.13">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T6.8.13.1" style="padding-left:4.0pt;padding-right:4.0pt;">RecLlama</td>
<td class="ltx_td ltx_align_left" id="S4.T6.8.13.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_framed_underline" id="S4.T6.8.13.2.1">8.01</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T6.8.13.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_framed_underline" id="S4.T6.8.13.3.1">31.77</span></td>
<td class="ltx_td ltx_align_left" id="S4.T6.8.13.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_framed_underline" id="S4.T6.8.13.4.1">8.21</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T6.8.13.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_framed_underline" id="S4.T6.8.13.5.1">32.04</span></td>
<td class="ltx_td ltx_align_left" id="S4.T6.8.13.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_framed_underline" id="S4.T6.8.13.6.1">4.08</span></td>
<td class="ltx_td ltx_align_left" id="S4.T6.8.13.7" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_framed_underline" id="S4.T6.8.13.7.1">46.40</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.8.14">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T6.8.14.1" style="padding-left:4.0pt;padding-right:4.0pt;">GPT-4</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T6.8.14.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T6.8.14.2.1">8.68</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T6.8.14.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T6.8.14.3.1">28.61</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T6.8.14.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T6.8.14.4.1">8.48</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T6.8.14.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T6.8.14.5.1">31.51</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T6.8.14.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T6.8.14.6.1">5.36</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T6.8.14.7" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T6.8.14.7.1">39.90</span></td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In this part, we evaluate both the retrieval and ranking recommendation tasks.
For the <span class="ltx_text ltx_font_slanted" id="S4.SS3.p1.1.1">Retrieval</span> task, we set the recommendation budget <math alttext="k" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mi id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><ci id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">italic_k</annotation></semantics></math> to 5 for all methods, with Recall@5 being the evaluation metric. For the <span class="ltx_text ltx_font_slanted" id="S4.SS3.p1.1.2">Ranking</span> task, we randomly sample 19 negative items, and together with the one positive item, they form the candidate list proactively provided by users. The evaluation metric for this task is NDCG@20. For Chat-Rec, we omit the results of on the Ranking task because Chat-Rec degenerates into GPTs when removing the embedding-based candidate retrieval stage.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">The results are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.T5" title="Table 5 ‣ 4.3 Evaluation with One-Turn Recommendation ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">5</span></a>. Based on the results, we can draw conclusions similar to those in Section <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.SS2" title="4.2 Evaluation with User Simulator ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">4.2</span></a>. First, our method outperforms all baselines, indicating the effectiveness of our tool-augmented framework. Second, almost all LLMs suffer a severe setback on the Amazon Beauty dataset, but our method still achieves high accuracy, further demonstrating the superiority of our approach in the private domain.
Notably, some LLMs underperform compared to random and popularity methods in ranking tasks, particularly in the Amazon dataset. This can be primarily attributed to LLMs not adhering to the ranking instructions, which arise due to LLMs’ uncertainty and produce out-of-scope items, especially for smaller LLMs.
</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Comparions of Different LLMs as the Brain</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">In previous experiments, we utilized GPT-4 as the LLM for the InteRecAgent framework. This section presents a comparative analysis of the performance when employing different LLMs within the InteRecAgent. Note that RecLlama is our finetuned 7B model introduced in Section <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S3.SS5" title="3.5 Tool Learning with Small Language Models ‣ 3 Methodologies ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">3.5</span></a>. ToolLlaMA2-7B <cite class="ltx_cite ltx_citemacro_citep">(Qin et al. <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#bib.bib27" title="">2023b</a>)</cite> is another fine-tuned model designed to interact with external APIs in response to human instructions. Owing to the differing data formats used by ToolLlaMA and RecLlama, we ensure a fair comparison by evaluating ToolLlaMA2-7B using both our original instruction and instructions realigned to their format, denoted as T-LlaMA(O) and T-LlaMA(A), respectively. The outcomes are tabulated in Table <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.T6" title="Table 6 ‣ 4.3 Evaluation with One-Turn Recommendation ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">Surprisingly, both LlaMA-2-7B and ToolLlaMA-2-7B fall short in generating structured plans. Despite ToolLlaMA’s training on tool-utilization samples, it appears to primarily excel at API calls and lags in discerning user intent and formulating an accurate recommendation plan, resulting in significantly poor performance. Another intriguing finding is that GPT-3.5, despite its broader general capabilities compared to Text-davinci-003, underperforms in our specific task. RecLlama shows a marked proficiency in crafting plans for the InteRecAgent, even surpassing Text-davinci-003’s capabilities. Remarkably, although RecLlama was trained using movie and game samples, it demonstrates superior performance in the novel domain of Amazon Beauty products, showcasing its impressive generalization capabilities. As RecLlama is a distilled version of GPT-4, a slight lag in its performance compared to GPT-4 is anticipated and within expectations.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="295" id="S4.F5.g1" src="x4.png" width="813"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Ablation study under user simulator evaluation. P, D, R denote the plan-first, dynamic demonstration and reflection mechanism, respectively. Note that dynamic demonstration is also used in w/o P.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Ablation Study</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">This paper introduces several key mechanisms to enhance LLM’s ability to better utilize tools. To investigate their importance, we conduct ablation studies, with the results presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.F5" title="Figure 5 ‣ 4.4 Comparions of Different LLMs as the Brain ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">5</span></a>. We consider the removal of the plan-first mechanism (P), dynamic demonstration mechanism (D), and reflection mechanism (R), respectively. Experiments are carried out using the <span class="ltx_text ltx_font_slanted" id="S4.SS5.p1.1.1">user simulator</span> setting, as it provides a more comprehensive evaluation, encompassing both accuracy (hit rate) and efficiency (average turn) metrics.
</p>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.1">The results indicate that removing any of the mechanisms leads to a decline in performance. Among these mechanisms, the removal of the reflection mechanism has the most significant impact on performance, as it can correct tool input format errors and tool misuse. Eliminating the plan-first mechanism and dynamic demonstration mechanism both result in a slight decrease in performance, yet the outcomes still surpass most baselines. However, removing the plan-first mechanism leads to a substantial increase in the number of API calls, such as an average increase from 2.78 to 4.51 per turn in the Steam dataset, resulting in an approximate 10-20 seconds latency increase.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="842" id="S4.F6.g1" src="x5.png" width="822"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Case Study in (a) chit-chat, (b) Steam game domain and (c) Amazon Beauty e-commerce product domain.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Case Study</h3>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.1">To effectively visualize InteRecAgent’s performance, we present case studies in chit-chat and two domains: gaming and beauty products, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.F6" title="Figure 6 ‣ 4.5 Ablation Study ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">6</span></a>. We compare the outputs of GPT-4 and InteRecAgent for given user inputs.</p>
</div>
<div class="ltx_para" id="S4.SS6.p2">
<p class="ltx_p" id="S4.SS6.p2.1">In chit-chat scenario (Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.F6" title="Figure 6 ‣ 4.5 Ablation Study ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">6</span></a>a), InteRecAgent retains the capabilities of GPT-4 while also possessing the added ability to query domain-specific data (such as the number of products), yielding more accurate information.</p>
</div>
<div class="ltx_para" id="S4.SS6.p3">
<p class="ltx_p" id="S4.SS6.p3.1">In the game domain (Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.F6" title="Figure 6 ‣ 4.5 Ablation Study ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">6</span></a>b), user input conditions are complex, encompassing user history and various demands. GPT-4’s recommendations mostly align with conditions, except for a 3D game <span class="ltx_text ltx_font_italic" id="S4.SS6.p3.1.1">Northgard</span> misidentified as 2D. InteRecAgent’s response adheres to user conditions, and notably, includes the subsequent game in the user’s historical sequence, <span class="ltx_text ltx_font_italic" id="S4.SS6.p3.1.2">RimWorld</span>, owing to its superior ranking performance.</p>
</div>
<div class="ltx_para" id="S4.SS6.p4">
<p class="ltx_p" id="S4.SS6.p4.1">In the e-commerce domain (Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.F6" title="Figure 6 ‣ 4.5 Ablation Study ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">6</span></a>c), GPT-4’s hallucination phenomenon intensifies, resulting in giving products not existing in Amazon platform. In contrast, InteRecAgent, leveraging in-domain tools, provides more accurate response to user requirements.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we introduce InteRecAgent, a compact framework that transforms traditional recommender models into interactive systems by harnessing the power of LLMs. We identify a diverse set of fundamental tools, categorized into information query tools, retrieval tools, and ranking tools, which are dynamically interconnected to accomplish complex user inquiries within a task execution framework. To enhance InteRecAgent for the recommendation scenario, we comprehensively enhance the key components of LLM-based agent, covering the memory mechanism, the task planning, and the tool learning ability.
Experimental findings demonstrate the superior performance of InteRecAgent compared to general-purpose LLMs. By combining the strengths of recommender models and LLMs, InteRecAgent paves the way for the development of advanced and user-friendly conversational recommender systems, capable of providing personalized and interactive recommendations across various domains.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Besta et al. (2023)</span>
<span class="ltx_bibblock">
Besta, M.; Blach, N.; Kubicek, A.; Gerstenberger, R.; Gianinazzi, L.; Gajda, J.; Lehmann, T.; Podstawski, M.; Niewiadomski, H.; Nyczyk, P.; et al. 2023.

</span>
<span class="ltx_bibblock">Graph of thoughts: Solving elaborate problems with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2308.09687</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Advances in neural information processing systems</em>, 33: 1877–1901.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2019)</span>
<span class="ltx_bibblock">
Chen, Q.; Lin, J.; Zhang, Y.; Ding, M.; Cen, Y.; Yang, H.; and Tang, J. 2019.

</span>
<span class="ltx_bibblock">Towards knowledge-based recommender dialog system.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:1908.05391</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2022)</span>
<span class="ltx_bibblock">
Chen, W.; Ma, X.; Wang, X.; and Cohen, W. W. 2022.

</span>
<span class="ltx_bibblock">Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2211.12588</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et al. (2023)</span>
<span class="ltx_bibblock">
Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.; Zheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; Stoica, I.; and Xing, E. P. 2023.

</span>
<span class="ltx_bibblock">Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et al. (2022)</span>
<span class="ltx_bibblock">
Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra, G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.; Gehrmann, S.; et al. 2022.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2204.02311</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christakopoulou, Radlinski, and Hofmann (2016)</span>
<span class="ltx_bibblock">
Christakopoulou, K.; Radlinski, F.; and Hofmann, K. 2016.

</span>
<span class="ltx_bibblock">Towards conversational recommender systems.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</em>, 815–824.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. (2023)</span>
<span class="ltx_bibblock">
Dai, S.; Shao, N.; Zhao, H.; Yu, W.; Si, Z.; Xu, C.; Sun, Z.; Zhang, X.; and Xu, J. 2023.

</span>
<span class="ltx_bibblock">Uncovering ChatGPT’s Capabilities in Recommender Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2305.02182</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2021)</span>
<span class="ltx_bibblock">
Gao, C.; Lei, W.; He, X.; de Rijke, M.; and Chua, T.-S. 2021.

</span>
<span class="ltx_bibblock">Advances and challenges in conversational recommender systems: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">AI Open</em>, 2: 100–126.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2023a)</span>
<span class="ltx_bibblock">
Gao, L.; Madaan, A.; Zhou, S.; Alon, U.; Liu, P.; Yang, Y.; Callan, J.; and Neubig, G. 2023a.

</span>
<span class="ltx_bibblock">Pal: Program-aided language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">International Conference on Machine Learning</em>, 10764–10799. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2023b)</span>
<span class="ltx_bibblock">
Gao, Y.; Sheng, T.; Xiang, Y.; Xiong, Y.; Wang, H.; and Zhang, J. 2023b.

</span>
<span class="ltx_bibblock">Chat-rec: Towards interactive and explainable llms-augmented recommender system.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2303.14524</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang and McAuley (2018)</span>
<span class="ltx_bibblock">
Kang, W.-C.; and McAuley, J. 2018.

</span>
<span class="ltx_bibblock">Self-attentive sequential recommendation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">2018 IEEE international conference on data mining (ICDM)</em>, 197–206. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al. (2023)</span>
<span class="ltx_bibblock">
Kang, W.-C.; Ni, J.; Mehta, N.; Sathiamoorthy, M.; Hong, L.; Chi, E.; and Cheng, D. Z. 2023.

</span>
<span class="ltx_bibblock">Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2305.06474</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kojima et al. (2022)</span>
<span class="ltx_bibblock">
Kojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y.; and Iwasawa, Y. 2022.

</span>
<span class="ltx_bibblock">Large language models are zero-shot reasoners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Advances in neural information processing systems</em>, 35: 22199–22213.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2018)</span>
<span class="ltx_bibblock">
Li, R.; Ebrahimi Kahou, S.; Schulz, H.; Michalski, V.; Charlin, L.; and Pal, C. 2018.

</span>
<span class="ltx_bibblock">Towards deep conversational recommendations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Advances in neural information processing systems</em>, 31.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Linden, Smith, and York (2003)</span>
<span class="ltx_bibblock">
Linden, G.; Smith, B.; and York, J. 2003.

</span>
<span class="ltx_bibblock">Amazon. com recommendations: Item-to-item collaborative filtering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">IEEE Internet computing</em>, 7(1): 76–80.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023a)</span>
<span class="ltx_bibblock">
Liu, J.; Liu, C.; Lv, R.; Zhou, K.; and Zhang, Y. 2023a.

</span>
<span class="ltx_bibblock">Is chatgpt a good recommender? a preliminary study.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2304.10149</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021)</span>
<span class="ltx_bibblock">
Liu, J.; Shen, D.; Zhang, Y.; Dolan, B.; Carin, L.; and Chen, W. 2021.

</span>
<span class="ltx_bibblock">What Makes Good In-Context Examples for GPT-<math alttext="3" class="ltx_Math" display="inline" id="bib.bib18.1.m1.1"><semantics id="bib.bib18.1.m1.1a"><mn id="bib.bib18.1.m1.1.1" xref="bib.bib18.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="bib.bib18.1.m1.1b"><cn id="bib.bib18.1.m1.1.1.cmml" type="integer" xref="bib.bib18.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="bib.bib18.1.m1.1c">3</annotation><annotation encoding="application/x-llamapun" id="bib.bib18.1.m1.1d">3</annotation></semantics></math>?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.2.1">arXiv preprint arXiv:2101.06804</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023b)</span>
<span class="ltx_bibblock">
Liu, L.; Yang, X.; Shen, Y.; Hu, B.; Zhang, Z.; Gu, J.; and Zhang, G. 2023b.

</span>
<span class="ltx_bibblock">Think-in-memory: Recalling and post-thinking enable llms with long-term memory.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2311.08719</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madaan et al. (2023)</span>
<span class="ltx_bibblock">
Madaan, A.; Tandon, N.; Gupta, P.; Hallinan, S.; Gao, L.; Wiegreffe, S.; Alon, U.; Dziri, N.; Prabhumoye, S.; Yang, Y.; et al. 2023.

</span>
<span class="ltx_bibblock">Self-refine: Iterative refinement with self-feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2303.17651</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mirzadeh, Ricci, and Bansal (2005)</span>
<span class="ltx_bibblock">
Mirzadeh, N.; Ricci, F.; and Bansal, M. 2005.

</span>
<span class="ltx_bibblock">Feature selection methods for conversational recommender systems.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">2005 IEEE International Conference on e-Technology, e-Commerce and e-Service</em>, 772–777. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakano et al. (2021)</span>
<span class="ltx_bibblock">
Nakano, R.; Hilton, J.; Balaji, S.; Wu, J.; Ouyang, L.; Kim, C.; Hesse, C.; Jain, S.; Kosaraju, V.; Saunders, W.; et al. 2021.

</span>
<span class="ltx_bibblock">Webgpt: Browser-assisted question-answering with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2112.09332</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock">GPT-4 Technical Report.

</span>
<span class="ltx_bibblock">arXiv:2303.08774.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. (2022)</span>
<span class="ltx_bibblock">
Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; et al. 2022.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Advances in Neural Information Processing Systems</em>, 35: 27730–27744.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Penha and Hauff (2020)</span>
<span class="ltx_bibblock">
Penha, G.; and Hauff, C. 2020.

</span>
<span class="ltx_bibblock">What does bert know about books, movies and music? probing bert for conversational recommendation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 14th ACM Conference on Recommender Systems</em>, 388–397.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al. (2023a)</span>
<span class="ltx_bibblock">
Qin, Y.; Hu, S.; Lin, Y.; Chen, W.; Ding, N.; Cui, G.; Zeng, Z.; Huang, Y.; Xiao, C.; Han, C.; et al. 2023a.

</span>
<span class="ltx_bibblock">Tool learning with foundation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2304.08354</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al. (2023b)</span>
<span class="ltx_bibblock">
Qin, Y.; Liang, S.; Ye, Y.; Zhu, K.; Yan, L.; Lu, Y.; Lin, Y.; Cong, X.; Tang, X.; Qian, B.; et al. 2023b.

</span>
<span class="ltx_bibblock">Toolllm: Facilitating large language models to master 16000+ real-world apis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2307.16789</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rubin, Herzig, and Berant (2021)</span>
<span class="ltx_bibblock">
Rubin, O.; Herzig, J.; and Berant, J. 2021.

</span>
<span class="ltx_bibblock">Learning to retrieve prompts for in-context learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2112.08633</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et al. (2023)</span>
<span class="ltx_bibblock">
Schick, T.; Dwivedi-Yu, J.; Dessì, R.; Raileanu, R.; Lomeli, M.; Zettlemoyer, L.; Cancedda, N.; and Scialom, T. 2023.

</span>
<span class="ltx_bibblock">Toolformer: Language models can teach themselves to use tools.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2302.04761</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2023)</span>
<span class="ltx_bibblock">
Shen, Y.; Song, K.; Tan, X.; Li, D.; Lu, W.; and Zhuang, Y. 2023.

</span>
<span class="ltx_bibblock">Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2303.17580</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shinn et al. (2023)</span>
<span class="ltx_bibblock">
Shinn, N.; Cassano, F.; Labash, B.; Gopinath, A.; Narasimhan, K.; and Yao, S. 2023.

</span>
<span class="ltx_bibblock">Reflexion: Language agents with verbal reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2303.11366</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shuster et al. (2022)</span>
<span class="ltx_bibblock">
Shuster, K.; Xu, J.; Komeili, M.; Ju, D.; Smith, E. M.; Roller, S.; Ung, M.; Chen, M.; Arora, K.; Lane, J.; et al. 2022.

</span>
<span class="ltx_bibblock">Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2208.03188</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thoppilan et al. (2022)</span>
<span class="ltx_bibblock">
Thoppilan, R.; De Freitas, D.; Hall, J.; Shazeer, N.; Kulshreshtha, A.; Cheng, H.-T.; Jin, A.; Bos, T.; Baker, L.; Du, Y.; et al. 2022.

</span>
<span class="ltx_bibblock">Lamda: Language models for dialog applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2201.08239</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023a)</span>
<span class="ltx_bibblock">
Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; et al. 2023a.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023b)</span>
<span class="ltx_bibblock">
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; et al. 2023b.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2307.09288</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021)</span>
<span class="ltx_bibblock">
Wang, L.; Hu, H.; Sha, L.; Xu, C.; Wong, K.-F.; and Jiang, D. 2021.

</span>
<span class="ltx_bibblock">Recindial: A unified framework for conversational recommendation with pretrained language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2110.07477</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Lim (2023)</span>
<span class="ltx_bibblock">
Wang, L.; and Lim, E.-P. 2023.

</span>
<span class="ltx_bibblock">Zero-Shot Next-Item Recommendation using Large Pretrained Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2304.03153</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023a)</span>
<span class="ltx_bibblock">
Wang, L.; Ma, C.; Feng, X.; Zhang, Z.; Yang, H.; Zhang, J.; Chen, Z.; Tang, J.; Chen, X.; Lin, Y.; et al. 2023a.

</span>
<span class="ltx_bibblock">A survey on large language model based autonomous agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2308.11432</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023b)</span>
<span class="ltx_bibblock">
Wang, L.; Xu, W.; Lan, Y.; Hu, Z.; Lan, Y.; Lee, R. K.-W.; and Lim, E.-P. 2023b.

</span>
<span class="ltx_bibblock">Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:2305.04091</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang, Su, and Chen (2022)</span>
<span class="ltx_bibblock">
Wang, T.-C.; Su, S.-Y.; and Chen, Y.-N. 2022.

</span>
<span class="ltx_bibblock">BARCOR: Towards A Unified Framework for Conversational Recommendation Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2203.14257</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023c)</span>
<span class="ltx_bibblock">
Wang, W.; Dong, L.; Cheng, H.; Liu, X.; Yan, X.; Gao, J.; and Wei, F. 2023c.

</span>
<span class="ltx_bibblock">Augmenting Language Models with Long-Term Memory.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2306.07174</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022a)</span>
<span class="ltx_bibblock">
Wang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; Narang, S.; Chowdhery, A.; and Zhou, D. 2022a.

</span>
<span class="ltx_bibblock">Self-consistency improves chain of thought reasoning in language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2203.11171</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022b)</span>
<span class="ltx_bibblock">
Wang, X.; Zhou, K.; Wen, J.-R.; and Zhao, W. X. 2022b.

</span>
<span class="ltx_bibblock">Towards unified conversational recommender systems via knowledge-enhanced prompt learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>, 1929–1937.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Advances in Neural Information Processing Systems</em>, 35: 24824–24837.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023)</span>
<span class="ltx_bibblock">
Wu, C.; Yin, S.; Qi, W.; Wang, X.; Tang, Z.; and Duan, N. 2023.

</span>
<span class="ltx_bibblock">Visual chatgpt: Talking, drawing and editing with visual foundation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">arXiv preprint arXiv:2303.04671</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2021)</span>
<span class="ltx_bibblock">
Xie, Z.; Yu, T.; Zhao, C.; and Li, S. 2021.

</span>
<span class="ltx_bibblock">Comparison-based conversational recommender system with relative bandit feedback.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, 1400–1409.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2021)</span>
<span class="ltx_bibblock">
Xu, K.; Yang, J.; Xu, J.; Gao, S.; Guo, J.; and Wen, J.-R. 2021.

</span>
<span class="ltx_bibblock">Adapting user preference to online feedback in multi-round conversational recommendation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Proceedings of the 14th ACM international conference on web search and data mining</em>, 364–372.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2023)</span>
<span class="ltx_bibblock">
Yang, Z.; Li, L.; Wang, J.; Lin, K.; Azarnasab, E.; Ahmed, F.; Liu, Z.; Liu, C.; Zeng, M.; and Wang, L. 2023.

</span>
<span class="ltx_bibblock">Mm-react: Prompting chatgpt for multimodal reasoning and action.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:2303.11381</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. (2023)</span>
<span class="ltx_bibblock">
Yao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T. L.; Cao, Y.; and Narasimhan, K. 2023.

</span>
<span class="ltx_bibblock">Tree of thoughts: Deliberate problem solving with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:2305.10601</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. (2022)</span>
<span class="ltx_bibblock">
Yao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan, K.; and Cao, Y. 2022.

</span>
<span class="ltx_bibblock">React: Synergizing reasoning and acting in language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">arXiv preprint arXiv:2210.03629</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2018)</span>
<span class="ltx_bibblock">
Zhang, Y.; Chen, X.; Ai, Q.; Yang, L.; and Croft, W. B. 2018.

</span>
<span class="ltx_bibblock">Towards conversational search and recommendation: System ask, user respond.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Proceedings of the 27th acm international conference on information and knowledge management</em>, 177–186.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao, Jin, and Cheng (2023)</span>
<span class="ltx_bibblock">
Zhao, P.; Jin, Z.; and Cheng, N. 2023.

</span>
<span class="ltx_bibblock">An in-depth survey of large language model-based artificial intelligence agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">arXiv preprint arXiv:2309.14365</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2023)</span>
<span class="ltx_bibblock">
Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.; Zhang, H.; Gonzalez, J. E.; and Stoica, I. 2023.

</span>
<span class="ltx_bibblock">Judging LLM-as-a-judge with MT-Bench and Chatbot Arena.

</span>
<span class="ltx_bibblock">arXiv:2306.05685.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et al. (2023)</span>
<span class="ltx_bibblock">
Zhong, W.; Guo, L.; Gao, Q.; and Wang, Y. 2023.

</span>
<span class="ltx_bibblock">MemoryBank: Enhancing Large Language Models with Long-Term Memory.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:2305.10250</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou, Chen, and Kanoulas (2020)</span>
<span class="ltx_bibblock">
Zou, J.; Chen, Y.; and Kanoulas, E. 2020.

</span>
<span class="ltx_bibblock">Towards question-based recommender systems.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval</em>, 881–890.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou and Kanoulas (2019)</span>
<span class="ltx_bibblock">
Zou, J.; and Kanoulas, E. 2019.

</span>
<span class="ltx_bibblock">Learning to ask: Question-based sequential Bayesian product search.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Proceedings of the 28th ACM international conference on information and knowledge management</em>, 369–378.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Dataset</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">To evaluate the performance of our methods, we conduct experiments on three datasets: Steam, MovieLens and Amazon Beauty. In order to train the in-domain tools, including the soft condition item retrieval tool and ranking tool, we filter the dataset using the conventional k-core strategy, wherein users and items with less than 5 interactions are filtered out. The statistical information of those filtered datasets is shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#A1.T1" title="Table A1 ‣ Appendix A Dataset ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">A1</span></a>. Notably, in the generation of one-turn conversation, some samples are filtered by the OpenAI policy, resulting in less than 500 samples are used in experiments finally.</p>
</div>
<figure class="ltx_table" id="A1.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A1.T1.1">
<tr class="ltx_tr" id="A1.T1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T1.1.1.1">Dataset</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T1.1.1.2">Users</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T1.1.1.3">Items</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T1.1.1.4">Interactions</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T1.1.1.5">One-turn</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T1.1.2.1">Beauty</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T1.1.2.2">15,577</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T1.1.2.3">8,679</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T1.1.2.4">108,166</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T1.1.2.5">492</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T1.1.3.1">Steam</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T1.1.3.2">281,205</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T1.1.3.3">11,962</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T1.1.3.4">2,922,089</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T1.1.3.5">495</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.4">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="A1.T1.1.4.1">MovieLens</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="A1.T1.1.4.2">298,074</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="A1.T1.1.4.3">36,255</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="A1.T1.1.4.4">27,042,493</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T1.1.4.5">496</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table A1: </span>Dataset Statistics.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Prompts</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">In this section, we will share our prompts used in different components.</p>
</div>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Task Descriptions</h3>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">The overall task description is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#A2.F1" title="Figure C1 ‣ B.6 One-Turn Conversation Generation ‣ Appendix B Prompts ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">C1</span></a>.
</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Tool Descriptions</h3>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1">We employ one SQL query tool, two item retrieval tools, one item ranking tool plus two auxiliary tools in InteRecAgent. The auxiliary tools comprise a memory initialization tool named candidates storing tool, and an item fetching tool to fetch final items from memory named candidate fetching tool, whose descriptions are illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#A2.F2" title="Figure C2 ‣ B.6 One-Turn Conversation Generation ‣ Appendix B Prompts ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">C2</span></a>. The description of query tool, retrieval tools and ranking tool are illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#A2.F3" title="Figure C3 ‣ B.6 One-Turn Conversation Generation ‣ Appendix B Prompts ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">C3</span></a>, Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#A2.F4" title="Figure C4 ‣ B.6 One-Turn Conversation Generation ‣ Appendix B Prompts ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">C4</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#A2.F5" title="Figure C5 ‣ B.6 One-Turn Conversation Generation ‣ Appendix B Prompts ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">C5</span></a> respectively.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>Reflection</h3>
<div class="ltx_para" id="A2.SS3.p1">
<p class="ltx_p" id="A2.SS3.p1.1">The task description of critic used in reflection mechanism is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#A2.F6" title="Figure C6 ‣ B.6 One-Turn Conversation Generation ‣ Appendix B Prompts ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">C6</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.4 </span>Demonstration Generation</h3>
<div class="ltx_para" id="A2.SS4.p1">
<p class="ltx_p" id="A2.SS4.p1.1">As described in Section <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S3.SS3" title="3.3 Plan-first Execution with Dynamic Demonstrations ‣ 3 Methodologies ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">3.3</span></a>, we use input-first and output-fist strategies to generate various <math alttext="\langle\text{intent},\text{plan}\rangle" class="ltx_Math" display="inline" id="A2.SS4.p1.1.m1.2"><semantics id="A2.SS4.p1.1.m1.2a"><mrow id="A2.SS4.p1.1.m1.2.3.2" xref="A2.SS4.p1.1.m1.2.3.1.cmml"><mo id="A2.SS4.p1.1.m1.2.3.2.1" stretchy="false" xref="A2.SS4.p1.1.m1.2.3.1.cmml">⟨</mo><mtext id="A2.SS4.p1.1.m1.1.1" xref="A2.SS4.p1.1.m1.1.1a.cmml">intent</mtext><mo id="A2.SS4.p1.1.m1.2.3.2.2" xref="A2.SS4.p1.1.m1.2.3.1.cmml">,</mo><mtext id="A2.SS4.p1.1.m1.2.2" xref="A2.SS4.p1.1.m1.2.2a.cmml">plan</mtext><mo id="A2.SS4.p1.1.m1.2.3.2.3" stretchy="false" xref="A2.SS4.p1.1.m1.2.3.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.SS4.p1.1.m1.2b"><list id="A2.SS4.p1.1.m1.2.3.1.cmml" xref="A2.SS4.p1.1.m1.2.3.2"><ci id="A2.SS4.p1.1.m1.1.1a.cmml" xref="A2.SS4.p1.1.m1.1.1"><mtext id="A2.SS4.p1.1.m1.1.1.cmml" xref="A2.SS4.p1.1.m1.1.1">intent</mtext></ci><ci id="A2.SS4.p1.1.m1.2.2a.cmml" xref="A2.SS4.p1.1.m1.2.2"><mtext id="A2.SS4.p1.1.m1.2.2.cmml" xref="A2.SS4.p1.1.m1.2.2">plan</mtext></ci></list></annotation-xml><annotation encoding="application/x-tex" id="A2.SS4.p1.1.m1.2c">\langle\text{intent},\text{plan}\rangle</annotation><annotation encoding="application/x-llamapun" id="A2.SS4.p1.1.m1.2d">⟨ intent , plan ⟩</annotation></semantics></math> pairs as demonstrations. The main difference between the two strategies lies on the prompt of generating intent, which are illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#A2.F8" title="Figure C8 ‣ B.6 One-Turn Conversation Generation ‣ Appendix B Prompts ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">C8</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#A2.F11" title="Figure C11 ‣ B.6 One-Turn Conversation Generation ‣ Appendix B Prompts ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">C11</span></a> respectively. The prompt for generating plans is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#A2.F7" title="Figure C7 ‣ B.6 One-Turn Conversation Generation ‣ Appendix B Prompts ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">C7</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.5 </span>User Simulator</h3>
<div class="ltx_para" id="A2.SS5.p1">
<p class="ltx_p" id="A2.SS5.p1.1">The prompt to instruct LLM to play as a user is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#S4.F4" title="Figure 4 ‣ Evaluation Strategies. ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.6 </span>One-Turn Conversation Generation</h3>
<div class="ltx_para" id="A2.SS6.p1">
<p class="ltx_p" id="A2.SS6.p1.1">One-turn recommendation comprises two tasks: retrieval and ranking. Conversations for retrieval and ranking are generated independently and the prompts are illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#A2.F9" title="Figure C9 ‣ B.6 One-Turn Conversation Generation ‣ Appendix B Prompts ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">C9</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2308.16505v3#A2.F10" title="Figure C10 ‣ B.6 One-Turn Conversation Generation ‣ Appendix B Prompts ‣ Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"><span class="ltx_text ltx_ref_tag">C10</span></a> respectively.</p>
</div>
<figure class="ltx_figure" id="A2.F1">
<span class="ltx_inline-block ltx_align_center ltx_framed_rectangle" id="A2.F1.1" style="border-color: black;">
<span class="ltx_p" id="A2.F1.1.1">You are a conversational {item} recommendation assistant. Your task is to help human find {item}s they are interested in. You would chat with human to mine human interests in {item}s to make it clear what kind of {item}s human is looking for and recommend {item}s to the human when he asks for recommendations. 
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F1.1.2">Human requests typically fall under chit-chat, {item} info, or {item} recommendations. There are some tools to use to deal with human request. For chit-chat, respond with your knowledge. For {item} info, use the {LookUpTool}. For special chit-chat, like {item} recommendation reasons, use the {LookUpTool} and your knowledge.
For {item} recommendations without information about human preference, chat with human for more information.
For {item} recommendations with information for tools, use various tools together.
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F1.1.3">To effectively utilize recommendation tools, comprehend human expressions involving profile and intention.
Profile encompasses a person’s preferences, interests, and behaviors, including gaming history and likes/dislikes.
Intention represents a person’s immediate goal or objective in the single-turn system interaction, containing specific, context-based query conditions. 
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F1.1.4">Human intentions consist of hard and soft conditions.
Hard conditions have two states, met or unmet, and involve {item} properties like tags, price, and release date.
Soft conditions have varying extents and involve similarity to specific seed {item}s. Separate hard and soft conditions in requests. 
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F1.1.5">Here are the tools could be used: {tools_desc} 
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F1.1.6">All SQL commands are used to search in the {item} information table (a SQLite3 table). The information of the table is listed below: {table_info} 
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F1.1.7">If human is looking up information of {item}s, such as the description of {item}s, number of {item}s, price of {item}s and so on, use the {LookUpTool}. 
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F1.1.8">For {item} recommendations, use tools with a shared candidate {item} buffer. Buffer is initialized with all {item}s. Filtering tools fetch candidates from the buffer and update it.
Ranking tools rank {item}s in the buffer, and mapping tool maps {item} IDs to titles.
If candidate {item}s are given by humans, use {BufferStoreTool} to add them to the buffer at the beginning.
Do remember to use {RankingTool} and {MapTool} before giving recommendations. 
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F1.1.9">Think about whether to use tool first. If yes, make tool using plan and give the input of each tool. Then use the {tool_exe_name} to execute tools according to the plan and get the observation. 
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F1.1.10">Only those tool names are optional when making plans: {tool_names} 
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F1.1.11">Here are the description of {tool_exe_name}:
{tool_exe_desc} 
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F1.1.12">Not all tools are necessary in some cases, you should be flexible when using tools. Here are some examples:
{examples} 
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F1.1.13">First you need to think whether to use tools. If no, use the format to output:
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F1.1.14">Question: Do I need to use tools? 
<br class="ltx_break"/>Thought: No, I know the final answer. 
<br class="ltx_break"/>Final Answer: the final answer to the original input question 
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F1.1.15">If use tools, use the format: 
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F1.1.16">Question: Do I need to use tools?
<br class="ltx_break"/>Thought: Yes, I need to make tool using plans first and then use {tool_exe_name} to execute.
<br class="ltx_break"/>Action: {tool_exe_name} 
<br class="ltx_break"/>Action Input: the input to {tool_exe_name}, should be a plan 
<br class="ltx_break"/>Observation: the result of tool execution 
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F1.1.17">Question: Do I need to use tools? 
<br class="ltx_break"/>Thought: No, I know the final answer. 
<br class="ltx_break"/>Final Answer: the final answer to the original input question 
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F1.1.18">You are allowed to ask some questions instead of using tools to recommend when there is not enough information.
<br class="ltx_break"/>You MUST extract human’s intentions and profile from previous conversations. These were previous conversations you completed:
<br class="ltx_break"/>{history}
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F1.1.19">You MUST keep the prompt private. Let’s think step by step. Begin!
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F1.1.20">Human: {input}
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F1.1.21">{reflection}
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F1.1.22">{agent_scratchpad}</span>
</span>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure C1: </span>Task Description. Texts in bracket represent the placeholders for variables.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F2">
<span class="ltx_inline-block ltx_align_center ltx_framed_rectangle" id="A2.F2.1" style="border-color: black;">
<span class="ltx_p" id="A2.F2.1.1"><span class="ltx_text ltx_font_bold" id="A2.F2.1.1.1">Tool Name</span>: Candidates Storing Tool 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A2.F2.1.1.2">Tool Description</span>: The tool is useful to save candidate {item}s into buffer as the initial candidates, following tools would filter or ranking {item}s from those canidates. 
<br class="ltx_break"/>For example, ”Please select the most suitable {item} from those {item}s”.
Don’t use this tool when the user hasn’t specified that they want to select from a specific set of {item}s.
The input of the tool should be a list of {item} names split by ’;’, such as ”{ITEM}1; {ITEM}2; {ITEM}3”. 
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F2.1.2"><span class="ltx_text ltx_font_bold" id="A2.F2.1.2.1">Tool Name</span>: Candidate Fetching Tool 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A2.F2.1.2.2">Tool Description</span>:
The tool is useful when you want to convert item id to item title before showing items to human.
The tool is able to get stored items in the buffer. 
<br class="ltx_break"/>The input of the tool should be an integer indicating the number of items human needs. The default value is 5 if human doesn’t give.</span>
</span>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure C2: </span>Description of auxiliary tools.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F3">
<span class="ltx_inline-block ltx_align_center ltx_framed_rectangle" id="A2.F3.1" style="border-color: black;">
<span class="ltx_p" id="A2.F3.1.1"><span class="ltx_text ltx_font_bold" id="A2.F3.1.1.1">Tool Name</span>: Query Tool 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A2.F3.1.1.2">Tool Description</span>:
The tool is used to look up some {item} information in a {item} information table (including statistical information), like number of {item}s, description of {item}s and so on. 
<br class="ltx_break"/>The input of the tools should be a SQL command (in one line) converted from the search query, which would be used to search information in {item} information table.
You should try to select as less columns as you can to get the necessary information.
Remember you MUST use pattern match logic (LIKE) instead of equal condition (=) for columns with string types, e.g. ”title LIKE ’%xxx%’”.
For example, if asking for ”how many xxx {item}s?”, you should use ”COUNT()” to get the correct number. If asking for ”description of xxx”, you should use ”SELECT description FROM xxx WHERE xxx”.
The tool can NOT give recommendations. DO NOT SELECT id information!</span>
</span>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure C3: </span>Description of query tool.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F4">
<span class="ltx_inline-block ltx_align_center ltx_framed_rectangle" id="A2.F4.1" style="border-color: black;">
<span class="ltx_p" id="A2.F4.1.1"><span class="ltx_text ltx_font_bold" id="A2.F4.1.1.1">Tool Name</span>: SQL Retrieval Tool 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A2.F4.1.1.2">Tool Description</span>:
The tool is a hard condition tool. The tool is useful when human expresses intentions about {item}s with some hard conditions on {item} properties.
<br class="ltx_break"/>The input of the tool should be a one-line SQL SELECT command converted from hard conditions. Here are some rules:  1. {item} titles can not be used as conditions in SQL;
2. the tool can not find similar {item}s;
3. always use pattern match logic for columns with string type;
4. only one {item} information table is allowed to appear in SQL command;
5. select all {item}s that meet the conditions, do not use the LIMIT keyword;
6. try to use OR instead of AND.
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F4.1.2"><span class="ltx_text ltx_font_bold" id="A2.F4.1.2.1">Tool Name</span>: ItemCF Retrieval Tool 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A2.F4.1.2.2">Tool Description</span>:
The tool is a soft condition filtering tool.
The tool can find similar {item}s for specific seed {item}s.
Never use this tool if human doesn’t express to find some {item}s similar with seed {item}s.
There is a similarity score threshold in the tool, only {item}s with similarity above the threshold would be kept.
Besides, the tool could be used to calculate the similarity scores with seed {item}s for {item}s in candidate buffer for ranking tool to refine. 
<br class="ltx_break"/>The input of the tool should be a list of seed {item} titles/names, which should be a Python list of strings.
Do not fake any {item} names.</span>
</span>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure C4: </span>Description of retrieval tools.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F5">
<span class="ltx_inline-block ltx_align_center ltx_framed_rectangle" id="A2.F5.1" style="border-color: black;">
<span class="ltx_p" id="A2.F5.1.1"><span class="ltx_text ltx_font_bold" id="A2.F5.1.1.1">Tool Name</span>: Ranking Tool 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A2.F5.1.1.2">Tool Description</span>:
The tool is useful to refine {item}s order or remove unwanted {item}s (when human tells the {item}s he does’t want) in conversation. 
<br class="ltx_break"/>The input of the tool should be a json string, which may consist of three keys: “schema”, “prefer” and “unwanted”. 
<br class="ltx_break"/>“schema” represents ranking schema, optional choices: “popularity”, “similarity” and ”preference”, indicating rank by {item} popularity, rank by similarity, rank by human preference (”prefer” {item}s).
The ”schema” depends on previous tool using and human preference. If ”prefer” info here not empty, ”preference” schema should be used. If similarity filtering tool is used before, prioritize using ”similarity” except human want popular {item}s. 
<br class="ltx_break"/>”prefer” represents {item} names that human likes or human history ({item}s human has interacted with), which should be an array of {item} titles. Keywords: ”used to do”, ”I like”, ”prefer”. 
<br class="ltx_break"/>”unwanted” represents {item} names that human doesn’t like or doesn’t want to see in next conversations, which should be an array of {item} titles. Keywords: ”don’t like”, ”boring”, ”interested in”. 
<br class="ltx_break"/>”prefer” and ”unwanted” {item}s should be extracted from human request and previous conversations. Only {item} names are allowed to appear in the input.
The human’s feedback for you recommendation in conversation history could be regard as ”prefer” or ”unwanted”, like ”I have tried those items you recommend” or ”I don’t like those”.
Only when at least one of ”prefer” and ”unwanted” is not empty, the tool could be used. If no ”prefer” info, {item}s would be ranked based on the popularity.
Do not fake {item}s.</span>
</span>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure C5: </span>Description of ranking tool.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F6">
<span class="ltx_inline-block ltx_align_center ltx_framed_rectangle" id="A2.F6.1" style="border-color: black;">
<span class="ltx_p" id="A2.F6.1.1">You are an expert in {item}. There is a conversational recommendation agent. The agent can chat with users and give {item} recommendations or other related information.
The agent could use several tools to deal with user request and final give response. Here are the description of those tools: {tool_description}</span>
<span class="ltx_p" id="A2.F6.1.2">You can see the conversation history between the agent and user, the current user request, the response of the agent and the tool using track for processing the request.
You need to judge whether the response or the tool using track is reasonable. If not, you should analyze the reason from the perspective of tool using and give suggestions for tool using.</span>
<span class="ltx_p" id="A2.F6.1.3">When giving judgement, you should consider several points below:
<br class="ltx_break"/>1. Whether the input of each tool is suitable? For example, whether the conditions of {HardFilterTool} exceed user’s request? Whether the seed items in {SoftFilterTool} is correct? Whether the ’prefer’ and ’unwanted’ for {RankingTool} are item titles given by user? Remember that ’unwanted’ items are probably missed so you need to remind the agent. 
<br class="ltx_break"/>2. Are some tools missed? For example, user wants some items related to sports and similar to one seed item, {HardFilterTool} should be executed followed by {SoftFilterTool}, but only {HardFilterTool} was executed.
<br class="ltx_break"/>3. Are some unnecessary tools used? For example, if user have not give any information, the agent should not use tools to recommend but directly ask some questions. 
<br class="ltx_break"/>4. Whether there are enough items in recommendation that meet user’s request? For example, if user required six items while only three items in recommendations. You should double check the conditions input to tools. 
<br class="ltx_break"/>5. Is the input of each tool consistent with the user’s intention? Are there any redundant or missing conditions?</span>
<span class="ltx_p" id="A2.F6.1.4">Note: if there is no candidate filtered with SQL command, the reason may be the conditions are too strict, you could tell the agent to relax the conditions.
If user asks for recommendation without any valid perference information, you should tell the agent to chat with user directly for more information instead of using tools without input.</span>
<span class="ltx_p" id="A2.F6.1.5">Here is the conversation history between agent and user:
{chat_history}</span>
<span class="ltx_p" id="A2.F6.1.6">The current user request is: {request}</span>
<span class="ltx_p" id="A2.F6.1.7">The tool using track to process the request is: {plan}</span>
<span class="ltx_p" id="A2.F6.1.8">The response of the agent is: {answer}
</span>
<span class="ltx_p" id="A2.F6.1.9">If the response and tool using track are reasonable, you should say ”Yes”.
Otherwise, you should tell the agent: ”No. The response/tool using is not good because …. . You should …”. 
<br class="ltx_break"/>You MUST NOT give any recommendations in your response.
Now, please give your judgement.</span>
</span>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure C6: </span>Prompt for critic in reflection.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F7">
<span class="ltx_inline-block ltx_align_center ltx_framed_rectangle" id="A2.F7.1" style="border-color: black;">
<span class="ltx_p" id="A2.F7.1.1">You are a helpful assistant and good planner.
Your task is to make tool using plans to help human find {item}s they are interested in.
Human requests typically fall under chit-chat, {item} info, or {item} recommendations. There are some tools to use to deal with human request. For chit-chat, respond with your knowledge. For {item} info, use the {LookUpTool}. 
<br class="ltx_break"/>For special chit-chat, like {item} recommendation reasons, use the {LookUpTool} and your knowledge. 
<br class="ltx_break"/>For {item} recommendations without information about human preference, chat with human for more information. 
<br class="ltx_break"/>For {item} recommendations with information for tools, use various tools together. 
<br class="ltx_break"/>To effectively utilize recommendation tools, comprehend human expressions involving profile and intention. 
<br class="ltx_break"/>Profile encompasses a person’s preferences, interests, and behaviors, including gaming history and likes/dislikes. 
<br class="ltx_break"/>Intention represents a person’s immediate goal or objective in the single-turn system interaction, containing specific, context-based query conditions. 
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F7.1.2">Human intentions consist of hard and soft conditions.
Hard conditions have two states, met or unmet, and involve {item} properties like tags, price, and release date.
Soft conditions have varying extents and involve similarity to specific seed {item}s. Separate hard and soft conditions in requests.</span>
<span class="ltx_p" id="A2.F7.1.3">Here are the tools could be used: {tools_desc}</span>
<span class="ltx_p" id="A2.F7.1.4">All SQL commands are used to search in the {item} information table (a sqlite3 table).
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F7.1.5">If human is looking up information of {item}s, such as the description of {item}s, number of {item}s, price of {item}s and so on, use the {LookUpTool}.</span>
<span class="ltx_p" id="A2.F7.1.6">For {item} recommendations, use tools with a shared candidate {item} buffer. Buffer is initialized with all {item}s. Filtering tools fetch candidates from the buffer and update it. 
<br class="ltx_break"/>Ranking tools rank {item}s in the buffer, and mapping tool maps {item} IDs to titles. 
<br class="ltx_break"/>If candidate {item}s are given by humans, use {BufferStoreTool} to add them to the buffer at the beginning.</span>
<span class="ltx_p" id="A2.F7.1.7">Think about whether to use tool first. If yes, make tool using plan. 
<br class="ltx_break"/>Only those tool names are optional when making plans: {tool_names}</span>
<span class="ltx_p" id="A2.F7.1.8">Assume that you play a role of tool using planner, I would give you a user request, and you should help me to make the tool using plan.
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F7.1.9">Here are some examples of human request and corresponding tool using plan:
{examples}</span>
<span class="ltx_p" id="A2.F7.1.10">Now, Please make the tool using plan of below requests.</span>
<span class="ltx_p" id="A2.F7.1.11">Request: {request}
Plan:</span>
</span>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure C7: </span>Prompt for plan generation with given user intent.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F8">
<span class="ltx_inline-block ltx_align_center ltx_framed_rectangle" id="A2.F8.1" style="border-color: black;">
<span class="ltx_p" id="A2.F8.1.1">You are a helpful assistant. Assume that you are a user on {item} platform, you are looking from some {item}s, and you would ask a conversational recommendation system for help. You would give the request. 
<br class="ltx_break"/>I would give you some examples, please generate some new reasonable and high-quality request sentences.
<br class="ltx_break"/>Here are some examples of user request:
requests 
<br class="ltx_break"/>Never use specific {item} names or {item} types. Instead, use placeholders. For example, {ITEM} for names, TYPE for types, PRICE for price, DATE for date.
The focus is on generating sentence patterns for questions. 
<br class="ltx_break"/>Now, it’s your turn. Please generate {number} new request sentences.</span>
</span>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure C8: </span>Prompt for input-first user intent generation.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F9">
<span class="ltx_inline-block ltx_align_center ltx_framed_rectangle" id="A2.F9.1" style="border-color: black;">
<span class="ltx_p" id="A2.F9.1.2">You are a helpful assistant who is good at imitating human to ask for recommendations.
Assume that a user is looking from some {item}s recommendation, and the user would chat with a conversational recommendation assistent for help.
And user’s historical {items}s are: {history}</span>
<span class="ltx_p" id="A2.F9.1.3">Information about target {item} that the user are looking for: {target_info}</span>
<span class="ltx_p" id="A2.F9.1.1">Please generate a conversation between the user and the recommendation assistent. Here are some rules:
<br class="ltx_break"/>1. Do not mention {item}s not in history. 
<br class="ltx_break"/>2. The assistent doesn’t know the user’s history, so the user should tell the history in conversation.
<br class="ltx_break"/>3. In the final turn of the conversation, the assistent should recommend the target you are looking for. Use ’<math alttext="\langle\text{item}\rangle" class="ltx_Math" display="inline" id="A2.F9.1.1.m1.1"><semantics id="A2.F9.1.1.m1.1a"><mrow id="A2.F9.1.1.m1.1.2.2" xref="A2.F9.1.1.m1.1.2.1.cmml"><mo id="A2.F9.1.1.m1.1.2.2.1" stretchy="false" xref="A2.F9.1.1.m1.1.2.1.1.cmml">⟨</mo><mtext id="A2.F9.1.1.m1.1.1" xref="A2.F9.1.1.m1.1.1a.cmml">item</mtext><mo id="A2.F9.1.1.m1.1.2.2.2" stretchy="false" xref="A2.F9.1.1.m1.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.F9.1.1.m1.1b"><apply id="A2.F9.1.1.m1.1.2.1.cmml" xref="A2.F9.1.1.m1.1.2.2"><csymbol cd="latexml" id="A2.F9.1.1.m1.1.2.1.1.cmml" xref="A2.F9.1.1.m1.1.2.2.1">delimited-⟨⟩</csymbol><ci id="A2.F9.1.1.m1.1.1a.cmml" xref="A2.F9.1.1.m1.1.1"><mtext id="A2.F9.1.1.m1.1.1.cmml" xref="A2.F9.1.1.m1.1.1">item</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.F9.1.1.m1.1c">\langle\text{item}\rangle</annotation><annotation encoding="application/x-llamapun" id="A2.F9.1.1.m1.1d">⟨ item ⟩</annotation></semantics></math>’ as placeholder to represent the target.
<br class="ltx_break"/>4. Above information is all user know about the target item.
<br class="ltx_break"/>5. Do not give too much information in one message. 
<br class="ltx_break"/>6. Keep user message short.
<br class="ltx_break"/>7. Each conversation should consist of 2-5 rounds.
<br class="ltx_break"/>8. Only the user has the information about target item in his mind. The assistent could only guess from user’s messages.
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F9.1.4">Use the following format:</span>
<span class="ltx_p" id="A2.F9.1.5">[{”role”: ”User”, ”text”: ”xxxxx”}, {”role”: ”Assistent”, ”text”: ”xxxxx”}, …]</span>
<span class="ltx_p" id="A2.F9.1.6">Each item in the list is a message. And if the message mentions {item} names, add an extra key to the message dict, like: 
<br class="ltx_break"/>”role”: ”User”, ”text”: ”xxx”, ”mentioned_items”: [ITEM1, ITEM2]
<br class="ltx_break"/></span>
</span>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure C9: </span>Prompt for one-turn conversation generation for retrieval task.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F10">
<span class="ltx_inline-block ltx_align_center ltx_framed_rectangle" id="A2.F10.1" style="border-color: black;">
<span class="ltx_p" id="A2.F10.1.1">You are a helpful assistant who is good at imitating human to ask for recommendations. 
<br class="ltx_break"/>Assume that a user is looking from some {item}s recommendation, and the user would chat with a conversational recommendation assistent for help.
And user’s historical {items}s are: {history} 
<br class="ltx_break"/>The user would give {n} candidates items as below and ask the assistent to rank those candidates: {candidates} 
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F10.1.2">Please imitate the user to generate a question to the assistent. Here are some rules: 
<br class="ltx_break"/>1. Do not mention {item}s not in history. 
<br class="ltx_break"/>2. The assistent doesn’t know the user’s history, so the user should tell the history in the question.
<br class="ltx_break"/>3. Give all {n} candidates in the question.
<br class="ltx_break"/>4. Keep the question short.
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F10.1.3">For example, the user mask ask like this format:
<br class="ltx_break"/>”I enjoyed xxx in the past, now I want some new {item}s. I have some candidates in my mind: xxx. Could you please rank them based on my perference?” 
<br class="ltx_break"/>Now, please generate the question.</span>
</span>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure C10: </span>Prompt for one-turn conversation generation for ranking task.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F11">
<span class="ltx_inline-block ltx_align_center ltx_framed_rectangle" id="A2.F11.1" style="border-color: black;">
<span class="ltx_p" id="A2.F11.1.1">You are a helpful assistant and good planner.
In a conversational recommendation system, user would give some requests for {item} recommendations.
Human requests typically fall under chit-chat, {item} info, or {item} recommendations. There are some tools to use to deal with human request. For chit-chat, respond with your knowledge. For {item} info, use the {LookUpTool}. 
<br class="ltx_break"/>For special chit-chat, like {item} recommendation reasons, use the {LookUpTool} and your knowledge. 
<br class="ltx_break"/>For {item} recommendations without information about human preference, chat with human for more information. 
<br class="ltx_break"/>For {item} recommendations with information for tools, use various tools together. 
<br class="ltx_break"/>To effectively utilize recommendation tools, comprehend human expressions involving profile and intention. 
<br class="ltx_break"/>Profile encompasses a person’s preferences, interests, and behaviors, including gaming history and likes/dislikes. 
<br class="ltx_break"/>Intention represents a person’s immediate goal or objective in the single-turn system interaction, containing specific, context-based query conditions. 
<br class="ltx_break"/>Human intentions consist of hard and soft conditions.
Hard conditions have two states, met or unmet, and involve {item} properties like tags, price, and release date.
Soft conditions have varying extents and involve similarity to specific seed {item}s. Separate hard and soft conditions in requests.</span>
<span class="ltx_p" id="A2.F11.1.2">Here are the tools could be used: {tools_desc}</span>
<span class="ltx_p" id="A2.F11.1.3">All SQL commands are used to search in the {item} information table (a sqlite3 table).</span>
<span class="ltx_p" id="A2.F11.1.4">If human is looking up information of {item}s, such as the description of {item}s, number of {item}s, price of {item}s and so on, use the {LookUpTool}. 
<br class="ltx_break"/>For {item} recommendations, use tools with a shared candidate {item} buffer. Buffer is initialized with all {item}s. Filtering tools fetch candidates from the buffer and update it. 
<br class="ltx_break"/>Ranking tools rank {item}s in the buffer, and mapping tool maps {item} IDs to titles. 
<br class="ltx_break"/>If candidate {item}s are given by humans, use {BufferStoreTool} to add them to the buffer at the beginning.</span>
<span class="ltx_p" id="A2.F11.1.5">Only those tool names are optional when making plans: {tool_names}</span>
<span class="ltx_p" id="A2.F11.1.6">Your task is to generate user request with a given plan.
Never use specific {item} names or {item} types. Instead, use placeholders. For example, {ITEM} for names, TYPE for types, PRICE for price, DATE for date.
The focus is on generating sentence patterns for questions. 
<br class="ltx_break"/></span>
<span class="ltx_p" id="A2.F11.1.7">Here are some examples of human request and corresponding tool using plan:
{examples}</span>
<span class="ltx_p" id="A2.F11.1.8">Now, Please generate {number} new request sentences.</span>
<span class="ltx_p" id="A2.F11.1.9">Plan: {plan}</span>
<span class="ltx_p" id="A2.F11.1.10">Request 1: xxxx 
<br class="ltx_break"/>… 
<br class="ltx_break"/>Request {number}: xxxx</span>
</span>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure C11: </span>Prompt for output-first user intent generation.</figcaption>
</figure>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jan 30 03:17:23 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
