<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Tool Learning with Foundation Models</title>
<!--Generated on Tue Aug  6 15:16:14 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2304.08354v3/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S1" title="In Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2" title="In Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2.SS1" title="In 2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Cognitive Origins of Tool Use</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2.SS2" title="In 2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Tool Categorization: A User-Interface Perspective</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2.SS3" title="In 2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Paradigm Shift of Foundation Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2.SS4" title="In 2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Complementary Roles of Tools and Foundation Models</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2.SS4.SSS0.Px1" title="In 2.4 Complementary Roles of Tools and Foundation Models ‣ 2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Benefits of Tools.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2.SS4.SSS0.Px2" title="In 2.4 Complementary Roles of Tools and Foundation Models ‣ 2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Benefits of Foundation Models.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3" title="In Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Tool Learning</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS1" title="In 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Components of Tool Learning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS1.SSS1" title="In 3.1 Components of Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Understanding the Components</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS1.SSS1.Px1" title="In 3.1.1 Understanding the Components ‣ 3.1 Components of Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Tool Set.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS1.SSS1.Px2" title="In 3.1.1 Understanding the Components ‣ 3.1 Components of Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Environment.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS1.SSS1.Px3" title="In 3.1.1 Understanding the Components ‣ 3.1 Components of Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Controller.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS1.SSS1.Px4" title="In 3.1.1 Understanding the Components ‣ 3.1 Components of Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Perceiver.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS1.SSS2" title="In 3.1 Components of Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Connecting the Components</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2" title="In 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>The General Procedure: From Intent to Plan</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2.SSS1" title="In 3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Understanding Intent and Tools</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2.SSS1.Px1" title="In 3.2.1 Understanding Intent and Tools ‣ 3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Intent Understanding.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2.SSS1.Px2" title="In 3.2.1 Understanding Intent and Tools ‣ 3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Tool Understanding.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2.SSS2" title="In 3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Planning with Reasoning</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2.SSS2.Px1" title="In 3.2.2 Planning with Reasoning ‣ 3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Eliciting Reasoning in Foundation Models.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2.SSS2.Px2" title="In 3.2.2 Planning with Reasoning ‣ 3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Introspective Reasoning.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2.SSS2.Px3" title="In 3.2.2 Planning with Reasoning ‣ 3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Extrospective Reasoning.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2.SSS2.Px4" title="In 3.2.2 Planning with Reasoning ‣ 3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Challenges in Multi-Step Multi-Tool Scenario.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3" title="In 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Training Models for Improved Tool Learning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3.SSS1" title="In 3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Learning from Demonstrations</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3.SSS1.Px1" title="In 3.3.1 Learning from Demonstrations ‣ 3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Supervised Learning.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3.SSS1.Px2" title="In 3.3.1 Learning from Demonstrations ‣ 3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Semi-supervised Learning.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3.SSS1.Px3" title="In 3.3.1 Learning from Demonstrations ‣ 3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Self-supervised Learning.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3.SSS2" title="In 3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Learning from Feedback</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3.SSS2.Px1" title="In 3.3.2 Learning from Feedback ‣ 3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Reinforcement Learning (RL) for Tool Learning.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3.SSS2.Px2" title="In 3.3.2 Learning from Feedback ‣ 3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Environment Feedback.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3.SSS2.Px3" title="In 3.3.2 Learning from Feedback ‣ 3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Human Feedback.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3.SSS3" title="In 3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.3 </span>Generalizable Tool Learning</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3.SSS3.Px1" title="In 3.3.3 Generalizable Tool Learning ‣ 3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Foundation of Generalization: Interface Unification.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3.SSS3.Px2" title="In 3.3.3 Generalizable Tool Learning ‣ 3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Strategies of Generalizable Tool Learning.</span></a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S4" title="In Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Application and Experiment</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S4.SS1" title="In 4 Application and Experiment ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Evaluated Tools</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S4.SS2" title="In 4 Application and Experiment ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S4.SS2.SSS0.Px1" title="In 4.2 Experiments ‣ 4 Application and Experiment ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Settings.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S4.SS2.SSS0.Px2" title="In 4.2 Experiments ‣ 4 Application and Experiment ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Results.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5" title="In Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS1" title="In 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Safe and Trustworthy Tool Learning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS1.SSS0.Px1" title="In 5.1 Safe and Trustworthy Tool Learning ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Adversaries.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS1.SSS0.Px2" title="In 5.1 Safe and Trustworthy Tool Learning ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Governance.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS1.SSS0.Px3" title="In 5.1 Safe and Trustworthy Tool Learning ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Trustworthiness.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS2" title="In 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Tool Learning for Large Complex Systems</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS2.SSS0.Px1" title="In 5.2 Tool Learning for Large Complex Systems ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">System Learning.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS2.SSS0.Px2" title="In 5.2 Tool Learning for Large Complex Systems ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Efficiency Requirements.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS2.SSS0.Px3" title="In 5.2 Tool Learning for Large Complex Systems ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Privacy Concerns.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS3" title="In 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>From Tool User to Tool Maker: AI’s Evolutionary Role</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS3.SSS0.Px1" title="In 5.3 From Tool User to Tool Maker: AI’s Evolutionary Role ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Tools for AI.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS3.SSS0.Px2" title="In 5.3 From Tool User to Tool Maker: AI’s Evolutionary Role ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Tools by AI.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS3.SSS0.Px3" title="In 5.3 From Tool User to Tool Maker: AI’s Evolutionary Role ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Creativity of AI.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS4" title="In 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>From General Intelligence to Personalized Intelligence</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS4.SSS0.Px1" title="In 5.4 From General Intelligence to Personalized Intelligence ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Aligning User Preference with Tool Manipulation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS4.SSS0.Px2" title="In 5.4 From General Intelligence to Personalized Intelligence ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">From Reactive Systems to Proactive Systems.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS4.SSS0.Px3" title="In 5.4 From General Intelligence to Personalized Intelligence ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Privacy Preserving Technologies.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS5" title="In 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Tool Learning and Embodied Learning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS5.SSS0.Px1" title="In 5.5 Tool Learning and Embodied Learning ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Tool Learning Enables Digital Embodiment.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS5.SSS0.Px2" title="In 5.5 Tool Learning and Embodied Learning ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Learning to Use Embodied Tools.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS6" title="In 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6 </span>Knowledge Conflicts in Tool Augmentation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS6.SSS0.Px1" title="In 5.6 Knowledge Conflicts in Tool Augmentation ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Knowledge Conflicts.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS6.SSS0.Px2" title="In 5.6 Knowledge Conflicts in Tool Augmentation ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Potential Solutions for Knowledge Conflicts.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS7" title="In 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.7 </span>Open Problems</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS7.SSS0.Px1" title="In 5.7 Open Problems ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Striking a Balance between Internalized Capabilities and External Tools.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS7.SSS0.Px2" title="In 5.7 Open Problems ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Tool Use as a Gauge for Machine Intelligence.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS7.SSS0.Px3" title="In 5.7 Open Problems ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Ethical Human-Model Collaboration in Tool Use.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS7.SSS0.Px4" title="In 5.7 Open Problems ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Safety Issues of Foundation Models Accessing Physical Tools.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS7.SSS0.Px5" title="In 5.7 Open Problems ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Tool Learning for Scientific Discovery.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S6" title="In Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1" title="In Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Case Study</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS1" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>3D Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS2" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Stock</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS3" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Making Slides</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS4" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Movie Hunter</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS5" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5 </span>Search Engine</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS6" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.6 </span>Chemicals Mining</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS7" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.7 </span>Cooking Assistant</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS8" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.8 </span>AI Painting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS9" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.9 </span>Navigating Knowledge Graphs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS10" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.10 </span>ALFWorld</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS11" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.11 </span>Calculator</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS12" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.12 </span>Weather</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS13" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.13 </span>Online Shopping</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS14" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.14 </span>Map</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS15" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.15 </span>Processing Tables</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS16" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.16 </span>Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS17" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.17 </span>Wikipedia</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS18" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.18 </span>Database</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
Tool Learning with Foundation Models
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yujia Qin<sup class="ltx_sup" id="id53.52.id1">1</sup>, Shengding Hu<sup class="ltx_sup" id="id54.53.id2">1</sup>, Yankai Lin<sup class="ltx_sup" id="id55.54.id3"><span class="ltx_text ltx_font_italic" id="id55.54.id3.1">2</span></sup>  , Weize Chen<sup class="ltx_sup" id="id56.55.id4">1</sup>, Ning Ding<sup class="ltx_sup" id="id57.56.id5">1</sup>, Ganqu Cui<sup class="ltx_sup" id="id58.57.id6">1</sup>, 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id12.12.6">Zheni Zeng<sup class="ltx_sup" id="id12.12.6.1"><span class="ltx_text ltx_font_medium" id="id12.12.6.1.1">1</span></sup>, Xuanhe Zhou<sup class="ltx_sup" id="id12.12.6.2"><span class="ltx_text ltx_font_medium" id="id12.12.6.2.1">1</span></sup>, Yufei Huang<sup class="ltx_sup" id="id12.12.6.3"><span class="ltx_text ltx_font_medium" id="id12.12.6.3.1">1</span></sup>, Chaojun Xiao<sup class="ltx_sup" id="id12.12.6.4"><span class="ltx_text ltx_font_medium" id="id12.12.6.4.1">1</span></sup>, Chi Han<sup class="ltx_sup" id="id12.12.6.5"><span class="ltx_text ltx_font_medium" id="id12.12.6.5.1">3</span></sup>, Yi Ren Fung<sup class="ltx_sup" id="id12.12.6.6"><span class="ltx_text ltx_font_medium" id="id12.12.6.6.1">3</span></sup>,</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id18.18.12">Yusheng Su<sup class="ltx_sup" id="id18.18.12.1"><span class="ltx_text ltx_font_medium" id="id18.18.12.1.1">1</span></sup>, Huadong Wang<sup class="ltx_sup" id="id18.18.12.2"><span class="ltx_text ltx_font_medium" id="id18.18.12.2.1">1</span></sup>, Cheng Qian<sup class="ltx_sup" id="id18.18.12.3"><span class="ltx_text ltx_font_medium" id="id18.18.12.3.1">1</span></sup>, Runchu Tian<sup class="ltx_sup" id="id18.18.12.4"><span class="ltx_text ltx_font_medium" id="id18.18.12.4.1">1</span></sup>, Kunlun Zhu<sup class="ltx_sup" id="id18.18.12.5"><span class="ltx_text ltx_font_medium" id="id18.18.12.5.1">8</span></sup>, Shihao Liang<sup class="ltx_sup" id="id18.18.12.6"><span class="ltx_text ltx_font_medium" id="id18.18.12.6.1">8</span></sup>, </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id25.25.19">Xingyu Shen<sup class="ltx_sup" id="id25.25.19.1"><span class="ltx_text ltx_font_medium" id="id25.25.19.1.1">1</span></sup>, Bokai Xu<sup class="ltx_sup" id="id25.25.19.2"><span class="ltx_text ltx_font_medium" id="id25.25.19.2.1">1</span></sup>, Zhen Zhang<sup class="ltx_sup" id="id25.25.19.3"><span class="ltx_text ltx_font_medium" id="id25.25.19.3.1">1</span></sup>, Yining Ye<sup class="ltx_sup" id="id25.25.19.4"><span class="ltx_text ltx_font_medium" id="id25.25.19.4.1">1</span></sup>, Bowen Li<sup class="ltx_sup" id="id25.25.19.5"><span class="ltx_text ltx_font_medium" id="id25.25.19.5.1">1</span></sup>, Ziwei Tang<sup class="ltx_sup" id="id25.25.19.6"><span class="ltx_text ltx_font_medium" id="id25.25.19.6.1">5</span></sup>, Jing Yi<sup class="ltx_sup" id="id25.25.19.7"><span class="ltx_text ltx_font_medium" id="id25.25.19.7.1">1</span></sup>, </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id31.31.25">Yuzhang Zhu<sup class="ltx_sup" id="id31.31.25.1"><span class="ltx_text ltx_font_medium" id="id31.31.25.1.1">1</span></sup>, Zhenning Dai<sup class="ltx_sup" id="id31.31.25.2"><span class="ltx_text ltx_font_medium" id="id31.31.25.2.1">1</span></sup>, Lan Yan<sup class="ltx_sup" id="id31.31.25.3"><span class="ltx_text ltx_font_medium" id="id31.31.25.3.1">1</span></sup>, Xin Cong<sup class="ltx_sup" id="id31.31.25.4"><span class="ltx_text ltx_font_medium" id="id31.31.25.4.1">1</span></sup>, Yaxi Lu<sup class="ltx_sup" id="id31.31.25.5"><span class="ltx_text ltx_font_medium" id="id31.31.25.5.1">1</span></sup>, Weilin Zhao<sup class="ltx_sup" id="id31.31.25.6"><span class="ltx_text ltx_font_medium" id="id31.31.25.6.1">1</span></sup>, </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id37.37.31">Yuxiang Huang<sup class="ltx_sup" id="id37.37.31.1"><span class="ltx_text ltx_font_medium" id="id37.37.31.1.1">1</span></sup>, Junxi Yan<sup class="ltx_sup" id="id37.37.31.2"><span class="ltx_text ltx_font_medium" id="id37.37.31.2.1">1</span></sup>, Xu Han<sup class="ltx_sup" id="id37.37.31.3"><span class="ltx_text ltx_font_medium" id="id37.37.31.3.1">1</span></sup>, Xian Sun<sup class="ltx_sup" id="id37.37.31.4"><span class="ltx_text ltx_font_medium" id="id37.37.31.4.1">7</span></sup>, Dahai Li<sup class="ltx_sup" id="id37.37.31.5"><span class="ltx_text ltx_font_medium" id="id37.37.31.5.1">7</span></sup>, Jason Phang<sup class="ltx_sup" id="id37.37.31.6"><span class="ltx_text ltx_font_medium" id="id37.37.31.6.1">4</span></sup>, </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id43.43.37">Cheng Yang<sup class="ltx_sup" id="id43.43.37.1"><span class="ltx_text ltx_font_medium" id="id43.43.37.1.1">5</span></sup>, Tongshuang Wu<sup class="ltx_sup" id="id43.43.37.2"><span class="ltx_text ltx_font_medium" id="id43.43.37.2.1">6</span></sup>, Heng Ji<sup class="ltx_sup" id="id43.43.37.3"><span class="ltx_text ltx_font_medium" id="id43.43.37.3.1">3</span></sup>, Guoliang Li<sup class="ltx_sup" id="id43.43.37.4"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id43.43.37.4.1">1</span></sup>, Zhiyuan Liu<sup class="ltx_sup" id="id43.43.37.5"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id43.43.37.5.1">1∗</span></sup>, Maosong Sun<sup class="ltx_sup" id="id43.43.37.6"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id43.43.37.6.1">1∗</span></sup></span>
<br class="ltx_break"/><sup class="ltx_sup" id="id59.58.id7">1</sup>Tsinghua University, <sup class="ltx_sup" id="id60.59.id8">2</sup>Renmin University of China, <sup class="ltx_sup" id="id61.60.id9">3</sup>University of Illinois Urbana-Champaign,
<br class="ltx_break"/><sup class="ltx_sup" id="id62.61.id10">4</sup>New York University, <sup class="ltx_sup" id="id63.62.id11">5</sup>Beijing University of Posts and Telecommunications, 
<br class="ltx_break"/><sup class="ltx_sup" id="id64.63.id12">6</sup>Carnegie Mellon University, <sup class="ltx_sup" id="id65.64.id13">7</sup>Zhihu Inc., <sup class="ltx_sup" id="id66.65.id14">8</sup>ModelBest Inc.
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id67.66.id15">qyj20@mails.tsinghua.edu.cn
<br class="ltx_break"/></span>
</span><span class="ltx_author_notes"> Corresponding authors.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id52.1">Humans possess an extraordinary ability to create and utilize tools, allowing them to overcome physical limitations and explore new frontiers. With the advent of recent powerful foundation models, artificial intelligence systems have the potential to be equally adept in tool use as humans. This paradigm, which is dubbed as <span class="ltx_text ltx_font_italic" id="id52.1.1">tool learning with foundation models</span>, combines the strengths of specialized tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors in this field. To this end, we present a systematic investigation and comprehensive review of tool learning in this paper. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. We recapitulate existing tool learning research and formulate a general tool learning framework: starting from understanding the user instruction, models should learn to decompose a complex task into several subtasks, dynamically adjust their plan through reasoning, and effectively conquer each sub-task by selecting appropriate tools. We also discuss how to train models for improved tool-use capabilities and facilitate the generalization in tool learning.
Considering the lack of a systematic tool learning evaluation in prior works, we experiment with <math alttext="18" class="ltx_Math" display="inline" id="id52.1.m1.1"><semantics id="id52.1.m1.1a"><mn id="id52.1.m1.1.1" xref="id52.1.m1.1.1.cmml">18</mn><annotation-xml encoding="MathML-Content" id="id52.1.m1.1b"><cn id="id52.1.m1.1.1.cmml" type="integer" xref="id52.1.m1.1.1">18</cn></annotation-xml><annotation encoding="application/x-tex" id="id52.1.m1.1c">18</annotation><annotation encoding="application/x-llamapun" id="id52.1.m1.1d">18</annotation></semantics></math> representative tools and show the potential of current foundation models in skillfully utilizing tools.
Finally, we discuss several open problems that require further investigation for tool learning, such as ensuring safe and trustworthy tool use, enabling tool creation with foundation models, and addressing personalization challenges.
Overall, we hope this paper could inspire future research in integrating tools with foundation models. Relevant codes and datasets are publicly available for further research exploration<span class="ltx_note ltx_role_footnote" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/OpenBMB/BMTools" title="">https://github.com/OpenBMB/BMTools</a>
<br class="ltx_break"/> Author contributions are listed in <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#Sx1" title="Contributions ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title">Contributions</span></a>.</span></span></span>.</p>
</div>
<div class="ltx_para ltx_parbox ltx_align_bottom" id="p1" style="width:195.1pt;">
<blockquote class="ltx_quote ltx_epigraph" id="p1.1" style="width:216.81pt; margin-left:auto;;">
<div class="ltx_block ltx_epigraph_text" id="p1.1.1" style="text-align:left; ;">
<p class="ltx_p" id="p1.1.1.1"><span class="ltx_text ltx_font_italic" id="p1.1.1.1.1">“It is not only the violin that shapes the violinist, we are all shaped by the tools we train ourselves to use.”</span></p>
</div>
<div class="ltx_block ltx_epigraph_source" id="p1.1.2" style="border-top:solid 0pt; text-align:right; ;">
<p class="ltx_p" id="p1.1.2.1">— Edsger W. Dijkstra</p>
</div>
</blockquote>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<nav class="ltx_TOC ltx_list_toc ltx_toc_toc"><h6 class="ltx_title ltx_title_contents">Contents</h6>
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S1" title="In Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2" title="In Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2.SS1" title="In 2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Cognitive Origins of Tool Use</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2.SS2" title="In 2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Tool Categorization: A User-Interface Perspective</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2.SS3" title="In 2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Paradigm Shift of Foundation Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2.SS4" title="In 2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Complementary Roles of Tools and Foundation Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3" title="In Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Tool Learning</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS1" title="In 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Components of Tool Learning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS1.SSS1" title="In 3.1 Components of Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Understanding the Components</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS1.SSS2" title="In 3.1 Components of Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Connecting the Components</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2" title="In 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>The General Procedure: From Intent to Plan</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2.SSS1" title="In 3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Understanding Intent and Tools</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2.SSS2" title="In 3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Planning with Reasoning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3" title="In 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Training Models for Improved Tool Learning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3.SSS1" title="In 3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Learning from Demonstrations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3.SSS2" title="In 3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Learning from Feedback</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3.SSS3" title="In 3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.3 </span>Generalizable Tool Learning</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S4" title="In Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Application and Experiment</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S4.SS1" title="In 4 Application and Experiment ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Evaluated Tools</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S4.SS2" title="In 4 Application and Experiment ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Experiments</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5" title="In Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS1" title="In 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Safe and Trustworthy Tool Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS2" title="In 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Tool Learning for Large Complex Systems</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS3" title="In 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>From Tool User to Tool Maker: AI’s Evolutionary Role</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS4" title="In 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>From General Intelligence to Personalized Intelligence</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS5" title="In 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Tool Learning and Embodied Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS6" title="In 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6 </span>Knowledge Conflicts in Tool Augmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS7" title="In 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.7 </span>Open Problems</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S6" title="In Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1" title="In Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Case Study</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS1" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>3D Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS2" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Stock</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS3" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Making Slides</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS4" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Movie Hunter</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS5" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5 </span>Search Engine</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS6" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.6 </span>Chemicals Mining</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS7" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.7 </span>Cooking Assistant</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS8" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.8 </span>AI Painting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS9" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.9 </span>Navigating Knowledge Graphs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS10" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.10 </span>ALFWorld</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS11" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.11 </span>Calculator</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS12" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.12 </span>Weather</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS13" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.13 </span>Online Shopping</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS14" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.14 </span>Map</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS15" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.15 </span>Processing Tables</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS16" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.16 </span>Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS17" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.17 </span>Wikipedia</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS18" title="In Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.18 </span>Database</span></a></li>
</ol>
</li>
</ol></nav>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Tools are extensions of human capabilities designed to enhance productivity, efficiency, and problem-solving in human activities. Since the dawn of civilization, tools have been integral to the very essence of our existence <cite class="ltx_cite ltx_citemacro_citep">(Washburn, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib178" title="">1960</a>)</cite>. Tool creation and utilization are motivated by a deep-rooted desire to overcome our physical limitations and discover new territories.
More specifically, with advancements in tools, we can accomplish increasingly complex tasks with ease and efficiency, liberating time and resources to pursue more ambitious ventures.
As such, tools have served as the crucial foundation upon which our cultural and social practices are built, transforming our modes of learning, communication, working, and entertainment, infusing these domains with new dimensions of accessibility and interactivity <cite class="ltx_cite ltx_citemacro_citep">(Gibson et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib45" title="">1993</a>)</cite>.
Throughout history, it is undeniable that human beings have played a pivotal role in the invention and manipulation of tools, which is a striking manifestation of intelligence <cite class="ltx_cite ltx_citemacro_citep">(Shumaker et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib146" title="">2011</a>)</cite>.
Given the rise of Artificial Intelligence (AI), one natural question is, does AI possess the potential to be equally adept and capable as its creators?</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The prerequisite of the manipulation of tools is a thorough comprehension of the tools’ functionalities, as well as the ability to understand user intents and perform planning and reasoning for tool use.
Before the advent of powerful foundation models <cite class="ltx_cite ltx_citemacro_citep">(Bommasani et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib17" title="">2021</a>)</cite>, conducting tool-oriented AI research was exceedingly challenging. While certain basic tools could be fitted using shallow statistical models or deep neural models <cite class="ltx_cite ltx_citemacro_citep">(Pomerleau, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib125" title="">1988</a>; Mnih et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib108" title="">2013</a>; Akkaya et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib3" title="">2019</a>)</cite>, their performance and stability remained inadequate to meet the demands of practical applications, let alone generalizing across various tools. This is due to the limitations of traditional supervised learning in capturing the complex operations essential for tool utilization and the insufficiency of trial-and-error approaches like reinforcement learning in mastering the extensive decision space associated with tool use.
In a nutshell, the fundamental limitations in tool use by earlier AI lie in the insufficient capabilities of the models.
Recently, the emergence of more capable foundation models, characterized by significantly improved capabilities, has rendered tool learning practicable.
They have shown enormous semantic understanding capacity in diverse tasks, spanning the fields of natural language processing (NLP) <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib19" title="">2020</a>)</cite>, computer vision (CV) <cite class="ltx_cite ltx_citemacro_citep">(Ramesh et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib130" title="">2022</a>)</cite>, biology <cite class="ltx_cite ltx_citemacro_citep">(Jumper et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib61" title="">2021</a>)</cite>, etc.
Additionally, they have demonstrated superior reasoning and decision-making abilities in complex interactive environments <cite class="ltx_cite ltx_citemacro_citep">(Nakano et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib110" title="">2021</a>)</cite>.
By harnessing the extensive world knowledge garnered during pre-training, they can perform grounded actions and interact with the real world.
Notably, the emergence of ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib113" title="">2022</a>)</cite> highlights the potential of foundation models to understand human intentions, automate intricate processes, and generate natural responses; the advent of GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib114" title="">2023</a>)</cite> offers immense potential for multi-modal perception, which is essential to the real-world grounding ability.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="407" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Tool learning paradigm aims to combine the strengths of specialized tools and foundation models.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Therefore, foundation models enable AI to harness tools, which can lead to more potent and streamlined solutions for real-world tasks.
Foundation models are able to decipher complex data, simulate human-like planning capabilities, and generate a broad spectrum of outputs.
Concurrently, specialized tools can be employed to refine and target specific goals. The amalgamation of tools and models unveils vast potential where sophisticated procedures can be automated with limited human involvement.
This paradigm, dubbed as <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">tool learning with foundation models</span> in this paper (Figure <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">1</span></a>), aims to combine the strengths of specialized tools and foundation models, thereby culminating in greater accuracy, efficiency, and autonomy in problem-solving.
Recent research has shed light on foundation models’ potential to exhibit a level of dexterity and finesse in tool use <cite class="ltx_cite ltx_citemacro_citep">(Lazaridou et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib76" title="">2022</a>; Nakano et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib110" title="">2021</a>; Cobbe et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib30" title="">2021</a>; Thoppilan et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib161" title="">2022</a>; Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib54" title="">2022b</a>; Ahn et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib2" title="">2022</a>; Yao et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib193" title="">2022a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib194" title="">b</a>; Schick et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib139" title="">2023</a>; Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib185" title="">2023</a>; Bubeck et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib20" title="">2023</a>)</cite>.
Despite these breakthroughs, the efforts mainly focus on applying foundation models to specific tasks and domains with delicate algorithm designs.
The current understanding of tool learning is still not comprehensive enough to estimate its characteristics and future developments.
We believe that it is crucial to examine and summarize the current progress of tool learning with foundation models to explore their potential and challenges and to better pave the way for future technological advancements.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we conduct a systematic investigation and comprehensive review of tool learning, attempting to build a full grasp of the key challenges, opportunities, and directions in this field.
Before delving into the tool learning framework, we introduce essential <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">backgrounds</span> (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2" title="2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">2</span></a>), covering both tools and foundation models and their interaction. Specifically, we first recapitulate the cognitive origins of tool use in human history and its potential implications for tool use in AI systems (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2.SS1" title="2.1 Cognitive Origins of Tool Use ‣ 2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">2.1</span></a>), followed by a categorization of tools from the perspective of the user interface (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2.SS2" title="2.2 Tool Categorization: A User-Interface Perspective ‣ 2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">2.2</span></a>). Then we review the AI paradigm shift brought by foundation models and highlight the emergence and significance of tool learning (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2.SS3" title="2.3 Paradigm Shift of Foundation Models ‣ 2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">2.3</span></a>). After that, we discuss the complementary roles of tools and foundation models, and argue that integrating both can bring various advantages, such as improving interpretability, enhancing robustness, and delivering better user experiences (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2.SS4" title="2.4 Complementary Roles of Tools and Foundation Models ‣ 2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">2.4</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We present a comprehensive literature review for existing exploration in tool learning. While previous works often focus on specific aspects in isolation, we strive to formulate a general tool learning <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">framework</span> (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS1" title="3.1 Components of Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>), which comprises the controller (typically modeled using a foundation model), tool set, environment, perceiver, and human. Based on the unified framework, we review existing works of tool learning, highlight core research problems, and introduce their existing solutions as well as future explorations. The whole procedure (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2" title="3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>) of tool learning starts with a user instruction, and models are required to make an executable plan for tool execution. To bridge user instructions with appropriate tools, models should first learn to understand the user intents underlying the instruction (i.e., <span class="ltx_text ltx_font_italic" id="S1.p5.1.2">intent understanding</span>) and understand the functionalities and usage of tools (i.e., <span class="ltx_text ltx_font_italic" id="S1.p5.1.3">tool understanding</span>). Models should also learn to decompose a complex task into several subtasks, dynamically adjust their plan through reasoning, and effectively conquer each sub-task with the appropriate tools. Regarding the training strategy (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3" title="3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>) to facilitate models for improved tool utilization, we conclude with two mainstream methods: learning from demonstrations and learning from feedback. We discuss how to construct effective training supervision under different settings. To facilitate transferring the learned tool-use skills to new tools and situations, i.e., generalizable tool learning, it is important to design a unified interface that enables the model to interact with different tools in a standardized manner.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Considering the lack of a systematic tool learning evaluation in prior works, we conduct <span class="ltx_text ltx_font_bold" id="S1.p6.1.1">experiments</span> (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S4" title="4 Application and Experiment ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">4</span></a>) on <math alttext="18" class="ltx_Math" display="inline" id="S1.p6.1.m1.1"><semantics id="S1.p6.1.m1.1a"><mn id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml">18</mn><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><cn id="S1.p6.1.m1.1.1.cmml" type="integer" xref="S1.p6.1.m1.1.1">18</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">18</annotation><annotation encoding="application/x-llamapun" id="S1.p6.1.m1.1d">18</annotation></semantics></math> representative tools based on our framework to investigate the efficacy and limitations of foundation models in tool manipulation. We demonstrate that state-of-the-art foundation models (e.g., ChatGPT) can effectively use tools to solve tasks with simple prompting. These results highlight the potential of using the foundation model as a general agent for tool learning.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Finally, we discuss the remaining important <span class="ltx_text ltx_font_bold" id="S1.p7.1.1">research topics</span> (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5" title="5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">5</span></a>) for applying our general framework to real-world scenarios, including (1) <span class="ltx_text ltx_font_bold" id="S1.p7.1.2">safety and trustworthiness</span>, where we emphasize the potential risks from adversaries, governance, and trustworthiness. We contend that careful considerations are necessary before deploying tool learning models in high-stakes scenarios (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS1" title="5.1 Safe and Trustworthy Tool Learning ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">5.1</span></a>); (2) <span class="ltx_text ltx_font_bold" id="S1.p7.1.3">tool learning for large complex systems</span>, where we showcase the unique characters of large complex systems and discuss the challenges in applying tool learning to these systems, such as complicated knowledge and function learning, representative data sampling with privacy concerns, and the strict requirements of efficient tool learning (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS2" title="5.2 Tool Learning for Large Complex Systems ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">5.2</span></a>); (3) <span class="ltx_text ltx_font_bold" id="S1.p7.1.4">tool creation</span>, where we discuss the possibility that AI can also create new tools, challenging the long-held beliefs about what makes humans unique (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS3" title="5.3 From Tool User to Tool Maker: AI’s Evolutionary Role ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">5.3</span></a>);
(4) <span class="ltx_text ltx_font_bold" id="S1.p7.1.5">personalized tool learning</span>, where models provide tailored assistance to users in tool use. We highlight the challenges of aligning user preference with tool manipulation and introduce the shift from reactive to proactive systems, and the privacy-preserving concerns (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS4" title="5.4 From General Intelligence to Personalized Intelligence ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">5.4</span></a>);
(5) <span class="ltx_text ltx_font_bold" id="S1.p7.1.6">embodied learning</span>, where the intersection of tool learning and embodied agent enables digital embodiment and manipulation of embodied tools (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS5" title="5.5 Tool Learning and Embodied Learning ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">5.5</span></a>); (6) <span class="ltx_text ltx_font_bold" id="S1.p7.1.7">knowledge conflicts in tool augmentation</span>, where we review how tools can be leveraged to enhance models’ generation and the practical problems of knowledge conflicts, which can lead to inaccurate and unreliable model predictions (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS6" title="5.6 Knowledge Conflicts in Tool Augmentation ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">5.6</span></a>); (7) other <span class="ltx_text ltx_font_bold" id="S1.p7.1.8">open problems</span>, such as viewing tool use capability as a measure for machine intelligence and tool learning for scientific discovery (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS7" title="5.7 Open Problems ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">5.7</span></a>). Overall, we hope this paper could inspire further research in integrating tools with foundation models and developing more intelligent and capable AI systems.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section, we first discuss the cognitive origins of human tool use (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2.SS1" title="2.1 Cognitive Origins of Tool Use ‣ 2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">2.1</span></a>), followed by a tool categorization through the lens of the user interface (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2.SS2" title="2.2 Tool Categorization: A User-Interface Perspective ‣ 2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">2.2</span></a>). Then we review the recent AI paradigm shift brought by foundation models (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2.SS3" title="2.3 Paradigm Shift of Foundation Models ‣ 2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">2.3</span></a>) and its significance in tool learning. After that, we examine the respective roles of specialized tools and foundation models in problem-solving, and discuss the benefits and challenges of their integration (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2.SS4" title="2.4 Complementary Roles of Tools and Foundation Models ‣ 2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">2.4</span></a>).</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Cognitive Origins of Tool Use</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Tools have played a critical role in the long history of thousands of years of human evolution. Tools are commonly viewed as extensions of human beings, just as ancient fighting equipment and agricultural machinery were. Tool use is defined as a unique characteristic of human beings that is distinguished from other species <cite class="ltx_cite ltx_citemacro_citep">(Von Eckardt, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib167" title="">1995</a>)</cite>. Throughout evolution, the ability to use tools has been essential for animals, particularly those with advanced intellectual development <cite class="ltx_cite ltx_citemacro_citep">(Shumaker et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib146" title="">2011</a>)</cite>.
For example, chimpanzees have been observed using stones or other materials to crack nuts <cite class="ltx_cite ltx_citemacro_citep">(Boesch et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib15" title="">2019</a>)</cite>, while New Caledonian crows can craft and utilize two distinct types of hook tools to aid in capturing prey <cite class="ltx_cite ltx_citemacro_citep">(Hunt, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib55" title="">1996</a>)</cite>. However, human tool behavior diverges from these observations in several ways. Humans can create much more complicated tools than other animals, such as converting our actions into fundamentally different mechanical actions in tools like hammers and pencils <cite class="ltx_cite ltx_citemacro_citep">(Frey, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib41" title="">2007</a>)</cite>. Additionally, we can harness natural forces such as wind turbines to create tools <cite class="ltx_cite ltx_citemacro_citep">(Shumaker et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib146" title="">2011</a>)</cite>. This ability may be attributed to our deep comprehension of cause-and-effect relations, which allows us to engage in technical reasoning <cite class="ltx_cite ltx_citemacro_citep">(Osiurak &amp; Reynaud, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib118" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p2.1.1">Neural Basis of Tool Use.</span>
To better understand human tool use behaviors, researchers analyze the neural basis of tool observation and execution. It is verified that humans have parietal systems involved in grasping objects and using tools, and the anterior supramarginal gyrus activation of observing tool use is typical of human subjects, of which macaques do not exhibit <cite class="ltx_cite ltx_citemacro_citep">(Orban &amp; Caruana, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib115" title="">2014</a>)</cite>. This neurocognitive bases of tool observation may be related to the origins of cumulative technological evolution (e.g., the improvement in the efficiency and complexity of human tools and techniques over generations <cite class="ltx_cite ltx_citemacro_citep">(Osiurak &amp; Reynaud, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib118" title="">2020</a>)</cite>) and some other human phenomena <cite class="ltx_cite ltx_citemacro_citep">(Reynaud et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib133" title="">2019</a>)</cite>. While for the tool execution, researchers hold different views on manipulation-based versus reasoning-based approaches <cite class="ltx_cite ltx_citemacro_citep">(Osiurak &amp; Badets, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib116" title="">2016</a>)</cite>. The former claims that tool use has to be supported by the simulation of sensorimotor experiences, and the latter demonstrates the importance of reasoning based on mechanical knowledge in tool use. Nevertheless, the overall trend in cognitive science is understanding cognition as an enactive process that emphasizes interaction with the external world <cite class="ltx_cite ltx_citemacro_citep">(Engel et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib39" title="">2013</a>)</cite>, and the feedback from observation, communication, and hands-on practice is important for mastering tool use.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p3.1.1">Three Intelligence Levels of Tool Use.</span>
Besides, there are specific frameworks designed to discuss the level of intelligence represented by human tool use. For instance, “intoolligence” <cite class="ltx_cite ltx_citemacro_citep">(Osiurak &amp; Heinke, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib117" title="">2018</a>)</cite> divides the tool use behavior into three modes: <span class="ltx_text ltx_font_italic" id="S2.SS1.p3.1.2">assistive tool use</span> is usually passive and unaware (e.g., walking in the rain shelter corridor); <span class="ltx_text ltx_font_italic" id="S2.SS1.p3.1.3">arbitrary tool use</span> requires active interaction (e.g., driving, using smart phones); <span class="ltx_text ltx_font_italic" id="S2.SS1.p3.1.4">free tool use</span> further needs to comprehend and choose appropriate tools for the scenarios (e.g., cooking new dishes). In this framework, the three modes of tool use present a progressive relationship, and the authors assume that the key cognitive process for achieving free tool use is technical reasoning, which allows someone to learn new actions by observing others using, selecting, or making a tool instead of numerous practices.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p4.1.1">Transition from Physical Tools to Conceptual Tools.</span>
Apart from tools in the physical world, we can also turn to more abstract tools. Take cognitive tools <cite class="ltx_cite ltx_citemacro_citep">(Heyes, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib52" title="">2018</a>)</cite> as an example: it refers to an auxiliary aid that facilitates higher-order thinking (e.g., multi-step critical analysis, the generation of creative problem-solving solutions). Cognitive tools can be classified based on the functionalities they provide <cite class="ltx_cite ltx_citemacro_citep">(Lajoie &amp; Derry, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib74" title="">2013</a>)</cite>. These include (1) supporting cognitive processes (e.g., documenting intermediate reasoning outcomes), (2) alleviating lower-level cognitive load to free up resources for advanced-level thinking, (3) enabling learners to engage in activities out of their reach and (4) allowing learners to generate and test hypotheses (e.g., simulated diagnoses for medical students).</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p5.1.1">Bridging the Gap between Human and Machine Tool Use.</span>
First, the abilities to manipulate tools are deeply rooted in our cognitive and perceptual systems and have evolved over millions of years. In contrast, foundation models rely primarily on statistical patterns of pre-training data, and significant gaps still exist between the tool-use capabilities of foundation models and their human counterparts. Humans can perceive the properties of tools, understand their functionalities, and identify the appropriate tools for each task. Gaining insights from this, we investigate and discuss how foundation models can learn such a process in <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2.SSS1" title="3.2.1 Understanding Intent and Tools ‣ 3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.2.1</span></a>.
Second, humans excel at breaking down complex tasks into smaller, manageable subtasks and deftly manipulating tools to accomplish each sub-task. However, foundation models lack the physical embodiment and sensory experience necessary to fully understand and utilize tools. As a result, these models often struggle with tasks that require higher-order reasoning and adaptivity, and they cannot trustfully integrate multiple sources of knowledge and tools effectively. We will discuss how to better make executable plans leveraging models’ reasoning abilities in <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2.SSS2" title="3.2.2 Planning with Reasoning ‣ 3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.2.2</span></a>. Furthermore, current algorithms for adapting foundation models to learn specific tools generally require huge amounts of supervised data <cite class="ltx_cite ltx_citemacro_citep">(Nakano et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib110" title="">2021</a>; Reed et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib132" title="">2022</a>)</cite>, hindering their generalization and transferability to broader types of tools or novel situations. Hence we first summarize the training strategies for tool learning (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3.SSS1" title="3.3.1 Learning from Demonstrations ‣ 3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.3.1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3.SSS2" title="3.3.2 Learning from Feedback ‣ 3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.3.2</span></a>) and discuss how to facilitate the generalization and transferability of tool learning (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3.SSS3" title="3.3.3 Generalizable Tool Learning ‣ 3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.3.3</span></a>).</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Tool Categorization: A User-Interface Perspective</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The growing number and complexity of tools in our world make it increasingly important to understand and group them in a meaningful way. A system for classifying these tools helps us better grasp their uses, benefits, and potential for growth. In this paper, our focus is particularly on those tools that can be manipulated using instructions in conjunction with foundation models. We introduce a taxonomy that sorts these tools based on their modes of expression and interaction. As depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2.F2" title="Figure 2 ‣ 2.2 Tool Categorization: A User-Interface Perspective ‣ 2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">2</span></a>, this taxonomy incorporates three levels of interaction, arranged from the most tangible to the least. <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.1">The physical level</span> involves direct physical interactions with tools. <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.2">The graphical user interface</span> (GUI) level facilitates user interaction with visual representations of tools. <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.3">The program level</span> involves users engaging directly with the underlying source code of tools.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p2.1.1">Physical Interaction-based Tools.</span> We start with the most tangible genre of tools, physical interaction-based tools.
As the name suggests, this class of tools involves direct interactions with the physical world, including devices like robots, sensors, and wearables that could physically impact the environment.
Physical interaction tools have the capability to sense and respond to the physical environment of users, making them useful in a wide range of applications, from manufacturing to healthcare and education.
Physical interaction tools are close to the real world, and they have the potential to substantially improve efficiency and productivity.
For example, robots can perform from simple to intricate, even adventurous tasks to reduce human errors and labor costs. Sensors can collect valuable data, such as temperature and pressure, allowing for real-time monitoring and optimization of industrial processes.
Wearables, on the other hand, provide users with a personalized experience by tracking physiological or environmental parameters, promoting health and safety.
It is worth noting that although the output of such tools interacts with the real world at the physical level, users may also create the input of the tools at the GUI or source code level.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p3.1.1">GUI-based Tools.</span>
Some tools allow users to manipulate them through an interactive interface, i.e., visual representations of tools, with pre-defined operations.
These tools, defined as GUI-based tools, do not have a direct impact on the physical world.
The GUI interface typically includes buttons, menus, text boxes, and other graphical elements that allow users to interact with the underlying system.
These tools are extensively employed in various industries and applications such as software development, data analysis, and design.
Particularly, GUI-based tools can improve productivity by streamlining workflows and automating repetitive tasks.
GUI-based tools could considerably simplify complex tasks and reduce the learning curve for non-technical users.
From this viewpoint, tool learning with foundation models share the same primary goal, which simplifies intricate tasks to a natural language format.
Representative GUI-based tools are usually well-developed software such as browsers, Microsoft Office, Adobe PhotoShop, etc.
These applications showcase the versatility that graphical interfaces can provide and enable users to access and manipulate complex features within the software.
On the other hand, the main limitation of GUI-based tools is that they may not provide the flexibility and customizability of command-line interfaces or APIs.
In specific scenarios that require rapid and mass responses, as well as greater and flexible control, the visual interface may not always be the most effective way to interact with a system.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="291" id="S2.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Tool categorization from the perspective of the user interface: (1) physical interaction-based tools, (b) GUI-based tools, and (c) program-based tools.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p4.1.1">Program-based Tools.</span>
The innermost layer of tools that users can access is the source code, offering a high degree of flexibility for the input and output of these program-based tools.
Program-based tools are software tools primarily designed for use through programming interfaces rather than visual interfaces.
They can take various forms, including declarative languages, programming libraries, software development kits (SDKs), and even neural network-based tools.
These tools are typically used by developers or technical users who possess a deeper understanding of the underlying data, system or technology, with which the users could complete complex software applications.
The main advantage of program-based tools is that they provide greater flexibility and customizability than GUI-based tools, and users can build more sophisticated solutions for current problems. As a result, such tools also have a steeper learning curve than GUI-based tools, they require a greater degree of technical expertise and programming knowledge, which may not be accessible to non-technical users.
For example, program-based tools can be more time-consuming to set up and configure and may require more maintenance and support in the learning process.
It is noteworthy that, although these tools pose difficulties for human beings in terms of the learning curve, they may not have the same level of challenges for foundation models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.1">It can be seen that the above three interaction modes have varying levels of connectivity with the tool kernel. They are not strictly mutually exclusive but indicate a tendency to intermingle with each other. Human beings have the ability to deal with complex tasks by flexibly executing tools of different types. In this paper, we contend that regardless of the tool type, it is fundamentally possible to leverage foundation models to execute them by setting up intermediary interfaces. We will introduce ways to unify the interface of different tools for foundation models in <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3.SSS3" title="3.3.3 Generalizable Tool Learning ‣ 3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.3.3</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Paradigm Shift of Foundation Models</h3>
<div class="ltx_para ltx_noindent" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">In recent years, the field of natural language processing (NLP) has undergone a paradigm shift, marked by the advent of pre-trained language models (PLMs) <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib36" title="">2019</a>; Bommasani et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib17" title="">2021</a>; Han et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib47" title="">2021</a>)</cite>.
Prior to this breakthrough, NLP was a challenging field that necessitated designing separate learning objectives for distinct research domains, such as dependency parsing <cite class="ltx_cite ltx_citemacro_citep">(Kübler et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib72" title="">2009</a>)</cite>, named entity recognition <cite class="ltx_cite ltx_citemacro_citep">(Nadeau &amp; Sekine, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib109" title="">2007</a>)</cite>, and summarization <cite class="ltx_cite ltx_citemacro_citep">(Nenkova &amp; McKeown, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib111" title="">2012</a>)</cite>.
Despite the successful design of effective models and methods for these specific tasks, the separated nature of this paradigm impeded progress toward a holistic comprehension of language, thereby limiting its potential.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">The invention of PLMs changes this paradigm. Building on Transformers <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib165" title="">2017</a>)</cite>, PLMs are trained on massive corpora, from which general linguistic ability and world knowledge are learned.
This technique has expedited the unification of NLP tasks, giving rise to the <span class="ltx_text ltx_font_italic" id="S2.SS3.p2.1.1">pre-train-then-fine-tune</span> paradigm, which has achieved new state-of-the-art performance on several NLP benchmarks, such as GLUE <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib171" title="">2019b</a>)</cite> and SuperGLUE <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib170" title="">2019a</a>)</cite>.
At this stage, each task shares the same starting point and only diverges as the task-specific adaptation proceeds. The fusion of task paradigms is still ongoing. T5 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib128" title="">2020</a>)</cite> transforms all NLP tasks into a text-to-text format with textual descriptions, while GPT-3 <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib19" title="">2020</a>)</cite> has discovered that introducing appropriate textual prompts can yield the desired output for specific tasks. Prompts, essentially serving as a natural language interface, are widely believed to stimulate the knowledge learned by PLMs during pre-training. Prompts can enable downstream tasks to be executed without updating model parameters for big models such as GPT-3. Research even suggests that with appropriate prompt guidance, models can perform complex reasoning tasks <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib182" title="">2022c</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib175" title="">2022a</a>)</cite>.
Also, prompts formulated in a natural language format possess remarkable generalization capabilities. Specifically, models that have undergone fine-tuning using diverse instructions are able to effectively generalize to new, unseen data <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib180" title="">2022a</a>; Sanh et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib137" title="">2022</a>)</cite>.
Overall, prompts demonstrate a proof-of-concept that uses PLMs as the underlying infrastructure and natural language as the medium to uniformly perform various tasks.
A highly successful example is ChatGPT, where all the natural language understanding and generation processes are accomplished through conversational interactions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">Nevertheless, there exist numerous tasks that transcend the scope of purely natural language.
For instance, generating presentation slides<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.microsoft.com/en-us/microsoft-365" title="">https://www.microsoft.com/en-us/microsoft-365</a></span></span></span>, constructing 3D models via CAD applications, and scheduling meetings through the analysis of team member calendars are examples of complex tasks that have not been defined in traditional artificial intelligence.
Fortunately, the strong generalization ability of PLM enables us to use natural language as a medium to accomplish these tasks by manipulating tools <cite class="ltx_cite ltx_citemacro_citep">(Zeng et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib198" title="">2022</a>)</cite>.
Essentially, the key to tool learning is to decompose complex tasks into sub-actions, tokenize actions in the form of natural language and convert them into executable instructions that can be understood by specific tools.
Language models serve as “translators”, making complex tasks more accessible to individuals without specialized technical knowledge.
The potential applications of tool learning are vast and exciting, ranging from automated customer service and personal assistants to self-driving cars and even space exploration.
By enabling machines to understand and interact with human language in a more natural and nuanced way, we can unlock new possibilities for collaboration and problem-solving that were previously impossible.
We anticipate that tool learning will prove instrumental in facilitating the integration of diverse tasks through shared tooling.
Thus, while natural language interfaces have enabled unification within the realm of language <cite class="ltx_cite ltx_citemacro_citep">(Hao et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib49" title="">2022</a>)</cite>, the challenges posed by non-linguistic tasks necessitate a more advanced approach to leveraging both natural language and tool learning. By harnessing the power of natural language, we can create systems that are capable of understanding and adapting to the complex and dynamic world around us, opening up new avenues for innovation and discovery.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Complementary Roles of Tools and Foundation Models</h3>
<div class="ltx_para ltx_noindent" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">The integration of specialized tools and foundation models represents a promising approach for harnessing the unique strengths of both.
By incorporating foundation models’ understanding and reasoning capabilities into specialized tools, we can create intelligent tools capable of performing more complex tasks than either specialized tools or foundation models alone. Specifically, the amalgamation of both confers a multitude of benefits as follows.</p>
</div>
<section class="ltx_paragraph" id="S2.SS4.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Benefits of Tools.</h5>
<div class="ltx_para ltx_noindent" id="S2.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS4.SSS0.Px1.p1.1">Tools that are designed to streamline concrete and specific objectives bring several benefits for tool learning: (1) <span class="ltx_text ltx_font_bold" id="S2.SS4.SSS0.Px1.p1.1.1">Mitigation for Memorization.</span>
Although foundation models have demonstrated an exceptional ability to memorize <cite class="ltx_cite ltx_citemacro_citep">(Carlini et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib22" title="">2021</a>, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib23" title="">2022</a>, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib24" title="">2023</a>)</cite>, they are not capable of memorizing every piece of training data. Furthermore, foundation models are often prompted with a relatively short context during model generation, thus not all the memorized knowledge can be properly steered <cite class="ltx_cite ltx_citemacro_citep">(Mialon et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib100" title="">2023</a>)</cite>. Additionally, memorization alone does not support the real-time coverage of up-to-date knowledge, especially in light of the potentially infinite possibilities of novel requests from users. Besides, foundation models are also criticized to hallucinate knowledge <cite class="ltx_cite ltx_citemacro_citep">(Roller et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib134" title="">2021</a>; Shuster et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib147" title="">2021</a>)</cite> by generating seemingly plausible but non-factual content. This is unacceptable in applications like financial transactions that require the results are 100% correct. Given the above factors, it is necessary to augment foundation models with real-time tool execution to mitigate limitations in memorization. For instance, a significant proportion of the memorization burden can be offloaded to the search engine and database systems (of different modes) if foundation models can learn how to utilize it.
(2) <span class="ltx_text ltx_font_bold" id="S2.SS4.SSS0.Px1.p1.1.2">Enhanced Expertise.</span> Specialized tools are designed to cater to specific domains with functionalities that are not available in foundation models. As a result, they are better suited to address the needs of domain-specific tasks, such as Wolfram <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.wolframalpha.com/" title="">https://www.wolframalpha.com/</a></span></span></span> for scientific calculation, through the utilization of tailored algorithms. Instead of solely relying on the foundation model to accomplish the task, models could invoke appropriate tools to generalize to a wider range of tasks that are beyond their capabilities.
(3) <span class="ltx_text ltx_font_bold" id="S2.SS4.SSS0.Px1.p1.1.3">Better Interpretability.</span>
Foundation models are criticized for lacking transparency in their decision-making process <cite class="ltx_cite ltx_citemacro_citep">(Linardatos et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib86" title="">2020</a>)</cite>, which can be a significant concern in applications such as healthcare or finance, where interpretability is critical for making informed decisions. In contrast, the process of tool execution reflects the whole process of how models solve complex requests, which allows for better interpretability and transparency. Users can easily understand why certain tools are called and how they contribute to the final output, which can improve trust and facilitate human-machine collaboration.
(4) <span class="ltx_text ltx_font_bold" id="S2.SS4.SSS0.Px1.p1.1.4">Improved Robustness.</span>
Foundation models are susceptible to adversarial attacks <cite class="ltx_cite ltx_citemacro_citep">(Wallace et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib168" title="">2019</a>; Jin et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib60" title="">2020</a>)</cite>, where slight modifications to the input can flip the model prediction. This is because these models heavily rely on statistical patterns in the training data. Conversely, tools are designed specifically for their intended use cases, which may be agnostic to the input perturbation. This makes tools more resistant to adversarial attacks. Overall, incorporating tools into the workflow of foundation models can improve the robustness of the system and reduce the risk of malicious attacks. This harmonious interplay between tools and models can enhance the reliability of the system against unpredictable real-world environments.
In <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S4" title="4 Application and Experiment ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">4</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1" title="Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">appendix</span> <span class="ltx_text ltx_ref_tag">A</span></a>, we use concrete examples to show how tools can enhance the model’s capabilities in various tasks.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS4.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Benefits of Foundation Models.</h5>
<div class="ltx_para ltx_noindent" id="S2.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS4.SSS0.Px2.p1.1">Foundation models can provide a solid basis for understanding, planning, reasoning, and generation, which bring several benefits for tool learning as follows:
(1) <span class="ltx_text ltx_font_bold" id="S2.SS4.SSS0.Px2.p1.1.1">Improved Decision-Making and Reasoning Abilities.</span>
Foundation models are trained on vast amounts of data, enabling them to acquire world knowledge across a wide range of domains. If properly steered, such knowledge can be wielded to perform decision-making and planning over prolonged time horizons <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib53" title="">2022a</a>)</cite>.
Besides, foundation models have demonstrated remarkable reasoning abilities <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib182" title="">2022c</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib175" title="">2022a</a>)</cite>, thereby enabling them to extrapolate the consequences of actions and make judicious decisions. These reasoning abilities are particularly useful for tasks requiring a deep understanding of cause-and-effect relations (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2.SSS2" title="3.2.2 Planning with Reasoning ‣ 3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.2.2</span></a>).
(2) <span class="ltx_text ltx_font_bold" id="S2.SS4.SSS0.Px2.p1.1.2">Better User Experience.</span> Benefitting from the powerful intent understanding capability of foundation models, tool learning could revolutionize the way we interact with machines and liberate users from the cognition load, allowing them to engage in higher-order thinking and decision-making processes. This, in turn, fosters a seamless and more natural language-based interaction paradigm that revolutionizes traditional graphical user interfaces (GUIs). The user only needs to provide high-level guidance and direction, and the model will seamlessly comprehend the user’s intent, thereby delivering more personalized and precise responses. In addition, tool learning has the potential to democratize access to complex tools. With the aid of foundation models, even novice users can easily and quickly get started with a new tool, regardless of their prior experience or technical expertise. This not only reduces the barriers to entry for new users but also unlocks a wealth of possibilities for innovation and creativity. However, it should be noted that human-model collaboration in tool use also triggers ethical concerns, which will be discussed in <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS7" title="5.7 Open Problems ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">5.7</span></a>.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Tool Learning</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, to unify existing efforts and promote a comprehensive understanding of tool learning, we first present a general framework, which encompasses four fundamental components, namely tool set, environment, controller, and perceiver, as detailed in <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS1" title="3.1 Components of Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>. Subsequently, we provide an elaborate discussion of the general procedure of tool learning in <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2" title="3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>. Lastly, we delve into the training methods for tool learning and discuss how to achieve generalizable tool learning in <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3" title="3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Components of Tool Learning</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">How can we enable foundation models to leverage the strengths of specialized tools to accomplish complex tasks? To better answer this question, we frame tool learning with four components as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.F3" title="Figure 3 ‣ Perceiver. ‣ 3.1.1 Understanding the Components ‣ 3.1 Components of Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">3</span></a>. Each component has its own characteristics and functions (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS1.SSS1" title="3.1.1 Understanding the Components ‣ 3.1 Components of Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.1.1</span></a>), but they also interact with each other closely (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS1.SSS2" title="3.1.2 Connecting the Components ‣ 3.1 Components of Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.1.2</span></a>).</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Understanding the Components</h4>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">We first introduce each component and explain how they contribute to the tool learning process.</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Tool Set.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS1.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px1.p1.2">Serving as the fundamental ingredient of tool learning, the tool set <math alttext="\mathcal{T}=\{\mathcal{T}_{1},\mathcal{T}_{2},\cdots\}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.Px1.p1.1.m1.3"><semantics id="S3.SS1.SSS1.Px1.p1.1.m1.3a"><mrow id="S3.SS1.SSS1.Px1.p1.1.m1.3.3" xref="S3.SS1.SSS1.Px1.p1.1.m1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.Px1.p1.1.m1.3.3.4" xref="S3.SS1.SSS1.Px1.p1.1.m1.3.3.4.cmml">𝒯</mi><mo id="S3.SS1.SSS1.Px1.p1.1.m1.3.3.3" xref="S3.SS1.SSS1.Px1.p1.1.m1.3.3.3.cmml">=</mo><mrow id="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.2" xref="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.3.cmml"><mo id="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.2.3" stretchy="false" xref="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.3.cmml">{</mo><msub id="S3.SS1.SSS1.Px1.p1.1.m1.2.2.1.1.1" xref="S3.SS1.SSS1.Px1.p1.1.m1.2.2.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.Px1.p1.1.m1.2.2.1.1.1.2" xref="S3.SS1.SSS1.Px1.p1.1.m1.2.2.1.1.1.2.cmml">𝒯</mi><mn id="S3.SS1.SSS1.Px1.p1.1.m1.2.2.1.1.1.3" xref="S3.SS1.SSS1.Px1.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.2.4" xref="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.3.cmml">,</mo><msub id="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.2.2" xref="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.2.2.2" xref="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.2.2.2.cmml">𝒯</mi><mn id="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.2.2.3" xref="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.2.5" xref="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.3.cmml">,</mo><mi id="S3.SS1.SSS1.Px1.p1.1.m1.1.1" mathvariant="normal" xref="S3.SS1.SSS1.Px1.p1.1.m1.1.1.cmml">⋯</mi><mo id="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.2.6" stretchy="false" xref="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.Px1.p1.1.m1.3b"><apply id="S3.SS1.SSS1.Px1.p1.1.m1.3.3.cmml" xref="S3.SS1.SSS1.Px1.p1.1.m1.3.3"><eq id="S3.SS1.SSS1.Px1.p1.1.m1.3.3.3.cmml" xref="S3.SS1.SSS1.Px1.p1.1.m1.3.3.3"></eq><ci id="S3.SS1.SSS1.Px1.p1.1.m1.3.3.4.cmml" xref="S3.SS1.SSS1.Px1.p1.1.m1.3.3.4">𝒯</ci><set id="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.3.cmml" xref="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.2"><apply id="S3.SS1.SSS1.Px1.p1.1.m1.2.2.1.1.1.cmml" xref="S3.SS1.SSS1.Px1.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.Px1.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS1.SSS1.Px1.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.Px1.p1.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS1.SSS1.Px1.p1.1.m1.2.2.1.1.1.2">𝒯</ci><cn id="S3.SS1.SSS1.Px1.p1.1.m1.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS1.SSS1.Px1.p1.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.2.2.cmml" xref="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.2.2.1.cmml" xref="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.2.2.2.cmml" xref="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.2.2.2">𝒯</ci><cn id="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.2.2.3.cmml" type="integer" xref="S3.SS1.SSS1.Px1.p1.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS1.SSS1.Px1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.Px1.p1.1.m1.1.1">⋯</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.Px1.p1.1.m1.3c">\mathcal{T}=\{\mathcal{T}_{1},\mathcal{T}_{2},\cdots\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.Px1.p1.1.m1.3d">caligraphic_T = { caligraphic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , caligraphic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ⋯ }</annotation></semantics></math> contains a collection of different tools that have different functionalities. As we have elaborated in <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2.SS2" title="2.2 Tool Categorization: A User-Interface Perspective ‣ 2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">2.2</span></a>, a tool in <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.Px1.p1.2.m2.1"><semantics id="S3.SS1.SSS1.Px1.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.Px1.p1.2.m2.1.1" xref="S3.SS1.SSS1.Px1.p1.2.m2.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.Px1.p1.2.m2.1b"><ci id="S3.SS1.SSS1.Px1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS1.Px1.p1.2.m2.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.Px1.p1.2.m2.1c">\mathcal{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.Px1.p1.2.m2.1d">caligraphic_T</annotation></semantics></math> can have different interfaces. In the following sections, we mainly take Application Programming Interface (API) as the example to illustrate how to interact with tools. Here we define an API as any function that can take the output of the foundation model as its input. For instance, for a weather API, the input to the API may be a location and time, and the output may contain the temperature or wind speed.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Environment.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS1.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px2.p1.1">The environment <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.Px2.p1.1.m1.1"><semantics id="S3.SS1.SSS1.Px2.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.Px2.p1.1.m1.1.1" xref="S3.SS1.SSS1.Px2.p1.1.m1.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.Px2.p1.1.m1.1b"><ci id="S3.SS1.SSS1.Px2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.Px2.p1.1.m1.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.Px2.p1.1.m1.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.Px2.p1.1.m1.1d">caligraphic_E</annotation></semantics></math> is the world where the tools operate, which provides the <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.Px2.p1.1.1">perceiver</span> with the execution results of tools. It provides the infrastructure necessary for tool execution, which can be either virtual or real. The former refers to a simulated environment that allows the model to interact with a digital representation of the tool, while a real environment involves actual interaction with the physical tool.
Virtual environments have the advantage of being easily accessible and replicable, allowing for more cost-effective training for models. However, virtual environments may not fully replicate the complexities of the real-world environment, leading to overfitting and poor generalization <cite class="ltx_cite ltx_citemacro_citep">(Hansen et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib48" title="">2021</a>)</cite>. On the other hand, real environments provide a more realistic context but may be more challenging to access and involve greater costs.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px3">
<h5 class="ltx_title ltx_title_paragraph">Controller.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS1.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px3.p1.4">The controller <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.Px3.p1.1.m1.1"><semantics id="S3.SS1.SSS1.Px3.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.Px3.p1.1.m1.1.1" xref="S3.SS1.SSS1.Px3.p1.1.m1.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.Px3.p1.1.m1.1b"><ci id="S3.SS1.SSS1.Px3.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.Px3.p1.1.m1.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.Px3.p1.1.m1.1c">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.Px3.p1.1.m1.1d">caligraphic_C</annotation></semantics></math> serves as the “brain” for tool learning framework and is typically modeled using a foundation model. The purpose of the controller <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.Px3.p1.2.m2.1"><semantics id="S3.SS1.SSS1.Px3.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.Px3.p1.2.m2.1.1" xref="S3.SS1.SSS1.Px3.p1.2.m2.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.Px3.p1.2.m2.1b"><ci id="S3.SS1.SSS1.Px3.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS1.Px3.p1.2.m2.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.Px3.p1.2.m2.1c">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.Px3.p1.2.m2.1d">caligraphic_C</annotation></semantics></math> is to provide a feasible and precise plan for using tools to fulfill the user’s request. To this end, <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.Px3.p1.3.m3.1"><semantics id="S3.SS1.SSS1.Px3.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.Px3.p1.3.m3.1.1" xref="S3.SS1.SSS1.Px3.p1.3.m3.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.Px3.p1.3.m3.1b"><ci id="S3.SS1.SSS1.Px3.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS1.Px3.p1.3.m3.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.Px3.p1.3.m3.1c">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.Px3.p1.3.m3.1d">caligraphic_C</annotation></semantics></math> should understand user intent as well as the relationship between the intent and available tools, and then develop a plan to select the appropriate tools for tackling tasks, which will be discussed in <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2.SSS1" title="3.2.1 Understanding Intent and Tools ‣ 3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.2.1</span></a>. In cases where the query is complex and targets a high-level task, <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.Px3.p1.4.m4.1"><semantics id="S3.SS1.SSS1.Px3.p1.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.Px3.p1.4.m4.1.1" xref="S3.SS1.SSS1.Px3.p1.4.m4.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.Px3.p1.4.m4.1b"><ci id="S3.SS1.SSS1.Px3.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS1.Px3.p1.4.m4.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.Px3.p1.4.m4.1c">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.Px3.p1.4.m4.1d">caligraphic_C</annotation></semantics></math> may need to decompose the task into multiple sub-tasks, which requires foundational models to have powerful planning and reasoning capabilities (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2.SSS2" title="3.2.2 Planning with Reasoning ‣ 3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.2.2</span></a>).</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px4">
<h5 class="ltx_title ltx_title_paragraph">Perceiver.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS1.Px4.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px4.p1.1">The perceiver <math alttext="\mathcal{P}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.Px4.p1.1.m1.1"><semantics id="S3.SS1.SSS1.Px4.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.Px4.p1.1.m1.1.1" xref="S3.SS1.SSS1.Px4.p1.1.m1.1.1.cmml">𝒫</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.Px4.p1.1.m1.1b"><ci id="S3.SS1.SSS1.Px4.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.Px4.p1.1.m1.1.1">𝒫</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.Px4.p1.1.m1.1c">\mathcal{P}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.Px4.p1.1.m1.1d">caligraphic_P</annotation></semantics></math> is responsible for processing the user’s and the environment’s feedback and generating a summary for the controller. Simple forms of feedback processing include concatenating the user and environment feedback or formatting the feedback using a pre-defined template.
The summarized feedback is then passed to the controller to assist its decision-making. By observing this feedback, the controller can determine whether the generated plan is effective and whether there are anomalies during the execution that need to be addressed. Under more complex scenarios, the perceiver should be able to support multiple modalities, such as text, vision, and audio, to capture the diverse nature of feedback from the user and the environment.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="365" id="S3.F3.g1" src="x3.png" width="648"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Illustration of the tool learning framework, where we display the human user and four core ingredients of the framework: tool set, controller, perceiver, and environment. The user sends an instruction to the controller, which then makes decisions and executes tools in the environment. The perceiver receives feedback from both the environment and the user and summarizes them to the controller.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Connecting the Components</h4>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.14">Formally, assume we have a tool set <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.1.m1.1"><semantics id="S3.SS1.SSS2.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS2.p1.1.m1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.1.m1.1b"><ci id="S3.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.1.m1.1c">\mathcal{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.1.m1.1d">caligraphic_T</annotation></semantics></math>, which the controller can utilize to accomplish certain tasks. At time step <math alttext="t" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.2.m2.1"><semantics id="S3.SS1.SSS2.p1.2.m2.1a"><mi id="S3.SS1.SSS2.p1.2.m2.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.2.m2.1b"><ci id="S3.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.2.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.2.m2.1d">italic_t</annotation></semantics></math>, environment <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.3.m3.1"><semantics id="S3.SS1.SSS2.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS2.p1.3.m3.1.1" xref="S3.SS1.SSS2.p1.3.m3.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.3.m3.1b"><ci id="S3.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.3.m3.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.3.m3.1d">caligraphic_E</annotation></semantics></math> provides feedback <math alttext="e_{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.4.m4.1"><semantics id="S3.SS1.SSS2.p1.4.m4.1a"><msub id="S3.SS1.SSS2.p1.4.m4.1.1" xref="S3.SS1.SSS2.p1.4.m4.1.1.cmml"><mi id="S3.SS1.SSS2.p1.4.m4.1.1.2" xref="S3.SS1.SSS2.p1.4.m4.1.1.2.cmml">e</mi><mi id="S3.SS1.SSS2.p1.4.m4.1.1.3" xref="S3.SS1.SSS2.p1.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.4.m4.1b"><apply id="S3.SS1.SSS2.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.4.m4.1.1.1.cmml" xref="S3.SS1.SSS2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.4.m4.1.1.2.cmml" xref="S3.SS1.SSS2.p1.4.m4.1.1.2">𝑒</ci><ci id="S3.SS1.SSS2.p1.4.m4.1.1.3.cmml" xref="S3.SS1.SSS2.p1.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.4.m4.1c">e_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.4.m4.1d">italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> on the tool execution. The perceiver <math alttext="\mathcal{P}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.5.m5.1"><semantics id="S3.SS1.SSS2.p1.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS2.p1.5.m5.1.1" xref="S3.SS1.SSS2.p1.5.m5.1.1.cmml">𝒫</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.5.m5.1b"><ci id="S3.SS1.SSS2.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS2.p1.5.m5.1.1">𝒫</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.5.m5.1c">\mathcal{P}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.5.m5.1d">caligraphic_P</annotation></semantics></math> receives the user feedback <math alttext="f_{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.6.m6.1"><semantics id="S3.SS1.SSS2.p1.6.m6.1a"><msub id="S3.SS1.SSS2.p1.6.m6.1.1" xref="S3.SS1.SSS2.p1.6.m6.1.1.cmml"><mi id="S3.SS1.SSS2.p1.6.m6.1.1.2" xref="S3.SS1.SSS2.p1.6.m6.1.1.2.cmml">f</mi><mi id="S3.SS1.SSS2.p1.6.m6.1.1.3" xref="S3.SS1.SSS2.p1.6.m6.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.6.m6.1b"><apply id="S3.SS1.SSS2.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.6.m6.1.1.1.cmml" xref="S3.SS1.SSS2.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.6.m6.1.1.2.cmml" xref="S3.SS1.SSS2.p1.6.m6.1.1.2">𝑓</ci><ci id="S3.SS1.SSS2.p1.6.m6.1.1.3.cmml" xref="S3.SS1.SSS2.p1.6.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.6.m6.1c">f_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.6.m6.1d">italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and the environment feedback <math alttext="e_{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.7.m7.1"><semantics id="S3.SS1.SSS2.p1.7.m7.1a"><msub id="S3.SS1.SSS2.p1.7.m7.1.1" xref="S3.SS1.SSS2.p1.7.m7.1.1.cmml"><mi id="S3.SS1.SSS2.p1.7.m7.1.1.2" xref="S3.SS1.SSS2.p1.7.m7.1.1.2.cmml">e</mi><mi id="S3.SS1.SSS2.p1.7.m7.1.1.3" xref="S3.SS1.SSS2.p1.7.m7.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.7.m7.1b"><apply id="S3.SS1.SSS2.p1.7.m7.1.1.cmml" xref="S3.SS1.SSS2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.7.m7.1.1.1.cmml" xref="S3.SS1.SSS2.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.7.m7.1.1.2.cmml" xref="S3.SS1.SSS2.p1.7.m7.1.1.2">𝑒</ci><ci id="S3.SS1.SSS2.p1.7.m7.1.1.3.cmml" xref="S3.SS1.SSS2.p1.7.m7.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.7.m7.1c">e_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.7.m7.1d">italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, and generates summarized feedback <math alttext="x_{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.8.m8.1"><semantics id="S3.SS1.SSS2.p1.8.m8.1a"><msub id="S3.SS1.SSS2.p1.8.m8.1.1" xref="S3.SS1.SSS2.p1.8.m8.1.1.cmml"><mi id="S3.SS1.SSS2.p1.8.m8.1.1.2" xref="S3.SS1.SSS2.p1.8.m8.1.1.2.cmml">x</mi><mi id="S3.SS1.SSS2.p1.8.m8.1.1.3" xref="S3.SS1.SSS2.p1.8.m8.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.8.m8.1b"><apply id="S3.SS1.SSS2.p1.8.m8.1.1.cmml" xref="S3.SS1.SSS2.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.8.m8.1.1.1.cmml" xref="S3.SS1.SSS2.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.8.m8.1.1.2.cmml" xref="S3.SS1.SSS2.p1.8.m8.1.1.2">𝑥</ci><ci id="S3.SS1.SSS2.p1.8.m8.1.1.3.cmml" xref="S3.SS1.SSS2.p1.8.m8.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.8.m8.1c">x_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.8.m8.1d">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. Typically, the perceiver can be achieved by pre-defined rules (e.g., concatenating <math alttext="f_{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.9.m9.1"><semantics id="S3.SS1.SSS2.p1.9.m9.1a"><msub id="S3.SS1.SSS2.p1.9.m9.1.1" xref="S3.SS1.SSS2.p1.9.m9.1.1.cmml"><mi id="S3.SS1.SSS2.p1.9.m9.1.1.2" xref="S3.SS1.SSS2.p1.9.m9.1.1.2.cmml">f</mi><mi id="S3.SS1.SSS2.p1.9.m9.1.1.3" xref="S3.SS1.SSS2.p1.9.m9.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.9.m9.1b"><apply id="S3.SS1.SSS2.p1.9.m9.1.1.cmml" xref="S3.SS1.SSS2.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.9.m9.1.1.1.cmml" xref="S3.SS1.SSS2.p1.9.m9.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.9.m9.1.1.2.cmml" xref="S3.SS1.SSS2.p1.9.m9.1.1.2">𝑓</ci><ci id="S3.SS1.SSS2.p1.9.m9.1.1.3.cmml" xref="S3.SS1.SSS2.p1.9.m9.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.9.m9.1c">f_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.9.m9.1d">italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="e_{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.10.m10.1"><semantics id="S3.SS1.SSS2.p1.10.m10.1a"><msub id="S3.SS1.SSS2.p1.10.m10.1.1" xref="S3.SS1.SSS2.p1.10.m10.1.1.cmml"><mi id="S3.SS1.SSS2.p1.10.m10.1.1.2" xref="S3.SS1.SSS2.p1.10.m10.1.1.2.cmml">e</mi><mi id="S3.SS1.SSS2.p1.10.m10.1.1.3" xref="S3.SS1.SSS2.p1.10.m10.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.10.m10.1b"><apply id="S3.SS1.SSS2.p1.10.m10.1.1.cmml" xref="S3.SS1.SSS2.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.10.m10.1.1.1.cmml" xref="S3.SS1.SSS2.p1.10.m10.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.10.m10.1.1.2.cmml" xref="S3.SS1.SSS2.p1.10.m10.1.1.2">𝑒</ci><ci id="S3.SS1.SSS2.p1.10.m10.1.1.3.cmml" xref="S3.SS1.SSS2.p1.10.m10.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.10.m10.1c">e_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.10.m10.1d">italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>) to form <math alttext="x_{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.11.m11.1"><semantics id="S3.SS1.SSS2.p1.11.m11.1a"><msub id="S3.SS1.SSS2.p1.11.m11.1.1" xref="S3.SS1.SSS2.p1.11.m11.1.1.cmml"><mi id="S3.SS1.SSS2.p1.11.m11.1.1.2" xref="S3.SS1.SSS2.p1.11.m11.1.1.2.cmml">x</mi><mi id="S3.SS1.SSS2.p1.11.m11.1.1.3" xref="S3.SS1.SSS2.p1.11.m11.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.11.m11.1b"><apply id="S3.SS1.SSS2.p1.11.m11.1.1.cmml" xref="S3.SS1.SSS2.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.11.m11.1.1.1.cmml" xref="S3.SS1.SSS2.p1.11.m11.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.11.m11.1.1.2.cmml" xref="S3.SS1.SSS2.p1.11.m11.1.1.2">𝑥</ci><ci id="S3.SS1.SSS2.p1.11.m11.1.1.3.cmml" xref="S3.SS1.SSS2.p1.11.m11.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.11.m11.1c">x_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.11.m11.1d">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, or modeled with complex neural models.
The controller <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.12.m12.1"><semantics id="S3.SS1.SSS2.p1.12.m12.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS2.p1.12.m12.1.1" xref="S3.SS1.SSS2.p1.12.m12.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.12.m12.1b"><ci id="S3.SS1.SSS2.p1.12.m12.1.1.cmml" xref="S3.SS1.SSS2.p1.12.m12.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.12.m12.1c">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.12.m12.1d">caligraphic_C</annotation></semantics></math> generates a plan <math alttext="a_{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.13.m13.1"><semantics id="S3.SS1.SSS2.p1.13.m13.1a"><msub id="S3.SS1.SSS2.p1.13.m13.1.1" xref="S3.SS1.SSS2.p1.13.m13.1.1.cmml"><mi id="S3.SS1.SSS2.p1.13.m13.1.1.2" xref="S3.SS1.SSS2.p1.13.m13.1.1.2.cmml">a</mi><mi id="S3.SS1.SSS2.p1.13.m13.1.1.3" xref="S3.SS1.SSS2.p1.13.m13.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.13.m13.1b"><apply id="S3.SS1.SSS2.p1.13.m13.1.1.cmml" xref="S3.SS1.SSS2.p1.13.m13.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.13.m13.1.1.1.cmml" xref="S3.SS1.SSS2.p1.13.m13.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.13.m13.1.1.2.cmml" xref="S3.SS1.SSS2.p1.13.m13.1.1.2">𝑎</ci><ci id="S3.SS1.SSS2.p1.13.m13.1.1.3.cmml" xref="S3.SS1.SSS2.p1.13.m13.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.13.m13.1c">a_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.13.m13.1d">italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, which selects and executes an appropriate tool from <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.14.m14.1"><semantics id="S3.SS1.SSS2.p1.14.m14.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS2.p1.14.m14.1.1" xref="S3.SS1.SSS2.p1.14.m14.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.14.m14.1b"><ci id="S3.SS1.SSS2.p1.14.m14.1.1.cmml" xref="S3.SS1.SSS2.p1.14.m14.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.14.m14.1c">\mathcal{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.14.m14.1d">caligraphic_T</annotation></semantics></math>. This process can be formulated as the following probability distribution:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A1.EGx1">
<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle p_{\mathcal{C}}(a_{t})=p_{\theta_{\mathcal{C}}}(a_{t}\mid x_{t},%
\mathcal{H}_{t},q)," class="ltx_Math" display="inline" id="S3.E1.m1.2"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.3.cmml"><mi id="S3.E1.m1.2.2.1.1.1.3.2" xref="S3.E1.m1.2.2.1.1.1.3.2.cmml">p</mi><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.2.1.1.1.3.3" xref="S3.E1.m1.2.2.1.1.1.3.3.cmml">𝒞</mi></msub><mo id="S3.E1.m1.2.2.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.2.2.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml">a</mi><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.E1.m1.2.2.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.3.cmml">=</mo><mrow id="S3.E1.m1.2.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.cmml"><msub id="S3.E1.m1.2.2.1.1.2.3" xref="S3.E1.m1.2.2.1.1.2.3.cmml"><mi id="S3.E1.m1.2.2.1.1.2.3.2" xref="S3.E1.m1.2.2.1.1.2.3.2.cmml">p</mi><msub id="S3.E1.m1.2.2.1.1.2.3.3" xref="S3.E1.m1.2.2.1.1.2.3.3.cmml"><mi id="S3.E1.m1.2.2.1.1.2.3.3.2" xref="S3.E1.m1.2.2.1.1.2.3.3.2.cmml">θ</mi><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.2.1.1.2.3.3.3" xref="S3.E1.m1.2.2.1.1.2.3.3.3.cmml">𝒞</mi></msub></msub><mo id="S3.E1.m1.2.2.1.1.2.2" xref="S3.E1.m1.2.2.1.1.2.2.cmml">⁢</mo><mrow id="S3.E1.m1.2.2.1.1.2.1.1" xref="S3.E1.m1.2.2.1.1.2.1.1.1.cmml"><mo id="S3.E1.m1.2.2.1.1.2.1.1.2" stretchy="false" xref="S3.E1.m1.2.2.1.1.2.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.2.2.1.1.2.1.1.1" xref="S3.E1.m1.2.2.1.1.2.1.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.2.1.1.1.4" xref="S3.E1.m1.2.2.1.1.2.1.1.1.4.cmml"><mi id="S3.E1.m1.2.2.1.1.2.1.1.1.4.2" xref="S3.E1.m1.2.2.1.1.2.1.1.1.4.2.cmml">a</mi><mi id="S3.E1.m1.2.2.1.1.2.1.1.1.4.3" xref="S3.E1.m1.2.2.1.1.2.1.1.1.4.3.cmml">t</mi></msub><mo id="S3.E1.m1.2.2.1.1.2.1.1.1.3" xref="S3.E1.m1.2.2.1.1.2.1.1.1.3.cmml">∣</mo><mrow id="S3.E1.m1.2.2.1.1.2.1.1.1.2.2" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.3.cmml"><msub id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.3" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.3.cmml">,</mo><msub id="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.2" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.2.2" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.2.2.cmml">ℋ</mi><mi id="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.2.3" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.2.3.cmml">t</mi></msub><mo id="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.4" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.3.cmml">,</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">q</mi></mrow></mrow><mo id="S3.E1.m1.2.2.1.1.2.1.1.3" stretchy="false" xref="S3.E1.m1.2.2.1.1.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.2.2.1.2" xref="S3.E1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1"><eq id="S3.E1.m1.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.3"></eq><apply id="S3.E1.m1.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1"><times id="S3.E1.m1.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.2"></times><apply id="S3.E1.m1.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.1.3.2">𝑝</ci><ci id="S3.E1.m1.2.2.1.1.1.3.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3.3">𝒞</ci></apply><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2">𝑎</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3">𝑡</ci></apply></apply><apply id="S3.E1.m1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2"><times id="S3.E1.m1.2.2.1.1.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2"></times><apply id="S3.E1.m1.2.2.1.1.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.3.1.cmml" xref="S3.E1.m1.2.2.1.1.2.3">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.3.2.cmml" xref="S3.E1.m1.2.2.1.1.2.3.2">𝑝</ci><apply id="S3.E1.m1.2.2.1.1.2.3.3.cmml" xref="S3.E1.m1.2.2.1.1.2.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.3.3.1.cmml" xref="S3.E1.m1.2.2.1.1.2.3.3">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.3.3.2.cmml" xref="S3.E1.m1.2.2.1.1.2.3.3.2">𝜃</ci><ci id="S3.E1.m1.2.2.1.1.2.3.3.3.cmml" xref="S3.E1.m1.2.2.1.1.2.3.3.3">𝒞</ci></apply></apply><apply id="S3.E1.m1.2.2.1.1.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1"><csymbol cd="latexml" id="S3.E1.m1.2.2.1.1.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.3">conditional</csymbol><apply id="S3.E1.m1.2.2.1.1.2.1.1.1.4.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.1.1.1.4.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.4">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.1.1.1.4.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.4.2">𝑎</ci><ci id="S3.E1.m1.2.2.1.1.2.1.1.1.4.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.4.3">𝑡</ci></apply><list id="S3.E1.m1.2.2.1.1.2.1.1.1.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.2"><apply id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.3">𝑡</ci></apply><apply id="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.2.2">ℋ</ci><ci id="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.2.3">𝑡</ci></apply><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑞</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\displaystyle p_{\mathcal{C}}(a_{t})=p_{\theta_{\mathcal{C}}}(a_{t}\mid x_{t},%
\mathcal{H}_{t},q),</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.2d">italic_p start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT ( italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = italic_p start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∣ italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , caligraphic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_q ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.SSS2.p1.22">where <math alttext="\theta_{\mathcal{C}}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.15.m1.1"><semantics id="S3.SS1.SSS2.p1.15.m1.1a"><msub id="S3.SS1.SSS2.p1.15.m1.1.1" xref="S3.SS1.SSS2.p1.15.m1.1.1.cmml"><mi id="S3.SS1.SSS2.p1.15.m1.1.1.2" xref="S3.SS1.SSS2.p1.15.m1.1.1.2.cmml">θ</mi><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS2.p1.15.m1.1.1.3" xref="S3.SS1.SSS2.p1.15.m1.1.1.3.cmml">𝒞</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.15.m1.1b"><apply id="S3.SS1.SSS2.p1.15.m1.1.1.cmml" xref="S3.SS1.SSS2.p1.15.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.15.m1.1.1.1.cmml" xref="S3.SS1.SSS2.p1.15.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.15.m1.1.1.2.cmml" xref="S3.SS1.SSS2.p1.15.m1.1.1.2">𝜃</ci><ci id="S3.SS1.SSS2.p1.15.m1.1.1.3.cmml" xref="S3.SS1.SSS2.p1.15.m1.1.1.3">𝒞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.15.m1.1c">\theta_{\mathcal{C}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.15.m1.1d">italic_θ start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT</annotation></semantics></math> denotes the parameters of <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.16.m2.1"><semantics id="S3.SS1.SSS2.p1.16.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS2.p1.16.m2.1.1" xref="S3.SS1.SSS2.p1.16.m2.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.16.m2.1b"><ci id="S3.SS1.SSS2.p1.16.m2.1.1.cmml" xref="S3.SS1.SSS2.p1.16.m2.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.16.m2.1c">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.16.m2.1d">caligraphic_C</annotation></semantics></math>, <math alttext="q" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.17.m3.1"><semantics id="S3.SS1.SSS2.p1.17.m3.1a"><mi id="S3.SS1.SSS2.p1.17.m3.1.1" xref="S3.SS1.SSS2.p1.17.m3.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.17.m3.1b"><ci id="S3.SS1.SSS2.p1.17.m3.1.1.cmml" xref="S3.SS1.SSS2.p1.17.m3.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.17.m3.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.17.m3.1d">italic_q</annotation></semantics></math> denotes the user query or instruction, and <math alttext="\mathcal{H}_{t}=\{(x_{s},a_{s})\}_{s=0}^{t-1}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.18.m4.1"><semantics id="S3.SS1.SSS2.p1.18.m4.1a"><mrow id="S3.SS1.SSS2.p1.18.m4.1.1" xref="S3.SS1.SSS2.p1.18.m4.1.1.cmml"><msub id="S3.SS1.SSS2.p1.18.m4.1.1.3" xref="S3.SS1.SSS2.p1.18.m4.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS2.p1.18.m4.1.1.3.2" xref="S3.SS1.SSS2.p1.18.m4.1.1.3.2.cmml">ℋ</mi><mi id="S3.SS1.SSS2.p1.18.m4.1.1.3.3" xref="S3.SS1.SSS2.p1.18.m4.1.1.3.3.cmml">t</mi></msub><mo id="S3.SS1.SSS2.p1.18.m4.1.1.2" xref="S3.SS1.SSS2.p1.18.m4.1.1.2.cmml">=</mo><msubsup id="S3.SS1.SSS2.p1.18.m4.1.1.1" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.cmml"><mrow id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.2.cmml"><mo id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.2.cmml">{</mo><mrow id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.2" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.3.cmml"><mo id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.2.3" stretchy="false" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.3.cmml">(</mo><msub id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.1.1" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.1.1.2" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.1.1.3" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.1.1.3.cmml">s</mi></msub><mo id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.2.4" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.3.cmml">,</mo><msub id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.2.2" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.2.2.2" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.2.2.2.cmml">a</mi><mi id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.2.2.3" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.2.2.3.cmml">s</mi></msub><mo id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.2.5" stretchy="false" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.3.cmml">)</mo></mrow><mo id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.3" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.3.cmml"><mi id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.3.2" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.3.2.cmml">s</mi><mo id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.3.1" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.3.3" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.3.3.cmml">0</mn></mrow><mrow id="S3.SS1.SSS2.p1.18.m4.1.1.1.3" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.3.cmml"><mi id="S3.SS1.SSS2.p1.18.m4.1.1.1.3.2" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.3.2.cmml">t</mi><mo id="S3.SS1.SSS2.p1.18.m4.1.1.1.3.1" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.3.1.cmml">−</mo><mn id="S3.SS1.SSS2.p1.18.m4.1.1.1.3.3" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.3.3.cmml">1</mn></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.18.m4.1b"><apply id="S3.SS1.SSS2.p1.18.m4.1.1.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1"><eq id="S3.SS1.SSS2.p1.18.m4.1.1.2.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1.2"></eq><apply id="S3.SS1.SSS2.p1.18.m4.1.1.3.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.18.m4.1.1.3.1.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1.3">subscript</csymbol><ci id="S3.SS1.SSS2.p1.18.m4.1.1.3.2.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1.3.2">ℋ</ci><ci id="S3.SS1.SSS2.p1.18.m4.1.1.3.3.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1.3.3">𝑡</ci></apply><apply id="S3.SS1.SSS2.p1.18.m4.1.1.1.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.18.m4.1.1.1.2.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1.1">superscript</csymbol><apply id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1.1">subscript</csymbol><set id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1"><interval closure="open" id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.2"><apply id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.1.1.3">𝑠</ci></apply><apply id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.2.2.2">𝑎</ci><ci id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.1.1.1.2.2.3">𝑠</ci></apply></interval></set><apply id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.3.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.3"><eq id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.3.1.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.3.1"></eq><ci id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.3.2.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.3.2">𝑠</ci><cn id="S3.SS1.SSS2.p1.18.m4.1.1.1.1.3.3.cmml" type="integer" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.1.3.3">0</cn></apply></apply><apply id="S3.SS1.SSS2.p1.18.m4.1.1.1.3.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.3"><minus id="S3.SS1.SSS2.p1.18.m4.1.1.1.3.1.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.3.1"></minus><ci id="S3.SS1.SSS2.p1.18.m4.1.1.1.3.2.cmml" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.3.2">𝑡</ci><cn id="S3.SS1.SSS2.p1.18.m4.1.1.1.3.3.cmml" type="integer" xref="S3.SS1.SSS2.p1.18.m4.1.1.1.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.18.m4.1c">\mathcal{H}_{t}=\{(x_{s},a_{s})\}_{s=0}^{t-1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.18.m4.1d">caligraphic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = { ( italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_s = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT</annotation></semantics></math> denotes the history feedback and plans. In its simplest form, a generated plan <math alttext="a_{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.19.m5.1"><semantics id="S3.SS1.SSS2.p1.19.m5.1a"><msub id="S3.SS1.SSS2.p1.19.m5.1.1" xref="S3.SS1.SSS2.p1.19.m5.1.1.cmml"><mi id="S3.SS1.SSS2.p1.19.m5.1.1.2" xref="S3.SS1.SSS2.p1.19.m5.1.1.2.cmml">a</mi><mi id="S3.SS1.SSS2.p1.19.m5.1.1.3" xref="S3.SS1.SSS2.p1.19.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.19.m5.1b"><apply id="S3.SS1.SSS2.p1.19.m5.1.1.cmml" xref="S3.SS1.SSS2.p1.19.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.19.m5.1.1.1.cmml" xref="S3.SS1.SSS2.p1.19.m5.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.19.m5.1.1.2.cmml" xref="S3.SS1.SSS2.p1.19.m5.1.1.2">𝑎</ci><ci id="S3.SS1.SSS2.p1.19.m5.1.1.3.cmml" xref="S3.SS1.SSS2.p1.19.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.19.m5.1c">a_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.19.m5.1d">italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> can simply be a specific action for tool execution. <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.20.m6.1"><semantics id="S3.SS1.SSS2.p1.20.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS2.p1.20.m6.1.1" xref="S3.SS1.SSS2.p1.20.m6.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.20.m6.1b"><ci id="S3.SS1.SSS2.p1.20.m6.1.1.cmml" xref="S3.SS1.SSS2.p1.20.m6.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.20.m6.1c">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.20.m6.1d">caligraphic_C</annotation></semantics></math> can also synergize its reasoning process with the action prediction, where <math alttext="a_{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.21.m7.1"><semantics id="S3.SS1.SSS2.p1.21.m7.1a"><msub id="S3.SS1.SSS2.p1.21.m7.1.1" xref="S3.SS1.SSS2.p1.21.m7.1.1.cmml"><mi id="S3.SS1.SSS2.p1.21.m7.1.1.2" xref="S3.SS1.SSS2.p1.21.m7.1.1.2.cmml">a</mi><mi id="S3.SS1.SSS2.p1.21.m7.1.1.3" xref="S3.SS1.SSS2.p1.21.m7.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.21.m7.1b"><apply id="S3.SS1.SSS2.p1.21.m7.1.1.cmml" xref="S3.SS1.SSS2.p1.21.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.21.m7.1.1.1.cmml" xref="S3.SS1.SSS2.p1.21.m7.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.21.m7.1.1.2.cmml" xref="S3.SS1.SSS2.p1.21.m7.1.1.2">𝑎</ci><ci id="S3.SS1.SSS2.p1.21.m7.1.1.3.cmml" xref="S3.SS1.SSS2.p1.21.m7.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.21.m7.1c">a_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.21.m7.1d">italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> may additionally contain the reasoning traces that explain which sub-task should be solved next and which tool to choose for solving the sub-task. It is worth noting that if the dependence on <math alttext="x_{s}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.22.m8.1"><semantics id="S3.SS1.SSS2.p1.22.m8.1a"><msub id="S3.SS1.SSS2.p1.22.m8.1.1" xref="S3.SS1.SSS2.p1.22.m8.1.1.cmml"><mi id="S3.SS1.SSS2.p1.22.m8.1.1.2" xref="S3.SS1.SSS2.p1.22.m8.1.1.2.cmml">x</mi><mi id="S3.SS1.SSS2.p1.22.m8.1.1.3" xref="S3.SS1.SSS2.p1.22.m8.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.22.m8.1b"><apply id="S3.SS1.SSS2.p1.22.m8.1.1.cmml" xref="S3.SS1.SSS2.p1.22.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.22.m8.1.1.1.cmml" xref="S3.SS1.SSS2.p1.22.m8.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.22.m8.1.1.2.cmml" xref="S3.SS1.SSS2.p1.22.m8.1.1.2">𝑥</ci><ci id="S3.SS1.SSS2.p1.22.m8.1.1.3.cmml" xref="S3.SS1.SSS2.p1.22.m8.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.22.m8.1c">x_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.22.m8.1d">italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> is removed from Equation (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.E1" title="Equation 1 ‣ 3.1.2 Connecting the Components ‣ 3.1 Components of Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">1</span></a>), the resulting probability distribution becomes equivalent to autoregressive language modeling. From this perspective, the controller additionally grounds the foundation model to the environment and the tool set. Moreover, we can factorize Equation (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.E1" title="Equation 1 ‣ 3.1.2 Connecting the Components ‣ 3.1 Components of Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">1</span></a>) as follows:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A1.EGx2">
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle p_{\theta_{\mathcal{C}}}(a_{t}\mid x_{t},\mathcal{H}_{t},q)=\sum%
_{\mathcal{T}_{i}\in\mathcal{T}}p_{\theta_{\mathcal{C}}}(a_{t}\mid\mathcal{T}_%
{i},x_{t},\mathcal{H}_{t},q)\times p_{\theta_{\mathcal{C}}}(\mathcal{T}_{i}%
\mid x_{t},\mathcal{H}_{t},q)," class="ltx_Math" display="inline" id="S3.E2.m1.4"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4.1" xref="S3.E2.m1.4.4.1.1.cmml"><mrow id="S3.E2.m1.4.4.1.1" xref="S3.E2.m1.4.4.1.1.cmml"><mrow id="S3.E2.m1.4.4.1.1.1" xref="S3.E2.m1.4.4.1.1.1.cmml"><msub id="S3.E2.m1.4.4.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.3.cmml"><mi id="S3.E2.m1.4.4.1.1.1.3.2" xref="S3.E2.m1.4.4.1.1.1.3.2.cmml">p</mi><msub id="S3.E2.m1.4.4.1.1.1.3.3" xref="S3.E2.m1.4.4.1.1.1.3.3.cmml"><mi id="S3.E2.m1.4.4.1.1.1.3.3.2" xref="S3.E2.m1.4.4.1.1.1.3.3.2.cmml">θ</mi><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.4.4.1.1.1.3.3.3" xref="S3.E2.m1.4.4.1.1.1.3.3.3.cmml">𝒞</mi></msub></msub><mo id="S3.E2.m1.4.4.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.4.4.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.4.4.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.4.4.1.1.1.1.1.1.4" xref="S3.E2.m1.4.4.1.1.1.1.1.1.4.cmml"><mi id="S3.E2.m1.4.4.1.1.1.1.1.1.4.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.4.2.cmml">a</mi><mi id="S3.E2.m1.4.4.1.1.1.1.1.1.4.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.4.3.cmml">t</mi></msub><mo id="S3.E2.m1.4.4.1.1.1.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.cmml">∣</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1.1.2.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2.3.cmml"><msub id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.E2.m1.4.4.1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E2.m1.4.4.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.4.4.1.1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2.2.2.2.cmml">ℋ</mi><mi id="S3.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.cmml">t</mi></msub><mo id="S3.E2.m1.4.4.1.1.1.1.1.1.2.2.4" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2.3.cmml">,</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">q</mi></mrow></mrow><mo id="S3.E2.m1.4.4.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.4.4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.1.1.4" xref="S3.E2.m1.4.4.1.1.4.cmml">=</mo><mrow id="S3.E2.m1.4.4.1.1.3" xref="S3.E2.m1.4.4.1.1.3.cmml"><mstyle displaystyle="true" id="S3.E2.m1.4.4.1.1.3.3" xref="S3.E2.m1.4.4.1.1.3.3.cmml"><munder id="S3.E2.m1.4.4.1.1.3.3a" xref="S3.E2.m1.4.4.1.1.3.3.cmml"><mo id="S3.E2.m1.4.4.1.1.3.3.2" movablelimits="false" xref="S3.E2.m1.4.4.1.1.3.3.2.cmml">∑</mo><mrow id="S3.E2.m1.4.4.1.1.3.3.3" xref="S3.E2.m1.4.4.1.1.3.3.3.cmml"><msub id="S3.E2.m1.4.4.1.1.3.3.3.2" xref="S3.E2.m1.4.4.1.1.3.3.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.4.4.1.1.3.3.3.2.2" xref="S3.E2.m1.4.4.1.1.3.3.3.2.2.cmml">𝒯</mi><mi id="S3.E2.m1.4.4.1.1.3.3.3.2.3" xref="S3.E2.m1.4.4.1.1.3.3.3.2.3.cmml">i</mi></msub><mo id="S3.E2.m1.4.4.1.1.3.3.3.1" xref="S3.E2.m1.4.4.1.1.3.3.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.4.4.1.1.3.3.3.3" xref="S3.E2.m1.4.4.1.1.3.3.3.3.cmml">𝒯</mi></mrow></munder></mstyle><mrow id="S3.E2.m1.4.4.1.1.3.2" xref="S3.E2.m1.4.4.1.1.3.2.cmml"><mrow id="S3.E2.m1.4.4.1.1.2.1.1" xref="S3.E2.m1.4.4.1.1.2.1.1.cmml"><mrow id="S3.E2.m1.4.4.1.1.2.1.1.1" xref="S3.E2.m1.4.4.1.1.2.1.1.1.cmml"><msub id="S3.E2.m1.4.4.1.1.2.1.1.1.3" xref="S3.E2.m1.4.4.1.1.2.1.1.1.3.cmml"><mi id="S3.E2.m1.4.4.1.1.2.1.1.1.3.2" xref="S3.E2.m1.4.4.1.1.2.1.1.1.3.2.cmml">p</mi><msub id="S3.E2.m1.4.4.1.1.2.1.1.1.3.3" xref="S3.E2.m1.4.4.1.1.2.1.1.1.3.3.cmml"><mi id="S3.E2.m1.4.4.1.1.2.1.1.1.3.3.2" xref="S3.E2.m1.4.4.1.1.2.1.1.1.3.3.2.cmml">θ</mi><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.4.4.1.1.2.1.1.1.3.3.3" xref="S3.E2.m1.4.4.1.1.2.1.1.1.3.3.3.cmml">𝒞</mi></msub></msub><mo id="S3.E2.m1.4.4.1.1.2.1.1.1.2" xref="S3.E2.m1.4.4.1.1.2.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.5" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.5.cmml"><mi id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.5.2" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.5.2.cmml">a</mi><mi id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.5.3" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.5.3.cmml">t</mi></msub><mo id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.4" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.4.cmml">∣</mo><mrow id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.3.3" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.3.4.cmml"><msub id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.1.1.1.2.cmml">𝒯</mi><mi id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.3.3.4" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.2.2.2.2.cmml">x</mi><mi id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.2.2.2.3" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.2.2.2.3.cmml">t</mi></msub><mo id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.3.3.5" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.3.3.3" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.3.3.3.2" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.3.3.3.2.cmml">ℋ</mi><mi id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.3.3.3.3" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.3.3.3.3.cmml">t</mi></msub><mo id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.3.3.6" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.3.4.cmml">,</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">q</mi></mrow></mrow><mo id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.1.1.2.1.1.2" rspace="0.222em" xref="S3.E2.m1.4.4.1.1.2.1.1.2.cmml">×</mo><msub id="S3.E2.m1.4.4.1.1.2.1.1.3" xref="S3.E2.m1.4.4.1.1.2.1.1.3.cmml"><mi id="S3.E2.m1.4.4.1.1.2.1.1.3.2" xref="S3.E2.m1.4.4.1.1.2.1.1.3.2.cmml">p</mi><msub id="S3.E2.m1.4.4.1.1.2.1.1.3.3" xref="S3.E2.m1.4.4.1.1.2.1.1.3.3.cmml"><mi id="S3.E2.m1.4.4.1.1.2.1.1.3.3.2" xref="S3.E2.m1.4.4.1.1.2.1.1.3.3.2.cmml">θ</mi><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.4.4.1.1.2.1.1.3.3.3" xref="S3.E2.m1.4.4.1.1.2.1.1.3.3.3.cmml">𝒞</mi></msub></msub></mrow><mo id="S3.E2.m1.4.4.1.1.3.2.3" xref="S3.E2.m1.4.4.1.1.3.2.3.cmml">⁢</mo><mrow id="S3.E2.m1.4.4.1.1.3.2.2.1" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.cmml"><mo id="S3.E2.m1.4.4.1.1.3.2.2.1.2" stretchy="false" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.cmml">(</mo><mrow id="S3.E2.m1.4.4.1.1.3.2.2.1.1" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.cmml"><msub id="S3.E2.m1.4.4.1.1.3.2.2.1.1.4" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.4.4.1.1.3.2.2.1.1.4.2" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.4.2.cmml">𝒯</mi><mi id="S3.E2.m1.4.4.1.1.3.2.2.1.1.4.3" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.4.3.cmml">i</mi></msub><mo id="S3.E2.m1.4.4.1.1.3.2.2.1.1.3" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.3.cmml">∣</mo><mrow id="S3.E2.m1.4.4.1.1.3.2.2.1.1.2.2" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.2.3.cmml"><msub id="S3.E2.m1.4.4.1.1.3.2.2.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.1.1.1.cmml"><mi id="S3.E2.m1.4.4.1.1.3.2.2.1.1.1.1.1.2" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.1.1.1.2.cmml">x</mi><mi id="S3.E2.m1.4.4.1.1.3.2.2.1.1.1.1.1.3" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.E2.m1.4.4.1.1.3.2.2.1.1.2.2.3" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.2.3.cmml">,</mo><msub id="S3.E2.m1.4.4.1.1.3.2.2.1.1.2.2.2" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.4.4.1.1.3.2.2.1.1.2.2.2.2" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.2.2.2.2.cmml">ℋ</mi><mi id="S3.E2.m1.4.4.1.1.3.2.2.1.1.2.2.2.3" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.2.2.2.3.cmml">t</mi></msub><mo id="S3.E2.m1.4.4.1.1.3.2.2.1.1.2.2.4" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.2.3.cmml">,</mo><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">q</mi></mrow></mrow><mo id="S3.E2.m1.4.4.1.1.3.2.2.1.3" stretchy="false" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E2.m1.4.4.1.2" xref="S3.E2.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.4.1.1.cmml" xref="S3.E2.m1.4.4.1"><eq id="S3.E2.m1.4.4.1.1.4.cmml" xref="S3.E2.m1.4.4.1.1.4"></eq><apply id="S3.E2.m1.4.4.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1"><times id="S3.E2.m1.4.4.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1.2"></times><apply id="S3.E2.m1.4.4.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.1.3.1.cmml" xref="S3.E2.m1.4.4.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.1.3.2.cmml" xref="S3.E2.m1.4.4.1.1.1.3.2">𝑝</ci><apply id="S3.E2.m1.4.4.1.1.1.3.3.cmml" xref="S3.E2.m1.4.4.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.1.3.3.1.cmml" xref="S3.E2.m1.4.4.1.1.1.3.3">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.1.3.3.2.cmml" xref="S3.E2.m1.4.4.1.1.1.3.3.2">𝜃</ci><ci id="S3.E2.m1.4.4.1.1.1.3.3.3.cmml" xref="S3.E2.m1.4.4.1.1.1.3.3.3">𝒞</ci></apply></apply><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3">conditional</csymbol><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.4.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.1.1.1.1.4.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.4">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.4.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.4.2">𝑎</ci><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.4.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.4.3">𝑡</ci></apply><list id="S3.E2.m1.4.4.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2.2"><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3">𝑡</ci></apply><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2.2.2.2">ℋ</ci><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3">𝑡</ci></apply><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑞</ci></list></apply></apply><apply id="S3.E2.m1.4.4.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.3"><apply id="S3.E2.m1.4.4.1.1.3.3.cmml" xref="S3.E2.m1.4.4.1.1.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.3.3.1.cmml" xref="S3.E2.m1.4.4.1.1.3.3">subscript</csymbol><sum id="S3.E2.m1.4.4.1.1.3.3.2.cmml" xref="S3.E2.m1.4.4.1.1.3.3.2"></sum><apply id="S3.E2.m1.4.4.1.1.3.3.3.cmml" xref="S3.E2.m1.4.4.1.1.3.3.3"><in id="S3.E2.m1.4.4.1.1.3.3.3.1.cmml" xref="S3.E2.m1.4.4.1.1.3.3.3.1"></in><apply id="S3.E2.m1.4.4.1.1.3.3.3.2.cmml" xref="S3.E2.m1.4.4.1.1.3.3.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.3.3.3.2.1.cmml" xref="S3.E2.m1.4.4.1.1.3.3.3.2">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.3.3.3.2.2.cmml" xref="S3.E2.m1.4.4.1.1.3.3.3.2.2">𝒯</ci><ci id="S3.E2.m1.4.4.1.1.3.3.3.2.3.cmml" xref="S3.E2.m1.4.4.1.1.3.3.3.2.3">𝑖</ci></apply><ci id="S3.E2.m1.4.4.1.1.3.3.3.3.cmml" xref="S3.E2.m1.4.4.1.1.3.3.3.3">𝒯</ci></apply></apply><apply id="S3.E2.m1.4.4.1.1.3.2.cmml" xref="S3.E2.m1.4.4.1.1.3.2"><times id="S3.E2.m1.4.4.1.1.3.2.3.cmml" xref="S3.E2.m1.4.4.1.1.3.2.3"></times><apply id="S3.E2.m1.4.4.1.1.2.1.1.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1"><times id="S3.E2.m1.4.4.1.1.2.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.2"></times><apply id="S3.E2.m1.4.4.1.1.2.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1"><times id="S3.E2.m1.4.4.1.1.2.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.2"></times><apply id="S3.E2.m1.4.4.1.1.2.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.2.1.1.1.3.1.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.2.1.1.1.3.2.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.3.2">𝑝</ci><apply id="S3.E2.m1.4.4.1.1.2.1.1.1.3.3.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.2.1.1.1.3.3.1.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.3.3">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.2.1.1.1.3.3.2.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.3.3.2">𝜃</ci><ci id="S3.E2.m1.4.4.1.1.2.1.1.1.3.3.3.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.3.3.3">𝒞</ci></apply></apply><apply id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.4.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.4">conditional</csymbol><apply id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.5.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.5.1.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.5">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.5.2.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.5.2">𝑎</ci><ci id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.5.3.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.5.3">𝑡</ci></apply><list id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.3.4.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.3.3"><apply id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.1.1.1.2">𝒯</ci><ci id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.2.2.2.2">𝑥</ci><ci id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.2.2.2.3">𝑡</ci></apply><apply id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.3.3.3.1.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.3.3.3.2.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.3.3.3.2">ℋ</ci><ci id="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.3.3.3.3.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.1.1.1.1.3.3.3.3">𝑡</ci></apply><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝑞</ci></list></apply></apply><apply id="S3.E2.m1.4.4.1.1.2.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.2.1.1.3.1.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.3">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.2.1.1.3.2.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.3.2">𝑝</ci><apply id="S3.E2.m1.4.4.1.1.2.1.1.3.3.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.2.1.1.3.3.1.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.3.3">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.2.1.1.3.3.2.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.3.3.2">𝜃</ci><ci id="S3.E2.m1.4.4.1.1.2.1.1.3.3.3.cmml" xref="S3.E2.m1.4.4.1.1.2.1.1.3.3.3">𝒞</ci></apply></apply></apply><apply id="S3.E2.m1.4.4.1.1.3.2.2.1.1.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.1"><csymbol cd="latexml" id="S3.E2.m1.4.4.1.1.3.2.2.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.3">conditional</csymbol><apply id="S3.E2.m1.4.4.1.1.3.2.2.1.1.4.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.4"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.3.2.2.1.1.4.1.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.4">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.3.2.2.1.1.4.2.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.4.2">𝒯</ci><ci id="S3.E2.m1.4.4.1.1.3.2.2.1.1.4.3.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.4.3">𝑖</ci></apply><list id="S3.E2.m1.4.4.1.1.3.2.2.1.1.2.3.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.2.2"><apply id="S3.E2.m1.4.4.1.1.3.2.2.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.3.2.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.3.2.2.1.1.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.1.1.1.2">𝑥</ci><ci id="S3.E2.m1.4.4.1.1.3.2.2.1.1.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.1.1.1.3">𝑡</ci></apply><apply id="S3.E2.m1.4.4.1.1.3.2.2.1.1.2.2.2.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.3.2.2.1.1.2.2.2.1.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.2.2.2">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.3.2.2.1.1.2.2.2.2.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.2.2.2.2">ℋ</ci><ci id="S3.E2.m1.4.4.1.1.3.2.2.1.1.2.2.2.3.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.1.1.2.2.2.3">𝑡</ci></apply><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">𝑞</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">\displaystyle p_{\theta_{\mathcal{C}}}(a_{t}\mid x_{t},\mathcal{H}_{t},q)=\sum%
_{\mathcal{T}_{i}\in\mathcal{T}}p_{\theta_{\mathcal{C}}}(a_{t}\mid\mathcal{T}_%
{i},x_{t},\mathcal{H}_{t},q)\times p_{\theta_{\mathcal{C}}}(\mathcal{T}_{i}%
\mid x_{t},\mathcal{H}_{t},q),</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.4d">italic_p start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∣ italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , caligraphic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_q ) = ∑ start_POSTSUBSCRIPT caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ caligraphic_T end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∣ caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , caligraphic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_q ) × italic_p start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∣ italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , caligraphic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_q ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.SSS2.p1.25">The decomposition reveals that the construction of the plan <math alttext="a_{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.23.m1.1"><semantics id="S3.SS1.SSS2.p1.23.m1.1a"><msub id="S3.SS1.SSS2.p1.23.m1.1.1" xref="S3.SS1.SSS2.p1.23.m1.1.1.cmml"><mi id="S3.SS1.SSS2.p1.23.m1.1.1.2" xref="S3.SS1.SSS2.p1.23.m1.1.1.2.cmml">a</mi><mi id="S3.SS1.SSS2.p1.23.m1.1.1.3" xref="S3.SS1.SSS2.p1.23.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.23.m1.1b"><apply id="S3.SS1.SSS2.p1.23.m1.1.1.cmml" xref="S3.SS1.SSS2.p1.23.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.23.m1.1.1.1.cmml" xref="S3.SS1.SSS2.p1.23.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.23.m1.1.1.2.cmml" xref="S3.SS1.SSS2.p1.23.m1.1.1.2">𝑎</ci><ci id="S3.SS1.SSS2.p1.23.m1.1.1.3.cmml" xref="S3.SS1.SSS2.p1.23.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.23.m1.1c">a_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.23.m1.1d">italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> involves two subtasks: selecting the appropriate tool based on the user intent and deciding the actions to execute using the selected tool.
For instance, given an instruction such as “I want to book a flight to Beijing next week”, the controller <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.24.m2.1"><semantics id="S3.SS1.SSS2.p1.24.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS2.p1.24.m2.1.1" xref="S3.SS1.SSS2.p1.24.m2.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.24.m2.1b"><ci id="S3.SS1.SSS2.p1.24.m2.1.1.cmml" xref="S3.SS1.SSS2.p1.24.m2.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.24.m2.1c">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.24.m2.1d">caligraphic_C</annotation></semantics></math> first infers that the user’s goal is to reserve a flight, with Beijing as the destination and the next week as the travel time. The model then selects the airline reservation system as the tool. Finally, it inputs the time and destination as the preliminary plan. In the process of making a reservation, we may face unexpected situations such as the unavailability of flights to Beijing in the next week. To cope with these anomalies, we can further equip <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.25.m3.1"><semantics id="S3.SS1.SSS2.p1.25.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS2.p1.25.m3.1.1" xref="S3.SS1.SSS2.p1.25.m3.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.25.m3.1b"><ci id="S3.SS1.SSS2.p1.25.m3.1.1.cmml" xref="S3.SS1.SSS2.p1.25.m3.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.25.m3.1c">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.25.m3.1d">caligraphic_C</annotation></semantics></math> with the ability to reason about the current context and generate alternative plans, as we will discuss in detail in <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2.SSS2" title="3.2.2 Planning with Reasoning ‣ 3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.2.2</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.6">After a plan <math alttext="a_{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p2.1.m1.1"><semantics id="S3.SS1.SSS2.p2.1.m1.1a"><msub id="S3.SS1.SSS2.p2.1.m1.1.1" xref="S3.SS1.SSS2.p2.1.m1.1.1.cmml"><mi id="S3.SS1.SSS2.p2.1.m1.1.1.2" xref="S3.SS1.SSS2.p2.1.m1.1.1.2.cmml">a</mi><mi id="S3.SS1.SSS2.p2.1.m1.1.1.3" xref="S3.SS1.SSS2.p2.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.1.m1.1b"><apply id="S3.SS1.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p2.1.m1.1.1.1.cmml" xref="S3.SS1.SSS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p2.1.m1.1.1.2.cmml" xref="S3.SS1.SSS2.p2.1.m1.1.1.2">𝑎</ci><ci id="S3.SS1.SSS2.p2.1.m1.1.1.3.cmml" xref="S3.SS1.SSS2.p2.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.1.m1.1c">a_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p2.1.m1.1d">italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is generated, it will be executed in <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p2.2.m2.1"><semantics id="S3.SS1.SSS2.p2.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS2.p2.2.m2.1.1" xref="S3.SS1.SSS2.p2.2.m2.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.2.m2.1b"><ci id="S3.SS1.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p2.2.m2.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.2.m2.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p2.2.m2.1d">caligraphic_E</annotation></semantics></math>, and the resulting feedback <math alttext="e_{t+1}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p2.3.m3.1"><semantics id="S3.SS1.SSS2.p2.3.m3.1a"><msub id="S3.SS1.SSS2.p2.3.m3.1.1" xref="S3.SS1.SSS2.p2.3.m3.1.1.cmml"><mi id="S3.SS1.SSS2.p2.3.m3.1.1.2" xref="S3.SS1.SSS2.p2.3.m3.1.1.2.cmml">e</mi><mrow id="S3.SS1.SSS2.p2.3.m3.1.1.3" xref="S3.SS1.SSS2.p2.3.m3.1.1.3.cmml"><mi id="S3.SS1.SSS2.p2.3.m3.1.1.3.2" xref="S3.SS1.SSS2.p2.3.m3.1.1.3.2.cmml">t</mi><mo id="S3.SS1.SSS2.p2.3.m3.1.1.3.1" xref="S3.SS1.SSS2.p2.3.m3.1.1.3.1.cmml">+</mo><mn id="S3.SS1.SSS2.p2.3.m3.1.1.3.3" xref="S3.SS1.SSS2.p2.3.m3.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.3.m3.1b"><apply id="S3.SS1.SSS2.p2.3.m3.1.1.cmml" xref="S3.SS1.SSS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p2.3.m3.1.1.1.cmml" xref="S3.SS1.SSS2.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p2.3.m3.1.1.2.cmml" xref="S3.SS1.SSS2.p2.3.m3.1.1.2">𝑒</ci><apply id="S3.SS1.SSS2.p2.3.m3.1.1.3.cmml" xref="S3.SS1.SSS2.p2.3.m3.1.1.3"><plus id="S3.SS1.SSS2.p2.3.m3.1.1.3.1.cmml" xref="S3.SS1.SSS2.p2.3.m3.1.1.3.1"></plus><ci id="S3.SS1.SSS2.p2.3.m3.1.1.3.2.cmml" xref="S3.SS1.SSS2.p2.3.m3.1.1.3.2">𝑡</ci><cn id="S3.SS1.SSS2.p2.3.m3.1.1.3.3.cmml" type="integer" xref="S3.SS1.SSS2.p2.3.m3.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.3.m3.1c">e_{t+1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p2.3.m3.1d">italic_e start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT</annotation></semantics></math> from <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p2.4.m4.1"><semantics id="S3.SS1.SSS2.p2.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS2.p2.4.m4.1.1" xref="S3.SS1.SSS2.p2.4.m4.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.4.m4.1b"><ci id="S3.SS1.SSS2.p2.4.m4.1.1.cmml" xref="S3.SS1.SSS2.p2.4.m4.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.4.m4.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p2.4.m4.1d">caligraphic_E</annotation></semantics></math> will be passed on to the perceiver.
The above process repeats for multiple rounds until the controller accomplishes the task. The overall objective is to find an action sequence <math alttext="\{a_{t}\}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p2.5.m5.1"><semantics id="S3.SS1.SSS2.p2.5.m5.1a"><mrow id="S3.SS1.SSS2.p2.5.m5.1.1.1" xref="S3.SS1.SSS2.p2.5.m5.1.1.2.cmml"><mo id="S3.SS1.SSS2.p2.5.m5.1.1.1.2" stretchy="false" xref="S3.SS1.SSS2.p2.5.m5.1.1.2.cmml">{</mo><msub id="S3.SS1.SSS2.p2.5.m5.1.1.1.1" xref="S3.SS1.SSS2.p2.5.m5.1.1.1.1.cmml"><mi id="S3.SS1.SSS2.p2.5.m5.1.1.1.1.2" xref="S3.SS1.SSS2.p2.5.m5.1.1.1.1.2.cmml">a</mi><mi id="S3.SS1.SSS2.p2.5.m5.1.1.1.1.3" xref="S3.SS1.SSS2.p2.5.m5.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.SS1.SSS2.p2.5.m5.1.1.1.3" stretchy="false" xref="S3.SS1.SSS2.p2.5.m5.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.5.m5.1b"><set id="S3.SS1.SSS2.p2.5.m5.1.1.2.cmml" xref="S3.SS1.SSS2.p2.5.m5.1.1.1"><apply id="S3.SS1.SSS2.p2.5.m5.1.1.1.1.cmml" xref="S3.SS1.SSS2.p2.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p2.5.m5.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.p2.5.m5.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p2.5.m5.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.p2.5.m5.1.1.1.1.2">𝑎</ci><ci id="S3.SS1.SSS2.p2.5.m5.1.1.1.1.3.cmml" xref="S3.SS1.SSS2.p2.5.m5.1.1.1.1.3">𝑡</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.5.m5.1c">\{a_{t}\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p2.5.m5.1d">{ italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT }</annotation></semantics></math> that ultimately fulfills the task specified by the user instruction <math alttext="q" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p2.6.m6.1"><semantics id="S3.SS1.SSS2.p2.6.m6.1a"><mi id="S3.SS1.SSS2.p2.6.m6.1.1" xref="S3.SS1.SSS2.p2.6.m6.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.6.m6.1b"><ci id="S3.SS1.SSS2.p2.6.m6.1.1.cmml" xref="S3.SS1.SSS2.p2.6.m6.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.6.m6.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p2.6.m6.1d">italic_q</annotation></semantics></math>. Note after tool execution, the controller may additionally integrate the execution results into a plausible response for the user (see details in <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS6" title="5.6 Knowledge Conflicts in Tool Augmentation ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">5.6</span></a>).</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>The General Procedure: From Intent to Plan</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">As formulated in <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS1.SSS2" title="3.1.2 Connecting the Components ‣ 3.1 Components of Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.1.2</span></a>, the general procedure of tool learning necessitates intricate interplay among different components. In this section, we will further elaborate on the key issues involved in this procedure.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Understanding Intent and Tools</h4>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.5">To accurately fulfill the task specified by the user query <math alttext="q" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.1.m1.1"><semantics id="S3.SS2.SSS1.p1.1.m1.1a"><mi id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b"><ci id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.1.m1.1d">italic_q</annotation></semantics></math>, the controller needs to understand two aspects: (1) the underlying intent of the user, which involves recognizing and formalizing the natural language <math alttext="q" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.2.m2.1"><semantics id="S3.SS2.SSS1.p1.2.m2.1a"><mi id="S3.SS2.SSS1.p1.2.m2.1.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.2.m2.1b"><ci id="S3.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.2.m2.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.2.m2.1d">italic_q</annotation></semantics></math> as a high-level task (i.e., <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p1.5.1">intent understanding</span>); (2) the tool set <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.3.m3.1"><semantics id="S3.SS2.SSS1.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS1.p1.3.m3.1.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.3.m3.1b"><ci id="S3.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.3.m3.1c">\mathcal{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.3.m3.1d">caligraphic_T</annotation></semantics></math>, which entails comprehending the functionality and objective of each tool within it (i.e., <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p1.5.2">tool understanding</span>). By understanding both aspects, the controller can bridge the gap between the user’s intent and the tool set, which is the pre-requisite for connecting controller <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.4.m4.1"><semantics id="S3.SS2.SSS1.p1.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS1.p1.4.m4.1.1" xref="S3.SS2.SSS1.p1.4.m4.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.4.m4.1b"><ci id="S3.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.4.m4.1c">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.4.m4.1d">caligraphic_C</annotation></semantics></math>, the user, and tool set <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.5.m5.1"><semantics id="S3.SS2.SSS1.p1.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS1.p1.5.m5.1.1" xref="S3.SS2.SSS1.p1.5.m5.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.5.m5.1b"><ci id="S3.SS2.SSS1.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.5.m5.1c">\mathcal{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.5.m5.1d">caligraphic_T</annotation></semantics></math> in Figure <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.F3" title="Figure 3 ‣ Perceiver. ‣ 3.1.1 Understanding the Components ‣ 3.1 Components of Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Intent Understanding.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.Px1.p1.1">Understanding user intent is a long-standing research topic in NLP <cite class="ltx_cite ltx_citemacro_citep">(Jansen et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib59" title="">2007</a>; Sukthankar et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib156" title="">2014</a>)</cite>, which involves comprehending the underlying purpose of a user query. Intent understanding is essential in scenarios requiring human-computer interaction, such as developing advanced conversational agents capable of conducting intricate and nuanced dialogues with users. It requires learning a mapping from the instruction space to the model’s cognition space. By accurately identifying the user intent, the controller can provide more personalized responses with a better user experience.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.Px1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.Px1.p2.1">Recent explorations in instruction tuning <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib180" title="">2022a</a>)</cite> demonstrate that foundation models can possess extraordinary proficiency in comprehending user instructions. Prior work has shown that fine-tuning large language models on a collection of datasets templated with human instructions allows models to generalize even to instructions for unseen tasks <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib180" title="">2022a</a>; Mishra et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib105" title="">2022</a>; Sanh et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib137" title="">2022</a>; Bach et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib9" title="">2022</a>; Ouyang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib120" title="">2022</a>)</cite>. Promisingly, such generalization ability can further be enhanced by scaling up both the model size and the quantity or diversity of training instructions <cite class="ltx_cite ltx_citemacro_citep">(Iyer et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib57" title="">2022</a>)</cite>. Despite the impressive intent understanding capabilities, challenges still exist in real-world tool learning scenarios:
(1) <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.Px1.p2.1.1">Understanding Vague Instructions.</span> The first challenge is dealing with the inherent vagueness and ambiguity in the user query. Many user queries are inherently imprecise and can even be polysemous, requiring the controller to rely on contextual cues and background knowledge to infer the user’s intended meaning. One possible solution is to actively interact with users to clarify any ambiguity, such as asking for clarifications about a previous user query.
(2) <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.Px1.p2.1.2">Generalization to Diverse Instructions.</span> Another challenge is having the models generalize to more diverse user instructions. As the intent space is theoretically infinite, it is almost impractical for foundation models to be exposed to every real-world intention during training. In addition, the challenge of personalization arises from the fact that each individual has their own unique way of expressing intentions, which requires the model to adapt to the diverse expressions of intent of different individuals. One solution is to incorporate more diverse training data that covers a wide range of real-world scenarios, thereby enabling the models to learn the nuances of different instructions. Another solution is to leverage user feedback and actively adapt the model to individual users, i.e., personalized tool learning (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS4" title="5.4 From General Intelligence to Personalized Intelligence ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">5.4</span></a>).</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Tool Understanding.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS1.Px2.p1.1">As noted by <cite class="ltx_cite ltx_citemacro_citet">Hernik &amp; Csibra (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib51" title="">2009</a>)</cite>, when learning to utilize a specific tool, human beings perceive it as an object with particular functions, engaging in a cognitive process to understand its purpose and operation. By observing goal-directed demonstrations and following actions performed by other people, they gradually acquire the necessary knowledge and skills to use the tools effectively. Similarly, this understanding process is crucial for successfully solving tasks with tools. Analogously, a comprehensive understanding of the tools’ functionalities is indispensable for enabling the controller to use tools proficiently. The process of tool understanding encompasses grasping what the tool is used for and how to use the tool. Take the case of a calculator: the controller needs to know that a calculator is intended for arithmetic operations, its input should be numbers and mathematical operators, and its output should be a computed value.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.Px2.p2">
<p class="ltx_p" id="S3.SS2.SSS1.Px2.p2.1">In real-world scenarios, tools are typically accompanied by a manual (or tutorial), which provides sufficient relevant details about their functionalities and usage. Endowed with strong few-shot learning <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib19" title="">2020</a>)</cite> and zero-shot learning <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib180" title="">2022a</a>)</cite> capabilities, foundation models can be prompted to unravel tools’ functionalities and comprehend how to use them. To this end, we can construct suitable task-specific prompts either through manual design <cite class="ltx_cite ltx_citemacro_citep">(Vemprala et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib166" title="">2023</a>)</cite> or retrievial <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib201" title="">2023</a>)</cite>. These prompts should describe the API functionalities or exemplify with demonstrations of their usage.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="400" id="S3.F4.g1" src="x4.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Examples of zero-shot and few-shot prompting for tool understanding. The prompts are constructed by describing the functionalities (zero-shot prompting) or giving usage examples (few-shot prompting) of a weather API.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.Px2.p3">
<p class="ltx_p" id="S3.SS2.SSS1.Px2.p3.1">We categorize two prompting approaches as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.F4" title="Figure 4 ‣ Tool Understanding. ‣ 3.2.1 Understanding Intent and Tools ‣ 3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">4</span></a>: (1) <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.Px2.p3.1.1">zero-shot prompting</span>, which describes API functionalities, their input/output formats, possible parameters, etc. This approach allows the model to understand the tasks that each API can tackle; (2) <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.Px2.p3.1.2">few-shot prompting</span>, which provides concrete tool-use demonstrations to the model. By mimicking human behaviors from these demonstrations, the model can learn how to utilize these tools. We provide experimental results of both prompting methods in <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S4" title="4 Application and Experiment ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.Px2.p4">
<p class="ltx_p" id="S3.SS2.SSS1.Px2.p4.1">Prompting has been widely adopted as a lightweight approach to teach foundation models about tools <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib194" title="">2022b</a>; Driess et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib37" title="">2023</a>; OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib114" title="">2023</a>)</cite> with minimum human effort. Prompts can be easily adjusted to accommodate changes of tools. For instance, when tools are modified or upgraded, we can flexibly rewrite the prompts to adapt the model behaviors. Despite these advantages, prompting methods still face several challenges. First, since the effectiveness of prompting depends a lot on the model, smaller or less capable models cannot understand prompts well. Second, prompting is restricted by input context length. Although foundation models have been shown to learn to use simple tools through prompts, the situation may be more challenging with multiple complex tools with long descriptions. Especially when the tool set greatly expands, providing all possible tools within a prompt becomes infeasible given the limited context length.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.Px2.p5">
<p class="ltx_p" id="S3.SS2.SSS1.Px2.p5.1">A potential solution is to add an intermediate stage of tool selection, which first retrieves a small set of tools that are most suitable for the task at hand. Another solution is fine-tuning, which optimizes models with concrete tool-use examples to understand tools. This process involves leveraging the rich knowledge obtained from human tool-use experiences. When deployed in practice, a fine-tuned model alleviates the need for incorporating tool definitions in the input, which shrinks the input length and accelerates model inference. However, a major limitation of fine-tuning is that it often requires extensive human annotations or demonstrations. Additionally, tools are frequently updated, and fine-tuned models will need to be retrained with updated examples, incurring additional costs. More discussion about the tuning-based solution is left in <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3" title="3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Planning with Reasoning</h4>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">As discussed in <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2.SSS1" title="3.2.1 Understanding Intent and Tools ‣ 3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.2.1</span></a>, understanding the intent and tools lays a solid foundation for planning. Nevertheless, it is still insufficient for tackling intricate tasks. The user query <math alttext="q" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.1.m1.1"><semantics id="S3.SS2.SSS2.p1.1.m1.1a"><mi id="S3.SS2.SSS2.p1.1.m1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.1.m1.1b"><ci id="S3.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.1.m1.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p1.1.m1.1d">italic_q</annotation></semantics></math> often implies a complex task that should be divided into multiple sub-tasks with proper sequencing, thereby necessitating a process of <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p1.1.1">reasoning</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1">Recent research has revealed that reasoning capabilities can emerge when foundation models are scaled up to a certain size <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib181" title="">2022b</a>)</cite>. In particular, foundation models with tens or hundreds of billions of parameters can generate intermediate reasoning traces during complex problem-solving, thereby significantly enhancing their zero-shot and few-shot performances <cite class="ltx_cite ltx_citemacro_citep">(Nakano et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib110" title="">2021</a>; Nye et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib112" title="">2021</a>; Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib181" title="">2022b</a>)</cite>. The reasoning ability observed in the foundation models appears to facilitate a transition from System 1 to System 2 <cite class="ltx_cite ltx_citemacro_citep">(Kahneman, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib63" title="">2011</a>)</cite>, enabling the accomplishment of more complex tasks.</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">Eliciting Reasoning in Foundation Models.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS2.Px1.p1.1">Despite the extensive study of the concept of <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.Px1.p1.1.1">reasoning</span> in the psychology literature <cite class="ltx_cite ltx_citemacro_citep">(Wason, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib179" title="">1968</a>; Kelley, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib66" title="">2013</a>)</cite>, the notion of <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.Px1.p1.1.2">reasoning</span> as applied to foundation models is not clearly defined. However, in general terms, the reasoning ability in the literature of foundation models can be framed as the capacity to decompose a complex problem into sub-problems and solve the sub-problems step-by-step <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib182" title="">2022c</a>; Press et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib126" title="">2022</a>; Khot et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib68" title="">2022</a>)</cite>. Here we keep consistent with these works and discuss reasoning in the sense of problem decomposition and sub-problem solving.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.Px1.p2">
<p class="ltx_p" id="S3.SS2.SSS2.Px1.p2.1">The vanilla few-shot prompt learning <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib19" title="">2020</a>)</cite>, whereby models are provided with a prompt consisting of several examples for the given task, has been shown to fail when it comes to problems that require complex reasoning <cite class="ltx_cite ltx_citemacro_citep">(Creswell et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib33" title="">2022</a>)</cite>. To address this issue, <cite class="ltx_cite ltx_citemacro_citet">Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib182" title="">2022c</a>)</cite> propose Chain-of-Thought (CoT) prompting. Unlike vanilla few-shot prompt learning, CoT additionally inserts the reasoning trace required to derive the final answer for each example in the prompt. In this way, CoT prompts models to generate their “thoughts” on the necessary intermediate steps before arriving at the final answer. CoT has been proven to significantly boost performance on a wide range of tasks, including arithmetic reasoning, commonsense reasoning, and symbolic reasoning <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib182" title="">2022c</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.Px1.p3">
<p class="ltx_p" id="S3.SS2.SSS2.Px1.p3.4">In light of the remarkable reasoning abilities of foundation models, recent research has made successful attempts to employ them in the controller in tool learning. It is demonstrated that their reasoning capabilities enable the controller to effectively decompose a complex problem into several sub-problems, and determine which tool to call upon for each sub-problem. We categorize relevant research into two streams: <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.Px1.p3.4.1">introspective reasoning</span> and <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.Px1.p3.4.2">extrospective reasoning</span>. The former involves generating a static plan of tool use without interacting with the environment <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.Px1.p3.1.m1.1"><semantics id="S3.SS2.SSS2.Px1.p3.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.Px1.p3.1.m1.1.1" xref="S3.SS2.SSS2.Px1.p3.1.m1.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.Px1.p3.1.m1.1b"><ci id="S3.SS2.SSS2.Px1.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS2.Px1.p3.1.m1.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.Px1.p3.1.m1.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.Px1.p3.1.m1.1d">caligraphic_E</annotation></semantics></math>, while the latter generates plans incrementally by iteratively interacting with <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.Px1.p3.2.m2.1"><semantics id="S3.SS2.SSS2.Px1.p3.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.Px1.p3.2.m2.1.1" xref="S3.SS2.SSS2.Px1.p3.2.m2.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.Px1.p3.2.m2.1b"><ci id="S3.SS2.SSS2.Px1.p3.2.m2.1.1.cmml" xref="S3.SS2.SSS2.Px1.p3.2.m2.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.Px1.p3.2.m2.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.Px1.p3.2.m2.1d">caligraphic_E</annotation></semantics></math> and utilizing feedback obtained from previous executions. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.F5" title="Figure 5 ‣ Introspective Reasoning. ‣ 3.2.2 Planning with Reasoning ‣ 3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">5</span></a>, the environment <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.Px1.p3.3.m3.1"><semantics id="S3.SS2.SSS2.Px1.p3.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.Px1.p3.3.m3.1.1" xref="S3.SS2.SSS2.Px1.p3.3.m3.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.Px1.p3.3.m3.1b"><ci id="S3.SS2.SSS2.Px1.p3.3.m3.1.1.cmml" xref="S3.SS2.SSS2.Px1.p3.3.m3.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.Px1.p3.3.m3.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.Px1.p3.3.m3.1d">caligraphic_E</annotation></semantics></math> is invisible to the controller <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.Px1.p3.4.m4.1"><semantics id="S3.SS2.SSS2.Px1.p3.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.Px1.p3.4.m4.1.1" xref="S3.SS2.SSS2.Px1.p3.4.m4.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.Px1.p3.4.m4.1b"><ci id="S3.SS2.SSS2.Px1.p3.4.m4.1.1.cmml" xref="S3.SS2.SSS2.Px1.p3.4.m4.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.Px1.p3.4.m4.1c">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.Px1.p3.4.m4.1d">caligraphic_C</annotation></semantics></math> in introspective reasoning but is visible in extrospective reasoning, creating a closed-loop interaction among the four components.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">Introspective Reasoning.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.Px2.p1.1">This kind of reasoning directly generates multi-step plans for tool use without knowing intermediate execution results. For instance, <cite class="ltx_cite ltx_citemacro_citet">Huang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib53" title="">2022a</a>)</cite> investigate the planning ability of foundation models and show that they are capable of decomposing high-level tasks into semantically plausible sub-plans. Another representative work is Program-Aided Language Models (PAL) <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib44" title="">2022</a>)</cite>, which prompts models to generate Python codes for intermediate reasoning steps. PAL uses the Python program interpreter as the tool, enabling the model to act as a programmer writing detailed comments, and achieving significant improvements in arithmetic, symbolic, and algorithmic reasoning. Notably, the idea of model-as-programmer has also been shown to be successful in embodied agents, as evidenced by ProgPrompt <cite class="ltx_cite ltx_citemacro_citep">(Singh et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib150" title="">2022</a>)</cite> and Code-as-Policies <cite class="ltx_cite ltx_citemacro_citep">(Liang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib82" title="">2022a</a>)</cite>, which prompt models to generate executable programs for embodied agents. These studies reveal that, despite not having direct interaction with the environment, models are capable of generating executable programs for agents and anticipating possible anomalies in the plan execution. Another example is Visual ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib185" title="">2023</a>)</cite>, which interleaves various vision foundation models with ChatGPT to enable understanding and generating images. In their system, ChatGPT serves as the core controller and makes sequential decisions. At each step, ChatGPT might call a vision model to modify an existing image or respond to the user with plain text.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="212" id="S3.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Illustration of introspective reasoning and extrospective reasoning. Extrospective reasoning requires feedback from the environment and humans to carry out iterative plan generation. We omit the perceiver in the illustration for simplicity.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.Px2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.Px2.p2.1">However, since these models are not grounded in the environment, they may generate unrealistic and nonsensical plans. To this end, SayCan <cite class="ltx_cite ltx_citemacro_citep">(Ahn et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib2" title="">2022</a>)</cite> emphasizes those actions that the agent is “permitted” to execute instead of those it is “willing” to perform. In practice, they employ a value function to estimate the probability of each action being successfully executed. With this function, the agent becomes more physically grounded in the environment. Overall, despite the absence of environment feedback, foundation models exhibit a remarkable ability to plan effectively in introspective reasoning. They can anticipate potential anomalies in plan execution and adjust their plans accordingly. This ability not only enables the controller to generate executable programs but also enhances its capacity to plan for a wide range of tasks.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS2.Px3">
<h5 class="ltx_title ltx_title_paragraph">Extrospective Reasoning.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.Px3.p1">
<p class="ltx_p" id="S3.SS2.SSS2.Px3.p1.1">Despite its simplicity, introspective reasoning cannot adapt the plan in response to intermediate execution results. A more rational approach to planning is taking the environment <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.Px3.p1.1.m1.1"><semantics id="S3.SS2.SSS2.Px3.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.Px3.p1.1.m1.1.1" xref="S3.SS2.SSS2.Px3.p1.1.m1.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.Px3.p1.1.m1.1b"><ci id="S3.SS2.SSS2.Px3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.Px3.p1.1.m1.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.Px3.p1.1.m1.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.Px3.p1.1.m1.1d">caligraphic_E</annotation></semantics></math> into account, and generating plans incrementally (e.g., one step at a time) with subsequent plans dependent on previous execution results. This allows the four components described in <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS1" title="3.1 Components of Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.1</span></a> to be well integrated and to cooperate effectively to achieve complex tasks. We refer to such an incremental reasoning strategy as extrospective reasoning. Compared to introspective reasoning, extrospective reasoning additionally considers feedback from the user and environment (Figure <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.F5" title="Figure 5 ‣ Introspective Reasoning. ‣ 3.2.2 Planning with Reasoning ‣ 3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">5</span></a>), and is thus better suited to complex tasks <cite class="ltx_cite ltx_citemacro_citep">(Madaan et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib93" title="">2023</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib177" title="">2023b</a>)</cite>, such as multi-step QA and embodied learning, where decision-making at each step is dependent on the preceding context.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.Px3.p2">
<p class="ltx_p" id="S3.SS2.SSS2.Px3.p2.1">Recent works such as Self-Ask <cite class="ltx_cite ltx_citemacro_citep">(Press et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib126" title="">2022</a>)</cite>, ReAct <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib194" title="">2022b</a>)</cite>, and ToolFormer <cite class="ltx_cite ltx_citemacro_citep">(Schick et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib139" title="">2023</a>)</cite> have demonstrated that by providing access to search engine APIs, models are able to achieve improved accuracy on multi-step QA. Through CoT prompting (Self-Ask and ReAct) or fine-tuning (ToolFormer), models can learn to decompose complex questions and utilize the search API to find the answer to the first sub-question. Based on the response and the question, they can then iteratively determine the subsequent question to ask or give the final answer.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.Px3.p3">
<p class="ltx_p" id="S3.SS2.SSS2.Px3.p3.1">For embodied learning, while introspective reasoning methods have demonstrated the ability to generate executable programs and address potential execution anomalies, direct interaction with the environment can further enhance models’ planning capabilities. For instance, Inner Monologue <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib54" title="">2022b</a>)</cite> leverages multiple sources of feedback from the environment, such as whether a task is completed successfully and the current scene information. In this way, models can generate more feasible plans and improve their ability of high-level instruction completion. LLM-Planner <cite class="ltx_cite ltx_citemacro_citep">(Song et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib152" title="">2022</a>)</cite> explicitly considers anomalies that may arise during plan execution and utilizes environmental feedback to regenerate the plan in case of execution failure, enabling models to handle exceptions appropriately. Additionally, ReAct <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib194" title="">2022b</a>)</cite> grants models the autonomy to determine when to cease generating action tokens during planning, enabling them to reason about the current situation and develop more refined subsequent plans.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.Px3.p4">
<p class="ltx_p" id="S3.SS2.SSS2.Px3.p4.1">In summary, extrospective reasoning requires interaction between the controller and the environment, which is a more complex setting. However, the real-time feedback from the user and environment empowers models to have a more comprehensive understanding of the current situation, making it possible to eventually achieve long-term goals that necessitate extensive planning.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS2.Px4">
<h5 class="ltx_title ltx_title_paragraph">Challenges in Multi-Step Multi-Tool Scenario.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.Px4.p1">
<p class="ltx_p" id="S3.SS2.SSS2.Px4.p1.1">Humans do not stick to only one single tool to complete complex tasks. Instead, we carefully decompose the task into several sub-tasks, select the most suitable tool for each sub-task, and gradually accomplish them step by step. As discussed above, current research has shown satisfactory performance in task decomposition. However, there is a lack of exploration in utilizing different tools for different sub-tasks. Most of the research mentioned in this section is limited to either multi-step single-tool or single-step multi-tool scenarios.
However, there has been a recent emergence of research that addresses the multi-step multi-tool scenario. One such example is the ReAct model <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib194" title="">2022b</a>)</cite>, which integrates multiple APIs of Wikipedia and employs the foundation model to decide when to use which API. Later, MM-ReAct <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib191" title="">2023</a>)</cite> generalizes ReAct to the multi-modal domain by including several vision experts. Furthermore, some recent projects such as Auto-GPT<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Torantulino/Auto-GPT" title="">https://github.com/Torantulino/Auto-GPT</a></span></span></span> demonstrate the huge potential of GPT-4 in manipulating multiple tools and making long-term plans, pushing the boundaries of what is possible with tool learning. Given a user query, Auto-GPT will take step-by-step actions to accomplish the objective autonomously. In addition to reasoning about the current state, Auto-GPT can also reflect on past actions to refine decision-making. Although these works constitute a significant step in advancing tool learning in the multi-step multi-tool scenario, there are still several challenges and future directions that need to be investigated.</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Understanding the Interplay among Different Tools.</span> The multi-step multi-tool scenario typically involves complex tasks that require a higher level of intent understanding and reasoning capability. To effectively utilize multiple tools under this scenario, models need to grasp not only the individual functionalities of tools but also their interactions and dependencies. Models should be able to sequence the tools in a logical order so that the subsequent tools can leverage the information generated by the previous tools and effectively complete the task.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;padding-top:3.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">From Sequential Execution to Parallel Execution.</span> Tool executions do not have to be performed sequentially. In certain scenarios, parallel execution is possible for sub-tasks that do not depend on each other, which can potentially improve execution efficiency. For instance, given a user instruction <span class="ltx_text ltx_font_italic" id="S3.I1.i2.p1.1.2">“Generate two codes, one for drawing a rectangle, and one for drawing a circle.”</span>, the two tasks can be assigned to two agents, enabling the codes to be generated simultaneously. Determining the dependencies among different sub-tasks and effectively switching between parallel and sequential execution to optimize the overall process is a promising direction that merits further investigation.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;padding-top:3.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">From Single-agent Problem-Solving to Multi-agent Collaboration.</span>
Previous works typically assume that a single agent (controller) is solely responsible for the entire tool learning procedure. However,
in practice, complex tasks often demand collaboration among multiple agents, each possessing unique abilities and expertise. Embracing multi-agent collaboration can unlock more effective and efficient problem-solving approaches, necessitating the design of methods for communication, coordination, and negotiation among agents to ensure seamless collaboration and optimal task execution. Notably, recent work like <cite class="ltx_cite ltx_citemacro_citet">Park et al. (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib122" title="">2023</a>)</cite> demonstrates that multiple agents modeled with foundation models can simulate human behaviors (e.g., interpersonal communication) in interactive scenarios. This provides promising evidence for the adoption of multiple agents for tool learning.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.Px4.p2">
<p class="ltx_p" id="S3.SS2.SSS2.Px4.p2.1">We look forward to more work in the future moving towards more practical multi-step multi-tool scenarios and making efforts to address these challenges. As a prior exploration, we evaluate foundation models’ performance when multiple tools (APIs) are required to solve a task in <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S4" title="4 Application and Experiment ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Training Models for Improved Tool Learning</h3>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="249" id="S3.F6.g1" src="x6.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Training strategies for tool learning: (left) learning from human-annotated or model-annotated demonstrations; (right) learning from feedback, where the supervision could come from either the environment or humans.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Guidance, either from humans or environments, plays a critical role in training foundation models to use tools. In contrast to the prompting-based methods mentioned in <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2.SSS1" title="3.2.1 Understanding Intent and Tools ‣ 3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.2.1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2.SSS2" title="3.2.2 Planning with Reasoning ‣ 3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.2.2</span></a>, which rely on the frozen foundation models’ in-context learning abilities, the training-based method optimizes the model with supervision.
As noted by <cite class="ltx_cite ltx_citemacro_citet">Fagard et al. (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib40" title="">2016</a>)</cite>, there are two primary ways for infants to learn a new tool, that is either from demonstration by an adult modeling the action or relying on their own exploration. Analogously, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.F6" title="Figure 6 ‣ 3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">6</span></a>, we categorize training strategies for tool learning into two streams: (1) learning from concrete tool-use demonstrations <cite class="ltx_cite ltx_citemacro_citep">(Nakano et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib110" title="">2021</a>; Sasaki &amp; Yamashina, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib138" title="">2021</a>)</cite>, which often requires human annotation, and (2) learning from feedback, which typically involves reinforcement learning <cite class="ltx_cite ltx_citemacro_citep">(Reddy et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib131" title="">2020</a>; Baker et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib12" title="">2022</a>)</cite>. Finally, considering the existence of potentially massive tools, learning each of them one by one is infeasible in practice. Hence, we emphasize the importance of generalization in tool learning and discuss potential solutions (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3.SSS3" title="3.3.3 Generalizable Tool Learning ‣ 3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.3.3</span></a>).</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Learning from Demonstrations</h4>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">Models can be trained to mimic the behavior of human experts through imitation learning <cite class="ltx_cite ltx_citemacro_citep">(Hussein et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib56" title="">2017</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib90" title="">2018b</a>; Baker et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib12" title="">2022</a>)</cite>. Behavior cloning <cite class="ltx_cite ltx_citemacro_citep">(Bain &amp; Sammut, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib10" title="">1995</a>)</cite> can be viewed as a simplistic form of imitation learning that focuses on learning policies in a supervised fashion, with the general assumption that the expert’s behavior is optimal or near-optimal.
The objective of behavioral cloning is to train models to imitate human experts’ actions given certain inputs or conditions, and this approach is commonly adopted when the actions of an expert can be easily recorded and utilized for learning <cite class="ltx_cite ltx_citemacro_citep">(Torabi et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib163" title="">2018</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p2">
<p class="ltx_p" id="S3.SS3.SSS1.p2.6">Formally, assume that we have a dataset <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.1.m1.1"><semantics id="S3.SS3.SSS1.p2.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p2.1.m1.1.1" xref="S3.SS3.SSS1.p2.1.m1.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.1.m1.1b"><ci id="S3.SS3.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.1.m1.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.1.m1.1d">caligraphic_D</annotation></semantics></math> of size <math alttext="N" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.2.m2.1"><semantics id="S3.SS3.SSS1.p2.2.m2.1a"><mi id="S3.SS3.SSS1.p2.2.m2.1.1" xref="S3.SS3.SSS1.p2.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.2.m2.1b"><ci id="S3.SS3.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS3.SSS1.p2.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.2.m2.1d">italic_N</annotation></semantics></math> consisting of pairs of user query <math alttext="q" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.3.m3.1"><semantics id="S3.SS3.SSS1.p2.3.m3.1a"><mi id="S3.SS3.SSS1.p2.3.m3.1.1" xref="S3.SS3.SSS1.p2.3.m3.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.3.m3.1b"><ci id="S3.SS3.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS3.SSS1.p2.3.m3.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.3.m3.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.3.m3.1d">italic_q</annotation></semantics></math> and the human demonstration annotation <math alttext="a^{*}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.4.m4.1"><semantics id="S3.SS3.SSS1.p2.4.m4.1a"><msup id="S3.SS3.SSS1.p2.4.m4.1.1" xref="S3.SS3.SSS1.p2.4.m4.1.1.cmml"><mi id="S3.SS3.SSS1.p2.4.m4.1.1.2" xref="S3.SS3.SSS1.p2.4.m4.1.1.2.cmml">a</mi><mo id="S3.SS3.SSS1.p2.4.m4.1.1.3" xref="S3.SS3.SSS1.p2.4.m4.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.4.m4.1b"><apply id="S3.SS3.SSS1.p2.4.m4.1.1.cmml" xref="S3.SS3.SSS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.4.m4.1.1.1.cmml" xref="S3.SS3.SSS1.p2.4.m4.1.1">superscript</csymbol><ci id="S3.SS3.SSS1.p2.4.m4.1.1.2.cmml" xref="S3.SS3.SSS1.p2.4.m4.1.1.2">𝑎</ci><times id="S3.SS3.SSS1.p2.4.m4.1.1.3.cmml" xref="S3.SS3.SSS1.p2.4.m4.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.4.m4.1c">a^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.4.m4.1d">italic_a start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math>, i.e., <math alttext="\mathcal{D}=\{(q_{i},a^{*}_{i})\}_{i=0}^{N-1}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.5.m5.1"><semantics id="S3.SS3.SSS1.p2.5.m5.1a"><mrow id="S3.SS3.SSS1.p2.5.m5.1.1" xref="S3.SS3.SSS1.p2.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p2.5.m5.1.1.3" xref="S3.SS3.SSS1.p2.5.m5.1.1.3.cmml">𝒟</mi><mo id="S3.SS3.SSS1.p2.5.m5.1.1.2" xref="S3.SS3.SSS1.p2.5.m5.1.1.2.cmml">=</mo><msubsup id="S3.SS3.SSS1.p2.5.m5.1.1.1" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.cmml"><mrow id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.2.cmml"><mo id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.2.cmml">{</mo><mrow id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.3.cmml"><mo id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2.3" stretchy="false" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.3.cmml">(</mo><msub id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.1.1" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.1.1.2" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.1.1.2.cmml">q</mi><mi id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.1.1.3" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2.4" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.3.cmml">,</mo><msubsup id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2.2" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2.2.2.2" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2.2.2.2.cmml">a</mi><mi id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2.2.3" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2.2.3.cmml">i</mi><mo id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2.2.2.3" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2.2.2.3.cmml">∗</mo></msubsup><mo id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2.5" stretchy="false" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.3.cmml">)</mo></mrow><mo id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.3" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.3.cmml"><mi id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.3.2" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.3.2.cmml">i</mi><mo id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.3.1" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.3.3" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.3.3.cmml">0</mn></mrow><mrow id="S3.SS3.SSS1.p2.5.m5.1.1.1.3" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.3.cmml"><mi id="S3.SS3.SSS1.p2.5.m5.1.1.1.3.2" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.3.2.cmml">N</mi><mo id="S3.SS3.SSS1.p2.5.m5.1.1.1.3.1" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.3.1.cmml">−</mo><mn id="S3.SS3.SSS1.p2.5.m5.1.1.1.3.3" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.3.3.cmml">1</mn></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.5.m5.1b"><apply id="S3.SS3.SSS1.p2.5.m5.1.1.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1"><eq id="S3.SS3.SSS1.p2.5.m5.1.1.2.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1.2"></eq><ci id="S3.SS3.SSS1.p2.5.m5.1.1.3.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1.3">𝒟</ci><apply id="S3.SS3.SSS1.p2.5.m5.1.1.1.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.5.m5.1.1.1.2.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1.1">superscript</csymbol><apply id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.2.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1.1">subscript</csymbol><set id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.2.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1"><interval closure="open" id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.3.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2"><apply id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.1.1.2">𝑞</ci><ci id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2.2">subscript</csymbol><apply id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2.2">superscript</csymbol><ci id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2.2.2.2">𝑎</ci><times id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2.2.2.3"></times></apply><ci id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.1.1.1.2.2.3">𝑖</ci></apply></interval></set><apply id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.3.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.3"><eq id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.3.1.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.3.1"></eq><ci id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.3.2.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.3.2">𝑖</ci><cn id="S3.SS3.SSS1.p2.5.m5.1.1.1.1.3.3.cmml" type="integer" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.1.3.3">0</cn></apply></apply><apply id="S3.SS3.SSS1.p2.5.m5.1.1.1.3.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.3"><minus id="S3.SS3.SSS1.p2.5.m5.1.1.1.3.1.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.3.1"></minus><ci id="S3.SS3.SSS1.p2.5.m5.1.1.1.3.2.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.3.2">𝑁</ci><cn id="S3.SS3.SSS1.p2.5.m5.1.1.1.3.3.cmml" type="integer" xref="S3.SS3.SSS1.p2.5.m5.1.1.1.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.5.m5.1c">\mathcal{D}=\{(q_{i},a^{*}_{i})\}_{i=0}^{N-1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.5.m5.1d">caligraphic_D = { ( italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N - 1 end_POSTSUPERSCRIPT</annotation></semantics></math>. Learning from human demonstrations optimizes the controller’s parameters <math alttext="\theta_{\mathcal{C}}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.6.m6.1"><semantics id="S3.SS3.SSS1.p2.6.m6.1a"><msub id="S3.SS3.SSS1.p2.6.m6.1.1" xref="S3.SS3.SSS1.p2.6.m6.1.1.cmml"><mi id="S3.SS3.SSS1.p2.6.m6.1.1.2" xref="S3.SS3.SSS1.p2.6.m6.1.1.2.cmml">θ</mi><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p2.6.m6.1.1.3" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.cmml">𝒞</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.6.m6.1b"><apply id="S3.SS3.SSS1.p2.6.m6.1.1.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.6.m6.1.1.1.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p2.6.m6.1.1.2.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.2">𝜃</ci><ci id="S3.SS3.SSS1.p2.6.m6.1.1.3.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.3">𝒞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.6.m6.1c">\theta_{\mathcal{C}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.6.m6.1d">italic_θ start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT</annotation></semantics></math> with the following objective:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A1.EGx3">
<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\theta_{\mathcal{C}}^{*}=\operatorname*{arg\,max}_{\theta_{%
\mathcal{C}}}\operatorname*{\mathbb{E}}_{(q_{i},a^{*}_{i})\in\mathcal{D}}\prod%
_{t=0}^{T_{i}}p_{\theta_{\mathcal{C}}}(a^{*}_{i,t}\mid x_{i,t},\mathcal{H}_{i,%
t},q_{i})," class="ltx_Math" display="inline" id="S3.E3.m1.9"><semantics id="S3.E3.m1.9a"><mrow id="S3.E3.m1.9.9.1" xref="S3.E3.m1.9.9.1.1.cmml"><mrow id="S3.E3.m1.9.9.1.1" xref="S3.E3.m1.9.9.1.1.cmml"><msubsup id="S3.E3.m1.9.9.1.1.3" xref="S3.E3.m1.9.9.1.1.3.cmml"><mi id="S3.E3.m1.9.9.1.1.3.2.2" xref="S3.E3.m1.9.9.1.1.3.2.2.cmml">θ</mi><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.9.9.1.1.3.2.3" xref="S3.E3.m1.9.9.1.1.3.2.3.cmml">𝒞</mi><mo id="S3.E3.m1.9.9.1.1.3.3" xref="S3.E3.m1.9.9.1.1.3.3.cmml">∗</mo></msubsup><mo id="S3.E3.m1.9.9.1.1.2" xref="S3.E3.m1.9.9.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.9.9.1.1.1" xref="S3.E3.m1.9.9.1.1.1.cmml"><mrow id="S3.E3.m1.9.9.1.1.1.3" xref="S3.E3.m1.9.9.1.1.1.3.cmml"><munder id="S3.E3.m1.9.9.1.1.1.3.1" xref="S3.E3.m1.9.9.1.1.1.3.1.cmml"><mrow id="S3.E3.m1.9.9.1.1.1.3.1.2" xref="S3.E3.m1.9.9.1.1.1.3.1.2.cmml"><mi id="S3.E3.m1.9.9.1.1.1.3.1.2.2" xref="S3.E3.m1.9.9.1.1.1.3.1.2.2.cmml">arg</mi><mo id="S3.E3.m1.9.9.1.1.1.3.1.2.1" lspace="0.170em" xref="S3.E3.m1.9.9.1.1.1.3.1.2.1.cmml">⁢</mo><mi id="S3.E3.m1.9.9.1.1.1.3.1.2.3" xref="S3.E3.m1.9.9.1.1.1.3.1.2.3.cmml">max</mi></mrow><msub id="S3.E3.m1.9.9.1.1.1.3.1.3" xref="S3.E3.m1.9.9.1.1.1.3.1.3.cmml"><mi id="S3.E3.m1.9.9.1.1.1.3.1.3.2" xref="S3.E3.m1.9.9.1.1.1.3.1.3.2.cmml">θ</mi><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.9.9.1.1.1.3.1.3.3" xref="S3.E3.m1.9.9.1.1.1.3.1.3.3.cmml">𝒞</mi></msub></munder><mo id="S3.E3.m1.9.9.1.1.1.3a" lspace="0.167em" xref="S3.E3.m1.9.9.1.1.1.3.cmml">⁡</mo><munder id="S3.E3.m1.9.9.1.1.1.3.2" xref="S3.E3.m1.9.9.1.1.1.3.2.cmml"><mo id="S3.E3.m1.9.9.1.1.1.3.2.2" xref="S3.E3.m1.9.9.1.1.1.3.2.2.cmml">𝔼</mo><mrow id="S3.E3.m1.2.2.2" xref="S3.E3.m1.2.2.2.cmml"><mrow id="S3.E3.m1.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.3.cmml"><mo id="S3.E3.m1.2.2.2.2.2.3" stretchy="false" xref="S3.E3.m1.2.2.2.2.3.cmml">(</mo><msub id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.2.cmml">q</mi><mi id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E3.m1.2.2.2.2.2.4" xref="S3.E3.m1.2.2.2.2.3.cmml">,</mo><msubsup id="S3.E3.m1.2.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.2.2.cmml"><mi id="S3.E3.m1.2.2.2.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.2.2.2.2.cmml">a</mi><mi id="S3.E3.m1.2.2.2.2.2.2.3" xref="S3.E3.m1.2.2.2.2.2.2.3.cmml">i</mi><mo id="S3.E3.m1.2.2.2.2.2.2.2.3" xref="S3.E3.m1.2.2.2.2.2.2.2.3.cmml">∗</mo></msubsup><mo id="S3.E3.m1.2.2.2.2.2.5" stretchy="false" xref="S3.E3.m1.2.2.2.2.3.cmml">)</mo></mrow><mo id="S3.E3.m1.2.2.2.3" xref="S3.E3.m1.2.2.2.3.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.2.2.2.4" xref="S3.E3.m1.2.2.2.4.cmml">𝒟</mi></mrow></munder></mrow><mo id="S3.E3.m1.9.9.1.1.1.2" lspace="0.167em" xref="S3.E3.m1.9.9.1.1.1.2.cmml">⁢</mo><mrow id="S3.E3.m1.9.9.1.1.1.1" xref="S3.E3.m1.9.9.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E3.m1.9.9.1.1.1.1.2" xref="S3.E3.m1.9.9.1.1.1.1.2.cmml"><munderover id="S3.E3.m1.9.9.1.1.1.1.2a" xref="S3.E3.m1.9.9.1.1.1.1.2.cmml"><mo id="S3.E3.m1.9.9.1.1.1.1.2.2.2" movablelimits="false" xref="S3.E3.m1.9.9.1.1.1.1.2.2.2.cmml">∏</mo><mrow id="S3.E3.m1.9.9.1.1.1.1.2.2.3" xref="S3.E3.m1.9.9.1.1.1.1.2.2.3.cmml"><mi id="S3.E3.m1.9.9.1.1.1.1.2.2.3.2" xref="S3.E3.m1.9.9.1.1.1.1.2.2.3.2.cmml">t</mi><mo id="S3.E3.m1.9.9.1.1.1.1.2.2.3.1" xref="S3.E3.m1.9.9.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E3.m1.9.9.1.1.1.1.2.2.3.3" xref="S3.E3.m1.9.9.1.1.1.1.2.2.3.3.cmml">0</mn></mrow><msub id="S3.E3.m1.9.9.1.1.1.1.2.3" xref="S3.E3.m1.9.9.1.1.1.1.2.3.cmml"><mi id="S3.E3.m1.9.9.1.1.1.1.2.3.2" xref="S3.E3.m1.9.9.1.1.1.1.2.3.2.cmml">T</mi><mi id="S3.E3.m1.9.9.1.1.1.1.2.3.3" xref="S3.E3.m1.9.9.1.1.1.1.2.3.3.cmml">i</mi></msub></munderover></mstyle><mrow id="S3.E3.m1.9.9.1.1.1.1.1" xref="S3.E3.m1.9.9.1.1.1.1.1.cmml"><msub id="S3.E3.m1.9.9.1.1.1.1.1.3" xref="S3.E3.m1.9.9.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.9.9.1.1.1.1.1.3.2" xref="S3.E3.m1.9.9.1.1.1.1.1.3.2.cmml">p</mi><msub id="S3.E3.m1.9.9.1.1.1.1.1.3.3" xref="S3.E3.m1.9.9.1.1.1.1.1.3.3.cmml"><mi id="S3.E3.m1.9.9.1.1.1.1.1.3.3.2" xref="S3.E3.m1.9.9.1.1.1.1.1.3.3.2.cmml">θ</mi><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.9.9.1.1.1.1.1.3.3.3" xref="S3.E3.m1.9.9.1.1.1.1.1.3.3.3.cmml">𝒞</mi></msub></msub><mo id="S3.E3.m1.9.9.1.1.1.1.1.2" xref="S3.E3.m1.9.9.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E3.m1.9.9.1.1.1.1.1.1.1" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.9.9.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.cmml"><msubsup id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.5" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.5.cmml"><mi id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.5.2.2" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.5.2.2.cmml">a</mi><mrow id="S3.E3.m1.4.4.2.4" xref="S3.E3.m1.4.4.2.3.cmml"><mi id="S3.E3.m1.3.3.1.1" xref="S3.E3.m1.3.3.1.1.cmml">i</mi><mo id="S3.E3.m1.4.4.2.4.1" xref="S3.E3.m1.4.4.2.3.cmml">,</mo><mi id="S3.E3.m1.4.4.2.2" xref="S3.E3.m1.4.4.2.2.cmml">t</mi></mrow><mo id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.5.2.3" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.5.2.3.cmml">∗</mo></msubsup><mo id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.4" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.4.cmml">∣</mo><mrow id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.3.4.cmml"><msub id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mrow id="S3.E3.m1.6.6.2.4" xref="S3.E3.m1.6.6.2.3.cmml"><mi id="S3.E3.m1.5.5.1.1" xref="S3.E3.m1.5.5.1.1.cmml">i</mi><mo id="S3.E3.m1.6.6.2.4.1" xref="S3.E3.m1.6.6.2.3.cmml">,</mo><mi id="S3.E3.m1.6.6.2.2" xref="S3.E3.m1.6.6.2.2.cmml">t</mi></mrow></msub><mo id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.3.3.4" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2.cmml">ℋ</mi><mrow id="S3.E3.m1.8.8.2.4" xref="S3.E3.m1.8.8.2.3.cmml"><mi id="S3.E3.m1.7.7.1.1" xref="S3.E3.m1.7.7.1.1.cmml">i</mi><mo id="S3.E3.m1.8.8.2.4.1" xref="S3.E3.m1.8.8.2.3.cmml">,</mo><mi id="S3.E3.m1.8.8.2.2" xref="S3.E3.m1.8.8.2.2.cmml">t</mi></mrow></msub><mo id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.3.3.5" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.cmml"><mi id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.2" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.2.cmml">q</mi><mi id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.3" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.3.cmml">i</mi></msub></mrow></mrow><mo id="S3.E3.m1.9.9.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.E3.m1.9.9.1.2" xref="S3.E3.m1.9.9.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.9b"><apply id="S3.E3.m1.9.9.1.1.cmml" xref="S3.E3.m1.9.9.1"><eq id="S3.E3.m1.9.9.1.1.2.cmml" xref="S3.E3.m1.9.9.1.1.2"></eq><apply id="S3.E3.m1.9.9.1.1.3.cmml" xref="S3.E3.m1.9.9.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.9.9.1.1.3.1.cmml" xref="S3.E3.m1.9.9.1.1.3">superscript</csymbol><apply id="S3.E3.m1.9.9.1.1.3.2.cmml" xref="S3.E3.m1.9.9.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.9.9.1.1.3.2.1.cmml" xref="S3.E3.m1.9.9.1.1.3">subscript</csymbol><ci id="S3.E3.m1.9.9.1.1.3.2.2.cmml" xref="S3.E3.m1.9.9.1.1.3.2.2">𝜃</ci><ci id="S3.E3.m1.9.9.1.1.3.2.3.cmml" xref="S3.E3.m1.9.9.1.1.3.2.3">𝒞</ci></apply><times id="S3.E3.m1.9.9.1.1.3.3.cmml" xref="S3.E3.m1.9.9.1.1.3.3"></times></apply><apply id="S3.E3.m1.9.9.1.1.1.cmml" xref="S3.E3.m1.9.9.1.1.1"><times id="S3.E3.m1.9.9.1.1.1.2.cmml" xref="S3.E3.m1.9.9.1.1.1.2"></times><apply id="S3.E3.m1.9.9.1.1.1.3.cmml" xref="S3.E3.m1.9.9.1.1.1.3"><apply id="S3.E3.m1.9.9.1.1.1.3.1.cmml" xref="S3.E3.m1.9.9.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E3.m1.9.9.1.1.1.3.1.1.cmml" xref="S3.E3.m1.9.9.1.1.1.3.1">subscript</csymbol><apply id="S3.E3.m1.9.9.1.1.1.3.1.2.cmml" xref="S3.E3.m1.9.9.1.1.1.3.1.2"><times id="S3.E3.m1.9.9.1.1.1.3.1.2.1.cmml" xref="S3.E3.m1.9.9.1.1.1.3.1.2.1"></times><ci id="S3.E3.m1.9.9.1.1.1.3.1.2.2.cmml" xref="S3.E3.m1.9.9.1.1.1.3.1.2.2">arg</ci><ci id="S3.E3.m1.9.9.1.1.1.3.1.2.3.cmml" xref="S3.E3.m1.9.9.1.1.1.3.1.2.3">max</ci></apply><apply id="S3.E3.m1.9.9.1.1.1.3.1.3.cmml" xref="S3.E3.m1.9.9.1.1.1.3.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.9.9.1.1.1.3.1.3.1.cmml" xref="S3.E3.m1.9.9.1.1.1.3.1.3">subscript</csymbol><ci id="S3.E3.m1.9.9.1.1.1.3.1.3.2.cmml" xref="S3.E3.m1.9.9.1.1.1.3.1.3.2">𝜃</ci><ci id="S3.E3.m1.9.9.1.1.1.3.1.3.3.cmml" xref="S3.E3.m1.9.9.1.1.1.3.1.3.3">𝒞</ci></apply></apply><apply id="S3.E3.m1.9.9.1.1.1.3.2.cmml" xref="S3.E3.m1.9.9.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.9.9.1.1.1.3.2.1.cmml" xref="S3.E3.m1.9.9.1.1.1.3.2">subscript</csymbol><ci id="S3.E3.m1.9.9.1.1.1.3.2.2.cmml" xref="S3.E3.m1.9.9.1.1.1.3.2.2">𝔼</ci><apply id="S3.E3.m1.2.2.2.cmml" xref="S3.E3.m1.2.2.2"><in id="S3.E3.m1.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.3"></in><interval closure="open" id="S3.E3.m1.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2.2"><apply id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2">𝑞</ci><ci id="S3.E3.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E3.m1.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.2.2">subscript</csymbol><apply id="S3.E3.m1.2.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.2.2">superscript</csymbol><ci id="S3.E3.m1.2.2.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2.2.2">𝑎</ci><times id="S3.E3.m1.2.2.2.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2.2.2.2.3"></times></apply><ci id="S3.E3.m1.2.2.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2.2.2.3">𝑖</ci></apply></interval><ci id="S3.E3.m1.2.2.2.4.cmml" xref="S3.E3.m1.2.2.2.4">𝒟</ci></apply></apply></apply><apply id="S3.E3.m1.9.9.1.1.1.1.cmml" xref="S3.E3.m1.9.9.1.1.1.1"><apply id="S3.E3.m1.9.9.1.1.1.1.2.cmml" xref="S3.E3.m1.9.9.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.9.9.1.1.1.1.2.1.cmml" xref="S3.E3.m1.9.9.1.1.1.1.2">superscript</csymbol><apply id="S3.E3.m1.9.9.1.1.1.1.2.2.cmml" xref="S3.E3.m1.9.9.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.9.9.1.1.1.1.2.2.1.cmml" xref="S3.E3.m1.9.9.1.1.1.1.2">subscript</csymbol><csymbol cd="latexml" id="S3.E3.m1.9.9.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.9.9.1.1.1.1.2.2.2">product</csymbol><apply id="S3.E3.m1.9.9.1.1.1.1.2.2.3.cmml" xref="S3.E3.m1.9.9.1.1.1.1.2.2.3"><eq id="S3.E3.m1.9.9.1.1.1.1.2.2.3.1.cmml" xref="S3.E3.m1.9.9.1.1.1.1.2.2.3.1"></eq><ci id="S3.E3.m1.9.9.1.1.1.1.2.2.3.2.cmml" xref="S3.E3.m1.9.9.1.1.1.1.2.2.3.2">𝑡</ci><cn id="S3.E3.m1.9.9.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.E3.m1.9.9.1.1.1.1.2.2.3.3">0</cn></apply></apply><apply id="S3.E3.m1.9.9.1.1.1.1.2.3.cmml" xref="S3.E3.m1.9.9.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.9.9.1.1.1.1.2.3.1.cmml" xref="S3.E3.m1.9.9.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E3.m1.9.9.1.1.1.1.2.3.2.cmml" xref="S3.E3.m1.9.9.1.1.1.1.2.3.2">𝑇</ci><ci id="S3.E3.m1.9.9.1.1.1.1.2.3.3.cmml" xref="S3.E3.m1.9.9.1.1.1.1.2.3.3">𝑖</ci></apply></apply><apply id="S3.E3.m1.9.9.1.1.1.1.1.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1"><times id="S3.E3.m1.9.9.1.1.1.1.1.2.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.2"></times><apply id="S3.E3.m1.9.9.1.1.1.1.1.3.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.9.9.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.9.9.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.3.2">𝑝</ci><apply id="S3.E3.m1.9.9.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.9.9.1.1.1.1.1.3.3.1.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E3.m1.9.9.1.1.1.1.1.3.3.2.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.3.3.2">𝜃</ci><ci id="S3.E3.m1.9.9.1.1.1.1.1.3.3.3.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.3.3.3">𝒞</ci></apply></apply><apply id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.4">conditional</csymbol><apply id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.5.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.5.1.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.5">subscript</csymbol><apply id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.5.2.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.5.2.1.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.5">superscript</csymbol><ci id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.5.2.2.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.5.2.2">𝑎</ci><times id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.5.2.3.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.5.2.3"></times></apply><list id="S3.E3.m1.4.4.2.3.cmml" xref="S3.E3.m1.4.4.2.4"><ci id="S3.E3.m1.3.3.1.1.cmml" xref="S3.E3.m1.3.3.1.1">𝑖</ci><ci id="S3.E3.m1.4.4.2.2.cmml" xref="S3.E3.m1.4.4.2.2">𝑡</ci></list></apply><list id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.3.4.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.3.3"><apply id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><list id="S3.E3.m1.6.6.2.3.cmml" xref="S3.E3.m1.6.6.2.4"><ci id="S3.E3.m1.5.5.1.1.cmml" xref="S3.E3.m1.5.5.1.1">𝑖</ci><ci id="S3.E3.m1.6.6.2.2.cmml" xref="S3.E3.m1.6.6.2.2">𝑡</ci></list></apply><apply id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2">ℋ</ci><list id="S3.E3.m1.8.8.2.3.cmml" xref="S3.E3.m1.8.8.2.4"><ci id="S3.E3.m1.7.7.1.1.cmml" xref="S3.E3.m1.7.7.1.1">𝑖</ci><ci id="S3.E3.m1.8.8.2.2.cmml" xref="S3.E3.m1.8.8.2.2">𝑡</ci></list></apply><apply id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.1.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.2.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.2">𝑞</ci><ci id="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.3">𝑖</ci></apply></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.9c">\displaystyle\theta_{\mathcal{C}}^{*}=\operatorname*{arg\,max}_{\theta_{%
\mathcal{C}}}\operatorname*{\mathbb{E}}_{(q_{i},a^{*}_{i})\in\mathcal{D}}\prod%
_{t=0}^{T_{i}}p_{\theta_{\mathcal{C}}}(a^{*}_{i,t}\mid x_{i,t},\mathcal{H}_{i,%
t},q_{i}),</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.9d">italic_θ start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ∈ caligraphic_D end_POSTSUBSCRIPT ∏ start_POSTSUBSCRIPT italic_t = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_a start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i , italic_t end_POSTSUBSCRIPT ∣ italic_x start_POSTSUBSCRIPT italic_i , italic_t end_POSTSUBSCRIPT , caligraphic_H start_POSTSUBSCRIPT italic_i , italic_t end_POSTSUBSCRIPT , italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.SSS1.p2.12">where <math alttext="a^{*}_{i,t}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.7.m1.2"><semantics id="S3.SS3.SSS1.p2.7.m1.2a"><msubsup id="S3.SS3.SSS1.p2.7.m1.2.3" xref="S3.SS3.SSS1.p2.7.m1.2.3.cmml"><mi id="S3.SS3.SSS1.p2.7.m1.2.3.2.2" xref="S3.SS3.SSS1.p2.7.m1.2.3.2.2.cmml">a</mi><mrow id="S3.SS3.SSS1.p2.7.m1.2.2.2.4" xref="S3.SS3.SSS1.p2.7.m1.2.2.2.3.cmml"><mi id="S3.SS3.SSS1.p2.7.m1.1.1.1.1" xref="S3.SS3.SSS1.p2.7.m1.1.1.1.1.cmml">i</mi><mo id="S3.SS3.SSS1.p2.7.m1.2.2.2.4.1" xref="S3.SS3.SSS1.p2.7.m1.2.2.2.3.cmml">,</mo><mi id="S3.SS3.SSS1.p2.7.m1.2.2.2.2" xref="S3.SS3.SSS1.p2.7.m1.2.2.2.2.cmml">t</mi></mrow><mo id="S3.SS3.SSS1.p2.7.m1.2.3.2.3" xref="S3.SS3.SSS1.p2.7.m1.2.3.2.3.cmml">∗</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.7.m1.2b"><apply id="S3.SS3.SSS1.p2.7.m1.2.3.cmml" xref="S3.SS3.SSS1.p2.7.m1.2.3"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.7.m1.2.3.1.cmml" xref="S3.SS3.SSS1.p2.7.m1.2.3">subscript</csymbol><apply id="S3.SS3.SSS1.p2.7.m1.2.3.2.cmml" xref="S3.SS3.SSS1.p2.7.m1.2.3"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.7.m1.2.3.2.1.cmml" xref="S3.SS3.SSS1.p2.7.m1.2.3">superscript</csymbol><ci id="S3.SS3.SSS1.p2.7.m1.2.3.2.2.cmml" xref="S3.SS3.SSS1.p2.7.m1.2.3.2.2">𝑎</ci><times id="S3.SS3.SSS1.p2.7.m1.2.3.2.3.cmml" xref="S3.SS3.SSS1.p2.7.m1.2.3.2.3"></times></apply><list id="S3.SS3.SSS1.p2.7.m1.2.2.2.3.cmml" xref="S3.SS3.SSS1.p2.7.m1.2.2.2.4"><ci id="S3.SS3.SSS1.p2.7.m1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p2.7.m1.1.1.1.1">𝑖</ci><ci id="S3.SS3.SSS1.p2.7.m1.2.2.2.2.cmml" xref="S3.SS3.SSS1.p2.7.m1.2.2.2.2">𝑡</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.7.m1.2c">a^{*}_{i,t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.7.m1.2d">italic_a start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i , italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is the human annotation at the <math alttext="t" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.8.m2.1"><semantics id="S3.SS3.SSS1.p2.8.m2.1a"><mi id="S3.SS3.SSS1.p2.8.m2.1.1" xref="S3.SS3.SSS1.p2.8.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.8.m2.1b"><ci id="S3.SS3.SSS1.p2.8.m2.1.1.cmml" xref="S3.SS3.SSS1.p2.8.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.8.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.8.m2.1d">italic_t</annotation></semantics></math>-th iteration for handling <math alttext="q_{i}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.9.m3.1"><semantics id="S3.SS3.SSS1.p2.9.m3.1a"><msub id="S3.SS3.SSS1.p2.9.m3.1.1" xref="S3.SS3.SSS1.p2.9.m3.1.1.cmml"><mi id="S3.SS3.SSS1.p2.9.m3.1.1.2" xref="S3.SS3.SSS1.p2.9.m3.1.1.2.cmml">q</mi><mi id="S3.SS3.SSS1.p2.9.m3.1.1.3" xref="S3.SS3.SSS1.p2.9.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.9.m3.1b"><apply id="S3.SS3.SSS1.p2.9.m3.1.1.cmml" xref="S3.SS3.SSS1.p2.9.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.9.m3.1.1.1.cmml" xref="S3.SS3.SSS1.p2.9.m3.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p2.9.m3.1.1.2.cmml" xref="S3.SS3.SSS1.p2.9.m3.1.1.2">𝑞</ci><ci id="S3.SS3.SSS1.p2.9.m3.1.1.3.cmml" xref="S3.SS3.SSS1.p2.9.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.9.m3.1c">q_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.9.m3.1d">italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="T_{i}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.10.m4.1"><semantics id="S3.SS3.SSS1.p2.10.m4.1a"><msub id="S3.SS3.SSS1.p2.10.m4.1.1" xref="S3.SS3.SSS1.p2.10.m4.1.1.cmml"><mi id="S3.SS3.SSS1.p2.10.m4.1.1.2" xref="S3.SS3.SSS1.p2.10.m4.1.1.2.cmml">T</mi><mi id="S3.SS3.SSS1.p2.10.m4.1.1.3" xref="S3.SS3.SSS1.p2.10.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.10.m4.1b"><apply id="S3.SS3.SSS1.p2.10.m4.1.1.cmml" xref="S3.SS3.SSS1.p2.10.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.10.m4.1.1.1.cmml" xref="S3.SS3.SSS1.p2.10.m4.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p2.10.m4.1.1.2.cmml" xref="S3.SS3.SSS1.p2.10.m4.1.1.2">𝑇</ci><ci id="S3.SS3.SSS1.p2.10.m4.1.1.3.cmml" xref="S3.SS3.SSS1.p2.10.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.10.m4.1c">T_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.10.m4.1d">italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the total iteration number of <math alttext="a_{i}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.11.m5.1"><semantics id="S3.SS3.SSS1.p2.11.m5.1a"><msub id="S3.SS3.SSS1.p2.11.m5.1.1" xref="S3.SS3.SSS1.p2.11.m5.1.1.cmml"><mi id="S3.SS3.SSS1.p2.11.m5.1.1.2" xref="S3.SS3.SSS1.p2.11.m5.1.1.2.cmml">a</mi><mi id="S3.SS3.SSS1.p2.11.m5.1.1.3" xref="S3.SS3.SSS1.p2.11.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.11.m5.1b"><apply id="S3.SS3.SSS1.p2.11.m5.1.1.cmml" xref="S3.SS3.SSS1.p2.11.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.11.m5.1.1.1.cmml" xref="S3.SS3.SSS1.p2.11.m5.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p2.11.m5.1.1.2.cmml" xref="S3.SS3.SSS1.p2.11.m5.1.1.2">𝑎</ci><ci id="S3.SS3.SSS1.p2.11.m5.1.1.3.cmml" xref="S3.SS3.SSS1.p2.11.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.11.m5.1c">a_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.11.m5.1d">italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, other varaibles follow the notations defined in Equation (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.E1" title="Equation 1 ‣ 3.1.2 Connecting the Components ‣ 3.1 Components of Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">1</span></a>). Based on how <math alttext="a^{*}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.12.m6.1"><semantics id="S3.SS3.SSS1.p2.12.m6.1a"><msup id="S3.SS3.SSS1.p2.12.m6.1.1" xref="S3.SS3.SSS1.p2.12.m6.1.1.cmml"><mi id="S3.SS3.SSS1.p2.12.m6.1.1.2" xref="S3.SS3.SSS1.p2.12.m6.1.1.2.cmml">a</mi><mo id="S3.SS3.SSS1.p2.12.m6.1.1.3" xref="S3.SS3.SSS1.p2.12.m6.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.12.m6.1b"><apply id="S3.SS3.SSS1.p2.12.m6.1.1.cmml" xref="S3.SS3.SSS1.p2.12.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.12.m6.1.1.1.cmml" xref="S3.SS3.SSS1.p2.12.m6.1.1">superscript</csymbol><ci id="S3.SS3.SSS1.p2.12.m6.1.1.2.cmml" xref="S3.SS3.SSS1.p2.12.m6.1.1.2">𝑎</ci><times id="S3.SS3.SSS1.p2.12.m6.1.1.3.cmml" xref="S3.SS3.SSS1.p2.12.m6.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.12.m6.1c">a^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.12.m6.1d">italic_a start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> is obtained, learning from demonstration can be categorized into three streams, with human intervention gradually becoming less:</p>
</div>
<section class="ltx_paragraph" id="S3.SS3.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Supervised Learning.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.Px1.p1.1">Traditionally, behavior cloning has been widely explored in learning end-to-end or modular perceiver-controller models for autonomous vehicles and robotic applications <cite class="ltx_cite ltx_citemacro_citep">(Ly &amp; Akhloufi, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib92" title="">2020</a>; Codevilla et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib31" title="">2019</a>)</cite>. Recently, there has been a surge of interest in fine-tuning foundation models to perform tool-oriented tasks in a supervised way. For instance, <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib81" title="">2022</a>)</cite> utilize foundation models as policy networks, whose input is the tokenized environment observations, the original goals, and action history. Benefiting from the task-general inductive bias brought by foundation models, behavior cloning using the policy network significantly improves both in-domain performance and out-of-distribution generalization. Another example is WebGPT <cite class="ltx_cite ltx_citemacro_citep">(Nakano et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib110" title="">2021</a>)</cite>, which interacts with a search engine by iteratively refining its search queries and recording important information. To achieve this, the authors first build a search interface backed up by Bing<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.microsoft.com/en-us/bing/apis/bing-web-search-api" title="">https://www.microsoft.com/en-us/bing/apis/bing-web-search-api</a></span></span></span> and then fine-tune GPT-3 <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib19" title="">2020</a>)</cite> to clone human web search behaviors. As a language model pre-trained on general domains, the original GPT-3 is not intrinsically anchored to valid browser commands.
Therefore, it is crucial to first gather demonstrations of human interactions with the browser and then learn state-to-action mappings. After fine-tuning, the model shows exceptional capabilities in manipulating search engines for information retrieval, even surpassing human experts. Similarly, WebShop <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib193" title="">2022a</a>)</cite> provides a web-based interactive environment where an agent could browse and purchase products. Through behavior cloning, the trained agent exhibits non-trivial performance in purchasing the right product given human instructions.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Semi-supervised Learning.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS1.Px2.p1.1">As is often the case, human behaviors cannot be easily recorded due to time and cost considerations. However, large-scale unlabeled data is often attainable, from which we could potentially construct weak, noisy supervision. Notably, recent works have shown that we could employ a less-capable model to annotate pseudo-labels on unlabeled data and convert them into weakly-supervised tool-use demonstrations. For instance, with a small amount of seed labeled data, <cite class="ltx_cite ltx_citemacro_cite">Baker et al. (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib12" title="">2022</a>)</cite> train a model to predict pseudo-labels of the action taken at each timestep in a Minecraft video game.
Learning from these pseudo-labels, a more powerful model can be trained without requiring the rollout of models in a target environment or large-scale gold-standard human behavior annotation.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS1.Px3">
<h5 class="ltx_title ltx_title_paragraph">Self-supervised Learning.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.Px3.p1">
<p class="ltx_p" id="S3.SS3.SSS1.Px3.p1.1">Despite reducing the heavy requirements on human behavior annotation, semi-supervised learning still requires a seed labeled dataset to attain the pseudo labels. Besides, the biases in the seed dataset may also be amplified during training, leading to poor generalization performance. To this end, researchers recently show that with a few demonstrations, foundation models can teach themselves how to utilize a tool in a self-supervised manner <cite class="ltx_cite ltx_citemacro_citep">(Parisi et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib121" title="">2022</a>; Schick et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib139" title="">2023</a>)</cite>. For instance, Toolformer <cite class="ltx_cite ltx_citemacro_citep">(Schick et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib139" title="">2023</a>)</cite> leverages the in-context learning ability of foundation models to iteratively bootstrap tool-use examples based on a handful of human-written examples. These auto-generated examples are further filtered to reduce noise. The final tool-use dataset contains sufficient supervision, significantly improving GPT-J’s <cite class="ltx_cite ltx_citemacro_citep">(Wang &amp; Komatsuzaki, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib172" title="">2021</a>)</cite> tool-use performance, highlighting the potential of self-supervised learning for enhancing tool-use capabilities.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Learning from Feedback</h4>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.2">Collecting manually annotated tool-use examples, which probably include complete traces of human behaviors and the final answers, is time-consuming and labor-intensive. Moreover, the learned model may not adapt effectively to new environments as it conforms to the recorded human behaviors. Besides, it is impractical to explicitly annotate every possible scenario of environment condition and agent behavior <cite class="ltx_cite ltx_citemacro_citep">(Codevilla et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib31" title="">2019</a>)</cite>. Alternatively, humans learn from trial and error to correct and rectify their tool-use behaviors <cite class="ltx_cite ltx_citemacro_citep">(Allen et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib5" title="">2019</a>)</cite>. Similarly, feedback from both the environment and humans can enable the model to understand the consequences of its actions and adapt its behaviors. The supervision from feedback can also enhance the capabilities of an agent trained in a supervised way <cite class="ltx_cite ltx_citemacro_citep">(Nakano et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib110" title="">2021</a>; Baker et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib12" title="">2022</a>)</cite>. Formally, learning from feedback can be described as optimizing the controllers’ parameters <math alttext="\theta_{\mathcal{C}}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.1.m1.1"><semantics id="S3.SS3.SSS2.p1.1.m1.1a"><msub id="S3.SS3.SSS2.p1.1.m1.1.1" xref="S3.SS3.SSS2.p1.1.m1.1.1.cmml"><mi id="S3.SS3.SSS2.p1.1.m1.1.1.2" xref="S3.SS3.SSS2.p1.1.m1.1.1.2.cmml">θ</mi><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS2.p1.1.m1.1.1.3" xref="S3.SS3.SSS2.p1.1.m1.1.1.3.cmml">𝒞</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.1.m1.1b"><apply id="S3.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1.2">𝜃</ci><ci id="S3.SS3.SSS2.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1.3">𝒞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.1.m1.1c">\theta_{\mathcal{C}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p1.1.m1.1d">italic_θ start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT</annotation></semantics></math> from open explorations with query set <math alttext="Q=\{q_{i}\}_{i}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.2.m2.1"><semantics id="S3.SS3.SSS2.p1.2.m2.1a"><mrow id="S3.SS3.SSS2.p1.2.m2.1.1" xref="S3.SS3.SSS2.p1.2.m2.1.1.cmml"><mi id="S3.SS3.SSS2.p1.2.m2.1.1.3" xref="S3.SS3.SSS2.p1.2.m2.1.1.3.cmml">Q</mi><mo id="S3.SS3.SSS2.p1.2.m2.1.1.2" xref="S3.SS3.SSS2.p1.2.m2.1.1.2.cmml">=</mo><msub id="S3.SS3.SSS2.p1.2.m2.1.1.1" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.cmml"><mrow id="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1.2.cmml"><mo id="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.2" stretchy="false" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1.2.cmml">{</mo><msub id="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.1" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.1.cmml"><mi id="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.1.2" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.1.2.cmml">q</mi><mi id="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.1.3" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.3" stretchy="false" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1.2.cmml">}</mo></mrow><mi id="S3.SS3.SSS2.p1.2.m2.1.1.1.3" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.2.m2.1b"><apply id="S3.SS3.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1"><eq id="S3.SS3.SSS2.p1.2.m2.1.1.2.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1.2"></eq><ci id="S3.SS3.SSS2.p1.2.m2.1.1.3.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1.3">𝑄</ci><apply id="S3.SS3.SSS2.p1.2.m2.1.1.1.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.2.m2.1.1.1.2.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1.1">subscript</csymbol><set id="S3.SS3.SSS2.p1.2.m2.1.1.1.1.2.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1"><apply id="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.1.2.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.1.2">𝑞</ci><ci id="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.1.3.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.1.3">𝑖</ci></apply></set><ci id="S3.SS3.SSS2.p1.2.m2.1.1.1.3.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.2.m2.1c">Q=\{q_{i}\}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p1.2.m2.1d">italic_Q = { italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A1.EGx4">
<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\theta_{\mathcal{C}}^{*}=\operatorname*{arg\,max}_{\theta_{%
\mathcal{C}}}\operatorname*{\mathbb{E}}_{q_{i}\in Q}\operatorname*{\mathbb{E}}%
_{\{a_{i,t}\}_{t=0}^{T_{i}}\in p_{\theta_{\mathcal{C}}}}\left[R({\{a_{i,t}\}}_%
{t=0}^{T_{i}})\right]," class="ltx_Math" display="inline" id="S3.E4.m1.6"><semantics id="S3.E4.m1.6a"><mrow id="S3.E4.m1.6.6.1" xref="S3.E4.m1.6.6.1.1.cmml"><mrow id="S3.E4.m1.6.6.1.1" xref="S3.E4.m1.6.6.1.1.cmml"><msubsup id="S3.E4.m1.6.6.1.1.4" xref="S3.E4.m1.6.6.1.1.4.cmml"><mi id="S3.E4.m1.6.6.1.1.4.2.2" xref="S3.E4.m1.6.6.1.1.4.2.2.cmml">θ</mi><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.6.6.1.1.4.2.3" xref="S3.E4.m1.6.6.1.1.4.2.3.cmml">𝒞</mi><mo id="S3.E4.m1.6.6.1.1.4.3" xref="S3.E4.m1.6.6.1.1.4.3.cmml">∗</mo></msubsup><mo id="S3.E4.m1.6.6.1.1.3" xref="S3.E4.m1.6.6.1.1.3.cmml">=</mo><mrow id="S3.E4.m1.6.6.1.1.2" xref="S3.E4.m1.6.6.1.1.2.cmml"><munder id="S3.E4.m1.6.6.1.1.2.3" xref="S3.E4.m1.6.6.1.1.2.3.cmml"><mrow id="S3.E4.m1.6.6.1.1.2.3.2" xref="S3.E4.m1.6.6.1.1.2.3.2.cmml"><mi id="S3.E4.m1.6.6.1.1.2.3.2.2" xref="S3.E4.m1.6.6.1.1.2.3.2.2.cmml">arg</mi><mo id="S3.E4.m1.6.6.1.1.2.3.2.1" lspace="0.170em" xref="S3.E4.m1.6.6.1.1.2.3.2.1.cmml">⁢</mo><mi id="S3.E4.m1.6.6.1.1.2.3.2.3" xref="S3.E4.m1.6.6.1.1.2.3.2.3.cmml">max</mi></mrow><msub id="S3.E4.m1.6.6.1.1.2.3.3" xref="S3.E4.m1.6.6.1.1.2.3.3.cmml"><mi id="S3.E4.m1.6.6.1.1.2.3.3.2" xref="S3.E4.m1.6.6.1.1.2.3.3.2.cmml">θ</mi><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.6.6.1.1.2.3.3.3" xref="S3.E4.m1.6.6.1.1.2.3.3.3.cmml">𝒞</mi></msub></munder><mo id="S3.E4.m1.6.6.1.1.2a" lspace="0.167em" xref="S3.E4.m1.6.6.1.1.2.cmml">⁡</mo><mrow id="S3.E4.m1.6.6.1.1.2.2" xref="S3.E4.m1.6.6.1.1.2.2.cmml"><munder id="S3.E4.m1.6.6.1.1.2.2.3" xref="S3.E4.m1.6.6.1.1.2.2.3.cmml"><mo id="S3.E4.m1.6.6.1.1.2.2.3.2" rspace="0.0835em" xref="S3.E4.m1.6.6.1.1.2.2.3.2.cmml">𝔼</mo><mrow id="S3.E4.m1.6.6.1.1.2.2.3.3" xref="S3.E4.m1.6.6.1.1.2.2.3.3.cmml"><msub id="S3.E4.m1.6.6.1.1.2.2.3.3.2" xref="S3.E4.m1.6.6.1.1.2.2.3.3.2.cmml"><mi id="S3.E4.m1.6.6.1.1.2.2.3.3.2.2" xref="S3.E4.m1.6.6.1.1.2.2.3.3.2.2.cmml">q</mi><mi id="S3.E4.m1.6.6.1.1.2.2.3.3.2.3" xref="S3.E4.m1.6.6.1.1.2.2.3.3.2.3.cmml">i</mi></msub><mo id="S3.E4.m1.6.6.1.1.2.2.3.3.1" xref="S3.E4.m1.6.6.1.1.2.2.3.3.1.cmml">∈</mo><mi id="S3.E4.m1.6.6.1.1.2.2.3.3.3" xref="S3.E4.m1.6.6.1.1.2.2.3.3.3.cmml">Q</mi></mrow></munder><mrow id="S3.E4.m1.6.6.1.1.2.2.2.2" xref="S3.E4.m1.6.6.1.1.2.2.2.3.cmml"><munder id="S3.E4.m1.6.6.1.1.1.1.1.1.1" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.cmml"><mo id="S3.E4.m1.6.6.1.1.1.1.1.1.1.2" lspace="0.0835em" rspace="0em" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.2.cmml">𝔼</mo><mrow id="S3.E4.m1.3.3.3" xref="S3.E4.m1.3.3.3.cmml"><msubsup id="S3.E4.m1.3.3.3.3" xref="S3.E4.m1.3.3.3.3.cmml"><mrow id="S3.E4.m1.3.3.3.3.1.1.1" xref="S3.E4.m1.3.3.3.3.1.1.2.cmml"><mo id="S3.E4.m1.3.3.3.3.1.1.1.2" stretchy="false" xref="S3.E4.m1.3.3.3.3.1.1.2.cmml">{</mo><msub id="S3.E4.m1.3.3.3.3.1.1.1.1" xref="S3.E4.m1.3.3.3.3.1.1.1.1.cmml"><mi id="S3.E4.m1.3.3.3.3.1.1.1.1.2" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2.cmml">a</mi><mrow id="S3.E4.m1.2.2.2.2.2.4" xref="S3.E4.m1.2.2.2.2.2.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml">i</mi><mo id="S3.E4.m1.2.2.2.2.2.4.1" xref="S3.E4.m1.2.2.2.2.2.3.cmml">,</mo><mi id="S3.E4.m1.2.2.2.2.2.2" xref="S3.E4.m1.2.2.2.2.2.2.cmml">t</mi></mrow></msub><mo id="S3.E4.m1.3.3.3.3.1.1.1.3" stretchy="false" xref="S3.E4.m1.3.3.3.3.1.1.2.cmml">}</mo></mrow><mrow id="S3.E4.m1.3.3.3.3.1.3" xref="S3.E4.m1.3.3.3.3.1.3.cmml"><mi id="S3.E4.m1.3.3.3.3.1.3.2" xref="S3.E4.m1.3.3.3.3.1.3.2.cmml">t</mi><mo id="S3.E4.m1.3.3.3.3.1.3.1" xref="S3.E4.m1.3.3.3.3.1.3.1.cmml">=</mo><mn id="S3.E4.m1.3.3.3.3.1.3.3" xref="S3.E4.m1.3.3.3.3.1.3.3.cmml">0</mn></mrow><msub id="S3.E4.m1.3.3.3.3.3" xref="S3.E4.m1.3.3.3.3.3.cmml"><mi id="S3.E4.m1.3.3.3.3.3.2" xref="S3.E4.m1.3.3.3.3.3.2.cmml">T</mi><mi id="S3.E4.m1.3.3.3.3.3.3" xref="S3.E4.m1.3.3.3.3.3.3.cmml">i</mi></msub></msubsup><mo id="S3.E4.m1.3.3.3.4" xref="S3.E4.m1.3.3.3.4.cmml">∈</mo><msub id="S3.E4.m1.3.3.3.5" xref="S3.E4.m1.3.3.3.5.cmml"><mi id="S3.E4.m1.3.3.3.5.2" xref="S3.E4.m1.3.3.3.5.2.cmml">p</mi><msub id="S3.E4.m1.3.3.3.5.3" xref="S3.E4.m1.3.3.3.5.3.cmml"><mi id="S3.E4.m1.3.3.3.5.3.2" xref="S3.E4.m1.3.3.3.5.3.2.cmml">θ</mi><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.3.3.3.5.3.3" xref="S3.E4.m1.3.3.3.5.3.3.cmml">𝒞</mi></msub></msub></mrow></munder><mrow id="S3.E4.m1.6.6.1.1.2.2.2.2.2" xref="S3.E4.m1.6.6.1.1.2.2.2.3.cmml"><mo id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2" xref="S3.E4.m1.6.6.1.1.2.2.2.3.cmml">[</mo><mrow id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.cmml"><mi id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.3" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.3.cmml">R</mi><mo id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.2" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.2.cmml">⁢</mo><mrow id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.cmml"><mo id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.2" stretchy="false" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.cmml">(</mo><msubsup id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.cmml"><mrow id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.1.1" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.1.2.cmml"><mo id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.1.2.cmml">{</mo><msub id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.1.1.1" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.1.1.1.2.cmml">a</mi><mrow id="S3.E4.m1.5.5.2.4" xref="S3.E4.m1.5.5.2.3.cmml"><mi id="S3.E4.m1.4.4.1.1" xref="S3.E4.m1.4.4.1.1.cmml">i</mi><mo id="S3.E4.m1.5.5.2.4.1" xref="S3.E4.m1.5.5.2.3.cmml">,</mo><mi id="S3.E4.m1.5.5.2.2" xref="S3.E4.m1.5.5.2.2.cmml">t</mi></mrow></msub><mo id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.3" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.3.2" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.3.2.cmml">t</mi><mo id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.3.1" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.3.1.cmml">=</mo><mn id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.3.3" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.3.3.cmml">0</mn></mrow><msub id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.3" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.3.cmml"><mi id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.3.2" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.3.2.cmml">T</mi><mi id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.3.3" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.3.3.cmml">i</mi></msub></msubsup><mo id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.3" stretchy="false" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.6.6.1.1.2.2.2.2.2.3" xref="S3.E4.m1.6.6.1.1.2.2.2.3.cmml">]</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.E4.m1.6.6.1.2" xref="S3.E4.m1.6.6.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.6b"><apply id="S3.E4.m1.6.6.1.1.cmml" xref="S3.E4.m1.6.6.1"><eq id="S3.E4.m1.6.6.1.1.3.cmml" xref="S3.E4.m1.6.6.1.1.3"></eq><apply id="S3.E4.m1.6.6.1.1.4.cmml" xref="S3.E4.m1.6.6.1.1.4"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.1.cmml" xref="S3.E4.m1.6.6.1.1.4">superscript</csymbol><apply id="S3.E4.m1.6.6.1.1.4.2.cmml" xref="S3.E4.m1.6.6.1.1.4"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.2.1.cmml" xref="S3.E4.m1.6.6.1.1.4">subscript</csymbol><ci id="S3.E4.m1.6.6.1.1.4.2.2.cmml" xref="S3.E4.m1.6.6.1.1.4.2.2">𝜃</ci><ci id="S3.E4.m1.6.6.1.1.4.2.3.cmml" xref="S3.E4.m1.6.6.1.1.4.2.3">𝒞</ci></apply><times id="S3.E4.m1.6.6.1.1.4.3.cmml" xref="S3.E4.m1.6.6.1.1.4.3"></times></apply><apply id="S3.E4.m1.6.6.1.1.2.cmml" xref="S3.E4.m1.6.6.1.1.2"><apply id="S3.E4.m1.6.6.1.1.2.3.cmml" xref="S3.E4.m1.6.6.1.1.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.2.3.1.cmml" xref="S3.E4.m1.6.6.1.1.2.3">subscript</csymbol><apply id="S3.E4.m1.6.6.1.1.2.3.2.cmml" xref="S3.E4.m1.6.6.1.1.2.3.2"><times id="S3.E4.m1.6.6.1.1.2.3.2.1.cmml" xref="S3.E4.m1.6.6.1.1.2.3.2.1"></times><ci id="S3.E4.m1.6.6.1.1.2.3.2.2.cmml" xref="S3.E4.m1.6.6.1.1.2.3.2.2">arg</ci><ci id="S3.E4.m1.6.6.1.1.2.3.2.3.cmml" xref="S3.E4.m1.6.6.1.1.2.3.2.3">max</ci></apply><apply id="S3.E4.m1.6.6.1.1.2.3.3.cmml" xref="S3.E4.m1.6.6.1.1.2.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.2.3.3.1.cmml" xref="S3.E4.m1.6.6.1.1.2.3.3">subscript</csymbol><ci id="S3.E4.m1.6.6.1.1.2.3.3.2.cmml" xref="S3.E4.m1.6.6.1.1.2.3.3.2">𝜃</ci><ci id="S3.E4.m1.6.6.1.1.2.3.3.3.cmml" xref="S3.E4.m1.6.6.1.1.2.3.3.3">𝒞</ci></apply></apply><apply id="S3.E4.m1.6.6.1.1.2.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2"><apply id="S3.E4.m1.6.6.1.1.2.2.3.cmml" xref="S3.E4.m1.6.6.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.2.2.3.1.cmml" xref="S3.E4.m1.6.6.1.1.2.2.3">subscript</csymbol><ci id="S3.E4.m1.6.6.1.1.2.2.3.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2.3.2">𝔼</ci><apply id="S3.E4.m1.6.6.1.1.2.2.3.3.cmml" xref="S3.E4.m1.6.6.1.1.2.2.3.3"><in id="S3.E4.m1.6.6.1.1.2.2.3.3.1.cmml" xref="S3.E4.m1.6.6.1.1.2.2.3.3.1"></in><apply id="S3.E4.m1.6.6.1.1.2.2.3.3.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2.3.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.2.2.3.3.2.1.cmml" xref="S3.E4.m1.6.6.1.1.2.2.3.3.2">subscript</csymbol><ci id="S3.E4.m1.6.6.1.1.2.2.3.3.2.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2.3.3.2.2">𝑞</ci><ci id="S3.E4.m1.6.6.1.1.2.2.3.3.2.3.cmml" xref="S3.E4.m1.6.6.1.1.2.2.3.3.2.3">𝑖</ci></apply><ci id="S3.E4.m1.6.6.1.1.2.2.3.3.3.cmml" xref="S3.E4.m1.6.6.1.1.2.2.3.3.3">𝑄</ci></apply></apply><apply id="S3.E4.m1.6.6.1.1.2.2.2.3.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2"><apply id="S3.E4.m1.6.6.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.6.6.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.2">𝔼</ci><apply id="S3.E4.m1.3.3.3.cmml" xref="S3.E4.m1.3.3.3"><in id="S3.E4.m1.3.3.3.4.cmml" xref="S3.E4.m1.3.3.3.4"></in><apply id="S3.E4.m1.3.3.3.3.cmml" xref="S3.E4.m1.3.3.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.3.3.2.cmml" xref="S3.E4.m1.3.3.3.3">superscript</csymbol><apply id="S3.E4.m1.3.3.3.3.1.cmml" xref="S3.E4.m1.3.3.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.3.3.1.2.cmml" xref="S3.E4.m1.3.3.3.3">subscript</csymbol><set id="S3.E4.m1.3.3.3.3.1.1.2.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1"><apply id="S3.E4.m1.3.3.3.3.1.1.1.1.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.3.3.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.3.3.3.3.1.1.1.1.2.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2">𝑎</ci><list id="S3.E4.m1.2.2.2.2.2.3.cmml" xref="S3.E4.m1.2.2.2.2.2.4"><ci id="S3.E4.m1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1">𝑖</ci><ci id="S3.E4.m1.2.2.2.2.2.2.cmml" xref="S3.E4.m1.2.2.2.2.2.2">𝑡</ci></list></apply></set><apply id="S3.E4.m1.3.3.3.3.1.3.cmml" xref="S3.E4.m1.3.3.3.3.1.3"><eq id="S3.E4.m1.3.3.3.3.1.3.1.cmml" xref="S3.E4.m1.3.3.3.3.1.3.1"></eq><ci id="S3.E4.m1.3.3.3.3.1.3.2.cmml" xref="S3.E4.m1.3.3.3.3.1.3.2">𝑡</ci><cn id="S3.E4.m1.3.3.3.3.1.3.3.cmml" type="integer" xref="S3.E4.m1.3.3.3.3.1.3.3">0</cn></apply></apply><apply id="S3.E4.m1.3.3.3.3.3.cmml" xref="S3.E4.m1.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.3.3.3.1.cmml" xref="S3.E4.m1.3.3.3.3.3">subscript</csymbol><ci id="S3.E4.m1.3.3.3.3.3.2.cmml" xref="S3.E4.m1.3.3.3.3.3.2">𝑇</ci><ci id="S3.E4.m1.3.3.3.3.3.3.cmml" xref="S3.E4.m1.3.3.3.3.3.3">𝑖</ci></apply></apply><apply id="S3.E4.m1.3.3.3.5.cmml" xref="S3.E4.m1.3.3.3.5"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.3.5.1.cmml" xref="S3.E4.m1.3.3.3.5">subscript</csymbol><ci id="S3.E4.m1.3.3.3.5.2.cmml" xref="S3.E4.m1.3.3.3.5.2">𝑝</ci><apply id="S3.E4.m1.3.3.3.5.3.cmml" xref="S3.E4.m1.3.3.3.5.3"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.3.5.3.1.cmml" xref="S3.E4.m1.3.3.3.5.3">subscript</csymbol><ci id="S3.E4.m1.3.3.3.5.3.2.cmml" xref="S3.E4.m1.3.3.3.5.3.2">𝜃</ci><ci id="S3.E4.m1.3.3.3.5.3.3.cmml" xref="S3.E4.m1.3.3.3.5.3.3">𝒞</ci></apply></apply></apply></apply><apply id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1"><times id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.2"></times><ci id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.3.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.3">𝑅</ci><apply id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1">superscript</csymbol><apply id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1">subscript</csymbol><set id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.1.1"><apply id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.1.1.1.2">𝑎</ci><list id="S3.E4.m1.5.5.2.3.cmml" xref="S3.E4.m1.5.5.2.4"><ci id="S3.E4.m1.4.4.1.1.cmml" xref="S3.E4.m1.4.4.1.1">𝑖</ci><ci id="S3.E4.m1.5.5.2.2.cmml" xref="S3.E4.m1.5.5.2.2">𝑡</ci></list></apply></set><apply id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.3.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.3"><eq id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.3.1"></eq><ci id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.3.2">𝑡</ci><cn id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.3.3.cmml" type="integer" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.1.3.3">0</cn></apply></apply><apply id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.3.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.3.1.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.3.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.3.2">𝑇</ci><ci id="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.3.3.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.1.1.1.1.3.3">𝑖</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.6c">\displaystyle\theta_{\mathcal{C}}^{*}=\operatorname*{arg\,max}_{\theta_{%
\mathcal{C}}}\operatorname*{\mathbb{E}}_{q_{i}\in Q}\operatorname*{\mathbb{E}}%
_{\{a_{i,t}\}_{t=0}^{T_{i}}\in p_{\theta_{\mathcal{C}}}}\left[R({\{a_{i,t}\}}_%
{t=0}^{T_{i}})\right],</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.6d">italic_θ start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ italic_Q end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT { italic_a start_POSTSUBSCRIPT italic_i , italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ∈ italic_p start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ italic_R ( { italic_a start_POSTSUBSCRIPT italic_i , italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.SSS2.p1.5">where <math alttext="R" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.3.m1.1"><semantics id="S3.SS3.SSS2.p1.3.m1.1a"><mi id="S3.SS3.SSS2.p1.3.m1.1.1" xref="S3.SS3.SSS2.p1.3.m1.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.3.m1.1b"><ci id="S3.SS3.SSS2.p1.3.m1.1.1.cmml" xref="S3.SS3.SSS2.p1.3.m1.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.3.m1.1c">R</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p1.3.m1.1d">italic_R</annotation></semantics></math> is the reward estimated from the sequence of feedback and <math alttext="T_{i}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.4.m2.1"><semantics id="S3.SS3.SSS2.p1.4.m2.1a"><msub id="S3.SS3.SSS2.p1.4.m2.1.1" xref="S3.SS3.SSS2.p1.4.m2.1.1.cmml"><mi id="S3.SS3.SSS2.p1.4.m2.1.1.2" xref="S3.SS3.SSS2.p1.4.m2.1.1.2.cmml">T</mi><mi id="S3.SS3.SSS2.p1.4.m2.1.1.3" xref="S3.SS3.SSS2.p1.4.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.4.m2.1b"><apply id="S3.SS3.SSS2.p1.4.m2.1.1.cmml" xref="S3.SS3.SSS2.p1.4.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.4.m2.1.1.1.cmml" xref="S3.SS3.SSS2.p1.4.m2.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p1.4.m2.1.1.2.cmml" xref="S3.SS3.SSS2.p1.4.m2.1.1.2">𝑇</ci><ci id="S3.SS3.SSS2.p1.4.m2.1.1.3.cmml" xref="S3.SS3.SSS2.p1.4.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.4.m2.1c">T_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p1.4.m2.1d">italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> denotes the number of iterations needed for handling <math alttext="q_{i}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.5.m3.1"><semantics id="S3.SS3.SSS2.p1.5.m3.1a"><msub id="S3.SS3.SSS2.p1.5.m3.1.1" xref="S3.SS3.SSS2.p1.5.m3.1.1.cmml"><mi id="S3.SS3.SSS2.p1.5.m3.1.1.2" xref="S3.SS3.SSS2.p1.5.m3.1.1.2.cmml">q</mi><mi id="S3.SS3.SSS2.p1.5.m3.1.1.3" xref="S3.SS3.SSS2.p1.5.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.5.m3.1b"><apply id="S3.SS3.SSS2.p1.5.m3.1.1.cmml" xref="S3.SS3.SSS2.p1.5.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.5.m3.1.1.1.cmml" xref="S3.SS3.SSS2.p1.5.m3.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p1.5.m3.1.1.2.cmml" xref="S3.SS3.SSS2.p1.5.m3.1.1.2">𝑞</ci><ci id="S3.SS3.SSS2.p1.5.m3.1.1.3.cmml" xref="S3.SS3.SSS2.p1.5.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.5.m3.1c">q_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p1.5.m3.1d">italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<section class="ltx_paragraph" id="S3.SS3.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">Reinforcement Learning (RL) for Tool Learning.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS2.Px1.p1.1">RL is a common solution to enabling artificial agents to learn from their environment in complex decision-making processes <cite class="ltx_cite ltx_citemacro_citep">(Silver et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib149" title="">2018</a>; Berner et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib14" title="">2019</a>; Schrittwieser et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib140" title="">2020</a>)</cite>. Tool learning can be considered an RL scenario, where the action space is defined by tools, and the agent learns to select the appropriate tool and perform the correct actions that maximize the reward signal. The policy model can be initialized by a foundation model <cite class="ltx_cite ltx_citemacro_citep">(Schulman et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib141" title="">2017</a>)</cite>.
Such initialization brings the policy model abundant prior knowledge, alleviating the need for the RL agent to learn basic skills. With a reward function that quantifies the performance of the agent in achieving the task goal, RL has been successfully used in various tool learning scenarios, such as robotic grasping <cite class="ltx_cite ltx_citemacro_citep">(Levine et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib77" title="">2018</a>)</cite> and multi-agent autocurricula <cite class="ltx_cite ltx_citemacro_citep">(Baker et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib11" title="">2020</a>)</cite>. By optimizing the loss function, the agent learns to reflect on the current state of the environment, select the appropriate tool, and perform the right actions that lead to the highest expected reward.
In the following, we introduce two sources of feedback: environment feedback and human feedback, which can be considered sources of reward signals in the context of tool learning. These two feedbacks are complementary and can be combined with each other.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">Environment Feedback.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.Px2.p1.1">The controller interacts with the environment and receives feedback about the consequences of its actions. The model then updates its policy based on this feedback to improve its tool-use behavior. Environment feedback can be categorized into two forms: (1) <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS2.Px2.p1.1.1">result feedback</span>, which is ultimate feedback returned from the environment, indicating whether the model’s actions have successfully completed the task or not. This type of feedback performs an overall assessment of the planning generated by the model. For instance, WebShop <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib193" title="">2022a</a>)</cite> uses a hand-coded reward to assess the similarity between human-bought and model-bought products, which indicates whether the actions performed by the controller lead to the correct final product. By receiving feedback on the success or failure of its actions, the model can iteratively update its planning strategy, and adjust its decision-making process; (2) <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS2.Px2.p1.1.2">intermediate feedback</span>, which refers to the state change of the environment triggered by an action. By observing the state changes, foundation models can learn whether each action is effective and appropriate, making the model better adjust its behaviors accordingly. This kind of feedback provides more detailed and timely information about the effectiveness of each tool execution. Take the case of interacting with a search engine to gather information for question-answering, models could update their policy for more efficient information retrieval by observing the rendered information of a search query.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS2.Px3">
<h5 class="ltx_title ltx_title_paragraph">Human Feedback.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.Px3.p1">
<p class="ltx_p" id="S3.SS3.SSS2.Px3.p1.2">Humans could give the model rewards and penalties based on its generated plans to regulate its behavior. Human feedback can be <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS2.Px3.p1.2.1">explicit</span>, which provides clear and direct insights into the model performance representing human preferences. For example, rating the quality of the model-generated action on a scale of <math alttext="1" class="ltx_Math" display="inline" id="S3.SS3.SSS2.Px3.p1.1.m1.1"><semantics id="S3.SS3.SSS2.Px3.p1.1.m1.1a"><mn id="S3.SS3.SSS2.Px3.p1.1.m1.1.1" xref="S3.SS3.SSS2.Px3.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.Px3.p1.1.m1.1b"><cn id="S3.SS3.SSS2.Px3.p1.1.m1.1.1.cmml" type="integer" xref="S3.SS3.SSS2.Px3.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.Px3.p1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.Px3.p1.1.m1.1d">1</annotation></semantics></math> to <math alttext="5" class="ltx_Math" display="inline" id="S3.SS3.SSS2.Px3.p1.2.m2.1"><semantics id="S3.SS3.SSS2.Px3.p1.2.m2.1a"><mn id="S3.SS3.SSS2.Px3.p1.2.m2.1.1" xref="S3.SS3.SSS2.Px3.p1.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.Px3.p1.2.m2.1b"><cn id="S3.SS3.SSS2.Px3.p1.2.m2.1.1.cmml" type="integer" xref="S3.SS3.SSS2.Px3.p1.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.Px3.p1.2.m2.1c">5</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.Px3.p1.2.m2.1d">5</annotation></semantics></math>; human feedback can also be <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS2.Px3.p1.2.2">implicit</span>, which is not directly specified by the user but can be derived from user behavior and interactions with the model. Examples include users’ comparison <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib120" title="">2022</a>)</cite>, response time, and actions taken after receiving a model’s output (e.g., clicking on a recommended link).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.Px3.p2">
<p class="ltx_p" id="S3.SS3.SSS2.Px3.p2.1">Though human feedback is accurate and stable, it is label-intensive and has high latency. To address this issue, reinforcement learning from human feedback (RLHF) <cite class="ltx_cite ltx_citemacro_citep">(Christiano et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib28" title="">2017</a>)</cite> is proposed to finetune a model to imitate humans to give rewards, which are then used to optimize the policy with RL algorithms such as PPO <cite class="ltx_cite ltx_citemacro_citep">(Schulman et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib141" title="">2017</a>)</cite>. RLHF has yielded exceptional performance in various domains such as text summarization <cite class="ltx_cite ltx_citemacro_citep">(Ziegler et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib203" title="">2019</a>; Stiennon et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib155" title="">2020</a>)</cite>. RLHF can also improve a model’s tool-use capabilities even if it has been trained on sufficient supervised human demonstrations. For instance, WebGPT <cite class="ltx_cite ltx_citemacro_citep">(Nakano et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib110" title="">2021</a>)</cite> utilizes human feedback to guide a policy model to align with human preferences, which helps better manipulate search engines to answer long-form questions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.Px3.p3">
<p class="ltx_p" id="S3.SS3.SSS2.Px3.p3.1">Despite its remarkable performance, RLHF still faces challenges: (1) <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS2.Px3.p3.1.1">task-specific nature</span>: the corresponding evaluation criteria for specific tasks need to be pre-defined, and the preference data annotated for one task is hard to be transferred to other settings, which limits the applicability of RLHF to a wider range of tasks. To this end, it is critical to develop a universal reward model that generalizes to various tasks; (2) <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS2.Px3.p3.1.2">biases</span>: RL agents optimize towards the pseudo-human reward model, thus can be up-bounded and biased by human preferences. Besides, societal biases or personal experiences may be amplified during RLHF, and it is essential to carefully evaluate the learned reward model for any biases and take measures to mitigate them.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3 </span>Generalizable Tool Learning</h4>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS3.p1">
<p class="ltx_p" id="S3.SS3.SSS3.p1.1">Generalization of tool use is a key characteristic of human intelligence <cite class="ltx_cite ltx_citemacro_citep">(Seed &amp; Byrne, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib142" title="">2010</a>; Teschke et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib160" title="">2013</a>; Osiurak et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib119" title="">2018</a>)</cite>. The ancient human, for instance, recognized that regardless of the specific tool being used, a sharp edge was essential for achieving clean cuts and efficiently carrying out tasks. This recognition allowed them to transfer their knowledge of sharpening a knife to sharpening other tools, such as scrapers or choppers. Generalization is also a critical aspect of tool learning, especially considering the existence of a massive and rapidly expanding array of tools. Although conducting supervised fine-tuning on a vast collection of tool-use data can be a potential solution to facilitating generalization, collecting enough supervised tool-use data and ensuring its quality and diversity is time-consuming and practically infeasible.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS3.p2">
<p class="ltx_p" id="S3.SS3.SSS3.p2.1">Generalizable tool learning highlights the importance of <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS3.p2.1.1">abstraction</span>, which is the process of identifying the essential features of a tool. Abstraction involves recognizing commonalities and patterns of tools so that models could synthesize and transfer their knowledge and skills, enabling them to use novel tools with ease. For instance, by abstracting essential features such as layers, filters, and color adjustments, users can transfer their knowledge of using Adobe Photoshop to Adobe Illustrator, even if the interface and specific tool names in these two figure-editing software are different. Abstracting these general features of tools can quickly help users learn a new tool effectively by building on previous experience.</p>
</div>
<section class="ltx_paragraph" id="S3.SS3.SSS3.Px1">
<h5 class="ltx_title ltx_title_paragraph">Foundation of Generalization: Interface Unification.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS3.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS3.Px1.p1.1">To facilitate knowledge transfer among tools, it is critical to design a unified interface that enables the model to manipulate various tools in a consistent and standardized manner, which serves as the foundation for generalizable tool learning. Through a unified interface, models can identify and abstract essential features of tools more easily in a unified tool protocol rather than grappling with the difficulty of understanding various tool interfaces. Currently, the manipulation of tools is through predicting discrete action tokens, and the action space is not aligned in different scenarios, which prohibits the models from quickly adapting to new scenarios and tools. Inspired by the aspect we categorize tools in <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2.SS2" title="2.2 Tool Categorization: A User-Interface Perspective ‣ 2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">2.2</span></a>, we identify three potential ways of interface unification: the semantic interface, GUI interface, and programming interface.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS3.Px1.p2">
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i1.p1.1.1">Semantic Interface.</span> The semantic interface operates by utilizing a specific text span (action name) as the action trigger, which is the most intuitive and natural way for interface unification. For instance, ReAct <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib194" title="">2022b</a>)</cite> employs <span class="ltx_text ltx_font_typewriter" id="S3.I2.i1.p1.1.2">Action:Search</span> as the trigger for the function that searches for relevant passages. In robotic manipulation <cite class="ltx_cite ltx_citemacro_citep">(Ahn et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib2" title="">2022</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib88" title="">2023</a>)</cite>, the generated natural language (e.g., <span class="ltx_text ltx_font_typewriter" id="S3.I2.i1.p1.1.3">pick up the sponge</span>) is mapped to specific actions. Despite its ease of implementation, the semantic interface poses certain challenges that must be addressed. First, the mapping between the generated text and the corresponding tool action should be pre-defined individually, which is a laborious task, particularly when the tool set expands quickly. Moreover, the model may fail to accurately produce the precise form to trigger the intended action, even leading to false triggering of actions.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;padding-top:3.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i2.p1.1.1">GUI Interface.</span> Humans primarily interact with the digital world through GUI interface (e.g., mouse and keyboard), which has been extensively optimized to follow human action efficiently. Nevertheless, before robots can learn to use a GUI interface flexibly, it is necessary to establish a virtual environment that can facilitate mapping predicted tokens to human-like mouse movements and keyboard inputs. Prior research has explored providing platforms for agents to complete web-based tasks using keyboard and mouse actions <cite class="ltx_cite ltx_citemacro_citep">(Shi et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib144" title="">2017</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib87" title="">2018a</a>)</cite>. However, these environments restrict models to a limited set of pre-defined mouse options and common keyboard actions such as copy and paste. By leveraging foundation models, it is possible to introduce prior knowledge regarding common combinations of keyword and mouse actions, thereby expanding the potential actions that a model can execute.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;padding-top:3.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i3.p1.1.1">Programming Interface.</span> This kind of interface allows the model to go beyond pure natural language and specify its action using a program. Such unification requires the model to be acquainted with the syntax of the function calls. The recent code-generating language models (CLM) such as Incoder <cite class="ltx_cite ltx_citemacro_citep">(Fried et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib42" title="">2022</a>)</cite> and CodeX <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib25" title="">2021</a>)</cite> provide the possibility of such unification. The programming interface has been applied widely. For example, Code-as-Policies <cite class="ltx_cite ltx_citemacro_citep">(Liang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib82" title="">2022a</a>)</cite> finds that with CLM as the backbone for robotic control, the robots can leverage the code grammar to execute complex actions, generalize to novel instructions, and give precise control with accurate parameter values to the functions. The programming interface provides promising opportunities for tool learning because (1) complex tool learning logic can be modeled using the control flow of programming language; (2) explicit calls of external APIs can be naturally implemented by executing programs.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS3.Px1.p3">
<p class="ltx_p" id="S3.SS3.SSS3.Px1.p3.1">It should be noted that the interface selection should align with the capabilities and limitations of the foundation model. For instance, language foundation models are trained to generate text and may be better suited for the semantic interface. Similarly, a multimodal foundation model that combines visual and textual information may be more appropriate for the GUI interface, as it can understand and generate human-like mouse movements and keyboard inputs. On the other hand, code foundation models may be more suitable for the programming interface, as it is trained to understand code syntax and function calls.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS3.Px1.p4">
<p class="ltx_p" id="S3.SS3.SSS3.Px1.p4.1">Under certain cases, we may face challenges where the tool’s output is not aligned with model’s input format. A common practice is to compose the functionality of the model and tool in the same modality. For example, <cite class="ltx_cite ltx_citemacro_citet">Zeng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib198" title="">2022</a>)</cite> chain together foundation models of various modalities by converting their outputs into natural languages. This simple method leverages prompting to compose new multimodal capabilities without fine-tuning. In contrast, another solution is to building multimodal foundation models that can perceive general modalities, based on the belief that multimodal foundation models can all be unified through a general-purpose interface <cite class="ltx_cite ltx_citemacro_citep">(Alayrac et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib4" title="">2022</a>; Hao et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib49" title="">2022</a>)</cite>. Gato <cite class="ltx_cite ltx_citemacro_citep">(Reed et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib132" title="">2022</a>)</cite> is a representative generalist multi-embodiment agent trained on tremendous datasets of agent experience. Gato can sense and act with different embodiments, such as playing Atari, captioning images, chatting, etc. Similarly, PaLM-E <cite class="ltx_cite ltx_citemacro_citep">(Driess et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib37" title="">2023</a>)</cite> incorporates continuous inputs from different modalities into a PLM. By joint training on multiple embodied tasks, PaLM-E could make grounded decisions in the real world.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS3.Px2">
<h5 class="ltx_title ltx_title_paragraph">Strategies of Generalizable Tool Learning.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS3.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS3.Px2.p1.1">In general, a unified interface enables models to learn and transfer knowledge more easily and efficiently, but it does not guarantee optimal learning outcomes in all scenarios. Generalizable tool learning requires models to further adapt, refine, and specialize their learned knowledge to specific tasks or domains. Here, we discuss two potential approaches to achieving this goal and facilitating generalization.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS3.Px2.p2">
<ul class="ltx_itemize" id="S3.I3">
<li class="ltx_item" id="S3.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I3.i1.p1">
<p class="ltx_p" id="S3.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I3.i1.p1.1.1">Meta Tool Learning.</span> Metacognition <cite class="ltx_cite ltx_citemacro_citep">(Clarebout et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib29" title="">2013</a>)</cite> is a crucial aspect of human intelligence that allows individuals to reflect on their own thinking and adapt their behaviors when faced with unfamiliar situations. In the context of tool learning, metacognition refers to the ability of a model to reflect on its own learning process and adapt new tool-use strategies when necessary. With metacognition, models can identify common underlying principles or patterns in tool-use strategies and transfer them to new tasks or domains. Take the case of the web search tool, when the model trained on a source search engine (e.g., Bing Search) is transferred to a target one (e.g., Google Search), the model can leverage its metacognitive awareness to adapt its tool-use strategies based on its previous experiences. This may include identifying common underlying patterns in tool-use strategies, such as effective search queries, relevant results, and user feedback, and using this metacognitive awareness to better align with the algorithms and user interface of the new search engine.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i2" style="list-style-type:none;padding-top:3.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I3.i2.p1">
<p class="ltx_p" id="S3.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I3.i2.p1.1.1">Curriculum Tool Learning.</span> Another approach to improving model generalization is through curriculum learning <cite class="ltx_cite ltx_citemacro_citep">(Bengio et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib13" title="">2009</a>)</cite>, which starts with simple tools and gradually introduces the model to more complex tools so that it can build upon its prior knowledge and develop a deeper understanding of the tool. For instance, we could start with a curriculum of basic algorithms and operations to effectively teach a model to use Mathematica<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.wolfram.com/mathematica" title="">https://www.wolfram.com/mathematica</a></span></span></span>, e.g., addition and subtraction, and then gradually move on to more complex mathematical concepts like calculus and linear algebra. This training strategy ensures that the model is introduced to the simple, essential features of the tool before moving on to more complex concepts in a way that is manageable and effective. Moreover, curriculum tool learning allows the model to learn how complex tools are built upon simple tools. It provides an understanding of how a complex tool can be seen as an updated high-level version of a simple tool, and how its function is a combination of several basic tools. This understanding of the relationship between simple and complex tools facilitates the transfer of previously learned knowledge to new tools, enabling the model to more effectively identify similarities and differences between situations and adjust its approach accordingly.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Application and Experiment</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we aim to explore the applications of tool learning and investigate the efficacy and limitations of state-of-the-art foundation models in utilizing tools. We select <math alttext="18" class="ltx_Math" display="inline" id="S4.p1.1.m1.1"><semantics id="S4.p1.1.m1.1a"><mn id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">18</mn><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><cn id="S4.p1.1.m1.1.1.cmml" type="integer" xref="S4.p1.1.m1.1.1">18</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">18</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.1d">18</annotation></semantics></math> representative tools for evaluation and place the main results in this section. For more case studies of ChatGPT, please refer to Appendix <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1" title="Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluated Tools</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We first briefly introduce the tools selected in experiments as follows:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Machine Translator.</span>
General-purpose language models may exhibit suboptimal proficiency when processing text from multiple linguistic domains. Machine translators can effectively alleviate this issue by enabling non-translation-dedicated language models to better comprehend multi-lingual texts.
Following Toolformer, we use NLLB <cite class="ltx_cite ltx_citemacro_citep">(Costa-jussà et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib32" title="">2022</a>)</cite> as our translator and choose MLQA <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib78" title="">2020a</a>)</cite>, a multilingual question answering benchmark, as the testbed. Given a context in English and a question in Arabian, the task requires answering the question using English. We randomly sample <math alttext="200" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><mn id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><cn id="S4.SS1.p2.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p2.1.m1.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">200</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">200</annotation></semantics></math> test instances from the original test data. For the evaluation metric, we choose F1-score.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.4"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.4.1">Calculator.</span>
Following the setting of Toolformer, we conduct experiments in which language models use a simple calculator to solve math word problems.
We choose a simple implementation for the calculator, which supports basic arithmetic operations (i.e., <math alttext="+" class="ltx_Math" display="inline" id="S4.SS1.p3.1.m1.1"><semantics id="S4.SS1.p3.1.m1.1a"><mo id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><plus id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.1.m1.1d">+</annotation></semantics></math>, <math alttext="-" class="ltx_Math" display="inline" id="S4.SS1.p3.2.m2.1"><semantics id="S4.SS1.p3.2.m2.1a"><mo id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><minus id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">-</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.2.m2.1d">-</annotation></semantics></math>, <math alttext="\times" class="ltx_Math" display="inline" id="S4.SS1.p3.3.m3.1"><semantics id="S4.SS1.p3.3.m3.1a"><mo id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><times id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.3.m3.1d">×</annotation></semantics></math>, <math alttext="\div" class="ltx_Math" display="inline" id="S4.SS1.p3.4.m4.1"><semantics id="S4.SS1.p3.4.m4.1a"><mo id="S4.SS1.p3.4.m4.1.1" xref="S4.SS1.p3.4.m4.1.1.cmml">÷</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m4.1b"><divide id="S4.SS1.p3.4.m4.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1"></divide></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m4.1c">\div</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.4.m4.1d">÷</annotation></semantics></math>). We evaluate two math word problem datasets: ASDiv <cite class="ltx_cite ltx_citemacro_citep">(Miao et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib101" title="">2020</a>)</cite> and MathQA <cite class="ltx_cite ltx_citemacro_citep">(Amini et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib7" title="">2019</a>)</cite> and choose accuracy as the metric.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">Map.</span>
We choose Bing Map API<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://learn.microsoft.com/en-us/bingmaps" title="">https://learn.microsoft.com/en-us/bingmaps</a></span></span></span> for location information retrieval, assisting in user queries related to the route, driving distance, latitude coordinates, nearby locations of interest, etc. We manually curate user queries through crowdsourcing.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p5.1.1">Weather.</span>
We choose Weather API<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.weatherapi.com" title="">https://www.weatherapi.com</a></span></span></span> and investigate whether models could use the tool to answer weather-related questions, such as questions about current weather in any city, forecasting the weather within two weeks in any city, and giving suggestions based on the weather information. Two APIs are supported, the first one is <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p5.1.2">GetWeatherToday&lt;city&gt;</span>, which provides the current weather condition of a city; another one is <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p5.1.3">ForecastWeather&lt;city, N&gt;</span>, which forecasts the weather of a city after N days. The detailed information returned includes the temperature, wind speed, UV index, sunrise, sunset time, etc. We manually curated <math alttext="100" class="ltx_Math" display="inline" id="S4.SS1.p5.1.m1.1"><semantics id="S4.SS1.p5.1.m1.1a"><mn id="S4.SS1.p5.1.m1.1.1" xref="S4.SS1.p5.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.1.m1.1b"><cn id="S4.SS1.p5.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p5.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.1.m1.1c">100</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p5.1.m1.1d">100</annotation></semantics></math> weather-related user queries.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p6.1.1">Stock.</span>
We choose Alpha Vantage Stock API<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.alphavantage.co/documentation" title="">https://www.alphavantage.co/documentation</a></span></span></span> for stock market querying. We aim to obtain specific information about the opening, closing, highest, or lowest price for one particular stock on one specific day or month. We manually curate <math alttext="1200" class="ltx_Math" display="inline" id="S4.SS1.p6.1.m1.1"><semantics id="S4.SS1.p6.1.m1.1a"><mn id="S4.SS1.p6.1.m1.1.1" xref="S4.SS1.p6.1.m1.1.1.cmml">1200</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.1.m1.1b"><cn id="S4.SS1.p6.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p6.1.m1.1.1">1200</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.1.m1.1c">1200</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p6.1.m1.1d">1200</annotation></semantics></math> question-answer pairs and choose accuracy as the evaluation metric.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p7">
<p class="ltx_p" id="S4.SS1.p7.3"><span class="ltx_text ltx_font_bold" id="S4.SS1.p7.3.1">Slides.</span>
Slides-making is traditionally performed by humans using a human-computer interface (e.g., mouse and keyboard). However, current models cannot directly move a mouse or press computer keys.
To address this limitation, we provide six APIs with high-level semantics for the model. Four APIs are built based on the open-source library python-pptx<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pypi.org/project/python-pptx" title="">https://pypi.org/project/python-pptx</a></span></span></span> to control the slides, one API allows the model to retrieve images from the internet based on a topic, and one API is used to submit and display the final slides to the user.
To collect the data, we brainstorm <math alttext="50" class="ltx_Math" display="inline" id="S4.SS1.p7.1.m1.1"><semantics id="S4.SS1.p7.1.m1.1a"><mn id="S4.SS1.p7.1.m1.1.1" xref="S4.SS1.p7.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p7.1.m1.1b"><cn id="S4.SS1.p7.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p7.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p7.1.m1.1c">50</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p7.1.m1.1d">50</annotation></semantics></math> different careers that require slides-making in their work, for each career, we brainstorm <math alttext="2" class="ltx_Math" display="inline" id="S4.SS1.p7.2.m2.1"><semantics id="S4.SS1.p7.2.m2.1a"><mn id="S4.SS1.p7.2.m2.1.1" xref="S4.SS1.p7.2.m2.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p7.2.m2.1b"><cn id="S4.SS1.p7.2.m2.1.1.cmml" type="integer" xref="S4.SS1.p7.2.m2.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p7.2.m2.1c">2</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p7.2.m2.1d">2</annotation></semantics></math> cases where practitioners have to make slides. The final dataset consists of <math alttext="100" class="ltx_Math" display="inline" id="S4.SS1.p7.3.m3.1"><semantics id="S4.SS1.p7.3.m3.1a"><mn id="S4.SS1.p7.3.m3.1.1" xref="S4.SS1.p7.3.m3.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p7.3.m3.1b"><cn id="S4.SS1.p7.3.m3.1.1.cmml" type="integer" xref="S4.SS1.p7.3.m3.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p7.3.m3.1c">100</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p7.3.m3.1d">100</annotation></semantics></math> slides-making tasks. We evaluate the model’s performance by counting the fraction of instances in which the model-generated API calls are correctly executed without errors.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p8">
<p class="ltx_p" id="S4.SS1.p8.2"><span class="ltx_text ltx_font_bold" id="S4.SS1.p8.2.1">Table Processing.</span>
We craft a suite of table processing APIs using pandas.DataFrame in Python.
By leveraging these tools, models can provide a more natural and streamlined experience for users, allowing them to perform data analysis and visualization tasks directly.
We manually construct a table processing dataset containing <math alttext="13" class="ltx_Math" display="inline" id="S4.SS1.p8.1.m1.1"><semantics id="S4.SS1.p8.1.m1.1a"><mn id="S4.SS1.p8.1.m1.1.1" xref="S4.SS1.p8.1.m1.1.1.cmml">13</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p8.1.m1.1b"><cn id="S4.SS1.p8.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p8.1.m1.1.1">13</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p8.1.m1.1c">13</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p8.1.m1.1d">13</annotation></semantics></math> tables and <math alttext="117" class="ltx_Math" display="inline" id="S4.SS1.p8.2.m2.1"><semantics id="S4.SS1.p8.2.m2.1a"><mn id="S4.SS1.p8.2.m2.1.1" xref="S4.SS1.p8.2.m2.1.1.cmml">117</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p8.2.m2.1b"><cn id="S4.SS1.p8.2.m2.1.1.cmml" type="integer" xref="S4.SS1.p8.2.m2.1.1">117</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p8.2.m2.1c">117</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p8.2.m2.1d">117</annotation></semantics></math> corresponding queries.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p9">
<p class="ltx_p" id="S4.SS1.p9.2"><span class="ltx_text ltx_font_bold" id="S4.SS1.p9.2.1">Knowledge Graphs.</span>
Knowledge graphs contain factual knowledge about the real world, which is stored in the form of RDF triplets. The triplets can be retrieved by SPARQL (Standard Protocol and RDF Query Language). We provide <math alttext="7" class="ltx_Math" display="inline" id="S4.SS1.p9.1.m1.1"><semantics id="S4.SS1.p9.1.m1.1a"><mn id="S4.SS1.p9.1.m1.1.1" xref="S4.SS1.p9.1.m1.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p9.1.m1.1b"><cn id="S4.SS1.p9.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p9.1.m1.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p9.1.m1.1c">7</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p9.1.m1.1d">7</annotation></semantics></math> APIs that mimic the process of human querying the knowledge graph, including showing the candidate entity/relation given a name surface form, showing a head entity’s home page, showing a tail entity’s home page, sending SPARQL queries, showing the result of SPARQL queries, and finding a keyword in the output of a SPARQL query. We curate <math alttext="64" class="ltx_Math" display="inline" id="S4.SS1.p9.2.m2.1"><semantics id="S4.SS1.p9.2.m2.1a"><mn id="S4.SS1.p9.2.m2.1.1" xref="S4.SS1.p9.2.m2.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p9.2.m2.1b"><cn id="S4.SS1.p9.2.m2.1.1.cmml" type="integer" xref="S4.SS1.p9.2.m2.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p9.2.m2.1c">64</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p9.2.m2.1d">64</annotation></semantics></math> questions that could be answered by querying knowledge graphs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p10">
<p class="ltx_p" id="S4.SS1.p10.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p10.1.1">Search Engine.</span>
We choose Bing Search API<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.microsoft.com/en-us/bing/apis/bing-web-search-api" title="">https://www.microsoft.com/en-us/bing/apis/bing-web-search-api</a></span></span></span> and test the model on real-time question answering. Two APIs are supported: the first one is <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p10.1.2">Search&lt;query&gt;</span>, which returns the top-related search results back to the model; another one is <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p10.1.3">LoadPage&lt;N&gt;</span>, which loads the detailed information of page N indexed in the search results, and returns the detailed contents. We experiment with RealTimeQA <cite class="ltx_cite ltx_citemacro_citep">(Kasai et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib65" title="">2022</a>)</cite>, which is a dynamic question-answering platform that inquires about novel events or information. Specifically, we choose the most recent release (20230217 version) of multiple-choice data for evaluation. Given the question and choices, the model is expected to interact with the search engine to extract the necessary information, before settling on the final answer.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p11">
<p class="ltx_p" id="S4.SS1.p11.3"><span class="ltx_text ltx_font_bold" id="S4.SS1.p11.3.1">Wikipedia.</span>
We largely build our Wikipedia Search tool upon ReAct <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib194" title="">2022b</a>)</cite> with slight modifications on the API designs. The tool consists of <math alttext="3" class="ltx_Math" display="inline" id="S4.SS1.p11.1.m1.1"><semantics id="S4.SS1.p11.1.m1.1a"><mn id="S4.SS1.p11.1.m1.1.1" xref="S4.SS1.p11.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p11.1.m1.1b"><cn id="S4.SS1.p11.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p11.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p11.1.m1.1c">3</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p11.1.m1.1d">3</annotation></semantics></math> APIs: <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p11.3.2">search&lt;entity&gt;</span>, which searches for an exact entity name on Wikipedia and returns the first <math alttext="5" class="ltx_Math" display="inline" id="S4.SS1.p11.2.m2.1"><semantics id="S4.SS1.p11.2.m2.1a"><mn id="S4.SS1.p11.2.m2.1.1" xref="S4.SS1.p11.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p11.2.m2.1b"><cn id="S4.SS1.p11.2.m2.1.1.cmml" type="integer" xref="S4.SS1.p11.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p11.2.m2.1c">5</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p11.2.m2.1d">5</annotation></semantics></math> sentences of the corresponding page if the entity exists; otherwise, it displays related entity names; <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p11.3.3">lookup&lt;keyword&gt;</span>, which looks up the keyword on the current page and returns the next sentence containing the keyword, similar to humans’ using the <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p11.3.4">CTRL+F</span> function on a web page; <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p11.3.5">disambiguate&lt;entity&gt;</span>, which inputs an entity name and displays all entities that share the same name. We focus on HotpotQA <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib192" title="">2018b</a>)</cite> for question answering. We conduct our experiments in an open-domain setting, where only the question is shown to the model. We randomly sample <math alttext="200" class="ltx_Math" display="inline" id="S4.SS1.p11.3.m3.1"><semantics id="S4.SS1.p11.3.m3.1a"><mn id="S4.SS1.p11.3.m3.1.1" xref="S4.SS1.p11.3.m3.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p11.3.m3.1b"><cn id="S4.SS1.p11.3.m3.1.1.cmml" type="integer" xref="S4.SS1.p11.3.m3.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p11.3.m3.1c">200</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p11.3.m3.1d">200</annotation></semantics></math> instances from the dataset.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p12">
<p class="ltx_p" id="S4.SS1.p12.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p12.1.1">Online Shopping.</span>
Amazon online shopping is a relatively complex web environment, in which models need to buy a commodity that satisfies various requirements mentioned in a user instruction.
Based on WebShop <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib193" title="">2022a</a>)</cite>, we build our online shopping tool, which covers mainstream online shopping actions including searching for an item, loading detailed information about an item, choosing a feature for an item, going to the previous/next page, deciding to purchase, etc. We use the dataset provided by WebShop and randomly sample <math alttext="100" class="ltx_Math" display="inline" id="S4.SS1.p12.1.m1.1"><semantics id="S4.SS1.p12.1.m1.1a"><mn id="S4.SS1.p12.1.m1.1.1" xref="S4.SS1.p12.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p12.1.m1.1b"><cn id="S4.SS1.p12.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p12.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p12.1.m1.1c">100</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p12.1.m1.1d">100</annotation></semantics></math> test instances, which cover instructions about various customers’ needs with specific requirements of commodities’ attributes.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p13">
<p class="ltx_p" id="S4.SS1.p13.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p13.1.1">Embodied Scene.</span>
ALFWorld <cite class="ltx_cite ltx_citemacro_citep">(Shridhar et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib145" title="">2021</a>)</cite> is an aligned text and embodied environment game, where agents need to interact with objects (e.g., fridge, microwave, drawer, etc.) in a house to complete a task (e.g., putting a clean spatula in a drawer).
We largely follow the setting of ReACT <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib194" title="">2022b</a>)</cite> and report the success rate on the valid set.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.1">
<tr class="ltx_tr" id="S4.T1.1.1" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1" style="background-color:#FFFFFF;">Tools</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.2.1" style="background-color:#FFFFFF;"># APIs</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.3.1" style="background-color:#FFFFFF;">Test Set</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.4.1" style="background-color:#FFFFFF;">Test Size</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.5.1" style="background-color:#FFFFFF;">No Tool</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.6"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.6.1" style="background-color:#FFFFFF;">Zero-shot</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.7"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.7.1" style="background-color:#FFFFFF;">Few-shot</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.2" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.2.1" rowspan="2"><span class="ltx_text" id="S4.T1.1.2.1.1" style="background-color:#FFFFFF;">Machine Translator</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.2" rowspan="2"><span class="ltx_text" id="S4.T1.1.2.2.1" style="background-color:#FFFFFF;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.3" rowspan="2"><span class="ltx_text" id="S4.T1.1.2.3.1" style="background-color:#FFFFFF;">MLQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.4" rowspan="2"><span class="ltx_text" id="S4.T1.1.2.4.1" style="background-color:#FFFFFF;">200</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.5"><span class="ltx_text" id="S4.T1.1.2.5.1" style="background-color:#FFFFFF;">49.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.6"><span class="ltx_text" id="S4.T1.1.2.6.1" style="background-color:#FFFFFF;">49.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.7"><span class="ltx_text" id="S4.T1.1.2.7.1" style="background-color:#FFFFFF;">54.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3">
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.1" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.3.1.1" style="background-color:#D4F2F2;">38.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.3.2.1" style="background-color:#D4F2F2;">38.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.3" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.3.3.1" style="background-color:#D4F2F2;">45.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.4.1" rowspan="2"><span class="ltx_text" id="S4.T1.1.4.1.1" style="background-color:#FFFFFF;">Calculator</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.4.2" rowspan="2"><span class="ltx_text" id="S4.T1.1.4.2.1" style="background-color:#FFFFFF;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.4.3" rowspan="2"><span class="ltx_text" id="S4.T1.1.4.3.1" style="background-color:#FFFFFF;">ASDiv</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.4.4" rowspan="2"><span class="ltx_text" id="S4.T1.1.4.4.1" style="background-color:#FFFFFF;">266</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.4.5"><span class="ltx_text" id="S4.T1.1.4.5.1" style="background-color:#FFFFFF;">85.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.4.6"><span class="ltx_text" id="S4.T1.1.4.6.1" style="background-color:#FFFFFF;">81.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.4.7"><span class="ltx_text" id="S4.T1.1.4.7.1" style="background-color:#FFFFFF;">92.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5">
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.1" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.5.1.1" style="background-color:#D4F2F2;">91.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.2" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.5.2.1" style="background-color:#D4F2F2;">74.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.3" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.5.3.1" style="background-color:#D4F2F2;">92.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.6.1" rowspan="2"><span class="ltx_text" id="S4.T1.1.6.1.1" style="background-color:#FFFFFF;">Map</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.6.2" rowspan="2"><span class="ltx_text" id="S4.T1.1.6.2.1" style="background-color:#FFFFFF;">11</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.6.3" rowspan="2"><span class="ltx_text" id="S4.T1.1.6.3.1" style="background-color:#FFFFFF;">Curated</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.6.4" rowspan="2"><span class="ltx_text" id="S4.T1.1.6.4.1" style="background-color:#FFFFFF;">129</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.6.5"><span class="ltx_text ltx_phantom ltx_ulem_sout" id="S4.T1.1.6.5.1" style="background-color:#FFFFFF;"><span style="visibility:hidden">00.0</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.6.6"><span class="ltx_text" id="S4.T1.1.6.6.1" style="background-color:#FFFFFF;">58.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.6.7"><span class="ltx_text" id="S4.T1.1.6.7.1" style="background-color:#FFFFFF;">86.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7">
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.1" style="background-color:#D4F2F2;"><span class="ltx_text ltx_phantom ltx_ulem_sout" id="S4.T1.1.7.1.1" style="background-color:#D4F2F2;"><span style="visibility:hidden">00.0</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.2" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.7.2.1" style="background-color:#D4F2F2;">29.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.3" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.7.3.1" style="background-color:#D4F2F2;">86.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.8.1" rowspan="2"><span class="ltx_text" id="S4.T1.1.8.1.1" style="background-color:#FFFFFF;">Weather</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.8.2" rowspan="2"><span class="ltx_text" id="S4.T1.1.8.2.1" style="background-color:#FFFFFF;">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.8.3" rowspan="2"><span class="ltx_text" id="S4.T1.1.8.3.1" style="background-color:#FFFFFF;">Curated</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.8.4" rowspan="2"><span class="ltx_text" id="S4.T1.1.8.4.1" style="background-color:#FFFFFF;">100</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.8.5"><span class="ltx_text ltx_phantom ltx_ulem_sout" id="S4.T1.1.8.5.1" style="background-color:#FFFFFF;"><span style="visibility:hidden">00.0</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.8.6"><span class="ltx_text" id="S4.T1.1.8.6.1" style="background-color:#FFFFFF;">39.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.8.7"><span class="ltx_text" id="S4.T1.1.8.7.1" style="background-color:#FFFFFF;">99.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.9">
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.1" style="background-color:#D4F2F2;"><span class="ltx_text ltx_phantom ltx_ulem_sout" id="S4.T1.1.9.1.1" style="background-color:#D4F2F2;"><span style="visibility:hidden">00.0</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.2" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.9.2.1" style="background-color:#D4F2F2;">92.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.3" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.9.3.1" style="background-color:#D4F2F2;">99.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.10" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.10.1" rowspan="2"><span class="ltx_text" id="S4.T1.1.10.1.1" style="background-color:#FFFFFF;">Stock</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.10.2" rowspan="2"><span class="ltx_text" id="S4.T1.1.10.2.1" style="background-color:#FFFFFF;">8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.10.3" rowspan="2"><span class="ltx_text" id="S4.T1.1.10.3.1" style="background-color:#FFFFFF;">Curated</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.10.4" rowspan="2"><span class="ltx_text" id="S4.T1.1.10.4.1" style="background-color:#FFFFFF;">122</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.10.5"><span class="ltx_text ltx_phantom ltx_ulem_sout" id="S4.T1.1.10.5.1" style="background-color:#FFFFFF;"><span style="visibility:hidden">00.0</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.10.6"><span class="ltx_text" id="S4.T1.1.10.6.1" style="background-color:#FFFFFF;">33.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.10.7"><span class="ltx_text" id="S4.T1.1.10.7.1" style="background-color:#FFFFFF;">63.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.11">
<td class="ltx_td ltx_align_center" id="S4.T1.1.11.1" style="background-color:#D4F2F2;"><span class="ltx_text ltx_phantom ltx_ulem_sout" id="S4.T1.1.11.1.1" style="background-color:#D4F2F2;"><span style="visibility:hidden">00.0</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.11.2" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.11.2.1" style="background-color:#D4F2F2;">39.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.11.3" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.11.3.1" style="background-color:#D4F2F2;">64.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.12" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.12.1" rowspan="2"><span class="ltx_text" id="S4.T1.1.12.1.1" style="background-color:#FFFFFF;">Slides</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.12.2" rowspan="2"><span class="ltx_text" id="S4.T1.1.12.2.1" style="background-color:#FFFFFF;">6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.12.3" rowspan="2"><span class="ltx_text" id="S4.T1.1.12.3.1" style="background-color:#FFFFFF;">Curated</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.12.4" rowspan="2"><span class="ltx_text" id="S4.T1.1.12.4.1" style="background-color:#FFFFFF;">100</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.12.5"><span class="ltx_text ltx_phantom ltx_ulem_sout" id="S4.T1.1.12.5.1" style="background-color:#FFFFFF;"><span style="visibility:hidden">00.0</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.12.6"><span class="ltx_text" id="S4.T1.1.12.6.1" style="background-color:#FFFFFF;">95.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.12.7"><span class="ltx_text" id="S4.T1.1.12.7.1" style="background-color:#FFFFFF;">97.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.13">
<td class="ltx_td ltx_align_center" id="S4.T1.1.13.1" style="background-color:#D4F2F2;"><span class="ltx_text ltx_phantom ltx_ulem_sout" id="S4.T1.1.13.1.1" style="background-color:#D4F2F2;"><span style="visibility:hidden">00.0</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.13.2" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.13.2.1" style="background-color:#D4F2F2;">94.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.13.3" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.13.3.1" style="background-color:#D4F2F2;">86.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.14" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.14.1" rowspan="2"><span class="ltx_text" id="S4.T1.1.14.1.1" style="background-color:#FFFFFF;">Tables</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.14.2" rowspan="2"><span class="ltx_text" id="S4.T1.1.14.2.1" style="background-color:#FFFFFF;">21</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.14.3" rowspan="2"><span class="ltx_text" id="S4.T1.1.14.3.1" style="background-color:#FFFFFF;">Curated</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.14.4" rowspan="2"><span class="ltx_text" id="S4.T1.1.14.4.1" style="background-color:#FFFFFF;">117</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.14.5"><span class="ltx_text" id="S4.T1.1.14.5.1" style="background-color:#FFFFFF;">54.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.14.6"><span class="ltx_text" id="S4.T1.1.14.6.1" style="background-color:#FFFFFF;">60.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.14.7"><span class="ltx_text" id="S4.T1.1.14.7.1" style="background-color:#FFFFFF;">85.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.15">
<td class="ltx_td ltx_align_center" id="S4.T1.1.15.1" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.15.1.1" style="background-color:#D4F2F2;">60.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.15.2" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.15.2.1" style="background-color:#D4F2F2;">73.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.15.3" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.15.3.1" style="background-color:#D4F2F2;">92.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.16" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.16.1" rowspan="2"><span class="ltx_text" id="S4.T1.1.16.1.1" style="background-color:#FFFFFF;">KGs</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.16.2" rowspan="2"><span class="ltx_text" id="S4.T1.1.16.2.1" style="background-color:#FFFFFF;">7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.16.3" rowspan="2"><span class="ltx_text" id="S4.T1.1.16.3.1" style="background-color:#FFFFFF;">Curated</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.16.4" rowspan="2"><span class="ltx_text" id="S4.T1.1.16.4.1" style="background-color:#FFFFFF;">64</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.16.5"><span class="ltx_text ltx_phantom ltx_ulem_sout" id="S4.T1.1.16.5.1" style="background-color:#FFFFFF;"><span style="visibility:hidden">00.0</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.16.6"><span class="ltx_text" id="S4.T1.1.16.6.1" style="background-color:#FFFFFF;">42.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.16.7"><span class="ltx_text" id="S4.T1.1.16.7.1" style="background-color:#FFFFFF;">46.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.17">
<td class="ltx_td ltx_align_center" id="S4.T1.1.17.1" style="background-color:#D4F2F2;"><span class="ltx_text ltx_phantom ltx_ulem_sout" id="S4.T1.1.17.1.1" style="background-color:#D4F2F2;"><span style="visibility:hidden">00.0</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.17.2" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.17.2.1" style="background-color:#D4F2F2;">  7.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.17.3" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.17.3.1" style="background-color:#D4F2F2;">14.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.18" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.18.1" rowspan="2"><span class="ltx_text" id="S4.T1.1.18.1.1" style="background-color:#FFFFFF;">Search Engine</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.18.2" rowspan="2"><span class="ltx_text" id="S4.T1.1.18.2.1" style="background-color:#FFFFFF;">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.18.3" rowspan="2"><span class="ltx_text" id="S4.T1.1.18.3.1" style="background-color:#FFFFFF;">RealTimeQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.18.4" rowspan="2"><span class="ltx_text" id="S4.T1.1.18.4.1" style="background-color:#FFFFFF;">30</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.18.5"><span class="ltx_text" id="S4.T1.1.18.5.1" style="background-color:#FFFFFF;">50.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.18.6"><span class="ltx_text" id="S4.T1.1.18.6.1" style="background-color:#FFFFFF;">50.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.18.7"><span class="ltx_text" id="S4.T1.1.18.7.1" style="background-color:#FFFFFF;">66.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.19">
<td class="ltx_td ltx_align_center" id="S4.T1.1.19.1" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.19.1.1" style="background-color:#D4F2F2;">50.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.19.2" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.19.2.1" style="background-color:#D4F2F2;">43.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.19.3" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.19.3.1" style="background-color:#D4F2F2;">63.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.20" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.20.1" rowspan="2"><span class="ltx_text" id="S4.T1.1.20.1.1" style="background-color:#FFFFFF;">Wikipedia</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.20.2" rowspan="2"><span class="ltx_text" id="S4.T1.1.20.2.1" style="background-color:#FFFFFF;">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.20.3" rowspan="2"><span class="ltx_text" id="S4.T1.1.20.3.1" style="background-color:#FFFFFF;">HotpotQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.20.4" rowspan="2"><span class="ltx_text" id="S4.T1.1.20.4.1" style="background-color:#FFFFFF;">200</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.20.5"><span class="ltx_text" id="S4.T1.1.20.5.1" style="background-color:#FFFFFF;">33.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.20.6"><span class="ltx_text" id="S4.T1.1.20.6.1" style="background-color:#FFFFFF;">28.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.20.7"><span class="ltx_text" id="S4.T1.1.20.7.1" style="background-color:#FFFFFF;">35.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.21">
<td class="ltx_td ltx_align_center" id="S4.T1.1.21.1" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.21.1.1" style="background-color:#D4F2F2;">34.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.21.2" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.21.2.1" style="background-color:#D4F2F2;">  8.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.21.3" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.21.3.1" style="background-color:#D4F2F2;">19.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.22" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.22.1" rowspan="2"><span class="ltx_text" id="S4.T1.1.22.1.1" style="background-color:#FFFFFF;">Online Shopping</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.22.2" rowspan="2"><span class="ltx_text" id="S4.T1.1.22.2.1" style="background-color:#FFFFFF;">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.22.3" rowspan="2"><span class="ltx_text" id="S4.T1.1.22.3.1" style="background-color:#FFFFFF;">Webshop</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.22.4" rowspan="2"><span class="ltx_text" id="S4.T1.1.22.4.1" style="background-color:#FFFFFF;">100</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.22.5"><span class="ltx_text ltx_phantom ltx_ulem_sout" id="S4.T1.1.22.5.1" style="background-color:#FFFFFF;"><span style="visibility:hidden">00.0</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.22.6"><span class="ltx_text" id="S4.T1.1.22.6.1" style="background-color:#FFFFFF;">38.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.22.7"><span class="ltx_text" id="S4.T1.1.22.7.1" style="background-color:#FFFFFF;">37.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.23">
<td class="ltx_td ltx_align_center" id="S4.T1.1.23.1" style="background-color:#D4F2F2;"><span class="ltx_text ltx_phantom ltx_ulem_sout" id="S4.T1.1.23.1.1" style="background-color:#D4F2F2;"><span style="visibility:hidden">00.0</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.23.2" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.23.2.1" style="background-color:#D4F2F2;">42.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.23.3" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.23.3.1" style="background-color:#D4F2F2;">35.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.24" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.24.1" rowspan="2"><span class="ltx_text" id="S4.T1.1.24.1.1" style="background-color:#FFFFFF;">Embodied Scene</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.24.2" rowspan="2"><span class="ltx_text" id="S4.T1.1.24.2.1" style="background-color:#FFFFFF;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.24.3" rowspan="2"><span class="ltx_text" id="S4.T1.1.24.3.1" style="background-color:#FFFFFF;">ALFWorld</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.24.4" rowspan="2"><span class="ltx_text" id="S4.T1.1.24.4.1" style="background-color:#FFFFFF;">134</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.24.5"><span class="ltx_text ltx_phantom ltx_ulem_sout" id="S4.T1.1.24.5.1" style="background-color:#FFFFFF;"><span style="visibility:hidden">00.0</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.24.6"><span class="ltx_text" id="S4.T1.1.24.6.1" style="background-color:#FFFFFF;">51.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.24.7"><span class="ltx_text" id="S4.T1.1.24.7.1" style="background-color:#FFFFFF;">78.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.25">
<td class="ltx_td ltx_align_center" id="S4.T1.1.25.1" style="background-color:#D4F2F2;"><span class="ltx_text ltx_phantom ltx_ulem_sout" id="S4.T1.1.25.1.1" style="background-color:#D4F2F2;"><span style="visibility:hidden">00.0</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.25.2" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.25.2.1" style="background-color:#D4F2F2;">23.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.25.3" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.25.3.1" style="background-color:#D4F2F2;">81.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.26" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.26.1" rowspan="2"><span class="ltx_text" id="S4.T1.1.26.1.1" style="background-color:#FFFFFF;">Cooking Assistant</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.26.2" rowspan="2"><span class="ltx_text" id="S4.T1.1.26.2.1" style="background-color:#FFFFFF;">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.26.3" rowspan="2"><span class="ltx_text" id="S4.T1.1.26.3.1" style="background-color:#FFFFFF;">Curated</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.26.4" rowspan="2"><span class="ltx_text" id="S4.T1.1.26.4.1" style="background-color:#FFFFFF;">50</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.26.5"><span class="ltx_text ltx_phantom ltx_ulem_sout" id="S4.T1.1.26.5.1" style="background-color:#FFFFFF;"><span style="visibility:hidden">00.0</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.26.6"><span class="ltx_text" id="S4.T1.1.26.6.1" style="background-color:#FFFFFF;">84.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.26.7"><span class="ltx_text" id="S4.T1.1.26.7.1" style="background-color:#FFFFFF;">98.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.27">
<td class="ltx_td ltx_align_center" id="S4.T1.1.27.1" style="background-color:#D4F2F2;"><span class="ltx_text ltx_phantom ltx_ulem_sout" id="S4.T1.1.27.1.1" style="background-color:#D4F2F2;"><span style="visibility:hidden">00.0</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.27.2" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.27.2.1" style="background-color:#D4F2F2;">82.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.27.3" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.27.3.1" style="background-color:#D4F2F2;">90.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.28" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.28.1" rowspan="2"><span class="ltx_text" id="S4.T1.1.28.1.1" style="background-color:#FFFFFF;">Movie Search</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.28.2" rowspan="2"><span class="ltx_text" id="S4.T1.1.28.2.1" style="background-color:#FFFFFF;">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.28.3" rowspan="2"><span class="ltx_text" id="S4.T1.1.28.3.1" style="background-color:#FFFFFF;">Curated</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.28.4" rowspan="2"><span class="ltx_text" id="S4.T1.1.28.4.1" style="background-color:#FFFFFF;">60</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.28.5"><span class="ltx_text ltx_phantom ltx_ulem_sout" id="S4.T1.1.28.5.1" style="background-color:#FFFFFF;"><span style="visibility:hidden">00.0</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.28.6"><span class="ltx_text" id="S4.T1.1.28.6.1" style="background-color:#FFFFFF;">77.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.28.7"><span class="ltx_text" id="S4.T1.1.28.7.1" style="background-color:#FFFFFF;">72.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.29">
<td class="ltx_td ltx_align_center" id="S4.T1.1.29.1" style="background-color:#D4F2F2;"><span class="ltx_text ltx_phantom ltx_ulem_sout" id="S4.T1.1.29.1.1" style="background-color:#D4F2F2;"><span style="visibility:hidden">00.0</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.29.2" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.29.2.1" style="background-color:#D4F2F2;">43.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.29.3" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.29.3.1" style="background-color:#D4F2F2;">75.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.30" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.30.1" rowspan="2"><span class="ltx_text" id="S4.T1.1.30.1.1" style="background-color:#FFFFFF;">AI Painting</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.30.2" rowspan="2"><span class="ltx_text" id="S4.T1.1.30.2.1" style="background-color:#FFFFFF;">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.30.3" rowspan="2"><span class="ltx_text" id="S4.T1.1.30.3.1" style="background-color:#FFFFFF;">Curated</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.30.4" rowspan="2"><span class="ltx_text" id="S4.T1.1.30.4.1" style="background-color:#FFFFFF;">25</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.30.5"><span class="ltx_text ltx_phantom ltx_ulem_sout" id="S4.T1.1.30.5.1" style="background-color:#FFFFFF;"><span style="visibility:hidden">00.0</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.30.6"><span class="ltx_text" id="S4.T1.1.30.6.1" style="background-color:#FFFFFF;">93.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.30.7"><span class="ltx_text" id="S4.T1.1.30.7.1" style="background-color:#FFFFFF;">100.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.31">
<td class="ltx_td ltx_align_center" id="S4.T1.1.31.1" style="background-color:#D4F2F2;"><span class="ltx_text ltx_phantom ltx_ulem_sout" id="S4.T1.1.31.1.1" style="background-color:#D4F2F2;"><span style="visibility:hidden">00.0</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.31.2" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.31.2.1" style="background-color:#D4F2F2;">90.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.31.3" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.31.3.1" style="background-color:#D4F2F2;">88.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.32" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.32.1" rowspan="2"><span class="ltx_text" id="S4.T1.1.32.1.1" style="background-color:#FFFFFF;">3D Model Construction</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.32.2" rowspan="2"><span class="ltx_text" id="S4.T1.1.32.2.1" style="background-color:#FFFFFF;">14</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.32.3" rowspan="2"><span class="ltx_text" id="S4.T1.1.32.3.1" style="background-color:#FFFFFF;">Curated</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.32.4" rowspan="2"><span class="ltx_text" id="S4.T1.1.32.4.1" style="background-color:#FFFFFF;">10</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.32.5"><span class="ltx_text ltx_phantom ltx_ulem_sout" id="S4.T1.1.32.5.1" style="background-color:#FFFFFF;"><span style="visibility:hidden">00.0</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.32.6"><span class="ltx_text" id="S4.T1.1.32.6.1" style="background-color:#FFFFFF;">20.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.32.7"><span class="ltx_text" id="S4.T1.1.32.7.1" style="background-color:#FFFFFF;">40.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.33">
<td class="ltx_td ltx_align_center" id="S4.T1.1.33.1" style="background-color:#D4F2F2;"><span class="ltx_text ltx_phantom ltx_ulem_sout" id="S4.T1.1.33.1.1" style="background-color:#D4F2F2;"><span style="visibility:hidden">00.0</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.33.2" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.33.2.1" style="background-color:#D4F2F2;">  0.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.33.3" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.33.3.1" style="background-color:#D4F2F2;">40.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.34" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.34.1" rowspan="2"><span class="ltx_text" id="S4.T1.1.34.1.1" style="background-color:#FFFFFF;">Chemical Properties</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.34.2" rowspan="2"><span class="ltx_text" id="S4.T1.1.34.2.1" style="background-color:#FFFFFF;">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.34.3" rowspan="2"><span class="ltx_text" id="S4.T1.1.34.3.1" style="background-color:#FFFFFF;">Curated</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.34.4" rowspan="2"><span class="ltx_text" id="S4.T1.1.34.4.1" style="background-color:#FFFFFF;">100</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.34.5"><span class="ltx_text" id="S4.T1.1.34.5.1" style="background-color:#FFFFFF;">35.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.34.6"><span class="ltx_text" id="S4.T1.1.34.6.1" style="background-color:#FFFFFF;">55.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.34.7"><span class="ltx_text" id="S4.T1.1.34.7.1" style="background-color:#FFFFFF;">73.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.35">
<td class="ltx_td ltx_align_center" id="S4.T1.1.35.1" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.35.1.1" style="background-color:#D4F2F2;">46.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.35.2" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.35.2.1" style="background-color:#D4F2F2;">67.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.35.3" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.35.3.1" style="background-color:#D4F2F2;">81.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.36" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T1.1.36.1" rowspan="2"><span class="ltx_text" id="S4.T1.1.36.1.1" style="background-color:#FFFFFF;">Database</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.1.36.2" rowspan="2"><span class="ltx_text" id="S4.T1.1.36.2.1" style="background-color:#FFFFFF;">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.1.36.3" rowspan="2"><span class="ltx_text" id="S4.T1.1.36.3.1" style="background-color:#FFFFFF;">Curated</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.1.36.4" rowspan="2"><span class="ltx_text" id="S4.T1.1.36.4.1" style="background-color:#FFFFFF;">50</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.36.5"><span class="ltx_text ltx_phantom ltx_ulem_sout" id="S4.T1.1.36.5.1" style="background-color:#FFFFFF;"><span style="visibility:hidden">00.0</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.36.6"><span class="ltx_text" id="S4.T1.1.36.6.1" style="background-color:#FFFFFF;">50.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.36.7"><span class="ltx_text" id="S4.T1.1.36.7.1" style="background-color:#FFFFFF;">75.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.37">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.37.1" style="background-color:#D4F2F2;"><span class="ltx_text ltx_phantom ltx_ulem_sout" id="S4.T1.1.37.1.1" style="background-color:#D4F2F2;"><span style="visibility:hidden">00.0</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.37.2" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.37.2.1" style="background-color:#D4F2F2;">58.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.37.3" style="background-color:#D4F2F2;"><span class="ltx_text" id="S4.T1.1.37.3.1" style="background-color:#D4F2F2;">75.0</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span> We list the overall results of different tools evaluated in this paper. “# APIs” denotes the number of APIs corresponding to each tool. The test set means the dataset we employed in conducting the experiments. We show the result of three settings i.e., <span class="ltx_text ltx_font_bold" id="S4.T1.6.1">No Tool</span>, <span class="ltx_text ltx_font_bold" id="S4.T1.7.2">Zero-shot</span>, <span class="ltx_text ltx_font_bold" id="S4.T1.8.3">Few-shot</span>. The results of text-davinci-003 are shown on white background, while the results of ChatGPT are shown in <span class="ltx_text" id="S4.T1.9.4" style="background-color:#D4F2F2;">cyan</span> background.
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS1.p14">
<p class="ltx_p" id="S4.SS1.p14.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p14.1.1">Cooking Assistant.</span>
We choose AllRecipe<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.allrecipes.com/" title="">https://www.allrecipes.com/</a></span></span></span> to investigate whether models can find the proper cooking recipe and extract important details. The tool is designed similarly to the search engine tool. With this tool, the model can perform: (1) finding the target recipe, and (2) answering questions based on observed details. We manually curate <math alttext="50" class="ltx_Math" display="inline" id="S4.SS1.p14.1.m1.1"><semantics id="S4.SS1.p14.1.m1.1a"><mn id="S4.SS1.p14.1.m1.1.1" xref="S4.SS1.p14.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p14.1.m1.1b"><cn id="S4.SS1.p14.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p14.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p14.1.m1.1c">50</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p14.1.m1.1d">50</annotation></semantics></math> queries for evaluation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p15">
<p class="ltx_p" id="S4.SS1.p15.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p15.1.1">Movie Search.</span>
We choose Douban Film API<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://movie.douban.com" title="">https://movie.douban.com</a></span></span></span> to search for movie-related information. Three APIs are devised with the aim of discovering movies that are currently playing or upcoming, as well as extracting detailed information about each movie. We curate <math alttext="60" class="ltx_Math" display="inline" id="S4.SS1.p15.1.m1.1"><semantics id="S4.SS1.p15.1.m1.1a"><mn id="S4.SS1.p15.1.m1.1.1" xref="S4.SS1.p15.1.m1.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p15.1.m1.1b"><cn id="S4.SS1.p15.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p15.1.m1.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p15.1.m1.1c">60</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p15.1.m1.1d">60</annotation></semantics></math> questions about the movies, such as recommending some movies which are on display or upcoming and providing a brief introduction to a movie.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p16">
<p class="ltx_p" id="S4.SS1.p16.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p16.1.1">AI Painting.</span>
AI image generation model has been widely used by human artists. To endow models with the capacity to create images using the AI image generation model, we provide the following APIs: one API generates an image given a prompt using stable diffusion <cite class="ltx_cite ltx_citemacro_citep">(Rombach et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib135" title="">2022</a>)</cite>, others are the image segmentation <span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/CIDAS/clipseg-rd64-refined" title="">https://huggingface.co/CIDAS/clipseg-rd64-refined</a></span></span></span> and image inpainting APIs <span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/runwayml/stable-diffusion-inpainting" title="">https://huggingface.co/runwayml/stable-diffusion-inpainting</a></span></span></span>, which replace a target object in an image with a new object described by a prompt. We curate <math alttext="25" class="ltx_Math" display="inline" id="S4.SS1.p16.1.m1.1"><semantics id="S4.SS1.p16.1.m1.1a"><mn id="S4.SS1.p16.1.m1.1.1" xref="S4.SS1.p16.1.m1.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p16.1.m1.1b"><cn id="S4.SS1.p16.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p16.1.m1.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p16.1.m1.1c">25</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p16.1.m1.1d">25</annotation></semantics></math> queries as the initial prompt, together with subsequent queries for modifying that image.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p17">
<p class="ltx_p" id="S4.SS1.p17.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p17.1.1">3D Model Construction.</span>
We investigate three-dimensional (3D) modeling by manually devising a collection of APIs that leverage the capabilities of the sophisticated 3D rendering engine Taichi<span class="ltx_note ltx_role_footnote" id="footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/taichi-dev/voxel-challenge" title="">https://github.com/taichi-dev/voxel-challenge</a></span></span></span>. Due to the complexity of executing this API (3D rendering), we only demonstrate the performance on <math alttext="10" class="ltx_Math" display="inline" id="S4.SS1.p17.1.m1.1"><semantics id="S4.SS1.p17.1.m1.1a"><mn id="S4.SS1.p17.1.m1.1.1" xref="S4.SS1.p17.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p17.1.m1.1b"><cn id="S4.SS1.p17.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p17.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p17.1.m1.1c">10</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p17.1.m1.1d">10</annotation></semantics></math> curated questions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p18">
<p class="ltx_p" id="S4.SS1.p18.2"><span class="ltx_text ltx_font_bold" id="S4.SS1.p18.2.1">Chemical Properties.</span> To evaluate the capability of tool learning in professional domains, we utilize the Chemical Property query, and more specifically, the PubChem<span class="ltx_note ltx_role_footnote" id="footnote16"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubchem.ncbi.nlm.nih.gov" title="">https://pubchem.ncbi.nlm.nih.gov</a></span></span></span> API for resolving scientific inquiries. <math alttext="4" class="ltx_Math" display="inline" id="S4.SS1.p18.1.m1.1"><semantics id="S4.SS1.p18.1.m1.1a"><mn id="S4.SS1.p18.1.m1.1.1" xref="S4.SS1.p18.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p18.1.m1.1b"><cn id="S4.SS1.p18.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p18.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p18.1.m1.1c">4</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p18.1.m1.1d">4</annotation></semantics></math> APIs are supported, which facilitate the retrieval of a chemical’s identification number based on the name or SMILES notation <cite class="ltx_cite ltx_citemacro_citep">(Weininger, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib183" title="">1988</a>)</cite>, as well as obtaining the chemical’s properties based on its identification number. We manually curated <math alttext="100" class="ltx_Math" display="inline" id="S4.SS1.p18.2.m2.1"><semantics id="S4.SS1.p18.2.m2.1a"><mn id="S4.SS1.p18.2.m2.1.1" xref="S4.SS1.p18.2.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p18.2.m2.1b"><cn id="S4.SS1.p18.2.m2.1.1.cmml" type="integer" xref="S4.SS1.p18.2.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p18.2.m2.1c">100</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p18.2.m2.1d">100</annotation></semantics></math> questions for evaluation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p19">
<p class="ltx_p" id="S4.SS1.p19.4"><span class="ltx_text ltx_font_bold" id="S4.SS1.p19.4.1">Database.</span> We explore the potential of tool learning in accessing database data via natural language. The fundamental APIs include <math alttext="(i)" class="ltx_Math" display="inline" id="S4.SS1.p19.1.m1.1"><semantics id="S4.SS1.p19.1.m1.1a"><mrow id="S4.SS1.p19.1.m1.1.2.2"><mo id="S4.SS1.p19.1.m1.1.2.2.1" stretchy="false">(</mo><mi id="S4.SS1.p19.1.m1.1.1" xref="S4.SS1.p19.1.m1.1.1.cmml">i</mi><mo id="S4.SS1.p19.1.m1.1.2.2.2" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p19.1.m1.1b"><ci id="S4.SS1.p19.1.m1.1.1.cmml" xref="S4.SS1.p19.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p19.1.m1.1c">(i)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p19.1.m1.1d">( italic_i )</annotation></semantics></math> obtaining the structural information of the target data (schema); <math alttext="(ii)" class="ltx_Math" display="inline" id="S4.SS1.p19.2.m2.1"><semantics id="S4.SS1.p19.2.m2.1a"><mrow id="S4.SS1.p19.2.m2.1.1.1" xref="S4.SS1.p19.2.m2.1.1.1.1.cmml"><mo id="S4.SS1.p19.2.m2.1.1.1.2" stretchy="false" xref="S4.SS1.p19.2.m2.1.1.1.1.cmml">(</mo><mrow id="S4.SS1.p19.2.m2.1.1.1.1" xref="S4.SS1.p19.2.m2.1.1.1.1.cmml"><mi id="S4.SS1.p19.2.m2.1.1.1.1.2" xref="S4.SS1.p19.2.m2.1.1.1.1.2.cmml">i</mi><mo id="S4.SS1.p19.2.m2.1.1.1.1.1" xref="S4.SS1.p19.2.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S4.SS1.p19.2.m2.1.1.1.1.3" xref="S4.SS1.p19.2.m2.1.1.1.1.3.cmml">i</mi></mrow><mo id="S4.SS1.p19.2.m2.1.1.1.3" stretchy="false" xref="S4.SS1.p19.2.m2.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p19.2.m2.1b"><apply id="S4.SS1.p19.2.m2.1.1.1.1.cmml" xref="S4.SS1.p19.2.m2.1.1.1"><times id="S4.SS1.p19.2.m2.1.1.1.1.1.cmml" xref="S4.SS1.p19.2.m2.1.1.1.1.1"></times><ci id="S4.SS1.p19.2.m2.1.1.1.1.2.cmml" xref="S4.SS1.p19.2.m2.1.1.1.1.2">𝑖</ci><ci id="S4.SS1.p19.2.m2.1.1.1.1.3.cmml" xref="S4.SS1.p19.2.m2.1.1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p19.2.m2.1c">(ii)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p19.2.m2.1d">( italic_i italic_i )</annotation></semantics></math> translating the input natural language text into an equivalent SQL query; <math alttext="(iii)" class="ltx_Math" display="inline" id="S4.SS1.p19.3.m3.1"><semantics id="S4.SS1.p19.3.m3.1a"><mrow id="S4.SS1.p19.3.m3.1.1.1" xref="S4.SS1.p19.3.m3.1.1.1.1.cmml"><mo id="S4.SS1.p19.3.m3.1.1.1.2" stretchy="false" xref="S4.SS1.p19.3.m3.1.1.1.1.cmml">(</mo><mrow id="S4.SS1.p19.3.m3.1.1.1.1" xref="S4.SS1.p19.3.m3.1.1.1.1.cmml"><mi id="S4.SS1.p19.3.m3.1.1.1.1.2" xref="S4.SS1.p19.3.m3.1.1.1.1.2.cmml">i</mi><mo id="S4.SS1.p19.3.m3.1.1.1.1.1" xref="S4.SS1.p19.3.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S4.SS1.p19.3.m3.1.1.1.1.3" xref="S4.SS1.p19.3.m3.1.1.1.1.3.cmml">i</mi><mo id="S4.SS1.p19.3.m3.1.1.1.1.1a" xref="S4.SS1.p19.3.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S4.SS1.p19.3.m3.1.1.1.1.4" xref="S4.SS1.p19.3.m3.1.1.1.1.4.cmml">i</mi></mrow><mo id="S4.SS1.p19.3.m3.1.1.1.3" stretchy="false" xref="S4.SS1.p19.3.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p19.3.m3.1b"><apply id="S4.SS1.p19.3.m3.1.1.1.1.cmml" xref="S4.SS1.p19.3.m3.1.1.1"><times id="S4.SS1.p19.3.m3.1.1.1.1.1.cmml" xref="S4.SS1.p19.3.m3.1.1.1.1.1"></times><ci id="S4.SS1.p19.3.m3.1.1.1.1.2.cmml" xref="S4.SS1.p19.3.m3.1.1.1.1.2">𝑖</ci><ci id="S4.SS1.p19.3.m3.1.1.1.1.3.cmml" xref="S4.SS1.p19.3.m3.1.1.1.1.3">𝑖</ci><ci id="S4.SS1.p19.3.m3.1.1.1.1.4.cmml" xref="S4.SS1.p19.3.m3.1.1.1.1.4">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p19.3.m3.1c">(iii)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p19.3.m3.1d">( italic_i italic_i italic_i )</annotation></semantics></math> rewriting the query into an execution-efficient one; and <math alttext="(iv)" class="ltx_Math" display="inline" id="S4.SS1.p19.4.m4.1"><semantics id="S4.SS1.p19.4.m4.1a"><mrow id="S4.SS1.p19.4.m4.1.1.1" xref="S4.SS1.p19.4.m4.1.1.1.1.cmml"><mo id="S4.SS1.p19.4.m4.1.1.1.2" stretchy="false" xref="S4.SS1.p19.4.m4.1.1.1.1.cmml">(</mo><mrow id="S4.SS1.p19.4.m4.1.1.1.1" xref="S4.SS1.p19.4.m4.1.1.1.1.cmml"><mi id="S4.SS1.p19.4.m4.1.1.1.1.2" xref="S4.SS1.p19.4.m4.1.1.1.1.2.cmml">i</mi><mo id="S4.SS1.p19.4.m4.1.1.1.1.1" xref="S4.SS1.p19.4.m4.1.1.1.1.1.cmml">⁢</mo><mi id="S4.SS1.p19.4.m4.1.1.1.1.3" xref="S4.SS1.p19.4.m4.1.1.1.1.3.cmml">v</mi></mrow><mo id="S4.SS1.p19.4.m4.1.1.1.3" stretchy="false" xref="S4.SS1.p19.4.m4.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p19.4.m4.1b"><apply id="S4.SS1.p19.4.m4.1.1.1.1.cmml" xref="S4.SS1.p19.4.m4.1.1.1"><times id="S4.SS1.p19.4.m4.1.1.1.1.1.cmml" xref="S4.SS1.p19.4.m4.1.1.1.1.1"></times><ci id="S4.SS1.p19.4.m4.1.1.1.1.2.cmml" xref="S4.SS1.p19.4.m4.1.1.1.1.2">𝑖</ci><ci id="S4.SS1.p19.4.m4.1.1.1.1.3.cmml" xref="S4.SS1.p19.4.m4.1.1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p19.4.m4.1c">(iv)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p19.4.m4.1d">( italic_i italic_v )</annotation></semantics></math> querying the result data by connecting to the database. We automatically curated 50 relatively complex queries under the TPC-H schema, which are of relatively complex structures (involving 2-6 tables and composite predicates) and take over 111.5 minutes to execute in total. The evaluation metric is the ratio of queries for which tool learning can accurately output the results.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p20">
<p class="ltx_p" id="S4.SS1.p20.1">To facilitate future research attempts, we implement and integrate all the above tools into <span class="ltx_text ltx_font_bold" id="S4.SS1.p20.1.1">BMTools<span class="ltx_note ltx_role_footnote" id="footnote17"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnote17.1.1.1">17</span></span><a class="ltx_ref ltx_url ltx_font_typewriter ltx_font_medium" href="https://github.com/OpenBMB/BMTools" title="">https://github.com/OpenBMB/BMTools</a></span></span></span></span>, which is an open-source repository that extends foundation models using tools and also serves as a platform for the community to build and share tools. With BMTools, users can easily build a new plugin by writing Python functions and also integrating external tools from other sources (e.g., ChatGPT Plugins).</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p21">
<p class="ltx_p" id="S4.SS1.p21.1">Building a <span class="ltx_text ltx_font_italic" id="S4.SS1.p21.1.1">tool library</span> for foundation models is critical to connecting foundation models with tools and we are glad to see there are emerging works in this direction. LangChain <span class="ltx_note ltx_role_footnote" id="footnote18"><sup class="ltx_note_mark">18</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">18</sup><span class="ltx_tag ltx_tag_note">18</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://docs.langchain.com" title="">https://docs.langchain.com</a></span></span></span> is the first open-sourced project that attempts to chain foundation models with tools. Under a unified interface, users could either build their own task pipelines or let the foundation models call APIs.
Most recently, TaskMatrix.AI <cite class="ltx_cite ltx_citemacro_citep">(Liang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib84" title="">2023</a>)</cite> and HuggingGPT <cite class="ltx_cite ltx_citemacro_citep">(Shen et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib143" title="">2023</a>)</cite> extend APIs and tasks to broader scenarios, including multimodal models for visual tasks, local software, and cloud service APIs. OpenAI also proposed its official tool library, ChatGPT Plugins <span class="ltx_note ltx_role_footnote" id="footnote19"><sup class="ltx_note_mark">19</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">19</sup><span class="ltx_tag ltx_tag_note">19</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/chatgpt-plugins" title="">https://openai.com/blog/chatgpt-plugins</a></span></span></span>, to empower ChatGPT with other applications. By simply providing APIs with descriptions, ChatGPT is enabled to call applications and complete more complex tasks. Different from third-party libraries, ChatGPT plugins are cautious about safety risks and establish strict standards for plugins. The library prioritizes the most essential tools such as the web browser, code interpreter, and retrieval plugin.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experiments</h3>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Settings.</h5>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">We conduct experiments on all the above tools and choose both text-davinci-003 and ChatGPT to evaluate their performance with zero-shot prompting and few-shot prompting as mentioned in <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2.SSS1" title="3.2.1 Understanding Intent and Tools ‣ 3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.2.1</span></a>:</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Zero-shot</span> prompting provides the instruction to model about the task description, and information about the APIs in the tool. Some basic guidelines can also be added to the instruction.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;padding-top:3.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Few-shot</span> prompting additionally adds concrete tool-use examples as a hint of how to use the APIs given a user query. Providing examples is expected to improve the performance.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px1.p2">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p2.1">Whenever feasible, we also compare the results with a baseline that does not involve the utilization of tools,i.e., <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p2.1.1">No Tool</span>. In such cases, we solely depend on the model’s internal knowledge to accomplish the given task (e.g., machine translation). Nonetheless, many tasks (e.g., slides-making) cannot be completed without the aid of tools. Consequently, we omit the “no tool” configuration in such cases.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px1.p3">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p3.1">In the experiment of machine translator, calculator, search engine, Wikipedia, online shopping, and ALFWorld, we employ existing datasets for evaluation. However, for other tools, a suitable dataset for experiments does not exist. To address this issue, we adopt a methodology similar to that of <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib176" title="">2022b</a>)</cite>, wherein we curate a set of user queries. Specifically, we manually write a few user queries as seed examples and use ChatGPT’s in-context learning ability to generate more instances. Then we manually filter those instances with low quality. We find empirically that the generated examples are diverse enough. Unless otherwise specified, for these manually curated test sets, we employ the trace of API calls as the metric for evaluating the models’ performance. Specifically, if humans judge that all the API calls are accurate for the given task, and they yield a reasonable result, the task is deemed to be correctly completed. The codes and our curated dataset will be made available to the academic community<span class="ltx_note ltx_role_footnote" id="footnote20"><sup class="ltx_note_mark">20</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">20</sup><span class="ltx_tag ltx_tag_note">20</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/OpenBMB/BMTools" title="">https://github.com/OpenBMB/BMTools</a></span></span></span>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Results.</h5>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.1">We present the results in Table <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S4.T1" title="Table 1 ‣ 4.1 Evaluated Tools ‣ 4 Application and Experiment ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">1</span></a>, from which we can conclude that:
(1) In most cases, models can learn how to effectively use tools with simple prompting, and improve their task performances.
(2) For the tasks that models can leverage their internal knowledge to solve (such as the cases of the calculator and search engine), utilizing tools with zero-shot prompting could sometimes lead to worse performance, which implies that sub-optimal utilization of tools may negatively impact performance. Nevertheless, incorporating tools with few-shot prompting still consistently yields superior performance than not incorporating tools. This underscores the concrete benefits that tools can bring to problem-solving, provided that they are employed effectively.
(3) Additionally, comparing the performance of ChatGPT and text-davinci-003, we observe that although ChatGPT has been fine-tuned with RLHF, it does not yield better results than text-davinci-003. We attribute this to two reasons: firstly, the alignment tax issue mentioned in <cite class="ltx_cite ltx_citemacro_cite">Ouyang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib120" title="">2022</a>)</cite>, that is, the specific task skills and in-context learning ability are undermined during RLHF training; secondly, the model size of ChatGPT, though not officially stated, might be much smaller than text-davinci-003, thus making ChatGPT harder to handle complex scenarios.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p2.1">Regarding the performance of different tools, it is important to acknowledge that the evaluation setups of these tools are inherently different, making direct comparison difficult. However, limiting our comparison to solely those tools that employ manually curated test sets and examining the successful rate of API calls, we have observed that under the few-shot prompting setting, certain tools such as Map, Weather, Slides, Tables, Cooking Assistant, and AI Painting exhibit a satisfying completion rate. These tools are deemed to be less challenging than other tools. In fact, we find empirically that both ChatGPT and text-davinci-003 can utilize these tools proficiently despite not directly being fine-tuned on them.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px2.p3">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p3.1">However, for several tools such as KGs, Wikipedia, online shopping, and 3D model construction, the model performance is still far from satisfactory even with few-shot prompting. The reason is perhaps that the usage of these tools cannot be easily learned with a few examples. For example, tools requiring the generation of executable code as the parameter to the API, such as the <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px2.p3.1.1">search_by_query</span> API in the KGs tool (see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1.SS9" title="A.9 Navigating Knowledge Graphs ‣ Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">A.9</span></a> for more details), are found to be significantly more arduous. This implies the necessity of training foundation models to use tools as mentioned in <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3" title="3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>. We provide the prompts and model responses of ChatGPT in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1" title="Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">A</span></a> as case studies for all the tools.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Safe and Trustworthy Tool Learning</h3>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Armed with external tools, AI systems can be unprecedentedly capable and human-like. With the ability to perceive, act, and make decisions, these models can potentially intervene and significantly influence human society. Although we are eager to witness how tool learning with foundation models will change our life, it is paramount to take a step back and contemplate the underlying risks. For responsible AI research, here we discuss the safety and trustworthiness problems of tool learning.</p>
</div>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Adversaries.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p1.1">Same as all the other AI systems, we could foresee that there will be external adversaries once the tool learning models are deployed in reality, and thus how to defend against these threats is of great significance <cite class="ltx_cite ltx_citemacro_citep">(Szegedy et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib158" title="">2014</a>; Wallace et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib168" title="">2019</a>; Jin et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib60" title="">2020</a>; Hendrycks et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib50" title="">2021</a>)</cite>. Recent works suggest that large foundation models like ChatGPT are more robust on hard and adversarial examples <cite class="ltx_cite ltx_citemacro_citep">(Taori et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib159" title="">2020</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib173" title="">2023a</a>)</cite>, which improves their utility in the complicated real world. But the attempt of crafting misleading or even harmful queries will undoubtfully persist as well <cite class="ltx_cite ltx_citemacro_citep">(Perez &amp; Ribeiro, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib123" title="">2022</a>)</cite>. Moreover, due to training on massive web data, foundation models are faced with long-lasting training-time security issues in deep learning, such as backdoor attacks <cite class="ltx_cite ltx_citemacro_citep">(Kurita et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib73" title="">2020</a>; Cui et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib34" title="">2022</a>)</cite> and data poisoning attacks <cite class="ltx_cite ltx_citemacro_citep">(Wallace et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib169" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p2.1">In addition to foundation models, the incorporated tools could be new attack targets for adversaries. For example, the attackers could maliciously modify the manual documentation or even the tools themselves (e.g. attacking a news API to give biased reports) to mislead the model into erroneous outcomes. The key challenge lies in the interplay between foundation models and tools, since a safe and robust system requires the models to not only learn to use tools, but also possess the ability to scrutinize, rectify, and secure them. Currently, most research endeavors aimed at defending against external adversaries focus solely on ensuring the model safety. Nonetheless, in light of the everchanging paradigm shift, safety research must also attend to tools to protect the entire system.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Governance.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p1.1">There is long-standing worry about the misuse of AI, especially the powerful foundation models <cite class="ltx_cite ltx_citemacro_citep">(Bommasani et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib17" title="">2021</a>)</cite>. Under the paradigm of tool learning, governance over foundation models is more urgently needed. The pertinent question at hand is <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px2.p1.1.1">which tools should be involved?</span> In <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S4" title="4 Application and Experiment ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">4</span></a>, we list a bunch of tools that may empower foundation models to solve complicated tasks. However, given the countless tools human beings have manufactured, we must consider if it is appropriate to allow models to master all of them. Certain tools, such as calculators and translators, may be deemed safe as they do not pose any harm to individuals. However, granting models access to the internet or permitting them to make decisions in the real world could be perilous, as they could cause negative or even dangerous influences such as disseminating falsehoods <cite class="ltx_cite ltx_citemacro_citep">(Zellers et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib197" title="">2019</a>)</cite> and harming human lives. In this regard, research communities and companies need to deliberate carefully before permitting machines to master a certain tool.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px2.p2">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p2.1">Apart from potentially engaged harmful tools, governance over tool usage is also a pertinent issue. As highlighted by <cite class="ltx_cite ltx_citemacro_citet">Amodei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib8" title="">2016</a>)</cite>, the end-to-end training paradigm in deep learning does not regulate how models achieve their objectives. Fortunately, such goal-oriented approaches did not result in catastrophic consequences due to the capability limitation of task-specific models, but it warrants serious consideration moving forward. Foundation models are not only expected to finish tasks with the help of tools but also should follow the regulations and constraints of tool usage.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Trustworthiness.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px3.p1.1">The goal of tool learning lies in creating advanced intelligent agents. However, determining whether these agents are trustworthy or not is a complex challenge. Even though tool learning delivers enhanced interpretability and robustness, the core foundation models are still considered “black boxes”. Recent research <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib27" title="">2022b</a>)</cite> shows that although large models achieve better performance, they are unable to predict when they will be wrong, rendering the calibration problem unresolved yet. Accompanied with tools, under what circumstances will the model call on the tools is unpredictable as well. Therefore, before we apply these models to high-stake scenarios such as autonomous driving <cite class="ltx_cite ltx_citemacro_citep">(Milakis et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib103" title="">2017</a>)</cite> and clinical trials <cite class="ltx_cite ltx_citemacro_citep">(Matheny et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib96" title="">2019</a>)</cite>, it is essential to thoroughly discuss to what extent should we allow AI to engage in human lives.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px3.p2">
<p class="ltx_p" id="S5.SS1.SSS0.Px3.p2.1">Moreover, the morality of foundation models has emerged as a contentious issue in recent times. Despite OpenAI’s commendable efforts to imbue InstructGPT <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib120" title="">2022</a>)</cite> and GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib114" title="">2023</a>)</cite> with human values and preferences, given the discomforting “jailbreak” responses by ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(Borji, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib18" title="">2023</a>)</cite> and New Bing <cite class="ltx_cite ltx_citemacro_citep">(Roose, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib136" title="">2023</a>)</cite>, whether these big models will be mild and compliant remains doubtful. Ironically, the very discourse that once centered around the potential recklessness of autonomous robots is now mirrored in the development of large language models, thereby fueling a self-fulfilling prophecy that further exacerbates the already frayed trustworthiness of these systems. When models could learn actively from the world via tools, the challenge of controlling their actions will become more daunting than ever before.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Tool Learning for Large Complex Systems</h3>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Different from tools with limited functionality, large complex systems (e.g., relational databases <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib202" title="">2020</a>)</cite>, manufacturing execution systems <cite class="ltx_cite ltx_citemacro_cite">Kletti (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib70" title="">2007</a>)</cite>, supply chain management systems <cite class="ltx_cite ltx_citemacro_cite">Misra et al. (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib106" title="">2010</a>)</cite>) are composed of numerous components (e.g., over 500 knobs that control different functions in relational databases <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib80" title="">2019</a>)</cite>). This complexity results in much more flexible and complicated interaction and management approaches. Consequently, there are three main challenges in applying tool learning in large complex systems.</p>
</div>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">System Learning.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px1.p1.1">Acquiring and comprehending the knowledge and functions of a large complex system is a labor-intensive task for human beings. Similarly, existing foundational models face challenges in memorizing and mastering the relevant skills associated with such systems. For instance, relational databases involve numerous built-in functions (e.g., selecting, inserting, updating, and managing user data), complicated query syntax (e.g., ORM code, SQL queries), and various maintenance problems during execution (e.g., connection anomalies, workload contention, resource problems). To address this issue, we first need to fine-tune foundation models with textual materials or even source code that describe the components, functions, execution mechanisms, and even anomaly cases of the system. This process deepens the models’ comprehension of the intricacies inherent in the system. Next, we must carefully design prompts to assist foundation models to become proficient in using specific functions (e.g., determining the calling order) and effectively manipulating this system. Finally, existing foundation models may occasionally generate erroneous or unintended actions. We can augment these models with a checking layer (e.g., SMT solver), which ensures accurate generation of function calls by the foundation model and derives both legal and reasonable interactions with the tools.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Efficiency Requirements.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px2.p1.1">In many scenarios, the efficiency of tool learning with foundation models is a critical metric when implementing inside real systems. For example, in real-time scenarios (e.g., fraud detection <cite class="ltx_cite ltx_citemacro_citep">(Wang, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib174" title="">2010</a>)</cite>), users expect fast and accurate responses within milliseconds. However, existing foundation models take a relatively long time to reason and plan how to call the functions, which is intolerable in practice. Thus, it is crucial to find efficient ways to speed up the response of tool learning while maintaining high performance. First, we can filter out irrelevant, redundant, or low-quality input information before sending it to the foundation model. Second, we can add hints in zero-prompting to enforce the foundation model to jump over redundant function calls and reasoning (e.g., ending the process right away after obtaining a good enough solution). Third, we can cache the relevant knowledge in high-speed storage (e.g., in-memory vector database) and the foundation models only retrieve the relevant knowledge to augment the reasoning procedure when necessary (e.g., the disk status information for system diagnosis).</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Privacy Concerns.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px3.p1.1">When training the foundation model for tool learning, it requires much user behavior data to simulate the thinking, planning, and decision-making processes. However, in modern systems, a significant portion of user data is of high sensitivity, which necessitates strict privacy and security regulations to ensure responsible data utilization. To address the privacy concerns, two possible solutions are federated learning and model distillation. First, federated learning enables the training of foundation models while preserving data privacy by allowing users to update model parameters using relevant data on their local machines (see <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS4" title="5.4 From General Intelligence to Personalized Intelligence ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">5.4</span></a>), which can be particularly useful in industry-specific scenarios such as scheduling high-concurrency transactions in the banking sector. Furthermore, model distillation involves the process of compressing the original foundational model into a smaller model that exhibits excellent performance on comparable tasks. By employing this approach, vendors can effectively retain both the model itself and user data within their systems, thereby ensuring efficient knowledge acquisition and data privacy.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>From Tool User to Tool Maker: AI’s Evolutionary Role</h3>
<div class="ltx_para ltx_noindent" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Throughout the annals of human civilization, the evolution of tools has occupied a pivotal position <cite class="ltx_cite ltx_citemacro_citep">(Mithen, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib107" title="">1996</a>; Ko, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib71" title="">2016</a>)</cite>. The Stone Age, in particular, witnessed the emergence of stone-based weaponry and hunting tools, which afforded humans a competitive edge over their animal counterparts. Subsequent epochs of human history were equally marked by significant societal transformations made possible by the introduction of novel tools. Notably, the invention of the steam engine heralded the onset of the first industrial revolution, while the widespread utilization of electricity catalyzed the second industrial revolution. The progression of human civilization is inextricably intertwined with the evolution of tools, and the relentless pursuit of innovative tool creation constitutes a vital aspect of human ingenuity.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">Human beings are the creators and users of almost all tools from the Stone Age to the 21st century. Although we take it as granted, things are different when foundation models are involved. Considering that they have proven tool-use capabilities to certain extents, it is also possible to put them into the lifecycle of tool creation.</p>
</div>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Tools for AI.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px1.p1.1">Humans create tools to satisfy our own needs, so the designation naturally suits human preference and convenience. However, current tool learning algorithms may not be optimal or efficient for models. This is because most tools (e.g., search engines) are specifically designed for human use, and models process information in a different way. Therefore, it is necessary to create tools that are specifically suited for models. Possible solutions may include: (1) <span class="ltx_text ltx_font_italic" id="S5.SS3.SSS0.Px1.p1.1.1">modularity</span>, which decomposes tools into smaller, more modular units, making them more adaptable and flexible for AI models. In this regard, models can learn to use these components in a more fine-grained and compositional manner; (2) <span class="ltx_text ltx_font_italic" id="S5.SS3.SSS0.Px1.p1.1.2">new input and output formats</span>: developing new input and output formats that are specifically tailored to the needs of AI models can improve their interaction and utilization of tools, enabling more seamless integration and communication between models and tools.</p>
</div>
<figure class="ltx_figure" id="S5.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="507" id="S5.F7.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Example of AI tool creation, where we ask ChatGPT to encapsulate a weather forecast API into a new function suited for a specific target.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Tools by AI.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px2.p1.1">The creation and utilization of tools have traditionally been considered exclusive to human intelligence. However, with the emergence of foundation models, this notion is being challenged. Increasing evidence indicates that the ability to create advanced tools is no longer limited to human beings. For instance, large code models <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib25" title="">2021</a>)</cite> can generate executable programs based on language description. These programs can be deemed as tools to help accomplish specific tasks. ChatGPT plugins<span class="ltx_note ltx_role_footnote" id="footnote21"><sup class="ltx_note_mark">21</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">21</sup><span class="ltx_tag ltx_tag_note">21</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/chatgpt-plugins" title="">https://openai.com/blog/chatgpt-plugins</a></span></span></span> present an awesome example about asking GPT-4 to write a TODO plugin and integrate it with ChatGPT. Besides writing codes from scratch, foundation models can also encapsulate existing tools into stronger tools. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.F7" title="Figure 7 ‣ Tools for AI. ‣ 5.3 From Tool User to Tool Maker: AI’s Evolutionary Role ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">7</span></a>, we show an example of ChatGPT encapsulating a weather forecast API into a new function that calculates the average temperature. All such evidence implies the potential for foundation models to transition from merely tool users to tool makers.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Creativity of AI.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px3.p1.1">Beyond the coding ability, other emergent abilities <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib181" title="">2022b</a>)</cite> also shed light on the possibility of more advanced tool creation. However, whether foundation models can exhibit genuine creativity in creating novel tools remains an open problem. This issue is important because the capacity for novel tool creation is a defining characteristic that distinguishes humans from animals <cite class="ltx_cite ltx_citemacro_citep">(Ambrose, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib6" title="">2010</a>)</cite>. Understanding the extent of creativity, beyond simply memorizing, composing, and interpolating between human tools encountered during pre-training, is crucial for assessing their potential to contribute to the development of new tools.
Such investigations may involve the development of novel evaluation metrics and benchmarks <cite class="ltx_cite ltx_citemacro_citep">(Liang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib83" title="">2022b</a>)</cite>, as well as the exploration of new techniques that prioritize creative problem-solving. In the future, we possess the wildest imagination that AI could create brand-new tools, such as a new language and rocket architecture.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>From General Intelligence to Personalized Intelligence</h3>
<div class="ltx_para ltx_noindent" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">Foundation models are typically trained on a generic domain and calibrated with broadly-defined human preferences that prioritize helpfulness and harmlessness <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib120" title="">2022</a>; Nakano et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib110" title="">2021</a>)</cite>. As a result, they struggle to process personal information and provide personalized assistance to users with varying needs for tool learning. For example,
when a user seeks advice on managing their finances, to provide helpful and relevant suggestions, models should first gain access to the user’s personalized data, such as income, expenses, and investment history, via financial tools. Subsequently, models may look for recent investment trends and relevant news through a search engine. By utilizing personalized information, models can provide more customized advice and offer a more tailored approach to financial management.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">User-centric and personalized natural language generation has received increasing attention in recent years <cite class="ltx_cite ltx_citemacro_citep">(Yang &amp; Flek, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib189" title="">2021</a>; Kirk et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib69" title="">2023</a>)</cite>. Existing works cover a wide range of tasks, such as dialogue generation <cite class="ltx_cite ltx_citemacro_citep">(Madotto et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib94" title="">2019</a>; Mazaré et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib98" title="">2018</a>; Song et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib153" title="">2021</a>; Zhong et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib200" title="">2022</a>)</cite>, machine translation <cite class="ltx_cite ltx_citemacro_citep">(Mirkin &amp; Meunier, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib104" title="">2015</a>; Michel &amp; Neubig, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib102" title="">2018</a>; Wuebker et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib187" title="">2018</a>)</cite>, and summarization <cite class="ltx_cite ltx_citemacro_citep">(Yan et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib188" title="">2011</a>)</cite>. These methods utilize external user-specific modules, such as user embeddings and user memory modules <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib199" title="">2018</a>; Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib186" title="">2021</a>)</cite>, to inject preferences, writing styles, and personal information of different users into the generated content. However, these works are often designed for specific tasks and experimented with limited user information. How to integrate user information into general-purpose tool learning models is still under-explored. We will discuss the key challenge of personalized tool learning in the following.</p>
</div>
<section class="ltx_paragraph" id="S5.SS4.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Aligning User Preference with Tool Manipulation.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS4.SSS0.Px1.p1.1">Personalized tool learning emphasizes the importance of considering user-specific information in tool manipulation.
There are two main challenges:
(1) <span class="ltx_text ltx_font_italic" id="S5.SS4.SSS0.Px1.p1.1.1">heterogeneous user information modeling</span>: in real-world scenarios, personal information can come from numerous heterogeneous sources.
For instance, when using an email tool, models need to consider the user’s language style from historical conversation records and gather relevant information from the user’s social networks. Other information, such as browsing history, purchase records, and behavioral data from daily life, can also reflect users’ personal preferences. This requires modelling user information with diverse structures into a unified semantic space, allowing models to utilize this information jointly;
(2) <span class="ltx_text ltx_font_italic" id="S5.SS4.SSS0.Px1.p1.1.2">personalized tool planning</span>: different users tend to have different preferences for tool planning and selection. For example, when completing the purchasing task, different users prefer to use different online shopping platforms. Similarly, when completing writing tasks, some users prefer to first search for sufficient references before writing, while others prefer to search for information while writing. Therefore, the models need to develop personalized tool execution plans based on user preferences;
(3) <span class="ltx_text ltx_font_italic" id="S5.SS4.SSS0.Px1.p1.1.3">personalized tool call</span>: adaptively calling tools according to the user’s preference is also an important direction in personalized tool learning. Most tools are designed without consideration of personalized information, which requires the model to generate different inputs for tools based on the user’s preferences. Taking the example of purchasing goods, different users have different preferences for the brand of the products. In this case, the model needs to input the user’s preferred brand into the purchasing tool to determine the product that needs to be purchased.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS4.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">From Reactive Systems to Proactive Systems.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS4.SSS0.Px2.p1.1">Currently, most of the foundation models are designed as <span class="ltx_text ltx_font_italic" id="S5.SS4.SSS0.Px2.p1.1.1">reactive systems</span>, which respond to user queries without initiating any actions on their own. A paradigm shift is underway toward <span class="ltx_text ltx_font_italic" id="S5.SS4.SSS0.Px2.p1.1.2">proactive systems</span> that can take action on behalf of the user. This shift presents both opportunities and challenges for tool learning. By leveraging the history of user interactions, proactive systems can continually improve their performance and tailor their responses to specific users, which provides a more personalized and seamless user experience. However, the introduction of proactive systems also raises several concerns regarding their safety and ethical implications. Proactive systems can initiate actions that have unintended consequences, particularly in complex and dynamic environments. This can lead to cascading failures, whereby the behavior of one assistant affects others, creating a chain reaction that is difficult to control or stop. This highlights the importance of designing proactive systems with safety in mind and incorporating fail-safe mechanisms to prevent catastrophic outcomes. To address these risks and challenges, proactive systems should be designed with the ability to identify and mitigate potential risks, as well as the flexibility to adapt and respond to unexpected situations.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS4.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Privacy Preserving Technologies.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS4.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS4.SSS0.Px3.p1.1">Personalized tool learning requires models to learn user preferences from private user information, which inevitably raises privacy-preserving concerns.
On the one hand, previous work has shown that training data extraction attacks can be applied to recover sensitive personal privacy from foundation models <cite class="ltx_cite ltx_citemacro_citep">(Carlini et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib22" title="">2021</a>)</cite>, which is a critical challenge for personalized tool learning.
On the other hand, models with high computational costs must be deployed on cloud servers, which require uploading private data to the cloud to enable personalized responses. It is crucial to develop secure and trustworthy mechanisms to access and process user data while protecting user privacy. Addressing these challenges will help unlock the potential of personalized tool learning, enabling more effective and tailored tool manipulation to meet individual user needs. To this end, it is worth exploring model-oriented distributed computing frameworks, such as edge computing and federated learning, in which cloud servers are responsible for hosting computationally intensive models, while edge devices like PCs or smartphones process personalized information to prevent its leakage.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Tool Learning and Embodied Learning</h3>
<div class="ltx_para ltx_noindent" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">The fundamental framework for tool learning entails a sequence of action and observation, where the model can perceive changes in the environment, aligning with the fundamental concept of embodied learning <cite class="ltx_cite ltx_citemacro_citep">(Duan et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib38" title="">2022</a>)</cite>. This section delves into the interplay between tool learning and embodied learning, elucidating their similarities, differences, and potential for intersection.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.1">Embodied learning posits that genuine intelligence can be acquired through interaction with the environment <cite class="ltx_cite ltx_citemacro_citep">(Smith &amp; Gasser, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib151" title="">2005</a>)</cite>. The embodiment of the agent in virtual simulation environments has been the primary focus of embodied learning research. Simulation environments provide agents with multi-modal feedback, predominantly visual feedback, which facilitates action execution within the environment’s dynamics. Different kinds of embodied environments have been proposed to facilitate the research. Some environments allow for simple object placement <cite class="ltx_cite ltx_citemacro_citep">(Puig et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib127" title="">2018</a>)</cite>, while others support more advanced physical simulation, such as collision <cite class="ltx_cite ltx_citemacro_citep">(Gan et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib43" title="">2020</a>)</cite>. Tasks typically assigned to agents include exploration <cite class="ltx_cite ltx_citemacro_citep">(Ramakrishnan et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib129" title="">2021</a>)</cite>, navigation <cite class="ltx_cite ltx_citemacro_citep">(Ye et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib195" title="">2021</a>)</cite>, question answering <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib196" title="">2019</a>)</cite> within the simulated environment, or more interactive embodied task <cite class="ltx_cite ltx_citemacro_citep">(Abramson et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib1" title="">2022</a>)</cite> based on human instructions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.p3">
<p class="ltx_p" id="S5.SS5.p3.1">While embodied learning emphasizes the use of physical interactions within a simulated environment, tool learning is not limited to a specific environment, but rather focuses on using interfaces that extend the language model’s capabilities. The intersection between these two paradigms could lead to the development of more advanced AI models capable of learning and adapting in complex and dynamic environments. Here we discuss two possible directions.</p>
</div>
<section class="ltx_paragraph" id="S5.SS5.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Tool Learning Enables Digital Embodiment.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS5.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS5.SSS0.Px1.p1.1">Tool learning broadens the scope of embodied learning research. At the core of embodied learning lies the dynamic interaction between an agent and its environment. In this sense, the model interacts with the world through tools. Even though the model might lack a physical body, it can also be seen as a kind of embodiment. We dub this form of embodiment as <span class="ltx_text ltx_font_italic" id="S5.SS5.SSS0.Px1.p1.1.1">digital embodiment</span>. To fully comprehend the concept of digital embodiment, one could envisage an agent utilizing various APIs to navigate the web, searching for relevant and up-to-date information, and constructing a personalized knowledge base. In addition, under strict safety constraints, the agent could interact with other agents using tools such as email interfaces, thereby facilitating communication and collaboration in a secure and controlled manner. This approach enables agents to exhibit a degree of autonomy and flexibility that is akin to human-like behavior.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.SSS0.Px1.p2">
<p class="ltx_p" id="S5.SS5.SSS0.Px1.p2.1">Digital embodiment serves as a testbed for the intelligent behaviors of agents. Firstly, digital embodiment presents a more accessible and practical approach to embodied learning compared to simulated environments. The ease of deployment and usage of digital embodiment makes it an attractive option for researchers investigating intelligent agent behaviors. Secondly, it is noteworthy that the challenges posed in digital embodiment tend to revolve around the increased emphasis on language-based inputs. Consequently, this necessitates agents to perform more advanced reasoning and decision-making operations, thereby promoting the development of higher-level cognitive skills. Thirdly, digital embodiment exhibits remarkable scalability, owing to the relative ease with which digital tools can be developed compared to the creation of additional interaction playgrounds in simulated environments. This feature enables the rapid scaling of digital embodiment and can facilitate the creation of increasingly complex environments and tasks for agents to operate in.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS5.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Learning to Use Embodied Tools.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS5.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS5.SSS0.Px2.p1.1">Traditional embodied learning learns directly from the environment, where the actions are often atomic and limited to basic tasks such as push, put, and drag, which fall short of the complexity of human problem-solving abilities. To narrow the gap between sim-to-real transfer <cite class="ltx_cite ltx_citemacro_citep">(Kadian et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib62" title="">2020</a>)</cite> and enhance agent performance, it is essential to incorporate embodied tools within simulated environments. For instance, by introducing objects such as hammers and knives, we can evaluate an agent’s capacity to choose the appropriate tool for cutting a piece of paper. Despite the potential benefits of such tools, to date, no studies have systematically explored the utilization of simulated tools in simulated environments, owing to the complexity of the simulation. Nevertheless, with the rapid growth of computational power in physical engines, such research directions are becoming increasingly practical. A starting point could be utilizing the assets of 3D model that has a more delicate interface and more realistic physical engine support. An additional avenue worth investigating is the automated generation of tools. Given that in tool learning, models can generate functions to define an API for their subsequent utilization, if the embodied agents are capable of generating assets from scratch or composing existing ones within a simulated environment, their intelligence quotient will be further amplified.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>Knowledge Conflicts in Tool Augmentation</h3>
<div class="ltx_para ltx_noindent" id="S5.SS6.p1">
<p class="ltx_p" id="S5.SS6.p1.1">Tools can be leveraged as complementary resources to augment foundation models to enhance their generation <cite class="ltx_cite ltx_citemacro_citep">(Mialon et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib100" title="">2023</a>)</cite>, which enables models to effectively incorporate domain-specific or up-to-date knowledge. Research in this area has primarily focused on augmenting models with external knowledge sources, such as unstructured raw text and domain-specific APIs. Below we first give a brief introduction of prior efforts in augmenting foundation models with tools.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS6.p2">
<p class="ltx_p" id="S5.SS6.p2.2">The most representative tool used for augmentation is the text retriever. Early endeavors resort to retrieving knowledge from local repositories to augment language generation. Some works propose retrieving knowledge using a <span class="ltx_text ltx_font_italic" id="S5.SS6.p2.2.1">frozen</span> knowledge retriever. For instance, <math alttext="k" class="ltx_Math" display="inline" id="S5.SS6.p2.1.m1.1"><semantics id="S5.SS6.p2.1.m1.1a"><mi id="S5.SS6.p2.1.m1.1.1" xref="S5.SS6.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS6.p2.1.m1.1b"><ci id="S5.SS6.p2.1.m1.1.1.cmml" xref="S5.SS6.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.p2.1.m1.1d">italic_k</annotation></semantics></math>NN-LM <cite class="ltx_cite ltx_citemacro_citep">(Khandelwal et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib67" title="">2020</a>)</cite> combines a pre-trained language model (PLM) and a <math alttext="k" class="ltx_Math" display="inline" id="S5.SS6.p2.2.m2.1"><semantics id="S5.SS6.p2.2.m2.1a"><mi id="S5.SS6.p2.2.m2.1.1" xref="S5.SS6.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS6.p2.2.m2.1b"><ci id="S5.SS6.p2.2.m2.1.1.cmml" xref="S5.SS6.p2.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.p2.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.p2.2.m2.1d">italic_k</annotation></semantics></math>-nearest neighbors model by linearly interpolating both models’ next word distributions, achieving lower perplexity in language modeling. Others train the retriever and the PLM in an end-to-end fashion, achieving superior performance in knowledge-intensive NLP tasks <cite class="ltx_cite ltx_citemacro_citep">(Guu et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib46" title="">2020</a>; Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib79" title="">2020b</a>; Izacard et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib58" title="">2022</a>)</cite>. Later works have gone beyond local repositories by leveraging the entire web as the knowledge source, which allows for improved temporal generalization and higher factual accuracy <cite class="ltx_cite ltx_citemacro_citep">(Piktus et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib124" title="">2021</a>; Lazaridou et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib76" title="">2022</a>; Menick et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib99" title="">2022</a>)</cite>. Instead of treating the retriever as a passive agent, researchers further demonstrate that PLMs can actively interact with a search engine like humans. For instance, BlenderBot <cite class="ltx_cite ltx_citemacro_citep">(Shuster et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib148" title="">2022</a>)</cite> is a dialogue agent that actively decides when and how to call a search engine in generating a dialogue response. LaMDA <cite class="ltx_cite ltx_citemacro_citep">(Thoppilan et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib161" title="">2022</a>)</cite> is another dialogue agent that augments its generation with sources from a search engine, a language translator, and a calculator. More recently, recitation-augmented models <cite class="ltx_cite ltx_citemacro_citep">(Sun et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib157" title="">2022</a>)</cite> are proposed, whereby relevant passages are first recited by sampling from a PLM and then used to generate the final answer. The intuition is that foundation models can also be seen as knowledge sources (i.e., model knowledge).</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS6.p3">
<p class="ltx_p" id="S5.SS6.p3.1">Apart from the retrieval tool, researchers have explored employing other tools to perform specific sub-tasks and then integrating the execution results into foundation models. For instance, <cite class="ltx_cite ltx_citemacro_citet">Cobbe et al. (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib30" title="">2021</a>)</cite> train a PLM to employ a calculator to perform basic arithmetic operations. Considering that PLMs are typically pre-trained on textual data only, thus are limited in understanding and interacting with the physical world, <cite class="ltx_cite ltx_citemacro_citet">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib89" title="">2022</a>)</cite> seek to bridge this gap and use a physics simulation engine (MuJoCo <cite class="ltx_cite ltx_citemacro_citep">(Todorov et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib162" title="">2012</a>)</cite>) to make PLMs’ reasoning grounded to the real world. Experiments show that augmenting physics simulation to PLMs could significantly enhance their physical understanding and reasoning abilities. <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib26" title="">2022a</a>); Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib44" title="">2022</a>)</cite> propose to augment PLMs with Python interpreters. Specifically, given a complex task, PLMs first understand it and generate <span class="ltx_text ltx_font_italic" id="S5.SS6.p3.1.1">programs</span> as intermediate thoughts. After that, the execution of <span class="ltx_text ltx_font_italic" id="S5.SS6.p3.1.2">programs</span> is offloaded to Python interpreters. This method exhibits superior performance in mathematical and symbolic reasoning tasks. <cite class="ltx_cite ltx_citemacro_citet">Nye et al. (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib112" title="">2021</a>)</cite> augment PLMs with a scratchpad, allowing them to emit intermediate task-solving procedures into a buffer before entering the final answer. The method significantly enhances PLMs in performing complex discrete computations.</p>
</div>
<section class="ltx_paragraph" id="S5.SS6.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Knowledge Conflicts.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS6.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS6.SSS0.Px1.p1.1">In practice, foundation models can be augmented by a variety of knowledge sources, including <span class="ltx_text ltx_font_italic" id="S5.SS6.SSS0.Px1.p1.1.1">model knowledge</span> memorized from training data and <span class="ltx_text ltx_font_italic" id="S5.SS6.SSS0.Px1.p1.1.2">augmented knowledge</span> derived from tool execution. Nonetheless, different sources of knowledge may inevitably contain conflicts, posing a challenge to the accuracy and reliability of model generation and planning in domains such as medical assistance and legal advice. In the following, we first introduce different types of knowledge conflicts and then discuss potential solutions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS6.SSS0.Px1.p2">
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">Conflicts between Model Knowledge and Augmented Knowledge.</span> Conflicts arise when there are discrepancies between the model knowledge and the knowledge augmented by tools. Such conflicts result from three primary reasons: (1) the model knowledge may become outdated, as most foundation models do not frequently update their parameters over time. In contrast, most tools provide real-time responses which are not covered in pre-training data; (2) the pre-training data is typically less curated than common AI datasets and may contain false knowledge such as human misconception and false beliefs <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib85" title="">2022</a>)</cite>. When augmented with responses from reliable sources like Wikipedia, this false knowledge can lead to conflicts; (3) the execution results from tools can also be misleading and biased, and it is crucial to carefully discriminate whether a knowledge source is trustworthy or not, as mentioned in <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS1" title="5.1 Safe and Trustworthy Tool Learning ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">5.1</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;padding-top:3.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">Conflicts among Augmented Knowledge from Different Tools.</span> In practice, the controller may retrieve knowledge from multiple tools to acquire more comprehensive and precise knowledge. However, the information returned by different tools may results in conflicts due to several reasons: (1) the credibility of different tools can vary significantly, meaning that not all tools are equally reliable or authoritative in all areas. For example, in the context of scientific research, using Google Scholar is likely to yield more reliable results than less credible sources; (2) different tools may have biases that can influence the information they provide. For example, a news aggregator may prioritize sensational headlines over accurate reporting, leading to a biased view of events; (3) even tools sharing the same functionality may produce various responses due to differences in their algorithms and implementation. For example, due to the different inner workings, Bing Translator and Google Translator may return different sequences for the same input.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S5.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="278" id="S5.F8.g1" src="x8.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>ChatGPT is able to correct its own belief by leveraging the knowledge provided by external tools.</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="362" id="S5.F9.g1" src="x9.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>When observing conflicting information retrieved from different sources, ChatGPT is able to detect such conflicts and adjust its response.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S5.SS6.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Potential Solutions for Knowledge Conflicts.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS6.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS6.SSS0.Px2.p1.1">Since the aforementioned conflicts can lead to a lack of explainability in model prediction and planning, it is crucial to guide models to integrate tool responses correctly and reliably. Research in open-domain QA has shown that small-scale models like T5 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib128" title="">2020</a>)</cite> may rely too heavily on their own knowledge after being fine-tuned on a specific dataset <cite class="ltx_cite ltx_citemacro_citep">(Longpre et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib91" title="">2021</a>)</cite>. In contrast, more advanced foundation models like ChatGPT handle such issues far better. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.F8" title="Figure 8 ‣ Knowledge Conflicts. ‣ 5.6 Knowledge Conflicts in Tool Augmentation ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">8</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.F9" title="Figure 9 ‣ Knowledge Conflicts. ‣ 5.6 Knowledge Conflicts in Tool Augmentation ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">9</span></a>, we conduct case studies of ChatGPT (Mar 23, 2023 version) by testing its behavior when conflicts arise. We find that ChatGPT is able to correct its own belief given retrieved information and discern the knowledge conflicts from different sources. Recent studies <cite class="ltx_cite ltx_citemacro_citep">(Nakano et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib110" title="">2021</a>; Menick et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib99" title="">2022</a>)</cite> have also attempted to guide models to rely more on augmented knowledge for faithful predictions. However, these works assume that the augmented responses come from a single reliable source, which may not always be the case in more complicated scenarios.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS6.SSS0.Px2.p2">
<p class="ltx_p" id="S5.SS6.SSS0.Px2.p2.1">We contend that models should have the ability to distinguish and verify the reliability of various sources. To achieve this goal, we suggest the following research directions: (1) <span class="ltx_text ltx_font_italic" id="S5.SS6.SSS0.Px2.p2.1.1">conflict detection</span>: models should first detect potential conflicts among different sources and flag them for further investigation; (2) <span class="ltx_text ltx_font_italic" id="S5.SS6.SSS0.Px2.p2.1.2">conflict resolution</span>: it is also important to make verification and choose reliable sources after conflict detection. Meanwhile, models should also provide explanations for their generation by interpreting which knowledge source is considered and how it is augmented into the final response.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.7 </span>Open Problems</h3>
<section class="ltx_paragraph" id="S5.SS7.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Striking a Balance between Internalized Capabilities and External Tools.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS7.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS7.SSS0.Px1.p1.1">The future development of foundation models for tool learning raises an intriguing question: should the capabilities of these models be primarily internalized, or should they rely more heavily on external tools? Recent advances in foundation models have exhibited these two contrasting trends, raising questions about their implications and potential trade-offs. We have discussed the tool learning ability of foundation models, suggesting the possibility of developing modular architectures that can be seamlessly integrated with a diverse array of external tools to enhance their capabilities. Such a modular approach could facilitate a more flexible and customizable AI system, allowing for rapid expansion of model capabilities to address various tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS7.SSS0.Px1.p2">
<p class="ltx_p" id="S5.SS7.SSS0.Px1.p2.1">Conversely, foundation models have increasingly displayed the ability to internalize and perform many AI tasks that previously required separate tools. For instance, the emergent multilingual abilities of foundation models can reduce the necessity for external translation APIs <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib19" title="">2020</a>)</cite>. This trend towards unified foundation models with versatile capabilities may streamline the development process and enable more efficient, self-contained AI systems that can address different tasks without additional tools.
The open question is to determine the optimal balance between internalized capabilities and external tool reliance, and where future models will lie on the spectrum between modular and uniform architectures.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS7.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Tool Use as a Gauge for Machine Intelligence.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS7.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS7.SSS0.Px2.p1.1">The ability to effectively use tools has long been considered a hallmark of human intelligence. We contend that the tool learning performance can serve as a next-generation gauge for measuring machine intelligence, offering several advantages over traditional evaluation metrics. Tool use evaluation requires AI systems to go beyond memorization and use their acquired knowledge to accomplish specific tasks, which better aligns with real-world applications and the notion of practical intelligence <cite class="ltx_cite ltx_citemacro_citep">(Sternberg, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib154" title="">1999</a>)</cite>. Hence, evaluating tool use performance is more closely aligned with human subjective perceptions of intelligence. Researchers can better assess the progress of AI systems in terms of their ability to assist human decision-making, collaborate with humans in solving problems, and contribute to a wider range of real-world applications.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS7.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Ethical Human-Model Collaboration in Tool Use.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS7.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS7.SSS0.Px3.p1.1">The integration of foundation models with human labor raises critical ethical concerns that warrant careful consideration.
Employing human labor in conjunction with AI systems could result in more robust and accurate knowledge.
However, this approach may also conflict with the widely accepted ethical principle that “human beings should be treated as ends in themselves, and not merely as means to an end” <cite class="ltx_cite ltx_citemacro_citep">(Kant &amp; Schneewind, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib64" title="">2002</a>)</cite>. Employing humans to augment the capabilities of foundation models can be seen as devaluing human dignity and commodifying human expertise, thereby undermining the intrinsic worth of individuals. To address these ethical concerns, it is essential for the community to establish guidelines and safeguards that prioritize human dignity and agency when integrating human labor with foundation models. This may involve setting clear boundaries on the types of tasks that can be delegated to humans, ensuring fair compensation and working conditions, and promoting transparency in the development of AI systems <cite class="ltx_cite ltx_citemacro_citep">(Mateescu &amp; Elish, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib95" title="">2019</a>)</cite>.
Moreover, fostering collaboration between AI researchers, ethicists, policymakers, and other stakeholders is crucial to develop a comprehensive understanding of the ethical implications of human-model collaboration and to create effective regulations that safeguard human rights and dignity <cite class="ltx_cite ltx_citemacro_citep">(Whittlestone et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib184" title="">2019</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS7.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Safety Issues of Foundation Models Accessing Physical Tools.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS7.SSS0.Px4.p1">
<p class="ltx_p" id="S5.SS7.SSS0.Px4.p1.1">The prospect of foundation models’ accessing and interacting with physical tools, such as drones, robots, and sensor-equipped devices, holds great promise for various applications, including automatic drive, agriculture, and smart home systems. Besides, by leveraging data from physical tools, models could potentially provide accurate recommendations to individuals, government agencies, and other stakeholders, resulting in significant benefits across various sectors <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib190" title="">2018a</a>)</cite>. However, this raises important safety concerns that must be thoroughly addressed before widespread implementation. Ensuring the trustworthiness of tool use is crucial, as any erroneous or malicious actions taken by these AI systems could have severe consequences, ranging from property damage and financial losses to threats <cite class="ltx_cite ltx_citemacro_citep">(Amodei et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib8" title="">2016</a>)</cite>. To mitigate these risks, researchers must focus on developing robust and reliable AI systems capable of safely interacting with physical tools. This may involve the development of novel safety mechanisms, such as uncertainty estimation, fail-safe strategies, and continuous monitoring of AI-generated actions <cite class="ltx_cite ltx_citemacro_citep">(Turner et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib164" title="">2022</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS7.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Tool Learning for Scientific Discovery.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS7.SSS0.Px5.p1">
<p class="ltx_p" id="S5.SS7.SSS0.Px5.p1.1">AI for science has drawn much attention in recent years, showing great potential in various scientific scenarios, such as HyperTree Proof Search for proving Metamath theorems <cite class="ltx_cite ltx_citemacro_citep">(Lample et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib75" title="">2022</a>)</cite>, protein structure prediction in structural biology <cite class="ltx_cite ltx_citemacro_citep">(Jumper et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib61" title="">2021</a>)</cite> and magnetic actuator coils controlling in nuclear physics <cite class="ltx_cite ltx_citemacro_citep">(Degrave et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib35" title="">2022</a>)</cite>. Overall, AI system has been proven effective in capturing rules and patterns from scientific data and providing hints for human researchers. Nevertheless, in the absence of professional scientific knowledge and reasoning ability training, the scientific problems that AI can solve are limited. Tool learning brings new solutions to this problem. Specifically, AI systems are promising to manipulate scientific tools and play more important roles in scientific discovery, and solve multidisciplinary problems (e.g., mathematics, cybernetics, materials). For instance, MATLAB <cite class="ltx_cite ltx_citemacro_citep">(Matlab, <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib97" title="">2012</a>)</cite> is designed for algorithm development, data visualization/analysis, and numerical computation. With MATLAB, AI systems can analyze raw materials, design algorithms, and verify assumptions by conducting simulations. Apart from the software level, it is also possible for AI systems to manipulate practical platforms such as the synthetic robots <cite class="ltx_cite ltx_citemacro_citep">(Burger et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib21" title="">2020</a>)</cite>, and to conduct synthetic experiments independently.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS7.SSS0.Px5.p2">
<p class="ltx_p" id="S5.SS7.SSS0.Px5.p2.1">It is not easy to realize the above ideas, though. We’ve mentioned the safety issues of accessing physical tools, and this is also one main challenge for scientific tool learning since many scientific problems need to be verified in actual situations, and this process may bring danger if decided by AIs. Meanwhile, foundation models are generally trained with natural language corpus or natural images, while scientific data are usually more heterogeneous, numerical, and structured. It is worth exploring how to fuse the general intelligence learned from plain text and the expertise needed for scientific discovery. Recently, <cite class="ltx_cite ltx_citemacro_citet">Boiko et al. (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#bib.bib16" title="">2023</a>)</cite> show the potential of this direction and build a system that uses foundation models to design, plan, and execute scientific experiments (e.g., catalyzed cross-coupling reactions).</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This paper studies the paradigm of tool learning with foundation models. We first recapitulate the cognitive origins of tool use in human history and categorize tools from the perspective of the user interface. Then we review the AI paradigm shift brought about by foundation models and discuss the complementary roles of tools and foundation models. We perform a comprehensive literature review for existing exploration in tool learning and start with formulating a general tool learning framework. Then we highlight core research problems such as bridging user intents with appropriate tools, better planning by leveraging the reasoning abilities of foundation models, training strategies for tool learning, and how to facilitate generalization for tool learning. Finally, we discuss important research topics, including safe and trustworthy tool learning, tool learning for large complex systems, AI tool creation, personalized tool learning, embodied tool learning, knowledge conflict issue in tool augmentation, etc. In general, this paper serves as a systematic investigation of tool learning. We hope this paper could facilitate research in integrating tools with foundation models in the future.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Contributions</h2>
<div class="ltx_para ltx_noindent" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">The contributions of all authors are listed as follows: Yujia Qin, Shengding Hu, Yankai Lin, Zhiyuan Liu, and Maosong Sun initiated (2022.8) and organized the research. Yujia Qin drafted the abstract. Yujia Qin and Ning Ding drafted the introduction. Zheni Zeng drafted <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2.SS1" title="2.1 Cognitive Origins of Tool Use ‣ 2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">2.1</span></a>. Ning Ding drafted <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2.SS2" title="2.2 Tool Categorization: A User-Interface Perspective ‣ 2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">2.2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2.SS3" title="2.3 Paradigm Shift of Foundation Models ‣ 2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">2.3</span></a>. Yujia Qin drafted <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S2.SS4" title="2.4 Complementary Roles of Tools and Foundation Models ‣ 2 Background ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">2.4</span></a>. Yujia Qin and Weize Chen drafted <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS1" title="3.1 Components of Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>. Yujia Qin, Yusheng Su, Kunlun Zhu, Shihao Liang, Ganqu Cui, Shengding Hu, and Weize Chen drafted <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2.SSS1" title="3.2.1 Understanding Intent and Tools ‣ 3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.2.1</span></a>. Weize Chen, Runchu Tian, and Yaxi Lu drafted <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS2.SSS2" title="3.2.2 Planning with Reasoning ‣ 3.2 The General Procedure: From Intent to Plan ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.2.2</span></a>. Yi Ren Fung and Yujia Qin drafted <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3.SSS1" title="3.3.1 Learning from Demonstrations ‣ 3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.3.1</span></a>, Yining Ye, Zhen Zhang, Shengding Hu, and Yujia Qin drafted <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3.SSS2" title="3.3.2 Learning from Feedback ‣ 3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.3.2</span></a>. Yujia Qin, Shengding Hu, and Cheng Qian drafted <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S3.SS3.SSS3" title="3.3.3 Generalizable Tool Learning ‣ 3.3 Training Models for Improved Tool Learning ‣ 3 Tool Learning ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">3.3.3</span></a>.
Shengding Hu and Yujia Qin drafted <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S4" title="4 Application and Experiment ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">4</span></a>. Ganqu Cui drafted <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS1" title="5.1 Safe and Trustworthy Tool Learning ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">5.1</span></a>. Xuanhe Zhou drafted <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS2" title="5.2 Tool Learning for Large Complex Systems ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">5.2</span></a>. Ganqu Cui and Chi Han drafted <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS3" title="5.3 From Tool User to Tool Maker: AI’s Evolutionary Role ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">5.3</span></a>. Chaojun Xiao and Yujia Qin drafted <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS4" title="5.4 From General Intelligence to Personalized Intelligence ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">5.4</span></a>. Shengding Hu drafted <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS5" title="5.5 Tool Learning and Embodied Learning ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">5.5</span></a>. Yufei Huang and Yujia Qin drafted <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS6" title="5.6 Knowledge Conflicts in Tool Augmentation ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">5.6</span></a>. Chi Han, Zheni Zeng, and Yujia Qin drafted <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S5.SS7" title="5.7 Open Problems ‣ 5 Discussion ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">5.7</span></a>. Yujia Qin drafted the conclusion.</p>
</div>
<div class="ltx_para ltx_noindent" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">The following authors conducted the experiments (<a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S4" title="4 Application and Experiment ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">4</span></a>) and drafted <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1" title="Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">appendix</span> <span class="ltx_text ltx_ref_tag">A</span></a>: 3D models (Xingyu Shen), translation (Shihao Liang), map and stock (Kunlun Zhu), making slides (Bokai Xu), movie hunter (Jing Yi), navigating knowledge graphs (Yuzhang Zhu, Zhenning Dai), AI painting (Xingyu Shen), search engine (Cheng Qian), calculator (Runchu Tian), chemicals mining (Zheni Zeng), ALFWorld (Yining Ye), weather (Cheng Qian), online shopping (Cheng Qian), processing tables (Bowen Li, Ziwei Tang), cooking assistant (Cheng Qian), Wikipedia (Yufei Huang), and database (Xuanhe Zhou). Yujia Qin and Shengding Hu led and organized the experiments. Shengding Hu organized and proofread <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#A1" title="Appendix A Case Study ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">appendix</span> <span class="ltx_text ltx_ref_tag">A</span></a>. Lan Yan, Kunlun Zhu, Shihao Liang, and Junxi Yan participated in the human evaluation for some experiments. Shengding Hu, Weilin Zhao, Yuxiang Huang, and Xin Cong built the first version of BMTools.</p>
</div>
<div class="ltx_para ltx_noindent" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1">Zhiyuan Liu, Tonshuang Wu, Heng Ji, Yankai Lin, Cheng Yang, Dahai Li, and Maosong Sun advised the project and participated in the discussion. Jason Phang, Tongshuang Wu, Xu Han, Xin Cong, and Huadong Wang provided detailed and important suggestions for the paper. Yujia Qin participated in all the sections. Yujia Qin, Yankai Lin, and Weize Chen proofread the whole paper.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abramson et al. (2022)</span>
<span class="ltx_bibblock">
Josh Abramson, Arun Ahuja, Federico Carnevale, Petko Georgiev, Alex Goldin,
Alden Hung, Jessica Landon, Jirka Lhotka, Timothy Lillicrap, Alistair Muldal,
et al.

</span>
<span class="ltx_bibblock">Improving multimodal interactive agents with reinforcement learning
from human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2211.11602</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahn et al. (2022)</span>
<span class="ltx_bibblock">
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron
David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog,
et al.

</span>
<span class="ltx_bibblock">Do as i can, not as i say: Grounding language in robotic affordances.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">ArXiv preprint</em>, abs/2204.01691, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2204.01691" title="">https://arxiv.org/abs/2204.01691</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Akkaya et al. (2019)</span>
<span class="ltx_bibblock">
Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew,
Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas,
et al.

</span>
<span class="ltx_bibblock">Solving rubik’s cube with a robot hand.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">ArXiv preprint</em>, abs/1910.07113, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1910.07113" title="">https://arxiv.org/abs/1910.07113</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alayrac et al. (2022)</span>
<span class="ltx_bibblock">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds,
et al.

</span>
<span class="ltx_bibblock">Flamingo: a visual language model for few-shot learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">ArXiv preprint</em>, abs/2204.14198, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2204.14198" title="">https://arxiv.org/abs/2204.14198</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Allen et al. (2019)</span>
<span class="ltx_bibblock">
Kelsey R Allen, Kevin A Smith, and Joshua B Tenenbaum.

</span>
<span class="ltx_bibblock">Rapid trial-and-error learning with simulation supports flexible tool
use and physical reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">ArXiv preprint</em>, abs/1907.09620, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1907.09620" title="">https://arxiv.org/abs/1907.09620</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ambrose (2010)</span>
<span class="ltx_bibblock">
Stanley H Ambrose.

</span>
<span class="ltx_bibblock">Coevolution of composite-tool technology, constructive memory, and
language: implications for the evolution of modern human behavior.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Current Anthropology</em>, 51(S1):S135–S147,
2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amini et al. (2019)</span>
<span class="ltx_bibblock">
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi,
and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">MathQA: Towards interpretable math word problem solving with
operation-based formalisms.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pp.  2357–2367,
Minneapolis, Minnesota, 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/N19-1245</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/N19-1245" title="">https://aclanthology.org/N19-1245</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amodei et al. (2016)</span>
<span class="ltx_bibblock">
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul F. Christiano, John Schulman,
and Dan Mané.

</span>
<span class="ltx_bibblock">Concrete problems in AI safety.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">ArXiv preprint</em>, abs/1606.06565, 2016.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1606.06565" title="">https://arxiv.org/abs/1606.06565</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bach et al. (2022)</span>
<span class="ltx_bibblock">
Stephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel,
Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry,
Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-david,
Canwen Xu, Gunjan Chhablani, Han Wang, Jason Fries, Maged Al-shaibani, Shanya
Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev, Mike
Tian-jian Jiang, and Alexander Rush.

</span>
<span class="ltx_bibblock">PromptSource: An integrated development environment and
repository for natural language prompts.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics: System Demonstrations</em>, pp.  93–104, Dublin,
Ireland, 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.acl-demo.9</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.acl-demo.9" title="">https://aclanthology.org/2022.acl-demo.9</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bain &amp; Sammut (1995)</span>
<span class="ltx_bibblock">
Michael Bain and Claude Sammut.

</span>
<span class="ltx_bibblock">A framework for behavioural cloning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Machine Intelligence 15</em>, pp.  103–129, 1995.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.cse.unsw.edu.au/~claude/papers/MI15.pdf" title="">http://www.cse.unsw.edu.au/~claude/papers/MI15.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baker et al. (2020)</span>
<span class="ltx_bibblock">
Bowen Baker, Ingmar Kanitscheider, Todor M. Markov, Yi Wu, Glenn Powell, Bob
McGrew, and Igor Mordatch.

</span>
<span class="ltx_bibblock">Emergent tool use from multi-agent autocurricula.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">8th International Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</em>. OpenReview.net, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=SkxpxJBKwS" title="">https://openreview.net/forum?id=SkxpxJBKwS</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baker et al. (2022)</span>
<span class="ltx_bibblock">
Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien
Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune.

</span>
<span class="ltx_bibblock">Video pretraining (vpt): Learning to act by watching unlabeled online
videos.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">ArXiv preprint</em>, abs/2206.11795, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2206.11795" title="">https://arxiv.org/abs/2206.11795</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bengio et al. (2009)</span>
<span class="ltx_bibblock">
Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston.

</span>
<span class="ltx_bibblock">Curriculum learning.

</span>
<span class="ltx_bibblock">In Andrea Pohoreckyj Danyluk, Léon Bottou, and Michael L.
Littman (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 26th Annual International Conference
on Machine Learning, ICML 2009, Montreal, Quebec, Canada, June 14-18,
2009</em>, volume 382 of <em class="ltx_emph ltx_font_italic" id="bib.bib13.2.2">ACM International Conference Proceeding Series</em>,
pp.  41–48. ACM, 2009.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/1553374.1553380</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/1553374.1553380" title="">https://doi.org/10.1145/1553374.1553380</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berner et al. (2019)</span>
<span class="ltx_bibblock">
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław
Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme,
Chris Hesse, et al.

</span>
<span class="ltx_bibblock">Dota 2 with large scale deep reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">ArXiv preprint</em>, abs/1912.06680, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1912.06680" title="">https://arxiv.org/abs/1912.06680</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Boesch et al. (2019)</span>
<span class="ltx_bibblock">
Christophe Boesch, Daša Bombjaková, Amelia Meier, and Roger Mundry.

</span>
<span class="ltx_bibblock">Learning curves and teaching when acquiring nut-cracking in humans
and chimpanzees.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Scientific Reports</em>, 9(1):1515, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Boiko et al. (2023)</span>
<span class="ltx_bibblock">
Daniil A. Boiko, Robert MacKnight, and Gabe Gomes.

</span>
<span class="ltx_bibblock">Emergent autonomous scientific research capabilities of large
language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bommasani et al. (2021)</span>
<span class="ltx_bibblock">
Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
Brunskill, et al.

</span>
<span class="ltx_bibblock">On the opportunities and risks of foundation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">ArXiv preprint</em>, abs/2108.07258, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2108.07258" title="">https://arxiv.org/abs/2108.07258</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borji (2023)</span>
<span class="ltx_bibblock">
Ali Borji.

</span>
<span class="ltx_bibblock">A categorical archive of chatgpt failures.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">ArXiv preprint</em>, abs/2302.03494, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2302.03494" title="">https://arxiv.org/abs/2302.03494</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom
Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens
Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock">In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,
Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Advances in Neural
Information Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>, 2020.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html" title="">https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bubeck et al. (2023)</span>
<span class="ltx_bibblock">
Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
et al.

</span>
<span class="ltx_bibblock">Sparks of artificial general intelligence: Early experiments with
gpt-4.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">ArXiv preprint</em>, abs/2303.12712, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2303.12712" title="">https://arxiv.org/abs/2303.12712</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burger et al. (2020)</span>
<span class="ltx_bibblock">
Benjamin Burger, Phillip M Maffettone, Vladimir V Gusev, Catherine M Aitchison,
Yang Bai, Xiaoyan Wang, Xiaobo Li, Ben M Alston, Buyi Li, Rob Clowes, et al.

</span>
<span class="ltx_bibblock">A mobile robotic chemist.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Nature</em>, 583(7815):237–241, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlini et al. (2021)</span>
<span class="ltx_bibblock">
Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel
Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar
Erlingsson, et al.

</span>
<span class="ltx_bibblock">Extracting training data from large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">30th USENIX Security Symposium (USENIX Security 21)</em>, pp. 2633–2650, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlini et al. (2022)</span>
<span class="ltx_bibblock">
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian
Tramer, and Chiyuan Zhang.

</span>
<span class="ltx_bibblock">Quantifying memorization across neural language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">ArXiv preprint</em>, abs/2202.07646, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2202.07646" title="">https://arxiv.org/abs/2202.07646</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlini et al. (2023)</span>
<span class="ltx_bibblock">
Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag,
Florian Tramèr, Borja Balle, Daphne Ippolito, and Eric Wallace.

</span>
<span class="ltx_bibblock">Extracting training data from diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">ArXiv preprint</em>, abs/2301.13188, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2301.13188" title="">https://arxiv.org/abs/2301.13188</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2021)</span>
<span class="ltx_bibblock">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira
Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
Brockman, et al.

</span>
<span class="ltx_bibblock">Evaluating large language models trained on code.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">ArXiv preprint</em>, abs/2107.03374, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2107.03374" title="">https://arxiv.org/abs/2107.03374</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2022a)</span>
<span class="ltx_bibblock">
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen.

</span>
<span class="ltx_bibblock">Program of thoughts prompting: Disentangling computation from
reasoning for numerical reasoning tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">ArXiv preprint</em>, abs/2211.12588, 2022a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2211.12588" title="">https://arxiv.org/abs/2211.12588</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2022b)</span>
<span class="ltx_bibblock">
Yangyi Chen, Lifan Yuan, Ganqu Cui, Zhiyuan Liu, and Heng Ji.

</span>
<span class="ltx_bibblock">A close look into the calibration of pre-trained language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">ArXiv preprint</em>, abs/2211.00151, 2022b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2211.00151" title="">https://arxiv.org/abs/2211.00151</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christiano et al. (2017)</span>
<span class="ltx_bibblock">
Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario
Amodei.

</span>
<span class="ltx_bibblock">Deep reinforcement learning from human preferences.

</span>
<span class="ltx_bibblock">In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Advances in Neural Information
Processing Systems</em>, volume 30. Curran Associates, Inc., 2017.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clarebout et al. (2013)</span>
<span class="ltx_bibblock">
Geraldine Clarebout, Jan Elen, Norma A Juarez Collazo, Griet Lust, and Lai
Jiang.

</span>
<span class="ltx_bibblock">Metacognition and the use of tools.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">International handbook of metacognition and learning
technologies</em>, pp.  187–195, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe et al. (2021)</span>
<span class="ltx_bibblock">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
et al.

</span>
<span class="ltx_bibblock">Training verifiers to solve math word problems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">ArXiv preprint</em>, abs/2110.14168, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2110.14168" title="">https://arxiv.org/abs/2110.14168</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Codevilla et al. (2019)</span>
<span class="ltx_bibblock">
Felipe Codevilla, Eder Santana, Antonio M. López, and Adrien Gaidon.

</span>
<span class="ltx_bibblock">Exploring the limitations of behavior cloning for autonomous driving.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">2019 IEEE/CVF International Conference on Computer Vision,
ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019</em>, pp. 9328–9337. IEEE, 2019.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ICCV.2019.00942</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ICCV.2019.00942" title="">https://doi.org/10.1109/ICCV.2019.00942</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Costa-jussà et al. (2022)</span>
<span class="ltx_bibblock">
Marta R Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth
Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean
Maillard, et al.

</span>
<span class="ltx_bibblock">No language left behind: Scaling human-centered machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">ArXiv preprint</em>, abs/2207.04672, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2207.04672" title="">https://arxiv.org/abs/2207.04672</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Creswell et al. (2022)</span>
<span class="ltx_bibblock">
Antonia Creswell, Murray Shanahan, and Irina Higgins.

</span>
<span class="ltx_bibblock">Selection-inference: Exploiting large language models for
interpretable logical reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">ArXiv preprint</em>, abs/2205.09712, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2205.09712" title="">https://arxiv.org/abs/2205.09712</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et al. (2022)</span>
<span class="ltx_bibblock">
Ganqu Cui, Lifan Yuan, Bingxiang He, Yangyi Chen, Zhiyuan Liu, and Maosong Sun.

</span>
<span class="ltx_bibblock">A unified evaluation of textual backdoor learning: Frameworks and
benchmarks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Advances in Neural Information Processing Systems</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Degrave et al. (2022)</span>
<span class="ltx_bibblock">
Jonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey,
Francesco Carpanese, Timo Ewalds, Roland Hafner, Abbas Abdolmaleki, Diego
de Las Casas, et al.

</span>
<span class="ltx_bibblock">Magnetic control of tokamak plasmas through deep reinforcement
learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Nature</em>, 602(7897):414–419, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">BERT: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pp.  4171–4186,
Minneapolis, Minnesota, 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/N19-1423</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/N19-1423" title="">https://aclanthology.org/N19-1423</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Driess et al. (2023)</span>
<span class="ltx_bibblock">
Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery,
Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al.

</span>
<span class="ltx_bibblock">Palm-e: An embodied multimodal language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">ArXiv preprint</em>, abs/2303.03378, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2303.03378" title="">https://arxiv.org/abs/2303.03378</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duan et al. (2022)</span>
<span class="ltx_bibblock">
Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, and Cheston Tan.

</span>
<span class="ltx_bibblock">A survey of embodied ai: From simulators to research tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">IEEE Transactions on Emerging Topics in Computational
Intelligence</em>, 6(2):230–244, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Engel et al. (2013)</span>
<span class="ltx_bibblock">
Andreas K Engel, Alexander Maye, Martin Kurthen, and Peter König.

</span>
<span class="ltx_bibblock">Where’s the action? the pragmatic turn in cognitive science.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Trends in cognitive sciences</em>, 17(5):202–209, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fagard et al. (2016)</span>
<span class="ltx_bibblock">
Jacqueline Fagard, Lauriane Rat-Fischer, Rana Esseily, Eszter Somogyi, and
JK O’Regan.

</span>
<span class="ltx_bibblock">What does it take for an infant to learn how to use a tool by
observation?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Frontiers in psychology</em>, 7:267, 2016.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4771934/" title="">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4771934/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frey (2007)</span>
<span class="ltx_bibblock">
Scott H Frey.

</span>
<span class="ltx_bibblock">What puts the how in where? tool use and the divided visual streams
hypothesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Cortex</em>, 43(3):368–375, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fried et al. (2022)</span>
<span class="ltx_bibblock">
Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi,
Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis.

</span>
<span class="ltx_bibblock">Incoder: A generative model for code infilling and synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">ArXiv preprint</em>, abs/2204.05999, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2204.05999" title="">https://arxiv.org/abs/2204.05999</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gan et al. (2020)</span>
<span class="ltx_bibblock">
Chuang Gan, Jeremy Schwartz, Seth Alter, Damian Mrowca, Martin Schrimpf, James
Traer, Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber,
et al.

</span>
<span class="ltx_bibblock">Threedworld: A platform for interactive multi-modal physical
simulation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:2007.04954</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2022)</span>
<span class="ltx_bibblock">
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie
Callan, and Graham Neubig.

</span>
<span class="ltx_bibblock">Pal: Program-aided language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">ArXiv preprint</em>, abs/2211.10435, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2211.10435" title="">https://arxiv.org/abs/2211.10435</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gibson et al. (1993)</span>
<span class="ltx_bibblock">
Kathleen R Gibson, Kathleen Rita Gibson, and Tim Ingold.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Tools, language and cognition in human evolution</em>.

</span>
<span class="ltx_bibblock">Cambridge University Press, 1993.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu et al. (2020)</span>
<span class="ltx_bibblock">
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.

</span>
<span class="ltx_bibblock">Retrieval augmented language model pre-training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the 37th International Conference on Machine
Learning, ICML 2020, 13-18 July 2020, Virtual Event</em>, volume 119 of
<em class="ltx_emph ltx_font_italic" id="bib.bib46.2.2">Proceedings of Machine Learning Research</em>, pp.  3929–3938. PMLR,
2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://proceedings.mlr.press/v119/guu20a.html" title="">http://proceedings.mlr.press/v119/guu20a.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al. (2021)</span>
<span class="ltx_bibblock">
Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu,
Liang Zhang, Wentao Han, Minlie Huang, Qin Jin, Yanyan Lan, Yang Liu, Zhiyuan
Liu, Zhiwu Lu, Xipeng Qiu, Ruihua Song, Jie Tang, Ji-Rong Wen, Jinhui Yuan,
Wayne Xin Zhao, and Jun Zhu.

</span>
<span class="ltx_bibblock">Pre-trained models: Past, present and future.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">AI Open</em>, 2021.

</span>
<span class="ltx_bibblock">ISSN 2666-6510.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">https://doi.org/10.1016/j.aiopen.2021.08.002</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hansen et al. (2021)</span>
<span class="ltx_bibblock">
Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Alenyà, Pieter Abbeel,
Alexei A. Efros, Lerrel Pinto, and Xiaolong Wang.

</span>
<span class="ltx_bibblock">Self-supervised policy adaptation during deployment.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">9th International Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>. OpenReview.net, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=o_V-MjyyGV_" title="">https://openreview.net/forum?id=o_V-MjyyGV_</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hao et al. (2022)</span>
<span class="ltx_bibblock">
Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming
Ma, and Furu Wei.

</span>
<span class="ltx_bibblock">Language models are general-purpose interfaces.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">ArXiv preprint</em>, abs/2206.06336, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2206.06336" title="">https://arxiv.org/abs/2206.06336</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et al. (2021)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Unsolved problems in ml safety.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">ArXiv preprint</em>, abs/2109.13916, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2109.13916" title="">https://arxiv.org/abs/2109.13916</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hernik &amp; Csibra (2009)</span>
<span class="ltx_bibblock">
Mikolaj Hernik and Gergely Csibra.

</span>
<span class="ltx_bibblock">Functional understanding facilitates learning about tools in human
children.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Current Opinion in Neurobiology</em>, 19(1):34–38, 2009.

</span>
<span class="ltx_bibblock">ISSN 0959-4388.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">https://doi.org/10.1016/j.conb.2009.05.003</span>.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S0959438809000415" title="">https://www.sciencedirect.com/science/article/pii/S0959438809000415</a>.

</span>
<span class="ltx_bibblock">Cognitive neuroscience.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heyes (2018)</span>
<span class="ltx_bibblock">
Cecilia Heyes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Cognitive gadgets: The cultural evolution of thinking</em>.

</span>
<span class="ltx_bibblock">Harvard University Press, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2022a)</span>
<span class="ltx_bibblock">
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.

</span>
<span class="ltx_bibblock">Language models as zero-shot planners: Extracting actionable
knowledge for embodied agents.

</span>
<span class="ltx_bibblock">In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba
Szepesvári, Gang Niu, and Sivan Sabato (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">International
Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore,
Maryland, USA</em>, volume 162 of <em class="ltx_emph ltx_font_italic" id="bib.bib53.2.2">Proceedings of Machine Learning
Research</em>, pp.  9118–9147. PMLR, 2022a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v162/huang22a.html" title="">https://proceedings.mlr.press/v162/huang22a.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2022b)</span>
<span class="ltx_bibblock">
Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy
Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al.

</span>
<span class="ltx_bibblock">Inner monologue: Embodied reasoning through planning with language
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">ArXiv preprint</em>, abs/2207.05608, 2022b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2207.05608" title="">https://arxiv.org/abs/2207.05608</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hunt (1996)</span>
<span class="ltx_bibblock">
Gavin R Hunt.

</span>
<span class="ltx_bibblock">Manufacture and use of hook-tools by new caledonian crows.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Nature</em>, 379(6562):249–251, 1996.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hussein et al. (2017)</span>
<span class="ltx_bibblock">
Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, and Chrisina Jayne.

</span>
<span class="ltx_bibblock">Imitation learning: A survey of learning methods.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">ACM Computing Surveys (CSUR)</em>, 50(2):1–35,
2017.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dl.acm.org/doi/abs/10.1145/3054912?casa_token=DlqMmdYdq8sAAAAA:2LEBqNJ9is6JtifUJCcvoUUiKu8zj75-Su3PBEDpmHqX1-PtZFKHv1SPfDyvSMi58HIYN7USbYw1" title="">https://dl.acm.org/doi/abs/10.1145/3054912?casa_token=DlqMmdYdq8sAAAAA:2LEBqNJ9is6JtifUJCcvoUUiKu8zj75-Su3PBEDpmHqX1-PtZFKHv1SPfDyvSMi58HIYN7USbYw1</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iyer et al. (2022)</span>
<span class="ltx_bibblock">
Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov,
Dániel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh
Koura, et al.

</span>
<span class="ltx_bibblock">Opt-iml: Scaling language model instruction meta learning through the
lens of generalization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">ArXiv preprint</em>, abs/2212.12017, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2212.12017" title="">https://arxiv.org/abs/2212.12017</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et al. (2022)</span>
<span class="ltx_bibblock">
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni,
Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard
Grave.

</span>
<span class="ltx_bibblock">Few-shot learning with retrieval augmented language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">ArXiv preprint</em>, abs/2208.03299, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2208.03299" title="">https://arxiv.org/abs/2208.03299</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jansen et al. (2007)</span>
<span class="ltx_bibblock">
Bernard J. Jansen, Danielle L. Booth, and Amanda Spink.

</span>
<span class="ltx_bibblock">Determining the user intent of web search engine queries.

</span>
<span class="ltx_bibblock">In Carey L. Williamson, Mary Ellen Zurko, Peter F. Patel-Schneider,
and Prashant J. Shenoy (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Proceedings of the 16th International
Conference on World Wide Web, WWW 2007, Banff, Alberta, Canada, May 8-12,
2007</em>, pp.  1149–1150. ACM, 2007.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/1242572.1242739</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/1242572.1242739" title="">https://doi.org/10.1145/1242572.1242739</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. (2020)</span>
<span class="ltx_bibblock">
Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits.

</span>
<span class="ltx_bibblock">Is BERT really robust? A strong baseline for natural language
attack on text classification and entailment.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">The Thirty-Fourth AAAI Conference on Artificial
Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of
Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium
on Educational Advances in Artificial Intelligence, EAAI 2020, New York,
NY, USA, February 7-12, 2020</em>, pp.  8018–8025. AAAI Press, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aaai.org/ojs/index.php/AAAI/article/view/6311" title="">https://aaai.org/ojs/index.php/AAAI/article/view/6311</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jumper et al. (2021)</span>
<span class="ltx_bibblock">
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov,
Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin
Žídek, Anna Potapenko, et al.

</span>
<span class="ltx_bibblock">Highly accurate protein structure prediction with alphafold.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Nature</em>, 596(7873):583–589, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s41586-021-03819-2" title="">https://www.nature.com/articles/s41586-021-03819-2</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kadian et al. (2020)</span>
<span class="ltx_bibblock">
Abhishek Kadian, Joanne Truong, Aaron Gokaslan, Alexander Clegg, Erik Wijmans,
Stefan Lee, Manolis Savva, Sonia Chernova, and Dhruv Batra.

</span>
<span class="ltx_bibblock">Sim2real predictivity: Does evaluation in simulation predict
real-world performance?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">IEEE Robotics and Automation Letters</em>, 5(4):6670–6677, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kahneman (2011)</span>
<span class="ltx_bibblock">
Daniel Kahneman.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">Thinking, fast and slow</em>.

</span>
<span class="ltx_bibblock">macmillan, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kant &amp; Schneewind (2002)</span>
<span class="ltx_bibblock">
Immanuel Kant and Jerome B Schneewind.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Groundwork for the Metaphysics of Morals</em>.

</span>
<span class="ltx_bibblock">Yale University Press, 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kasai et al. (2022)</span>
<span class="ltx_bibblock">
Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai,
Xinyan Yu, Dragomir Radev, Noah A. Smith, Yejin Choi, and Kentaro Inui.

</span>
<span class="ltx_bibblock">RealTime QA: What’s the answer right now?, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2207.13332" title="">https://arxiv.org/abs/2207.13332</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kelley (2013)</span>
<span class="ltx_bibblock">
David Kelley.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">The art of reasoning: An introduction to logic and critical
thinking</em>.

</span>
<span class="ltx_bibblock">WW Norton &amp; Company, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khandelwal et al. (2020)</span>
<span class="ltx_bibblock">
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis.

</span>
<span class="ltx_bibblock">Generalization through memorization: Nearest neighbor language
models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">8th International Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</em>. OpenReview.net, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=HklBjCEKvH" title="">https://openreview.net/forum?id=HklBjCEKvH</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khot et al. (2022)</span>
<span class="ltx_bibblock">
Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter
Clark, and Ashish Sabharwal.

</span>
<span class="ltx_bibblock">Decomposed prompting: A modular approach for solving complex tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">ArXiv preprint</em>, abs/2210.02406, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2210.02406" title="">https://arxiv.org/abs/2210.02406</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirk et al. (2023)</span>
<span class="ltx_bibblock">
Hannah Rose Kirk, Bertie Vidgen, Paul Röttger, and Scott A Hale.

</span>
<span class="ltx_bibblock">Personalisation within bounds: A risk taxonomy and policy framework
for the alignment of large language models with personalised feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">ArXiv preprint</em>, abs/2303.05453, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2303.05453" title="">https://arxiv.org/abs/2303.05453</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kletti (2007)</span>
<span class="ltx_bibblock">
Jürgen Kletti.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">Manufacturing Execution Systems—MES</em>.

</span>
<span class="ltx_bibblock">Springer, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ko (2016)</span>
<span class="ltx_bibblock">
Kwang Hyun Ko.

</span>
<span class="ltx_bibblock">Origins of human intelligence: The chain of tool-making and brain
evolution.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">Anthropological Notebooks</em>, 22(1), 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kübler et al. (2009)</span>
<span class="ltx_bibblock">
Sandra Kübler, Ryan McDonald, and Joakim Nivre.

</span>
<span class="ltx_bibblock">Dependency parsing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">Synthesis lectures on human language technologies</em>, 1(1):1–127, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kurita et al. (2020)</span>
<span class="ltx_bibblock">
Keita Kurita, Paul Michel, and Graham Neubig.

</span>
<span class="ltx_bibblock">Weight poisoning attacks on pretrained models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pp.  2793–2806, Online, 2020. Association
for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2020.acl-main.249</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.acl-main.249" title="">https://aclanthology.org/2020.acl-main.249</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lajoie &amp; Derry (2013)</span>
<span class="ltx_bibblock">
Susanne P Lajoie and Sharon J Derry.

</span>
<span class="ltx_bibblock">Computer environments as cognitive tools for enhancing learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">Computers as cognitive tools</em>, pp.  269–296. Routledge,
2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lample et al. (2022)</span>
<span class="ltx_bibblock">
Guillaume Lample, Timothee Lacroix, Marie-Anne Lachaux, Aurelien Rodriguez,
Amaury Hayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet.

</span>
<span class="ltx_bibblock">Hypertree proof search for neural theorem proving.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">Advances in Neural Information Processing Systems</em>,
35:26337–26349, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lazaridou et al. (2022)</span>
<span class="ltx_bibblock">
Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai
Grigorev.

</span>
<span class="ltx_bibblock">Internet-augmented language models through few-shot prompting for
open-domain question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">ArXiv preprint</em>, abs/2203.05115, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2203.05115" title="">https://arxiv.org/abs/2203.05115</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levine et al. (2018)</span>
<span class="ltx_bibblock">
Sergey Levine, Peter Pastor, Alex Krizhevsky, Julian Ibarz, and Deirdre
Quillen.

</span>
<span class="ltx_bibblock">Learning hand-eye coordination for robotic grasping with deep
learning and large-scale data collection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">The International journal of robotics research</em>, 37(4-5):421–436, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020a)</span>
<span class="ltx_bibblock">
Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk.

</span>
<span class="ltx_bibblock">MLQA: Evaluating cross-lingual extractive question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pp.  7315–7330, Online, 2020a.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2020.acl-main.653</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.acl-main.653" title="">https://aclanthology.org/2020.acl-main.653</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020b)</span>
<span class="ltx_bibblock">
Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,
Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive NLP tasks.

</span>
<span class="ltx_bibblock">In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,
Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">Advances in Neural
Information Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>,
2020b.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html" title="">https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2019)</span>
<span class="ltx_bibblock">
Guoliang Li, Xuanhe Zhou, Shifu Li, and Bo Gao.

</span>
<span class="ltx_bibblock">Qtune: A query-aware database tuning system with deep reinforcement
learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">Proceedings of the VLDB Endowment</em>, 12(12):2118–2130, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022)</span>
<span class="ltx_bibblock">
Shuang Li, Xavier Puig, Yilun Du, Clinton Wang, Ekin Akyurek, Antonio Torralba,
Jacob Andreas, and Igor Mordatch.

</span>
<span class="ltx_bibblock">Pre-trained language models for interactive decision-making.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">ArXiv preprint</em>, abs/2202.01771, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2202.01771" title="">https://arxiv.org/abs/2202.01771</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. (2022a)</span>
<span class="ltx_bibblock">
Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete
Florence, and Andy Zeng.

</span>
<span class="ltx_bibblock">Code as policies: Language model programs for embodied control.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">ArXiv preprint</em>, abs/2209.07753, 2022a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2209.07753" title="">https://arxiv.org/abs/2209.07753</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. (2022b)</span>
<span class="ltx_bibblock">
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,
et al.

</span>
<span class="ltx_bibblock">Holistic evaluation of language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">ArXiv preprint</em>, abs/2211.09110, 2022b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2211.09110" title="">https://arxiv.org/abs/2211.09110</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. (2023)</span>
<span class="ltx_bibblock">
Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai
Lu, Lei Ji, Shaoguang Mao, et al.

</span>
<span class="ltx_bibblock">Taskmatrix. ai: Completing tasks by connecting foundation models with
millions of apis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">ArXiv preprint</em>, abs/2303.16434, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2303.16434" title="">https://arxiv.org/abs/2303.16434</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2022)</span>
<span class="ltx_bibblock">
Stephanie Lin, Jacob Hilton, and Owain Evans.

</span>
<span class="ltx_bibblock">TruthfulQA: Measuring how models mimic human falsehoods.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pp.  3214–3252,
Dublin, Ireland, 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.acl-long.229</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.acl-long.229" title="">https://aclanthology.org/2022.acl-long.229</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Linardatos et al. (2020)</span>
<span class="ltx_bibblock">
Pantelis Linardatos, Vasilis Papastefanopoulos, and Sotiris Kotsiantis.

</span>
<span class="ltx_bibblock">Explainable ai: A review of machine learning interpretability
methods.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">Entropy</em>, 23(1):18, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mdpi.com/1099-4300/23/1/18" title="">https://www.mdpi.com/1099-4300/23/1/18</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2018a)</span>
<span class="ltx_bibblock">
Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang.

</span>
<span class="ltx_bibblock">Reinforcement learning on web interfaces using workflow-guided
exploration.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
Proceedings</em>. OpenReview.net, 2018a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=ryTp3f-0-" title="">https://openreview.net/forum?id=ryTp3f-0-</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Jason Xinyu Liu, Ziyi Yang, Ifrah Idrees, Sam Liang, Benjamin Schornstein,
Stefanie Tellex, and Ankit Shah.

</span>
<span class="ltx_bibblock">Lang2ltl: Translating natural language commands to temporal robot
task specification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">ArXiv preprint</em>, abs/2302.11649, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2302.11649" title="">https://arxiv.org/abs/2302.11649</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022)</span>
<span class="ltx_bibblock">
Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire
Cui, Denny Zhou, and Andrew M Dai.

</span>
<span class="ltx_bibblock">Mind’s eye: Grounded language model reasoning through simulation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">ArXiv preprint</em>, abs/2210.05359, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2210.05359" title="">https://arxiv.org/abs/2210.05359</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2018b)</span>
<span class="ltx_bibblock">
YuXuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine.

</span>
<span class="ltx_bibblock">Imitation from observation: Learning to imitate behaviors from raw
video via context translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">2018 IEEE International Conference on Robotics and
Automation (ICRA)</em>, pp.  1118–1125. IEEE, 2018b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Longpre et al. (2021)</span>
<span class="ltx_bibblock">
Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois,
and Sameer Singh.

</span>
<span class="ltx_bibblock">Entity-based knowledge conflicts in question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing</em>, pp.  7052–7063, Online and Punta Cana,
Dominican Republic, 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.emnlp-main.565</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.emnlp-main.565" title="">https://aclanthology.org/2021.emnlp-main.565</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ly &amp; Akhloufi (2020)</span>
<span class="ltx_bibblock">
Abdoulaye O Ly and Moulay Akhloufi.

</span>
<span class="ltx_bibblock">Learning to drive by imitation: An overview of deep behavior cloning
methods.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">IEEE Transactions on Intelligent Vehicles</em>, 6(2):195–209, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madaan et al. (2023)</span>
<span class="ltx_bibblock">
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah
Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al.

</span>
<span class="ltx_bibblock">Self-refine: Iterative refinement with self-feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">arXiv preprint arXiv:2303.17651</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/pdf/2303.17651.pdf" title="">https://arxiv.org/pdf/2303.17651.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madotto et al. (2019)</span>
<span class="ltx_bibblock">
Andrea Madotto, Zhaojiang Lin, Chien-Sheng Wu, and Pascale Fung.

</span>
<span class="ltx_bibblock">Personalizing dialogue agents via meta-learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pp.  5454–5459, Florence, Italy, 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/P19-1542</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P19-1542" title="">https://aclanthology.org/P19-1542</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mateescu &amp; Elish (2019)</span>
<span class="ltx_bibblock">
Alexandra Mateescu and Madeleine Elish.

</span>
<span class="ltx_bibblock">Ai in context: the labor of integrating new technologies.

</span>
<span class="ltx_bibblock">Technical report, Data &amp; Society Research Institute, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Matheny et al. (2019)</span>
<span class="ltx_bibblock">
Michael Matheny, S Thadaney Israni, Mahnoor Ahmed, and Danielle Whicher.

</span>
<span class="ltx_bibblock">Artificial intelligence in health care: The hope, the hype, the
promise, the peril.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">Washington, DC: National Academy of Medicine</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Matlab (2012)</span>
<span class="ltx_bibblock">
Starting Matlab.

</span>
<span class="ltx_bibblock">Matlab.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">The MathWorks, Natick, MA</em>, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mazaré et al. (2018)</span>
<span class="ltx_bibblock">
Pierre-Emmanuel Mazaré, Samuel Humeau, Martin Raison, and Antoine Bordes.

</span>
<span class="ltx_bibblock">Training millions of personalized dialogue agents.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</em>, pp.  2775–2779, Brussels, Belgium, 2018.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/D18-1298</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D18-1298" title="">https://aclanthology.org/D18-1298</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Menick et al. (2022)</span>
<span class="ltx_bibblock">
Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song,
Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham,
Geoffrey Irving, et al.

</span>
<span class="ltx_bibblock">Teaching language models to support answers with verified quotes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib99.1.1">ArXiv preprint</em>, abs/2203.11147, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2203.11147" title="">https://arxiv.org/abs/2203.11147</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mialon et al. (2023)</span>
<span class="ltx_bibblock">
Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis,
Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane
Dwivedi-Yu, Asli Celikyilmaz, et al.

</span>
<span class="ltx_bibblock">Augmented language models: a survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">ArXiv preprint</em>, abs/2302.07842, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2302.07842" title="">https://arxiv.org/abs/2302.07842</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miao et al. (2020)</span>
<span class="ltx_bibblock">
Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.

</span>
<span class="ltx_bibblock">A diverse corpus for evaluating and developing English math word
problem solvers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pp.  975–984, Online, 2020. Association for
Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2020.acl-main.92</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.acl-main.92" title="">https://aclanthology.org/2020.acl-main.92</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Michel &amp; Neubig (2018)</span>
<span class="ltx_bibblock">
Paul Michel and Graham Neubig.

</span>
<span class="ltx_bibblock">Extreme adaptation for personalized neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib102.1.1">Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Papers)</em>, pp.  312–318,
Melbourne, Australia, 2018. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/P18-2050</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P18-2050" title="">https://aclanthology.org/P18-2050</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Milakis et al. (2017)</span>
<span class="ltx_bibblock">
Dimitris Milakis, Bart Van Arem, and Bert Van Wee.

</span>
<span class="ltx_bibblock">Policy and society related implications of automated driving: A
review of literature and directions for future research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib103.1.1">Journal of Intelligent Transportation Systems</em>, 21(4):324–348, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mirkin &amp; Meunier (2015)</span>
<span class="ltx_bibblock">
Shachar Mirkin and Jean-Luc Meunier.

</span>
<span class="ltx_bibblock">Personalized machine translation: Predicting translational
preferences.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib104.1.1">Proceedings of the 2015 Conference on Empirical Methods in
Natural Language Processing</em>, pp.  2019–2025, Lisbon, Portugal, 2015.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/D15-1238</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D15-1238" title="">https://aclanthology.org/D15-1238</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et al. (2022)</span>
<span class="ltx_bibblock">
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">Cross-task generalization via natural language crowdsourcing
instructions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pp.  3470–3487,
Dublin, Ireland, 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.acl-long.244</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.acl-long.244" title="">https://aclanthology.org/2022.acl-long.244</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Misra et al. (2010)</span>
<span class="ltx_bibblock">
Vikas Misra, MI Khan, and UK Singh.

</span>
<span class="ltx_bibblock">Supply chain management systems: architecture, design and vision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">Journal of Strategic Innovation and Sustainability</em>, 6(4):96–101, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mithen (1996)</span>
<span class="ltx_bibblock">
Steven Mithen.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib107.1.1">The prehistory of the mind: The cognitive origins of art and
science</em>.

</span>
<span class="ltx_bibblock">Thames &amp; Hudson Ltd., 1996.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mnih et al. (2013)</span>
<span class="ltx_bibblock">
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
Antonoglou, Daan Wierstra, and Martin Riedmiller.

</span>
<span class="ltx_bibblock">Playing atari with deep reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib108.1.1">arXiv preprint arXiv:1312.5602</em>, 2013.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/pdf/1312.5602.pdf" title="">https://arxiv.org/pdf/1312.5602.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nadeau &amp; Sekine (2007)</span>
<span class="ltx_bibblock">
David Nadeau and Satoshi Sekine.

</span>
<span class="ltx_bibblock">A survey of named entity recognition and classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib109.1.1">Lingvisticae Investigationes</em>, 30(1):3–26,
2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakano et al. (2021)</span>
<span class="ltx_bibblock">
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina
Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders,
et al.

</span>
<span class="ltx_bibblock">Webgpt: Browser-assisted question-answering with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib110.1.1">ArXiv preprint</em>, abs/2112.09332, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2112.09332" title="">https://arxiv.org/abs/2112.09332</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nenkova &amp; McKeown (2012)</span>
<span class="ltx_bibblock">
Ani Nenkova and Kathleen McKeown.

</span>
<span class="ltx_bibblock">A survey of text summarization techniques.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib111.1.1">Mining text data</em>, pp.  43–76, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nye et al. (2021)</span>
<span class="ltx_bibblock">
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob
Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David
Luan, et al.

</span>
<span class="ltx_bibblock">Show your work: Scratchpads for intermediate computation with
language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib112.1.1">ArXiv preprint</em>, abs/2112.00114, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2112.00114" title="">https://arxiv.org/abs/2112.00114</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">OpenAI: Introducing ChatGPT, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/chatgpt" title="">https://openai.com/blog/chatgpt</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Orban &amp; Caruana (2014)</span>
<span class="ltx_bibblock">
Guy A Orban and Fausto Caruana.

</span>
<span class="ltx_bibblock">The neural basis of human tool use.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib115.1.1">Frontiers in psychology</em>, 5:310, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Osiurak &amp; Badets (2016)</span>
<span class="ltx_bibblock">
François Osiurak and Arnaud Badets.

</span>
<span class="ltx_bibblock">Tool use and affordance: Manipulation-based versus reasoning-based
approaches.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib116.1.1">Psychological review</em>, 123(5):534, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Osiurak &amp; Heinke (2018)</span>
<span class="ltx_bibblock">
François Osiurak and Dietmar Heinke.

</span>
<span class="ltx_bibblock">Looking for intoolligence: A unified framework for the cognitive
study of human tool use and technology.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib117.1.1">American Psychologist</em>, 73(2):169, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Osiurak &amp; Reynaud (2020)</span>
<span class="ltx_bibblock">
François Osiurak and Emanuelle Reynaud.

</span>
<span class="ltx_bibblock">The elephant in the room: what matters cognitively in cumulative
technological culture.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib118.1.1">Behavioral and Brain Sciences</em>, 43:e156, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Osiurak et al. (2018)</span>
<span class="ltx_bibblock">
François Osiurak, Mathieu Lesourd, Ludovic Delporte, and Yves Rossetti.

</span>
<span class="ltx_bibblock">Tool use and generalized motor programs: we all are natural born
poly-dexters.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib119.1.1">Scientific Reports</em>, 8(1):10429, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela
Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib120.1.1">ArXiv preprint</em>, abs/2203.02155, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2203.02155" title="">https://arxiv.org/abs/2203.02155</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parisi et al. (2022)</span>
<span class="ltx_bibblock">
Aaron Parisi, Yao Zhao, and Noah Fiedel.

</span>
<span class="ltx_bibblock">Talm: Tool augmented language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib121.1.1">ArXiv preprint</em>, abs/2205.12255, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2205.12255" title="">https://arxiv.org/abs/2205.12255</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. (2023)</span>
<span class="ltx_bibblock">
Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy
Liang, and Michael S Bernstein.

</span>
<span class="ltx_bibblock">Generative agents: Interactive simulacra of human behavior.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib122.1.1">arXiv preprint arXiv:2304.03442</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/pdf/2304.03442.pdf" title="">https://arxiv.org/pdf/2304.03442.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perez &amp; Ribeiro (2022)</span>
<span class="ltx_bibblock">
Fábio Perez and Ian Ribeiro.

</span>
<span class="ltx_bibblock">Ignore previous prompt: Attack techniques for language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib123.1.1">ArXiv preprint</em>, abs/2211.09527, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2211.09527" title="">https://arxiv.org/abs/2211.09527</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Piktus et al. (2021)</span>
<span class="ltx_bibblock">
Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Dmytro Okhonko, Samuel
Broscheit, Gautier Izacard, Patrick Lewis, Barlas Oğuz, Edouard Grave,
Wen-tau Yih, et al.

</span>
<span class="ltx_bibblock">The web is your oyster–knowledge-intensive nlp against a very large
web corpus.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib124.1.1">ArXiv preprint</em>, abs/2112.09924, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2112.09924" title="">https://arxiv.org/abs/2112.09924</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pomerleau (1988)</span>
<span class="ltx_bibblock">
Dean A Pomerleau.

</span>
<span class="ltx_bibblock">Alvinn: An autonomous land vehicle in a neural network.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib125.1.1">Advances in neural information processing systems</em>, 1, 1988.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Press et al. (2022)</span>
<span class="ltx_bibblock">
Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike
Lewis.

</span>
<span class="ltx_bibblock">Measuring and narrowing the compositionality gap in language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib126.1.1">ArXiv preprint</em>, abs/2210.03350, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2210.03350" title="">https://arxiv.org/abs/2210.03350</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Puig et al. (2018)</span>
<span class="ltx_bibblock">
Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and
Antonio Torralba.

</span>
<span class="ltx_bibblock">Virtualhome: Simulating household activities via programs.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib127.1.1">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>, June 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text
transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib128.1.1">J. Mach. Learn. Res.</em>, 21:140:1–140:67, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://jmlr.org/papers/v21/20-074.html" title="">http://jmlr.org/papers/v21/20-074.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramakrishnan et al. (2021)</span>
<span class="ltx_bibblock">
Santhosh K Ramakrishnan, Dinesh Jayaraman, and Kristen Grauman.

</span>
<span class="ltx_bibblock">An exploration of embodied visual exploration.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib129.1.1">International Journal of Computer Vision</em>, 129:1616–1649, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramesh et al. (2022)</span>
<span class="ltx_bibblock">
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.

</span>
<span class="ltx_bibblock">Hierarchical text-conditional image generation with clip latents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib130.1.1">ArXiv preprint</em>, abs/2204.06125, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2204.06125" title="">https://arxiv.org/abs/2204.06125</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddy et al. (2020)</span>
<span class="ltx_bibblock">
Siddharth Reddy, Anca D. Dragan, and Sergey Levine.

</span>
<span class="ltx_bibblock">SQIL: imitation learning via reinforcement learning with sparse
rewards.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib131.1.1">8th International Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</em>. OpenReview.net, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=S1xKd24twB" title="">https://openreview.net/forum?id=S1xKd24twB</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reed et al. (2022)</span>
<span class="ltx_bibblock">
Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander
Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay,
Jost Tobias Springenberg, et al.

</span>
<span class="ltx_bibblock">A generalist agent.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib132.1.1">ArXiv preprint</em>, abs/2205.06175, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2205.06175" title="">https://arxiv.org/abs/2205.06175</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reynaud et al. (2019)</span>
<span class="ltx_bibblock">
Emanuelle Reynaud, Jordan Navarro, Mathieu Lesourd, and François Osiurak.

</span>
<span class="ltx_bibblock">To watch is to work: a review of neuroimaging data on tool use
observation network.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib133.1.1">Neuropsychology Review</em>, 29:484–497, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roller et al. (2021)</span>
<span class="ltx_bibblock">
Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu,
Jing Xu, Myle Ott, Eric Michael Smith, Y-Lan Boureau, and Jason Weston.

</span>
<span class="ltx_bibblock">Recipes for building an open-domain chatbot.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib134.1.1">Proceedings of the 16th Conference of the European Chapter
of the Association for Computational Linguistics: Main Volume</em>, pp. 300–325, Online, 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.eacl-main.24</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.eacl-main.24" title="">https://aclanthology.org/2021.eacl-main.24</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et al. (2022)</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
Ommer.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib135.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, pp.  10684–10695, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roose (2023)</span>
<span class="ltx_bibblock">
Kevin Roose.

</span>
<span class="ltx_bibblock">A conversation with bing’s chatbot left me deeply unsettled.

</span>
<span class="ltx_bibblock">https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et al. (2022)</span>
<span class="ltx_bibblock">
Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika,
Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey,
M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza
Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta,
Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,
Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj,
Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason Alan
Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and
Alexander M. Rush.

</span>
<span class="ltx_bibblock">Multitask prompted training enables zero-shot task generalization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib137.1.1">The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>.
OpenReview.net, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=9Vrb9D0WI4" title="">https://openreview.net/forum?id=9Vrb9D0WI4</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sasaki &amp; Yamashina (2021)</span>
<span class="ltx_bibblock">
Fumihiro Sasaki and Ryota Yamashina.

</span>
<span class="ltx_bibblock">Behavioral cloning from noisy demonstrations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib138.1.1">9th International Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>. OpenReview.net, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=zrT3HcsWSAt" title="">https://openreview.net/forum?id=zrT3HcsWSAt</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et al. (2023)</span>
<span class="ltx_bibblock">
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria
Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.

</span>
<span class="ltx_bibblock">Toolformer: Language models can teach themselves to use tools.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib139.1.1">ArXiv preprint</em>, abs/2302.04761, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2302.04761" title="">https://arxiv.org/abs/2302.04761</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schrittwieser et al. (2020)</span>
<span class="ltx_bibblock">
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan,
Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis,
Thore Graepel, et al.

</span>
<span class="ltx_bibblock">Mastering atari, go, chess and shogi by planning with a learned
model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib140.1.1">Nature</em>, 588(7839):604–609, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schulman et al. (2017)</span>
<span class="ltx_bibblock">
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.

</span>
<span class="ltx_bibblock">Proximal policy optimization algorithms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib141.1.1">ArXiv preprint</em>, abs/1707.06347, 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1707.06347" title="">https://arxiv.org/abs/1707.06347</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seed &amp; Byrne (2010)</span>
<span class="ltx_bibblock">
Amanda Seed and Richard Byrne.

</span>
<span class="ltx_bibblock">Animal tool-use.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib142.1.1">Current biology</em>, 20(23):R1032–R1039,
2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2023)</span>
<span class="ltx_bibblock">
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting
Zhuang.

</span>
<span class="ltx_bibblock">Hugginggpt: Solving ai tasks with chatgpt and its friends in
huggingface, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2017)</span>
<span class="ltx_bibblock">
Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang.

</span>
<span class="ltx_bibblock">World of bits: An open-domain platform for web-based agents.

</span>
<span class="ltx_bibblock">In Doina Precup and Yee Whye Teh (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib144.1.1">Proceedings of the
34th International Conference on Machine Learning, ICML 2017, Sydney, NSW,
Australia, 6-11 August 2017</em>, volume 70 of <em class="ltx_emph ltx_font_italic" id="bib.bib144.2.2">Proceedings of Machine
Learning Research</em>, pp.  3135–3144. PMLR, 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://proceedings.mlr.press/v70/shi17a.html" title="">http://proceedings.mlr.press/v70/shi17a.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shridhar et al. (2021)</span>
<span class="ltx_bibblock">
Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk,
Adam Trischler, and Matthew J. Hausknecht.

</span>
<span class="ltx_bibblock">Alfworld: Aligning text and embodied environments for interactive
learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib145.1.1">9th International Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>. OpenReview.net, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=0IOX0YcCdTn" title="">https://openreview.net/forum?id=0IOX0YcCdTn</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shumaker et al. (2011)</span>
<span class="ltx_bibblock">
Robert W Shumaker, Kristina R Walkup, and Benjamin B Beck.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib146.1.1">Animal tool behavior: the use and manufacture of tools by
animals</em>.

</span>
<span class="ltx_bibblock">JHU Press, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shuster et al. (2021)</span>
<span class="ltx_bibblock">
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston.

</span>
<span class="ltx_bibblock">Retrieval augmentation reduces hallucination in conversation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib147.1.1">Findings of the Association for Computational Linguistics:
EMNLP 2021</em>, pp.  3784–3803, Punta Cana, Dominican Republic, 2021.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.findings-emnlp.320</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.findings-emnlp.320" title="">https://aclanthology.org/2021.findings-emnlp.320</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shuster et al. (2022)</span>
<span class="ltx_bibblock">
Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen
Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, et al.

</span>
<span class="ltx_bibblock">Blenderbot 3: a deployed conversational agent that continually learns
to responsibly engage.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib148.1.1">ArXiv preprint</em>, abs/2208.03188, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2208.03188" title="">https://arxiv.org/abs/2208.03188</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Silver et al. (2018)</span>
<span class="ltx_bibblock">
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew
Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore
Graepel, et al.

</span>
<span class="ltx_bibblock">A general reinforcement learning algorithm that masters chess, shogi,
and go through self-play.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib149.1.1">Science</em>, 362(6419):1140–1144, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. (2022)</span>
<span class="ltx_bibblock">
Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan
Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg.

</span>
<span class="ltx_bibblock">Progprompt: Generating situated robot task plans using large language
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib150.1.1">ArXiv preprint</em>, abs/2209.11302, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2209.11302" title="">https://arxiv.org/abs/2209.11302</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith &amp; Gasser (2005)</span>
<span class="ltx_bibblock">
Linda Smith and Michael Gasser.

</span>
<span class="ltx_bibblock">The development of embodied cognition: Six lessons from babies.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib151.1.1">Artificial life</em>, 11(1-2):13–29, 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2022)</span>
<span class="ltx_bibblock">
Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun Chao, and
Yu Su.

</span>
<span class="ltx_bibblock">Llm-planner: Few-shot grounded planning for embodied agents with
large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib152.1.1">ArXiv preprint</em>, abs/2212.04088, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2212.04088" title="">https://arxiv.org/abs/2212.04088</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2021)</span>
<span class="ltx_bibblock">
Haoyu Song, Yan Wang, Kaiyan Zhang, Wei-Nan Zhang, and Ting Liu.

</span>
<span class="ltx_bibblock">BoB: BERT over BERT for training persona-based dialogue
models from limited personalized data.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib153.1.1">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pp.  167–177, Online,
2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.acl-long.14</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.acl-long.14" title="">https://aclanthology.org/2021.acl-long.14</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sternberg (1999)</span>
<span class="ltx_bibblock">
Robert J Sternberg.

</span>
<span class="ltx_bibblock">The theory of successful intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib154.1.1">Review of General psychology</em>, 3(4):292–316, 1999.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stiennon et al. (2020)</span>
<span class="ltx_bibblock">
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea
Voss, Alec Radford, Dario Amodei, and Paul F. Christiano.

</span>
<span class="ltx_bibblock">Learning to summarize with human feedback.

</span>
<span class="ltx_bibblock">In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,
Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib155.1.1">Advances in Neural
Information Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>, 2020.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html" title="">https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sukthankar et al. (2014)</span>
<span class="ltx_bibblock">
Gita Sukthankar, Christopher Geib, Hung Bui, David Pynadath, and Robert P
Goldman.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib156.1.1">Plan, activity, and intent recognition: Theory and practice</em>.

</span>
<span class="ltx_bibblock">Newnes, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2022)</span>
<span class="ltx_bibblock">
Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou.

</span>
<span class="ltx_bibblock">Recitation-augmented language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib157.1.1">ArXiv preprint</em>, abs/2210.01296, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2210.01296" title="">https://arxiv.org/abs/2210.01296</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Szegedy et al. (2014)</span>
<span class="ltx_bibblock">
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian J. Goodfellow, and Rob Fergus.

</span>
<span class="ltx_bibblock">Intriguing properties of neural networks.

</span>
<span class="ltx_bibblock">In Yoshua Bengio and Yann LeCun (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib158.1.1">2nd International
Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April
14-16, 2014, Conference Track Proceedings</em>, 2014.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1312.6199" title="">http://arxiv.org/abs/1312.6199</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et al. (2020)</span>
<span class="ltx_bibblock">
Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht,
and Ludwig Schmidt.

</span>
<span class="ltx_bibblock">Measuring robustness to natural distribution shifts in image
classification.

</span>
<span class="ltx_bibblock">In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,
Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib159.1.1">Advances in Neural
Information Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>, 2020.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2020/hash/d8330f857a17c53d217014ee776bfd50-Abstract.html" title="">https://proceedings.neurips.cc/paper/2020/hash/d8330f857a17c53d217014ee776bfd50-Abstract.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teschke et al. (2013)</span>
<span class="ltx_bibblock">
Irmgard Teschke, Claudia AF Wascher, Madeleine F Scriba, Auguste MP von Bayern,
V Huml, B Siemers, and Sabine Tebbich.

</span>
<span class="ltx_bibblock">Did tool-use evolve with enhanced physical cognitive abilities?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib160.1.1">Philosophical Transactions of the Royal Society B: Biological
Sciences</em>, 368(1630):20120418, 2013.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4027416/" title="">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4027416/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thoppilan et al. (2022)</span>
<span class="ltx_bibblock">
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv
Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,
et al.

</span>
<span class="ltx_bibblock">Lamda: Language models for dialog applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib161.1.1">ArXiv preprint</em>, abs/2201.08239, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2201.08239" title="">https://arxiv.org/abs/2201.08239</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Todorov et al. (2012)</span>
<span class="ltx_bibblock">
Emanuel Todorov, Tom Erez, and Yuval Tassa.

</span>
<span class="ltx_bibblock">Mujoco: A physics engine for model-based control.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib162.1.1">2012 IEEE/RSJ international conference on intelligent robots
and systems</em>, pp.  5026–5033. IEEE, 2012.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/6386109" title="">https://ieeexplore.ieee.org/document/6386109</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Torabi et al. (2018)</span>
<span class="ltx_bibblock">
Faraz Torabi, Garrett Warnell, and Peter Stone.

</span>
<span class="ltx_bibblock">Behavioral cloning from observation.

</span>
<span class="ltx_bibblock">In Jérôme Lang (ed.), <em class="ltx_emph ltx_font_italic" id="bib.bib163.1.1">Proceedings of the
Twenty-Seventh International Joint Conference on Artificial Intelligence,
IJCAI 2018, July 13-19, 2018, Stockholm, Sweden</em>, pp.  4950–4957.
ijcai.org, 2018.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.24963/ijcai.2018/687</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.24963/ijcai.2018/687" title="">https://doi.org/10.24963/ijcai.2018/687</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Turner et al. (2022)</span>
<span class="ltx_bibblock">
Alexander Matt Turner, Aseem Saxena, and Prasad Tadepalli.

</span>
<span class="ltx_bibblock">Formalizing the problem of side effect regularization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib164.1.1">NeurIPS ML Safety Workshop</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach,
Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib165.1.1">Advances
in Neural Information Processing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA,
USA</em>, pp.  5998–6008, 2017.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" title="">https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vemprala et al. (2023)</span>
<span class="ltx_bibblock">
Sai Vemprala, Rogerio Bonatti, Arthur Bucker, and Ashish Kapoor.

</span>
<span class="ltx_bibblock">Chatgpt for robotics: Design principles and model abilities.

</span>
<span class="ltx_bibblock">Technical Report MSR-TR-2023-8, Microsoft, February 2023.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.microsoft.com/en-us/research/publication/chatgpt-for-robotics-design-principles-and-model-abilities/" title="">https://www.microsoft.com/en-us/research/publication/chatgpt-for-robotics-design-principles-and-model-abilities/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Von Eckardt (1995)</span>
<span class="ltx_bibblock">
Barbara Von Eckardt.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib167.1.1">What is cognitive science?</em>
</span>
<span class="ltx_bibblock">MIT press, 1995.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wallace et al. (2019)</span>
<span class="ltx_bibblock">
Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh.

</span>
<span class="ltx_bibblock">Universal adversarial triggers for attacking and analyzing NLP.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib168.1.1">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pp.  2153–2162, Hong Kong,
China, 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/D19-1221</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D19-1221" title="">https://aclanthology.org/D19-1221</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wallace et al. (2021)</span>
<span class="ltx_bibblock">
Eric Wallace, Tony Zhao, Shi Feng, and Sameer Singh.

</span>
<span class="ltx_bibblock">Concealed data poisoning attacks on NLP models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib169.1.1">Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pp.  139–150, Online, 2021. Association for Computational
Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.naacl-main.13</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.naacl-main.13" title="">https://aclanthology.org/2021.naacl-main.13</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2019a)</span>
<span class="ltx_bibblock">
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,
Felix Hill, Omer Levy, and Samuel R. Bowman.

</span>
<span class="ltx_bibblock">Superglue: A stickier benchmark for general-purpose language
understanding systems.

</span>
<span class="ltx_bibblock">In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence
d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib170.1.1">Advances
in Neural Information Processing Systems 32: Annual Conference on Neural
Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
Vancouver, BC, Canada</em>, pp.  3261–3275, 2019a.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html" title="">https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2019b)</span>
<span class="ltx_bibblock">
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
Samuel R. Bowman.

</span>
<span class="ltx_bibblock">GLUE: A multi-task benchmark and analysis platform for natural
language understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib171.1.1">7th International Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019</em>. OpenReview.net,
2019b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=rJ4km2R5t7" title="">https://openreview.net/forum?id=rJ4km2R5t7</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang &amp; Komatsuzaki (2021)</span>
<span class="ltx_bibblock">
Ben Wang and Aran Komatsuzaki.

</span>
<span class="ltx_bibblock">GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/kingoflolz/mesh-transformer-jax" title="">https://github.com/kingoflolz/mesh-transformer-jax</a>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023a)</span>
<span class="ltx_bibblock">
Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi
Yang, Haojun Huang, Wei Ye, Xiubo Geng, et al.

</span>
<span class="ltx_bibblock">On the robustness of chatgpt: An adversarial and out-of-distribution
perspective.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib173.1.1">ArXiv preprint</em>, abs/2302.12095, 2023a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2302.12095" title="">https://arxiv.org/abs/2302.12095</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang (2010)</span>
<span class="ltx_bibblock">
Shiguo Wang.

</span>
<span class="ltx_bibblock">A comprehensive survey of data mining-based accounting-fraud
detection research.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib174.1.1">2010 International Conference on Intelligent Computation
Technology and Automation</em>, volume 1, pp.  50–53. IEEE, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022a)</span>
<span class="ltx_bibblock">
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou.

</span>
<span class="ltx_bibblock">Self-consistency improves chain of thought reasoning in language
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib175.1.1">ArXiv preprint</em>, abs/2203.11171, 2022a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2203.11171" title="">https://arxiv.org/abs/2203.11171</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022b)</span>
<span class="ltx_bibblock">
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel
Khashabi, and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">Self-instruct: Aligning language model with self generated
instructions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib176.1.1">ArXiv preprint</em>, abs/2212.10560, 2022b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2212.10560" title="">https://arxiv.org/abs/2212.10560</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023b)</span>
<span class="ltx_bibblock">
Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang.

</span>
<span class="ltx_bibblock">Describe, explain, plan and select: Interactive planning with large
language models enables open-world multi-task agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib177.1.1">arXiv preprint arXiv:2302.01560</em>, 2023b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/pdf/2302.01560.pdf" title="">https://arxiv.org/pdf/2302.01560.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Washburn (1960)</span>
<span class="ltx_bibblock">
Sherwood L Washburn.

</span>
<span class="ltx_bibblock">Tools and human evolution.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib178.1.1">Scientific American</em>, 203(3):62–75, 1960.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wason (1968)</span>
<span class="ltx_bibblock">
Peter C Wason.

</span>
<span class="ltx_bibblock">Reasoning about a rule.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib179.1.1">Quarterly journal of experimental psychology</em>, 20(3):273–281, 1968.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022a)</span>
<span class="ltx_bibblock">
Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian
Lester, Nan Du, Andrew M. Dai, and Quoc V. Le.

</span>
<span class="ltx_bibblock">Finetuned language models are zero-shot learners.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib180.1.1">The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>.
OpenReview.net, 2022a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=gEZrGCozdqR" title="">https://openreview.net/forum?id=gEZrGCozdqR</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022b)</span>
<span class="ltx_bibblock">
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al.

</span>
<span class="ltx_bibblock">Emergent abilities of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib181.1.1">ArXiv preprint</em>, abs/2206.07682, 2022b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2206.07682" title="">https://arxiv.org/abs/2206.07682</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022c)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and
Denny Zhou.

</span>
<span class="ltx_bibblock">Chain of thought prompting elicits reasoning in large language
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib182.1.1">ArXiv preprint</em>, abs/2201.11903, 2022c.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2201.11903" title="">https://arxiv.org/abs/2201.11903</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weininger (1988)</span>
<span class="ltx_bibblock">
David Weininger.

</span>
<span class="ltx_bibblock">Smiles, a chemical language and information system. 1. introduction
to methodology and encoding rules.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib183.1.1">Journal of chemical information and computer sciences</em>,
28(1):31–36, 1988.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Whittlestone et al. (2019)</span>
<span class="ltx_bibblock">
Jess Whittlestone, Rune Nyrup, Anna Alexandrova, Kanta Dihal, and Stephen Cave.

</span>
<span class="ltx_bibblock">Ethical and societal implications of algorithms, data, and artificial
intelligence: a roadmap for research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib184.1.1">London: Nuffield Foundation</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023)</span>
<span class="ltx_bibblock">
Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan
Duan.

</span>
<span class="ltx_bibblock">Visual chatgpt: Talking, drawing and editing with visual foundation
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib185.1.1">ArXiv preprint</em>, abs/2303.04671, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2303.04671" title="">https://arxiv.org/abs/2303.04671</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2021)</span>
<span class="ltx_bibblock">
Yuwei Wu, Xuezhe Ma, and Diyi Yang.

</span>
<span class="ltx_bibblock">Personalized response generation via generative split memory network.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib186.1.1">Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pp.  1956–1970, Online, 2021. Association for Computational
Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.naacl-main.157</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.naacl-main.157" title="">https://aclanthology.org/2021.naacl-main.157</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wuebker et al. (2018)</span>
<span class="ltx_bibblock">
Joern Wuebker, Patrick Simianer, and John DeNero.

</span>
<span class="ltx_bibblock">Compact personalized models for neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib187.1.1">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</em>, pp.  881–886, Brussels, Belgium, 2018.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/D18-1104</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D18-1104" title="">https://aclanthology.org/D18-1104</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. (2011)</span>
<span class="ltx_bibblock">
Rui Yan, Jian-Yun Nie, and Xiaoming Li.

</span>
<span class="ltx_bibblock">Summarize what you are interested in: An optimization framework for
interactive personalized summarization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib188.1.1">Proceedings of the 2011 Conference on Empirical Methods in
Natural Language Processing</em>, pp.  1342–1351, Edinburgh, Scotland, UK.,
2011. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D11-1124" title="">https://aclanthology.org/D11-1124</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang &amp; Flek (2021)</span>
<span class="ltx_bibblock">
Diyi Yang and Lucie Flek.

</span>
<span class="ltx_bibblock">Towards user-centric text-to-text generation: A survey.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib189.1.1">Text, Speech, and Dialogue: 24th International Conference,
TSD 2021, Olomouc, Czech Republic, September 6–9, 2021, Proceedings 24</em>,
pp.  3–22. Springer, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018a)</span>
<span class="ltx_bibblock">
Guang-Zhong Yang, Jim Bellingham, Pierre E Dupont, Peer Fischer, Luciano
Floridi, Robert Full, Neil Jacobstein, Vijay Kumar, Marcia McNutt, Robert
Merrifield, et al.

</span>
<span class="ltx_bibblock">The grand challenges of science robotics.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib190.1.1">Science robotics</em>, 3(14):eaar7650,
2018a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2023)</span>
<span class="ltx_bibblock">
Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal
Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang.

</span>
<span class="ltx_bibblock">Mm-react: Prompting chatgpt for multimodal reasoning and action.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib191.1.1">ArXiv preprint</em>, abs/2303.11381, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2303.11381" title="">https://arxiv.org/abs/2303.11381</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib192">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018b)</span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan
Salakhutdinov, and Christopher D. Manning.

</span>
<span class="ltx_bibblock">HotpotQA: A dataset for diverse, explainable multi-hop question
answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib192.1.1">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</em>, pp.  2369–2380, Brussels, Belgium,
2018b. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/D18-1259</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D18-1259" title="">https://aclanthology.org/D18-1259</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib193">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. (2022a)</span>
<span class="ltx_bibblock">
Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.

</span>
<span class="ltx_bibblock">Webshop: Towards scalable real-world web interaction with grounded
language agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib193.1.1">ArXiv preprint</em>, abs/2207.01206, 2022a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2207.01206" title="">https://arxiv.org/abs/2207.01206</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib194">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. (2022b)</span>
<span class="ltx_bibblock">
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
and Yuan Cao.

</span>
<span class="ltx_bibblock">React: Synergizing reasoning and acting in language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib194.1.1">ArXiv preprint</em>, abs/2210.03629, 2022b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2210.03629" title="">https://arxiv.org/abs/2210.03629</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib195">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. (2021)</span>
<span class="ltx_bibblock">
Joel Ye, Dhruv Batra, Erik Wijmans, and Abhishek Das.

</span>
<span class="ltx_bibblock">Auxiliary tasks speed up learning point goal navigation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib195.1.1">Conference on Robot Learning</em>, pp.  498–516. PMLR, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib196">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2019)</span>
<span class="ltx_bibblock">
Licheng Yu, Xinlei Chen, Georgia Gkioxari, Mohit Bansal, Tamara L. Berg, and
Dhruv Batra.

</span>
<span class="ltx_bibblock">Multi-target embodied question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib196.1.1">IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019</em>, pp. 6309–6318. Computer Vision Foundation / IEEE, 2019.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/CVPR.2019.00647</span>.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Multi-Target_Embodied_Question_Answering_CVPR_2019_paper.html" title="">http://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Multi-Target_Embodied_Question_Answering_CVPR_2019_paper.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib197">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et al. (2019)</span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi,
Franziska Roesner, and Yejin Choi.

</span>
<span class="ltx_bibblock">Defending against neural fake news.

</span>
<span class="ltx_bibblock">In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence
d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib197.1.1">Advances
in Neural Information Processing Systems 32: Annual Conference on Neural
Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
Vancouver, BC, Canada</em>, pp.  9051–9062, 2019.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2019/hash/3e9f0fc9b2f89e043bc6233994dfcf76-Abstract.html" title="">https://proceedings.neurips.cc/paper/2019/hash/3e9f0fc9b2f89e043bc6233994dfcf76-Abstract.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib198">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. (2022)</span>
<span class="ltx_bibblock">
Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari,
Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke,
et al.

</span>
<span class="ltx_bibblock">Socratic models: Composing zero-shot multimodal reasoning with
language.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib198.1.1">ArXiv preprint</em>, abs/2204.00598, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2204.00598" title="">https://arxiv.org/abs/2204.00598</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib199">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2018)</span>
<span class="ltx_bibblock">
Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason
Weston.

</span>
<span class="ltx_bibblock">Personalizing dialogue agents: I have a dog, do you have pets too?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib199.1.1">Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pp.  2204–2213,
Melbourne, Australia, 2018. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/P18-1205</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P18-1205" title="">https://aclanthology.org/P18-1205</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib200">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et al. (2022)</span>
<span class="ltx_bibblock">
Hanxun Zhong, Zhicheng Dou, Yutao Zhu, Hongjin Qian, and Ji-Rong Wen.

</span>
<span class="ltx_bibblock">Less is more: Learning to refine dialogue history for personalized
dialogue generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib200.1.1">Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pp.  5808–5820, Seattle, United States, 2022. Association
for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.naacl-main.426</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.naacl-main.426" title="">https://aclanthology.org/2022.naacl-main.426</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib201">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2023)</span>
<span class="ltx_bibblock">
Shuyan Zhou, Uri Alon, Frank F. Xu, Zhiruo Wang, Zhengbao Jiang, and Graham
Neubig.

</span>
<span class="ltx_bibblock">Docprompting: Generating code by retrieving the docs, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib202">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2020)</span>
<span class="ltx_bibblock">
Xuanhe Zhou, Chengliang Chai, Guoliang Li, and Ji Sun.

</span>
<span class="ltx_bibblock">Database meets artificial intelligence: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib202.1.1">IEEE Transactions on Knowledge and Data Engineering</em>,
34(3):1096–1116, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib203">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ziegler et al. (2019)</span>
<span class="ltx_bibblock">
Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario
Amodei, Paul Christiano, and Geoffrey Irving.

</span>
<span class="ltx_bibblock">Fine-tuning language models from human preferences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib203.1.1">ArXiv preprint</em>, abs/1909.08593, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1909.08593" title="">https://arxiv.org/abs/1909.08593</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Case Study</h2>
<div class="ltx_para ltx_noindent" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">In this section, we provide the specific prompts and model responses of ChatGPT (Mar 23, 2023 version) for each tool studied in <a class="ltx_ref" href="https://arxiv.org/html/2304.08354v3#S4" title="4 Application and Experiment ‣ Tool Learning with Foundation Models"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">4</span></a>.
The implementations for different APIs and datasets will be available in <a class="ltx_ref ltx_href" href="https://github.com/OpenBMB/BMTools" title="">BMTools</a>.</p>
</div>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>3D Models</h3>
<div class="ltx_para ltx_noindent" id="A1.SS1.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="1074" id="A1.SS1.p1.g1" src="x10.png" width="829"/>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS1.p2">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="1228" id="A1.SS1.p2.g1" src="x11.png" width="831"/>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Stock</h3>
<div class="ltx_para ltx_noindent" id="A1.SS2.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="1052" id="A1.SS2.p1.g1" src="x12.png" width="830"/>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Making Slides</h3>
<div class="ltx_para ltx_noindent" id="A1.SS3.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="1197" id="A1.SS3.p1.g1" src="x13.png" width="830"/>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p2">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="1226" id="A1.SS3.p2.g1" src="x14.png" width="830"/>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p3">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="1203" id="A1.SS3.p3.g1" src="x15.png" width="830"/>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p4">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="1158" id="A1.SS3.p4.g1" src="x16.png" width="829"/>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p5">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="883" id="A1.SS3.p5.g1" src="x17.png" width="829"/>
</div>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Movie Hunter</h3>
<div class="ltx_para ltx_noindent" id="A1.SS4.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="1192" id="A1.SS4.p1.g1" src="x18.png" width="829"/>
</div>
</section>
<section class="ltx_subsection" id="A1.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5 </span>Search Engine</h3>
<div class="ltx_para ltx_noindent" id="A1.SS5.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="1175" id="A1.SS5.p1.g1" src="x19.png" width="829"/>
</div>
</section>
<section class="ltx_subsection" id="A1.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.6 </span>Chemicals Mining</h3>
<div class="ltx_para ltx_noindent" id="A1.SS6.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="1014" id="A1.SS6.p1.g1" src="x20.png" width="830"/>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_subsection" id="A1.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.7 </span>Cooking Assistant</h3>
<div class="ltx_para ltx_noindent" id="A1.SS7.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="1184" id="A1.SS7.p1.g1" src="x21.png" width="829"/>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_subsection" id="A1.SS8">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.8 </span>AI Painting</h3>
<div class="ltx_para ltx_noindent" id="A1.SS8.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="1165" id="A1.SS8.p1.g1" src="x22.png" width="831"/>
</div>
</section>
<section class="ltx_subsection" id="A1.SS9">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.9 </span>Navigating Knowledge Graphs</h3>
<div class="ltx_para ltx_noindent" id="A1.SS9.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="1204" id="A1.SS9.p1.g1" src="x23.png" width="831"/>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS9.p2">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="782" id="A1.SS9.p2.g1" src="x24.png" width="831"/>
</div>
</section>
<section class="ltx_subsection" id="A1.SS10">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.10 </span>ALFWorld</h3>
<div class="ltx_para ltx_noindent" id="A1.SS10.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="405" id="A1.SS10.p1.g1" src="x25.png" width="830"/>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS10.p2">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="1022" id="A1.SS10.p2.g1" src="x26.png" width="831"/>
</div>
</section>
<section class="ltx_subsection" id="A1.SS11">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.11 </span>Calculator</h3>
<div class="ltx_para ltx_noindent" id="A1.SS11.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="462" id="A1.SS11.p1.g1" src="x27.png" width="829"/>
</div>
</section>
<section class="ltx_subsection" id="A1.SS12">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.12 </span>Weather</h3>
<div class="ltx_para ltx_noindent" id="A1.SS12.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="687" id="A1.SS12.p1.g1" src="x28.png" width="830"/>
</div>
</section>
<section class="ltx_subsection" id="A1.SS13">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.13 </span>Online Shopping</h3>
<div class="ltx_para ltx_noindent" id="A1.SS13.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="1203" id="A1.SS13.p1.g1" src="x29.png" width="829"/>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_noindent" id="A1.SS13.p2">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="324" id="A1.SS13.p2.g1" src="x30.png" width="830"/>
</div>
</section>
<section class="ltx_subsection" id="A1.SS14">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.14 </span>Map</h3>
<div class="ltx_para ltx_noindent" id="A1.SS14.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="831" id="A1.SS14.p1.g1" src="x31.png" width="831"/>
</div>
</section>
<section class="ltx_subsection" id="A1.SS15">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.15 </span>Processing Tables</h3>
<div class="ltx_para ltx_noindent" id="A1.SS15.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="1203" id="A1.SS15.p1.g1" src="x32.png" width="830"/>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_noindent" id="A1.SS15.p2">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="1231" id="A1.SS15.p2.g1" src="x33.png" width="830"/>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_noindent" id="A1.SS15.p3">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="497" id="A1.SS15.p3.g1" src="x34.png" width="831"/>
</div>
</section>
<section class="ltx_subsection" id="A1.SS16">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.16 </span>Translation</h3>
<div class="ltx_para ltx_noindent" id="A1.SS16.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="638" id="A1.SS16.p1.g1" src="x35.png" width="829"/>
</div>
</section>
<section class="ltx_subsection" id="A1.SS17">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.17 </span>Wikipedia</h3>
<div class="ltx_para ltx_noindent" id="A1.SS17.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="1201" id="A1.SS17.p1.g1" src="x36.png" width="830"/>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_noindent" id="A1.SS17.p2">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="517" id="A1.SS17.p2.g1" src="x37.png" width="830"/>
</div>
</section>
<section class="ltx_subsection" id="A1.SS18">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.18 </span>Database</h3>
<div class="ltx_para ltx_noindent" id="A1.SS18.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="650" id="A1.SS18.p1.g1" src="x38.png" width="830"/>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS18.p2">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="1229" id="A1.SS18.p2.g1" src="x39.png" width="830"/>
</div>
<div class="ltx_para ltx_noindent ltx_align_center" id="A1.SS18.p3">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="324" id="A1.SS18.p3.g1" src="x40.png" width="831"/>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Aug  6 15:16:14 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
