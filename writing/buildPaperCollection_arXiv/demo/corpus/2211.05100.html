<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>BLOOM: A 176B-Parameter Open-Access Multilingual Language Model</title>
<!--Generated on Thu Jul 13 17:51:19 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="css/ar5iv.min.css" rel="stylesheet" type="text/css"/>
<link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" integrity="sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM" rel="stylesheet"/>
<script crossorigin="anonymous" integrity="sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz" src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script defer="" src="https://services.dev.arxiv.org/html/addons.js"></script>
<script defer="" src="https://services.dev.arxiv.org/html/feedbackOverlay.js"></script>
<link href="https://services.dev.arxiv.org/html/styles.css" rel="stylesheet" type="text/css"/></head>
<body>
<div class="ltx_page_main" id="main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">BLOOM: A 176B-Parameter Open-Access Multilingual Language Model</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">BigScience Workshop<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Please direct correspondence to <a class="ltx_ref ltx_url ltx_font_typewriter" href="bigscience-contact@googlegroups.com" title="">bigscience-contact@googlegroups.com</a>. A list of contributions is available in <a class="ltx_ref" href="#S6" title="6 Contributions â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">6</span></a>.</span></span></span>
<span class="ltx_ERROR undefined" id="id1.1.id1">\AND</span>Major Contributors 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id2.2.id2">\names</span>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana IliÄ‡, Daniel Hesslow, Roman CastagnÃ©, Alexandra Sasha Luccioni, FranÃ§ois Yvon, Matthias GallÃ©, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, BenoÃ®t Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Thomas Wolf, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo LaurenÃ§on, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel
<span class="ltx_ERROR undefined" id="id3.3.id3">\AND</span>Dataset 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id4.4.id4">\names</span>Aaron Gokaslan, Adi Simhi, Aitor Soroa, Albert Villanova del Moral, Alexandra Sasha Luccioni, Alham Fikri Aji, Amit Alfassy, Angelina McMillan-Major, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Akiki, Christopher Klamm, Colin Leong, Colin Raffel, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo GonzÃ¡lez Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, GÃ©rard Dupont, GermÃ¡n Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Hugo LaurenÃ§on, Huu Nguyen, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, JÃ¶rg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Lucile Saulnier, Ludovic Tanguy, Manan Dey, Manuel Romero MuÃ±oz, Maraim Masoud, Margaret Mitchell, MarÃ­a Grandury, Mario Å aÅ¡ko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Pawan Sasanka Ammanamanchi, Pedro Ortiz Suarez, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis LÃ³pez, Roman CastagnÃ©, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Samson Tan, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Stella Biderman, Suhas Pai, Suzana IliÄ‡, Sydney Zink, Teven Le Scao, Thomas Wang, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Yacine Jernite, Zaid Alyafeai, Zeerak Talat
<span class="ltx_ERROR undefined" id="id5.5.id5">\AND</span>Tokenization 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id6.6.id6">\names</span>Arun Raja, Benjamin Heinzerling, BenoÃ®t Sagot, Chenglei Si, Colin Raffel, Davut Emre TaÅŸar, Elizabeth Salesky, Lucile Saulnier, Manan Dey, Matthias GallÃ©, Pedro Ortiz Suarez, Roman CastagnÃ©, Sabrina J. Mielke, Samson Tan, Teven Le Scao, Thomas Wang, Wilson Y. Lee, Zaid Alyafeai
<span class="ltx_ERROR undefined" id="id7.7.id7">\AND</span>Prompt Engineering 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id8.8.id8">\names</span>Abheesht Sharma, Albert Webson, Alexander M. Rush, Alham Fikri Aji, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Canwen Xu, Colin Raffel, Debajyoti Datta, Dragomir Radev, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jonathan Chang, Jos Rozen, Khalid Almubarak, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Manan Dey, Matteo Manica, Mike Tian-Jian Jiang, Nihal Nayak, Niklas Muennighoff, Rachel Bawden, Ryan Teehan, Samuel Albanie, Shanya Sharma, Sheng Shen, Srulik Ben-David, Stella Biderman, Stephen H. Bach, Taewoon Kim, Tali Bers, Teven Le Scao, Thibault Fevry, Thomas Wang, Thomas Wolf, Trishala Neeraj, Urmish Thakker, Victor Sanh, Vikas Raunak, Xiangru Tang, Zaid Alyafeai, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh
<span class="ltx_ERROR undefined" id="id9.9.id9">\AND</span>Architecture and Objective 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id10.10.id10">\names</span>Adam Roberts, Colin Raffel, Daniel Hesslow, Hady Elsahar, Hyung Won Chung, Iz Beltagy, Jaesung Tae, Jason Phang, Julien Launay, Lintang Sutawika, Lucile Saulnier, M Saiful Bari, Niklas Muennighoff, Ofir Press, Sheng Shen, Stas Bekman, Stella Biderman, Teven Le Scao, Thomas Wang, Vassilina Nikoulina, Victor Sanh, Zheng-Xin Yong
<span class="ltx_ERROR undefined" id="id11.11.id11">\AND</span>Engineering 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id12.12.id12">\names</span>Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Niklas Muennighoff, Nouamane Tazi, Olatunji Ruwase, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre FranÃ§ois LavallÃ©e, RÃ©mi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, Stas Bekman, StÃ©phane Requena, Suraj Patil, Teven Le Scao, Thomas Wang, Tim Dettmers
<span class="ltx_ERROR undefined" id="id13.13.id13">\AND</span>Evaluation and Interpretability 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id14.14.id14">\names</span>Ahmed Baruwa, Albert Webson, Alexandra Sasha Luccioni, Alham Fikri Aji, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, AurÃ©lie NÃ©vÃ©ol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Dragomir Radev, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Ellie Pavlick, FranÃ§ois Yvon, Genta Indra Winata, Hailey Schoelkopf, Jaesung Tae, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Khalid Almubarak, Liam Hazan, Lintang Sutawika, Manan Dey, Maraim Masoud, Margaret Mitchell, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Niklas Muennighoff, Oleg Serikov, Omer Antverg, Oskar van der Wal, Pawan Sasanka Ammanamanchi, Pierre Colombo, Rachel Bawden, Rui Zhang, Ruochen Zhang, Samson Tan, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Shanya Sharma, Shayne Longpre, Stella Biderman, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Urmish Thakker, Vassilina Nikoulina, Verena Rieser, Vikas Raunak, Vitaly Protasov, Vladislav Mikhailov, Wilson Y. Lee, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, ZdenÄ›k Kasner, Zeerak Talat, Zheng-Xin Yong
<span class="ltx_ERROR undefined" id="id15.15.id15">\AND</span>Broader Impacts 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id16.16.id16">\names</span>Aaron Gokaslan, Alexandra Sasha Luccioni, Alham Fikri Aji, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Angelina McMillan-Major, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos MuÃ±oz Ferrandis, Chenghao Mou, Minh Chien Vu, Christopher Akiki, Daniel McDuff, Danish Contractor, David Ifeoluwa Adelani, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, GÃ©rard Dupont, Giada Pistilli, Habib Rezanejad, Hessie Jones, Huu Nguyen, Ian Yu, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jaesung Tae, Jenny Chim, Jesse Dodge, Jesse Passmore, Josh Seltzer, Julien Launay, Julio Bonis Sanz, Khalid Almubarak, Livia Dutra, Long Phan, Mairon Samagaio, Manan Dey, Maraim Masoud, Margaret Mitchell, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Niklas Muennighoff, Nishant Subramani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Olivier Nguyen, Paulo Villegas, Pawan Sasanka Ammanamanchi, Priscilla Amuok, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Shanya Sharma, Shayne Longpre, Silas Wang, Somaieh Nikpoor, Sourav Roy, Stas Bekman, Stella Biderman, Suhas Pai, Suzana IliÄ‡, Sylvain Viguier, Teven Le Scao, Thanh Le, Tobi Oyebade, Trieu Le, Tristan Thrush, Yacine Jernite, Yoyo Yang, Zach Nguyen, Zeerak Talat, Zheng-Xin Yong
<span class="ltx_ERROR undefined" id="id17.17.id17">\AND</span>Applications 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id18.18.id18">\names</span>Abhinav Ramesh Kashyap, Albert Villanova del Moral, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Carlos MuÃ±oz Ferrandis, Chenxi Zhou, Chirag Jain, Christopher Akiki, Chuxin Xu, ClÃ©mentine Fourrier, Daniel LeÃ³n PeriÃ±Ã¡n, Daniel Molano, Daniel van Strien, Danish Contractor, David Lansky, Debajyoti Datta, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Francesco De Toni, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jason Alan Fries, Javier de la Rosa, Jenny Chim, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Leon Weber, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc PÃ mies, Maria A Castillo, Marianna Nezhurina, Mario SÃ¤nger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minh Chien Vu, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shamik Bose, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Stella Biderman, Stephen H. Bach, Sushil Bharati, Tanmay Laud, ThÃ©o Gigant, Tomoya Kainuma, Trishala Neeraj, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye
<span class="ltx_ERROR undefined" id="id19.19.id19">\AND</span>Organization 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id20.20.id20">\names</span>Angela Fan, Christopher Akiki, Douwe Kiela, Giada Pistilli, Margot Mieskes, Mathilde Bras, Matthias GallÃ©, Suzana IliÄ‡, Yacine Jernite, Younes Belkada, Thomas Wolf
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id21.id1">Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions.
While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public.
As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers.
BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total).
We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning.
To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://hf.co/bigscience/bloom" title="">hf.co/bigscience/bloom</a></span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p" id="p1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1" style="font-size:90%;">Keywords:</span><span class="ltx_text" id="p1.1.2" style="font-size:90%;"> 
Language models, collaborative research</span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Pretrained language models have become a cornerstone of modern natural language processing (NLP) pipelines because they often produce better performance from smaller quantities of labeled data.
The development of ELMoÂ <cite class="ltx_cite ltx_citemacro_citep">(Peters etÂ al., <a class="ltx_ref" href="#bib.bib112" title="">2018</a>)</cite>, ULMFiTÂ <cite class="ltx_cite ltx_citemacro_citep">(Howard and Ruder, <a class="ltx_ref" href="#bib.bib62" title="">2018</a>)</cite>, GPTÂ <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a class="ltx_ref" href="#bib.bib116" title="">2018</a>)</cite>, and BERTÂ <cite class="ltx_cite ltx_citemacro_citep">(Devlin etÂ al., <a class="ltx_ref" href="#bib.bib39" title="">2019</a>)</cite> led to the widespread use of pretrained models as an initialization for finetuning on downstream tasks.
The subsequent finding that pretrained language models can perform useful tasks without any additional trainingÂ <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a class="ltx_ref" href="#bib.bib117" title="">2019</a>; Brown etÂ al., <a class="ltx_ref" href="#bib.bib27" title="">2020</a>)</cite> further demonstrated their utility.
In addition, the empirical observation that a language modelâ€™s performance tends to increase as the model is made largerâ€”sometimes predictablyÂ <cite class="ltx_cite ltx_citemacro_citep">(Hestness etÂ al., <a class="ltx_ref" href="#bib.bib59" title="">2017</a>; Kaplan etÂ al., <a class="ltx_ref" href="#bib.bib67" title="">2020</a>; Hoffmann etÂ al., <a class="ltx_ref" href="#bib.bib61" title="">2022</a>)</cite> and sometimes suddenlyÂ <cite class="ltx_cite ltx_citemacro_citep">(Wei etÂ al., <a class="ltx_ref" href="#bib.bib159" title="">2022</a>)</cite>â€”has led to a trend of increasing scaleÂ <cite class="ltx_cite ltx_citemacro_citep">(Zeng etÂ al., <a class="ltx_ref" href="#bib.bib168" title="">2021</a>; Rae etÂ al., <a class="ltx_ref" href="#bib.bib118" title="">2021</a>; Smith etÂ al., <a class="ltx_ref" href="#bib.bib138" title="">2022</a>; Chowdhery etÂ al., <a class="ltx_ref" href="#bib.bib31" title="">2022</a>)</cite>.
Apart from environmental concernsÂ <cite class="ltx_cite ltx_citemacro_citep">(Strubell etÂ al., <a class="ltx_ref" href="#bib.bib141" title="">2019</a>; Lacoste etÂ al., <a class="ltx_ref" href="#bib.bib72" title="">2019</a>; Schwartz etÂ al., <a class="ltx_ref" href="#bib.bib130" title="">2020</a>)</cite>, the costs of training large language models (LLMs) are only affordable for well-resourced organizations.
Furthermore, until recently, most LLMs were not publicly released.
As a result, the majority of the research community has been excluded from the development of LLMs.
This exclusion has had concrete consequences; for example, most LLMs are primarily trained on English-language text (with notable exceptions in Chinese and Korean, e.g.Â <cite class="ltx_cite ltx_citemacro_citep">Wang etÂ al., <a class="ltx_ref" href="#bib.bib155" title="">2021</a>; Zeng etÂ al., <a class="ltx_ref" href="#bib.bib168" title="">2021</a>; Kim etÂ al., <a class="ltx_ref" href="#bib.bib68" title="">2021</a></cite>).</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To address these issues, we present the BigScience Large Open-science Open-access Multilingual Language Model (BLOOM,Â <cite class="ltx_cite ltx_citemacro_citep">BigScience Workshop, <a class="ltx_ref" href="#bib.bib18" title="">2022</a></cite>).
BLOOM is a 176 billion parameter language model trained on 46 natural languages and 13 programming languages that was developed and released by a collaboration of hundreds of researchers.
The compute for training BLOOM was provided through a French public grant from GENCI and IDRIS, leveraging IDRISâ€™ Jean Zay supercomputer.
To build BLOOM, we undertook a thorough design process for each of its components, including the training dataset (<a class="ltx_ref" href="#S3.SS1" title="3.1 Training Dataset â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">3.1</span></a>), model architecture and training objective (<a class="ltx_ref" href="#S3.SS2" title="3.2 Model Architecture â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">3.2</span></a>), and engineering strategy for distributed learning (<a class="ltx_ref" href="#S3.SS4" title="3.4 Engineering â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">3.4</span></a>).
We also performed an analysis of the modelâ€™s capabilities (<a class="ltx_ref" href="#S4" title="4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">4</span></a>).
Our overall aim is not only to publicly release a large-scale multilingual language model with performance comparable to recently developed systems, but also to document the coordinated process that went into its development (<a class="ltx_ref" href="#S2.SS2" title="2.2 BigScience â€£ 2 Background â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">2.2</span></a>).
The purpose of this paper is to provide a high-level overview of these design steps while referencing the individual reports we produced over the course of developing BLOOM.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Before describing the BLOOM model itself, in this section we provide necessary background on LLMs as well as an organizational overview of the BigScience effort.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Language Modeling</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.6">Language modeling refers to the task of modeling the probability of a sequence of tokens in a text <cite class="ltx_cite ltx_citemacro_citep">(Shannon, <a class="ltx_ref" href="#bib.bib132" title="">1948</a>)</cite>, where a token is a unit of text (e.g.Â word, subword, character or byte, etc., as discussed byÂ <cite class="ltx_cite ltx_citemacro_citep">Mielke etÂ al., <a class="ltx_ref" href="#bib.bib91" title="">2021</a></cite>).
In this work (and in most current applications of language modeling) we model the joint probability of tokens in a text as:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p(x)=p(x_{1},\ldots,x_{T})=\prod_{t=1}^{T}p(x_{t}|x_{&lt;t})" class="ltx_Math" display="block" id="S2.E1.m1.5"><semantics id="S2.E1.m1.5a"><mrow id="S2.E1.m1.5.5" xref="S2.E1.m1.5.5.cmml"><mrow id="S2.E1.m1.5.5.5" xref="S2.E1.m1.5.5.5.cmml"><mi id="S2.E1.m1.5.5.5.2" xref="S2.E1.m1.5.5.5.2.cmml">p</mi><mo id="S2.E1.m1.5.5.5.1" xref="S2.E1.m1.5.5.5.1.cmml">â¢</mo><mrow id="S2.E1.m1.5.5.5.3.2" xref="S2.E1.m1.5.5.5.cmml"><mo id="S2.E1.m1.5.5.5.3.2.1" stretchy="false" xref="S2.E1.m1.5.5.5.cmml">(</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">x</mi><mo id="S2.E1.m1.5.5.5.3.2.2" stretchy="false" xref="S2.E1.m1.5.5.5.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.5.5.6" xref="S2.E1.m1.5.5.6.cmml">=</mo><mrow id="S2.E1.m1.4.4.2" xref="S2.E1.m1.4.4.2.cmml"><mi id="S2.E1.m1.4.4.2.4" xref="S2.E1.m1.4.4.2.4.cmml">p</mi><mo id="S2.E1.m1.4.4.2.3" xref="S2.E1.m1.4.4.2.3.cmml">â¢</mo><mrow id="S2.E1.m1.4.4.2.2.2" xref="S2.E1.m1.4.4.2.2.3.cmml"><mo id="S2.E1.m1.4.4.2.2.2.3" stretchy="false" xref="S2.E1.m1.4.4.2.2.3.cmml">(</mo><msub id="S2.E1.m1.3.3.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.2.cmml">x</mi><mn id="S2.E1.m1.3.3.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.E1.m1.4.4.2.2.2.4" xref="S2.E1.m1.4.4.2.2.3.cmml">,</mo><mi id="S2.E1.m1.2.2" mathvariant="normal" xref="S2.E1.m1.2.2.cmml">â€¦</mi><mo id="S2.E1.m1.4.4.2.2.2.5" xref="S2.E1.m1.4.4.2.2.3.cmml">,</mo><msub id="S2.E1.m1.4.4.2.2.2.2" xref="S2.E1.m1.4.4.2.2.2.2.cmml"><mi id="S2.E1.m1.4.4.2.2.2.2.2" xref="S2.E1.m1.4.4.2.2.2.2.2.cmml">x</mi><mi id="S2.E1.m1.4.4.2.2.2.2.3" xref="S2.E1.m1.4.4.2.2.2.2.3.cmml">T</mi></msub><mo id="S2.E1.m1.4.4.2.2.2.6" stretchy="false" xref="S2.E1.m1.4.4.2.2.3.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.5.5.7" rspace="0.111em" xref="S2.E1.m1.5.5.7.cmml">=</mo><mrow id="S2.E1.m1.5.5.3" xref="S2.E1.m1.5.5.3.cmml"><munderover id="S2.E1.m1.5.5.3.2" xref="S2.E1.m1.5.5.3.2.cmml"><mo id="S2.E1.m1.5.5.3.2.2.2" movablelimits="false" xref="S2.E1.m1.5.5.3.2.2.2.cmml">âˆ</mo><mrow id="S2.E1.m1.5.5.3.2.2.3" xref="S2.E1.m1.5.5.3.2.2.3.cmml"><mi id="S2.E1.m1.5.5.3.2.2.3.2" xref="S2.E1.m1.5.5.3.2.2.3.2.cmml">t</mi><mo id="S2.E1.m1.5.5.3.2.2.3.1" xref="S2.E1.m1.5.5.3.2.2.3.1.cmml">=</mo><mn id="S2.E1.m1.5.5.3.2.2.3.3" xref="S2.E1.m1.5.5.3.2.2.3.3.cmml">1</mn></mrow><mi id="S2.E1.m1.5.5.3.2.3" xref="S2.E1.m1.5.5.3.2.3.cmml">T</mi></munderover><mrow id="S2.E1.m1.5.5.3.1" xref="S2.E1.m1.5.5.3.1.cmml"><mi id="S2.E1.m1.5.5.3.1.3" xref="S2.E1.m1.5.5.3.1.3.cmml">p</mi><mo id="S2.E1.m1.5.5.3.1.2" xref="S2.E1.m1.5.5.3.1.2.cmml">â¢</mo><mrow id="S2.E1.m1.5.5.3.1.1.1" xref="S2.E1.m1.5.5.3.1.1.1.1.cmml"><mo id="S2.E1.m1.5.5.3.1.1.1.2" stretchy="false" xref="S2.E1.m1.5.5.3.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.5.5.3.1.1.1.1" xref="S2.E1.m1.5.5.3.1.1.1.1.cmml"><msub id="S2.E1.m1.5.5.3.1.1.1.1.2" xref="S2.E1.m1.5.5.3.1.1.1.1.2.cmml"><mi id="S2.E1.m1.5.5.3.1.1.1.1.2.2" xref="S2.E1.m1.5.5.3.1.1.1.1.2.2.cmml">x</mi><mi id="S2.E1.m1.5.5.3.1.1.1.1.2.3" xref="S2.E1.m1.5.5.3.1.1.1.1.2.3.cmml">t</mi></msub><mo fence="false" id="S2.E1.m1.5.5.3.1.1.1.1.1" xref="S2.E1.m1.5.5.3.1.1.1.1.1.cmml">|</mo><msub id="S2.E1.m1.5.5.3.1.1.1.1.3" xref="S2.E1.m1.5.5.3.1.1.1.1.3.cmml"><mi id="S2.E1.m1.5.5.3.1.1.1.1.3.2" xref="S2.E1.m1.5.5.3.1.1.1.1.3.2.cmml">x</mi><mrow id="S2.E1.m1.5.5.3.1.1.1.1.3.3" xref="S2.E1.m1.5.5.3.1.1.1.1.3.3.cmml"><mi id="S2.E1.m1.5.5.3.1.1.1.1.3.3.2" xref="S2.E1.m1.5.5.3.1.1.1.1.3.3.2.cmml"></mi><mo id="S2.E1.m1.5.5.3.1.1.1.1.3.3.1" xref="S2.E1.m1.5.5.3.1.1.1.1.3.3.1.cmml">&lt;</mo><mi id="S2.E1.m1.5.5.3.1.1.1.1.3.3.3" xref="S2.E1.m1.5.5.3.1.1.1.1.3.3.3.cmml">t</mi></mrow></msub></mrow><mo id="S2.E1.m1.5.5.3.1.1.1.3" stretchy="false" xref="S2.E1.m1.5.5.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.5b"><apply id="S2.E1.m1.5.5.cmml" xref="S2.E1.m1.5.5"><and id="S2.E1.m1.5.5a.cmml" xref="S2.E1.m1.5.5"></and><apply id="S2.E1.m1.5.5b.cmml" xref="S2.E1.m1.5.5"><eq id="S2.E1.m1.5.5.6.cmml" xref="S2.E1.m1.5.5.6"></eq><apply id="S2.E1.m1.5.5.5.cmml" xref="S2.E1.m1.5.5.5"><times id="S2.E1.m1.5.5.5.1.cmml" xref="S2.E1.m1.5.5.5.1"></times><ci id="S2.E1.m1.5.5.5.2.cmml" xref="S2.E1.m1.5.5.5.2">ğ‘</ci><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">ğ‘¥</ci></apply><apply id="S2.E1.m1.4.4.2.cmml" xref="S2.E1.m1.4.4.2"><times id="S2.E1.m1.4.4.2.3.cmml" xref="S2.E1.m1.4.4.2.3"></times><ci id="S2.E1.m1.4.4.2.4.cmml" xref="S2.E1.m1.4.4.2.4">ğ‘</ci><vector id="S2.E1.m1.4.4.2.2.3.cmml" xref="S2.E1.m1.4.4.2.2.2"><apply id="S2.E1.m1.3.3.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.2">ğ‘¥</ci><cn id="S2.E1.m1.3.3.1.1.1.1.3.cmml" type="integer" xref="S2.E1.m1.3.3.1.1.1.1.3">1</cn></apply><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">â€¦</ci><apply id="S2.E1.m1.4.4.2.2.2.2.cmml" xref="S2.E1.m1.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.2.2.2.2.1.cmml" xref="S2.E1.m1.4.4.2.2.2.2">subscript</csymbol><ci id="S2.E1.m1.4.4.2.2.2.2.2.cmml" xref="S2.E1.m1.4.4.2.2.2.2.2">ğ‘¥</ci><ci id="S2.E1.m1.4.4.2.2.2.2.3.cmml" xref="S2.E1.m1.4.4.2.2.2.2.3">ğ‘‡</ci></apply></vector></apply></apply><apply id="S2.E1.m1.5.5c.cmml" xref="S2.E1.m1.5.5"><eq id="S2.E1.m1.5.5.7.cmml" xref="S2.E1.m1.5.5.7"></eq><share href="#S2.E1.m1.4.4.2.cmml" id="S2.E1.m1.5.5d.cmml" xref="S2.E1.m1.5.5"></share><apply id="S2.E1.m1.5.5.3.cmml" xref="S2.E1.m1.5.5.3"><apply id="S2.E1.m1.5.5.3.2.cmml" xref="S2.E1.m1.5.5.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.3.2.1.cmml" xref="S2.E1.m1.5.5.3.2">superscript</csymbol><apply id="S2.E1.m1.5.5.3.2.2.cmml" xref="S2.E1.m1.5.5.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.3.2.2.1.cmml" xref="S2.E1.m1.5.5.3.2">subscript</csymbol><csymbol cd="latexml" id="S2.E1.m1.5.5.3.2.2.2.cmml" xref="S2.E1.m1.5.5.3.2.2.2">product</csymbol><apply id="S2.E1.m1.5.5.3.2.2.3.cmml" xref="S2.E1.m1.5.5.3.2.2.3"><eq id="S2.E1.m1.5.5.3.2.2.3.1.cmml" xref="S2.E1.m1.5.5.3.2.2.3.1"></eq><ci id="S2.E1.m1.5.5.3.2.2.3.2.cmml" xref="S2.E1.m1.5.5.3.2.2.3.2">ğ‘¡</ci><cn id="S2.E1.m1.5.5.3.2.2.3.3.cmml" type="integer" xref="S2.E1.m1.5.5.3.2.2.3.3">1</cn></apply></apply><ci id="S2.E1.m1.5.5.3.2.3.cmml" xref="S2.E1.m1.5.5.3.2.3">ğ‘‡</ci></apply><apply id="S2.E1.m1.5.5.3.1.cmml" xref="S2.E1.m1.5.5.3.1"><times id="S2.E1.m1.5.5.3.1.2.cmml" xref="S2.E1.m1.5.5.3.1.2"></times><ci id="S2.E1.m1.5.5.3.1.3.cmml" xref="S2.E1.m1.5.5.3.1.3">ğ‘</ci><apply id="S2.E1.m1.5.5.3.1.1.1.1.cmml" xref="S2.E1.m1.5.5.3.1.1.1"><csymbol cd="latexml" id="S2.E1.m1.5.5.3.1.1.1.1.1.cmml" xref="S2.E1.m1.5.5.3.1.1.1.1.1">conditional</csymbol><apply id="S2.E1.m1.5.5.3.1.1.1.1.2.cmml" xref="S2.E1.m1.5.5.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.3.1.1.1.1.2.1.cmml" xref="S2.E1.m1.5.5.3.1.1.1.1.2">subscript</csymbol><ci id="S2.E1.m1.5.5.3.1.1.1.1.2.2.cmml" xref="S2.E1.m1.5.5.3.1.1.1.1.2.2">ğ‘¥</ci><ci id="S2.E1.m1.5.5.3.1.1.1.1.2.3.cmml" xref="S2.E1.m1.5.5.3.1.1.1.1.2.3">ğ‘¡</ci></apply><apply id="S2.E1.m1.5.5.3.1.1.1.1.3.cmml" xref="S2.E1.m1.5.5.3.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.3.1.1.1.1.3.1.cmml" xref="S2.E1.m1.5.5.3.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.5.5.3.1.1.1.1.3.2.cmml" xref="S2.E1.m1.5.5.3.1.1.1.1.3.2">ğ‘¥</ci><apply id="S2.E1.m1.5.5.3.1.1.1.1.3.3.cmml" xref="S2.E1.m1.5.5.3.1.1.1.1.3.3"><lt id="S2.E1.m1.5.5.3.1.1.1.1.3.3.1.cmml" xref="S2.E1.m1.5.5.3.1.1.1.1.3.3.1"></lt><csymbol cd="latexml" id="S2.E1.m1.5.5.3.1.1.1.1.3.3.2.cmml" xref="S2.E1.m1.5.5.3.1.1.1.1.3.3.2">absent</csymbol><ci id="S2.E1.m1.5.5.3.1.1.1.1.3.3.3.cmml" xref="S2.E1.m1.5.5.3.1.1.1.1.3.3.3">ğ‘¡</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.5c">p(x)=p(x_{1},\ldots,x_{T})=\prod_{t=1}^{T}p(x_{t}|x_{&lt;t})</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.5d">italic_p ( italic_x ) = italic_p ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) = âˆ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_p ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT &lt; italic_t end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS1.p1.5">where <math alttext="x" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.1"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">italic_x</annotation></semantics></math> is a sequence of tokens, <math alttext="x_{t}" class="ltx_Math" display="inline" id="S2.SS1.p1.2.m2.1"><semantics id="S2.SS1.p1.2.m2.1a"><msub id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><mi id="S2.SS1.p1.2.m2.1.1.2" xref="S2.SS1.p1.2.m2.1.1.2.cmml">x</mi><mi id="S2.SS1.p1.2.m2.1.1.3" xref="S2.SS1.p1.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2">ğ‘¥</ci><ci id="S2.SS1.p1.2.m2.1.1.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">x_{t}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.2.m2.1d">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is the <math alttext="t^{\mathrm{th}}" class="ltx_Math" display="inline" id="S2.SS1.p1.3.m3.1"><semantics id="S2.SS1.p1.3.m3.1a"><msup id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml"><mi id="S2.SS1.p1.3.m3.1.1.2" xref="S2.SS1.p1.3.m3.1.1.2.cmml">t</mi><mi id="S2.SS1.p1.3.m3.1.1.3" xref="S2.SS1.p1.3.m3.1.1.3.cmml">th</mi></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><apply id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.3.m3.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">superscript</csymbol><ci id="S2.SS1.p1.3.m3.1.1.2.cmml" xref="S2.SS1.p1.3.m3.1.1.2">ğ‘¡</ci><ci id="S2.SS1.p1.3.m3.1.1.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3">th</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">t^{\mathrm{th}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.3.m3.1d">italic_t start_POSTSUPERSCRIPT roman_th end_POSTSUPERSCRIPT</annotation></semantics></math> token, and <math alttext="x_{&lt;t}" class="ltx_Math" display="inline" id="S2.SS1.p1.4.m4.1"><semantics id="S2.SS1.p1.4.m4.1a"><msub id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml"><mi id="S2.SS1.p1.4.m4.1.1.2" xref="S2.SS1.p1.4.m4.1.1.2.cmml">x</mi><mrow id="S2.SS1.p1.4.m4.1.1.3" xref="S2.SS1.p1.4.m4.1.1.3.cmml"><mi id="S2.SS1.p1.4.m4.1.1.3.2" xref="S2.SS1.p1.4.m4.1.1.3.2.cmml"></mi><mo id="S2.SS1.p1.4.m4.1.1.3.1" xref="S2.SS1.p1.4.m4.1.1.3.1.cmml">&lt;</mo><mi id="S2.SS1.p1.4.m4.1.1.3.3" xref="S2.SS1.p1.4.m4.1.1.3.3.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><apply id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.1.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS1.p1.4.m4.1.1.2.cmml" xref="S2.SS1.p1.4.m4.1.1.2">ğ‘¥</ci><apply id="S2.SS1.p1.4.m4.1.1.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3"><lt id="S2.SS1.p1.4.m4.1.1.3.1.cmml" xref="S2.SS1.p1.4.m4.1.1.3.1"></lt><csymbol cd="latexml" id="S2.SS1.p1.4.m4.1.1.3.2.cmml" xref="S2.SS1.p1.4.m4.1.1.3.2">absent</csymbol><ci id="S2.SS1.p1.4.m4.1.1.3.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3.3">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">x_{&lt;t}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.4.m4.1d">italic_x start_POSTSUBSCRIPT &lt; italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is the sequence of tokens preceding <math alttext="x_{t}" class="ltx_Math" display="inline" id="S2.SS1.p1.5.m5.1"><semantics id="S2.SS1.p1.5.m5.1a"><msub id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml"><mi id="S2.SS1.p1.5.m5.1.1.2" xref="S2.SS1.p1.5.m5.1.1.2.cmml">x</mi><mi id="S2.SS1.p1.5.m5.1.1.3" xref="S2.SS1.p1.5.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><apply id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.5.m5.1.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p1.5.m5.1.1.2.cmml" xref="S2.SS1.p1.5.m5.1.1.2">ğ‘¥</ci><ci id="S2.SS1.p1.5.m5.1.1.3.cmml" xref="S2.SS1.p1.5.m5.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">x_{t}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.5.m5.1d">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>.
This approach is referred to as autoregressive language modeling and can be seen as iteratively predicting the probability of the next token.</p>
</div>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Early Language Models</h5>
<div class="ltx_para" id="S2.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px1.p1.5">Language models have a long history of application in NLP.
Early language models (such as those developed byÂ <cite class="ltx_cite ltx_citemacro_citep">Shannon, <a class="ltx_ref" href="#bib.bib132" title="">1948</a></cite>) were primarily <math alttext="n" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S2.SS1.SSS0.Px1.p1.1.m1.1a"><mi id="S2.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.1.m1.1b"><ci id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px1.p1.1.m1.1d">italic_n</annotation></semantics></math>-gram models that estimate the probability of a length-<math alttext="n" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.2.m2.1"><semantics id="S2.SS1.SSS0.Px1.p1.2.m2.1a"><mi id="S2.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.2.m2.1b"><ci id="S2.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px1.p1.2.m2.1d">italic_n</annotation></semantics></math> sequence of tokens in accordance with the number of times it appears in a training corpus.
In practice, <math alttext="n" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.3.m3.1"><semantics id="S2.SS1.SSS0.Px1.p1.3.m3.1a"><mi id="S2.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S2.SS1.SSS0.Px1.p1.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.3.m3.1b"><ci id="S2.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.3.m3.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.3.m3.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px1.p1.3.m3.1d">italic_n</annotation></semantics></math>-gram models face two major issues: first, they grow exponentially in size as <math alttext="n" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.4.m4.1"><semantics id="S2.SS1.SSS0.Px1.p1.4.m4.1a"><mi id="S2.SS1.SSS0.Px1.p1.4.m4.1.1" xref="S2.SS1.SSS0.Px1.p1.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.4.m4.1b"><ci id="S2.SS1.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.4.m4.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px1.p1.4.m4.1d">italic_n</annotation></semantics></math> is increased; and second, they have no direct way of producing a probability for a sequence of tokens that does not appear in their training data.
Advances on these problems enabled <math alttext="n" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.5.m5.1"><semantics id="S2.SS1.SSS0.Px1.p1.5.m5.1a"><mi id="S2.SS1.SSS0.Px1.p1.5.m5.1.1" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.5.m5.1b"><ci id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.5.m5.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px1.p1.5.m5.1d">italic_n</annotation></semantics></math>-gram models to see widespread use across most areas of NLPÂ <cite class="ltx_cite ltx_citemacro_citep">(Goodman, <a class="ltx_ref" href="#bib.bib54" title="">2001</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Neural Language Models</h5>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px2.p1.1">An alternative to <math alttext="n" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.1.m1.1"><semantics id="S2.SS1.SSS0.Px2.p1.1.m1.1a"><mi id="S2.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p1.1.m1.1b"><ci id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px2.p1.1.m1.1d">italic_n</annotation></semantics></math>-gram models, first proposed byÂ <cite class="ltx_cite ltx_citemacro_citet">Miikkulainen and Dyer (<a class="ltx_ref" href="#bib.bib92" title="">1991</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Schmidhuber and Heil (<a class="ltx_ref" href="#bib.bib129" title="">1996</a>)</cite> and later popularized byÂ <cite class="ltx_cite ltx_citemacro_citet">Bengio etÂ al. (<a class="ltx_ref" href="#bib.bib16" title="">2000</a>)</cite>, is to use a neural network to estimate the probability of the next token given prior tokens.
While early work used feed-forward networks with a fixed-length history window,Â <cite class="ltx_cite ltx_citemacro_citet">Mikolov etÂ al. (<a class="ltx_ref" href="#bib.bib93" title="">2010</a>); Sutskever etÂ al. (<a class="ltx_ref" href="#bib.bib143" title="">2011</a>); Graves (<a class="ltx_ref" href="#bib.bib56" title="">2013</a>)</cite> proposed to use recurrent neural networks instead and found that this significantly improved performance.
More recently, language models based on the Transformer architectureÂ <cite class="ltx_cite ltx_citemacro_citep">(Vaswani etÂ al., <a class="ltx_ref" href="#bib.bib148" title="">2017</a>)</cite> were shown to be more effective than recurrent neural networksÂ <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a class="ltx_ref" href="#bib.bib116" title="">2018</a>; Al-Rfou etÂ al., <a class="ltx_ref" href="#bib.bib5" title="">2019</a>; Kaplan etÂ al., <a class="ltx_ref" href="#bib.bib67" title="">2020</a>)</cite>.
Consequently, the Transformer has become the <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS0.Px2.p1.1.1">de facto</span> choice for language models.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Transfer Learning</h5>
<div class="ltx_para" id="S2.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px3.p1.1">In tandem with advances in language modeling using neural networks, NLP pipelines have increasingly adopted the framework of transfer learning.
In transfer learning, the parameters of a model are first pretrained on a data-rich task before being finetuned on a downstream task.
A historically common approach to obtaining pretrained parameters were word vectorsÂ <cite class="ltx_cite ltx_citemacro_citep">(Mikolov etÂ al., <a class="ltx_ref" href="#bib.bib94" title="">2013</a>)</cite> trained so that the dot product of co-occurring word vectors is large.
However, subsequent work byÂ <cite class="ltx_cite ltx_citemacro_citet">Peters etÂ al. (<a class="ltx_ref" href="#bib.bib112" title="">2018</a>); Howard and Ruder (<a class="ltx_ref" href="#bib.bib62" title="">2018</a>); Radford etÂ al. (<a class="ltx_ref" href="#bib.bib116" title="">2018</a>); Devlin etÂ al. (<a class="ltx_ref" href="#bib.bib39" title="">2019</a>)</cite> showed that the framework ofÂ <cite class="ltx_cite ltx_citemacro_citet">Collobert etÂ al. (<a class="ltx_ref" href="#bib.bib33" title="">2011</a>)</cite>, where the entire model is pretrained before being finetuned, can attain stronger performance.
In particular,Â <cite class="ltx_cite ltx_citemacro_citet">Radford etÂ al. (<a class="ltx_ref" href="#bib.bib116" title="">2018</a>); Devlin etÂ al. (<a class="ltx_ref" href="#bib.bib39" title="">2019</a>)</cite> demonstrated strong results using pretrained Transformer language models, prompting work on progressively better modelsÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a class="ltx_ref" href="#bib.bib82" title="">2019</a>; Yang etÂ al., <a class="ltx_ref" href="#bib.bib166" title="">2019</a>; Lewis etÂ al., <a class="ltx_ref" href="#bib.bib76" title="">2020</a>; Raffel etÂ al., <a class="ltx_ref" href="#bib.bib119" title="">2020</a>; Zhang etÂ al., <a class="ltx_ref" href="#bib.bib171" title="">2019</a>, etc.)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Few- and Zero-Shot Learning</h5>
<div class="ltx_para" id="S2.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px4.p1.1">While finetuning a pretrained model remains an effective way of attaining high performance with limited labeled data, a parallel line of work has demonstrated that pretrained language models can be induced to perform tasks without any subsequent training.
AfterÂ <cite class="ltx_cite ltx_citemacro_citet">Vinyals and Le (<a class="ltx_ref" href="#bib.bib149" title="">2015</a>)</cite> observed limited task-performing behavior in a neural dialog model,Â <cite class="ltx_cite ltx_citemacro_citet">Radford etÂ al. (<a class="ltx_ref" href="#bib.bib117" title="">2019</a>)</cite> later demonstrated that Transformer-based language models trained on text scraped from the web could perform various tasks to varying degrees.
Notably,Â <cite class="ltx_cite ltx_citemacro_citet">Radford etÂ al. (<a class="ltx_ref" href="#bib.bib117" title="">2019</a>)</cite> found that performance improved with model scale, inspiring work to characterizeÂ <cite class="ltx_cite ltx_citemacro_citep">(Kaplan etÂ al., <a class="ltx_ref" href="#bib.bib67" title="">2020</a>; Hoffmann etÂ al., <a class="ltx_ref" href="#bib.bib61" title="">2022</a>)</cite> and exploitÂ <cite class="ltx_cite ltx_citemacro_citep">(Shoeybi etÂ al., <a class="ltx_ref" href="#bib.bib136" title="">2019</a>; Brown etÂ al., <a class="ltx_ref" href="#bib.bib27" title="">2020</a>; Smith etÂ al., <a class="ltx_ref" href="#bib.bib138" title="">2022</a>; Chowdhery etÂ al., <a class="ltx_ref" href="#bib.bib31" title="">2022</a>; Rae etÂ al., <a class="ltx_ref" href="#bib.bib118" title="">2021</a>; Wang etÂ al., <a class="ltx_ref" href="#bib.bib155" title="">2021</a>; Zeng etÂ al., <a class="ltx_ref" href="#bib.bib168" title="">2021</a>; Zhang etÂ al., <a class="ltx_ref" href="#bib.bib169" title="">2022</a>)</cite> the benefits of scale.
A major factor in the success of this approach is the way that task-specific examples are formatted when fed into the model.
<cite class="ltx_cite ltx_citemacro_citet">Brown etÂ al. (<a class="ltx_ref" href="#bib.bib27" title="">2020</a>)</cite> popularized the idea of designing â€œpromptsâ€ that provide natural-language descriptions of the task and also allow inputting a few demonstrations of input-output behavior.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Social Limitations of LLM Development</h5>
<div class="ltx_para" id="S2.SS1.SSS0.Px5.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px5.p1.1">While the continued increase in the size of large language models
has resulted in improvements
across a wide range of tasks,
it has also exacerbated issues with their development and useÂ <cite class="ltx_cite ltx_citemacro_citep">(Bender etÂ al., <a class="ltx_ref" href="#bib.bib15" title="">2021</a>)</cite>. The computational expense of large models also prohibits the majority of the research community from participating in their development, evaluation and routine use.
Moreover, the computational costs have also lead to concerns about the carbon footprint stemming from the training and use of large language models
Â <cite class="ltx_cite ltx_citemacro_citep">(Strubell etÂ al., <a class="ltx_ref" href="#bib.bib141" title="">2019</a>; Lacoste etÂ al., <a class="ltx_ref" href="#bib.bib72" title="">2019</a>; Schwartz etÂ al., <a class="ltx_ref" href="#bib.bib130" title="">2020</a>; Bannour etÂ al., <a class="ltx_ref" href="#bib.bib9" title="">2021</a>)</cite>, and existing carbon footprint studies have likely under-estimated emissionsÂ <cite class="ltx_cite ltx_citemacro_citep">(Bannour etÂ al., <a class="ltx_ref" href="#bib.bib9" title="">2021</a>)</cite>.
Contributing to an increase in the global carbon footprint exacerbates climate change which most severely affects already-marginalized communities <cite class="ltx_cite ltx_citemacro_citep">(Westra and Lawson, <a class="ltx_ref" href="#bib.bib160" title="">2001</a>)</cite>.
Furthermore, the concentration of resources within a handful of (typically industrial) institutions with primarily technical expertise hinders prospects for an inclusive, collaborative, and reliable governance of the technology. First, public narratives about the technology that are driven by industry actors can lead to inflated expectations about its suitability for useÂ <cite class="ltx_cite ltx_citemacro_citep">(Brennen, <a class="ltx_ref" href="#bib.bib25" title="">2018</a>; Brennen etÂ al., <a class="ltx_ref" href="#bib.bib26" title="">2022</a>)</cite>, leading to misaligned research and policy prioritiesÂ <cite class="ltx_cite ltx_citemacro_citep">(Raji etÂ al., <a class="ltx_ref" href="#bib.bib122" title="">2022</a>)</cite> and potentially dire consequences in e.g.Â medical applicationsÂ <cite class="ltx_cite ltx_citemacro_citep">(Wong etÂ al., <a class="ltx_ref" href="#bib.bib163" title="">2021</a>)</cite>. Second, in a world mediated by technology, choices at all stages of its development end up shaping peopleâ€™s lives in a way that can be most closely compared to regulationsÂ <cite class="ltx_cite ltx_citemacro_citep">(Winner, <a class="ltx_ref" href="#bib.bib161" title="">1977</a>, <a class="ltx_ref" href="#bib.bib162" title="">2017</a>)</cite>, albeit without the same explicit consultation of stakeholders in the process.
When the development efforts are guided by prioritizing internal definitions of performance over their impact on society, the values of the developers come to be emphasized over those of the direct and indirect users <cite class="ltx_cite ltx_citemacro_citep">(Birhane etÂ al., <a class="ltx_ref" href="#bib.bib20" title="">2022</a>)</cite>.
Despite the substantial social dangers in allowing this technology to be developed unilaterally by corporations, EleutherAI <cite class="ltx_cite ltx_citemacro_citep">(Phang etÂ al., <a class="ltx_ref" href="#bib.bib113" title="">2022</a>)</cite> was the only non-corporate entity outside of China that was developing large language models before the BigScience Workshop was convened.
</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>BigScience</h3>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Participants</h5>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p1.1">BLOOMâ€™s development was coordinated by BigScience, an open research collaboration whose goal was the public release of an LLM.
The project started after being awarded by GENCI a compute grant on its Jean Zay supercomputer at IDRIS/CNRS.
It was initially built around a concerted effort from Hugging Face and the French NLP community (the â€œfounding membersâ€), and quickly opened up to grow into a broader international collaboration to support its aims of linguistic, geographical, and scientific diversity.
In the end, over 1200 people registered as participants in BigScience and were given access to its communication channels.
They had background not only in machine learning and computer science, but also linguistics, statistics, socio-cultural anthropology, philosophy, law, and other fields.
Of those, hundreds of individuals have directly contributed to one of the projectâ€™s released artifacts.
While the largest number of participants ultimately originated from the US, 38 countries were represented.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Organization</h5>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p1.1">The set of related research questions tackled by the BigScience effort was reflected in the projectâ€™s organization into working groups.
Each working group comprised several participants with various levels of involvement, including chairs whose role was to self-organize around a specific aspect of the overall project.
Importantly, participants were encouraged to join more than one working group in order to share experiences and information, which resulted in the set of 30 working groups presented in FigureÂ <a class="ltx_ref" href="#S2.F1" title="Figure 1 â€£ Organization â€£ 2.2 BigScience â€£ 2 Background â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">1</span></a>. Most of the working groups focused on tasks directly linked to the development of BLOOM. In addition, a few groups focused on the evaluation of LLMs and dataset development in specific domains, such as biomedical textsÂ <cite class="ltx_cite ltx_citemacro_citep">(Fries etÂ al., <a class="ltx_ref" href="#bib.bib47" title="">2022b</a>)</cite> and historical textsÂ <cite class="ltx_cite ltx_citemacro_citep">(DeÂ Toni etÂ al., <a class="ltx_ref" href="#bib.bib37" title="">2022</a>)</cite>.
A larger overview of the motivations behind this initiative, its history and some of the lessons learned can be found in <cite class="ltx_cite ltx_citemacro_citet">Akiki etÂ al. (<a class="ltx_ref" href="#bib.bib4" title="">2022</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="255" id="S2.F1.g1" src="x1.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S2.F1.3.2" style="font-size:90%;">Organization of BigScience working groups.</span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Ethical Considerations within BigScience</h5>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px3.p1.1">In order to acknowledge and start addressing social limitations of LLM development within BigScience, the workshop relied on a collaboratively designed Ethical Charter<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://bigscience.huggingface.co/blog/bigscience-ethical-charter" title="">bigscience.huggingface.co/blog/bigscience-ethical-charter</a></span></span></span> and original research on applicable regulations in jurisdictions outside of the US<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://bigscience.huggingface.co/blog/legal-playbook-for-natural-language-processing-researchers" title="">bigscience.huggingface.co/blog/legal-playbook-for-natural-language-processing-researchers</a></span></span></span> to guide its choices throughout the project. In particular, the charter emphasizes values of <span class="ltx_text ltx_font_bold" id="S2.SS2.SSS0.Px3.p1.1.1">inclusivity and diversity</span>, <span class="ltx_text ltx_font_bold" id="S2.SS2.SSS0.Px3.p1.1.2">openness and reproducibility</span>, and <span class="ltx_text ltx_font_bold" id="S2.SS2.SSS0.Px3.p1.1.3">responsibility</span> in various aspects of the organizationÂ <cite class="ltx_cite ltx_citemacro_citep">(Akiki etÂ al., <a class="ltx_ref" href="#bib.bib4" title="">2022</a>)</cite>. Each of these values are showcased in different ways in the dataset curation (SectionÂ <a class="ltx_ref" href="#S3.SS1" title="3.1 Training Dataset â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">3.1</span></a>), modeling (SectionÂ <a class="ltx_ref" href="#S3.SS2" title="3.2 Model Architecture â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">3.2</span></a>), engineering (SectionÂ <a class="ltx_ref" href="#S3.SS4" title="3.4 Engineering â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">3.4</span></a>), evaluation (SectionÂ <a class="ltx_ref" href="#S4" title="4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">4</span></a>), and other social impact (throughout) aspects of the project.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>BLOOM</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we document the design of BLOOM, including its training dataset (<a class="ltx_ref" href="#S3.SS1" title="3.1 Training Dataset â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">3.1</span></a>), architecture (<a class="ltx_ref" href="#S3.SS2" title="3.2 Model Architecture â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">3.2</span></a>), tokenizer (<a class="ltx_ref" href="#S3.SS3" title="3.3 Tokenization â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">3.3</span></a>), computing infrastructure (<a class="ltx_ref" href="#S3.SS4" title="3.4 Engineering â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">3.4</span></a>), and training hyperparameters (<a class="ltx_ref" href="#S3.SS5" title="3.5 Training â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">3.5</span></a>).</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Training Dataset</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">BLOOM was trained on the ROOTS corpusÂ <cite class="ltx_cite ltx_citemacro_citep">(LaurenÃ§on etÂ al., <a class="ltx_ref" href="#bib.bib74" title="">2022</a>)</cite>, a composite collection of 498Â HuggingÂ FaceÂ datasetsÂ <cite class="ltx_cite ltx_citemacro_citep">(Lhoest etÂ al., <a class="ltx_ref" href="#bib.bib77" title="">2021</a>)</cite> amounting to 1.61Â terabytes of text that span 46 natural languages and 13 programming languages. A high-level overview of this dataset can be seen inÂ <a class="ltx_ref" href="#S3.F3" title="Figure 3 â€£ Deduplication and Privacy Redaction â€£ 3.1.3 Data Preprocessing â€£ 3.1 Training Dataset â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">3</span></a>, while a detailed itemized list of every language along with its linguistic genus, family and macroarea is presented inÂ <a class="ltx_ref" href="#S3.T1" title="Table 1 â€£ 3.1 Training Dataset â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.2" style="width:433.6pt;height:771.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-26.0pt,46.2pt) scale(0.893036783623019,0.893036783623019) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.2.1">
<tr class="ltx_tr" id="S3.T1.2.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.1.1">Language</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.2.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.2.1">ISO-639-3</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.2.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.3.1">catalog-ref</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.2.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.4.1">Genus</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.2.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.5.1">Family</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.2.1.1.6"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.6.1">Macroarea</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S3.T1.2.1.1.7"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.7.1">Size in Bytes</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.1.2.1">Akan</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.1.2.2">aka</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.1.2.3">ak</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.1.2.4">Kwa</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.1.2.5">Niger-Congo</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.1.2.6">Africa</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.2.1.2.7">70,1554</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.3">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.3.1">Arabic</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.3.2">arb</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.3.3">ar</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.3.4">Semitic</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.3.5">Afro-Asiatic</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.3.6">Eurasia</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.3.7">74,854,900,600</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.4">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.4.1">Assamese</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.4.2">asm</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.4.3">as</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.4.4">Indic</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.4.5">Indo-European</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.4.6">Eurasia</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.4.7">291,522,098</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.5">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.5.1">Bambara</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.5.2">bam</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.5.3">bm</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.5.4">Western Mande</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.5.5">Mande</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.5.6">Africa</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.5.7">391,747</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.6">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.6.1">Basque</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.6.2">eus</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.6.3">eu</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.6.4">Basque</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.6.5">Basque</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.6.6">Eurasia</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.6.7">2,360,470,848</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.7">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.7.1">Bengali</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.7.2">ben</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.7.3">bn</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.7.4">Indic</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.7.5">Indo-European</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.7.6">Eurasia</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.7.7">18,606,823,104</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.8">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.8.1">Catalan</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.8.2">cat</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.8.3">ca</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.8.4">Romance</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.8.5">Indo-European</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.8.6">Eurasia</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.8.7">17,792,493,289</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.9">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.9.1">Chichewa</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.9.2">nya</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.9.3">ny</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.9.4">Bantoid</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.9.5">Niger-Congo</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.9.6">Africa</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.9.7">1,187,405</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.10">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.10.1">chiShona</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.10.2">sna</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.10.3">sn</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.10.4">Bantoid</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.10.5">Niger-Congo</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.10.6">Africa</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.10.7">6,638,639</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.11">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.11.1">Chitumbuka</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.11.2">tum</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.11.3">tum</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.11.4">Bantoid</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.11.5">Niger-Congo</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.11.6">Africa</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.11.7">170,360</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.12">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.12.1">English</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.12.2">eng</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.12.3">en</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.12.4">Germanic</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.12.5">Indo-European</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.12.6">Eurasia</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.12.7">484,953,009,124</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.13">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.13.1">Fon</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.13.2">fon</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.13.3">fon</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.13.4">Kwa</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.13.5">Niger-Congo</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.13.6">Africa</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.13.7">2,478,546</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.14">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.14.1">French</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.14.2">fra</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.14.3">fr</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.14.4">Romance</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.14.5">Indo-European</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.14.6">Eurasia</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.14.7">208,242,620,434</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.15">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.15.1">Gujarati</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.15.2">guj</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.15.3">gu</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.15.4">Indic</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.15.5">Indo-European</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.15.6">Eurasia</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.15.7">1,199,986,460</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.16">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.16.1">Hindi</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.16.2">hin</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.16.3">hi</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.16.4">Indic</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.16.5">Indo-European</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.16.6">Eurasia</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.16.7">24,622,119,985</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.17">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.17.1">Igbo</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.17.2">ibo</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.17.3">ig</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.17.4">Igboid</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.17.5">Niger-Congo</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.17.6">Africa</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.17.7">14078,521</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.18">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.18.1">Indonesian</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.18.2">ind</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.18.3">id</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.18.4">Malayo-Sumbawan</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.18.5">Austronesian</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.18.6">Papunesia</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.18.7">19,972,325,222</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.19">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.19.1">isiXhosa</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.19.2">xho</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.19.3">xh</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.19.4">Bantoid</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.19.5">Niger-Congo</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.19.6">Africa</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.19.7">14,304,074</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.20">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.20.1">isiZulu</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.20.2">zul</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.20.3">zu</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.20.4">Bantoid</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.20.5">Niger-Congo</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.20.6">Africa</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.20.7">8,511,561</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.21">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.21.1">Kannada</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.21.2">kan</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.21.3">kn</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.21.4">Southern Dravidian</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.21.5">Dravidian</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.21.6">Eurasia</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.21.7">2,098,453,560</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.22">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.22.1">Kikuyu</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.22.2">kik</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.22.3">ki</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.22.4">Bantoid</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.22.5">Niger-Congo</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.22.6">Africa</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.22.7">359,615</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.23">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.23.1">Kinyarwanda</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.23.2">kin</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.23.3">rw</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.23.4">Bantoid</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.23.5">Niger-Congo</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.23.6">Africa</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.23.7">40,428,299</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.24">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.24.1">Kirundi</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.24.2">run</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.24.3">rn</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.24.4">Bantoid</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.24.5">Niger-Congo</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.24.6">Africa</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.24.7">3,272,550</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.25">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.25.1">Lingala</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.25.2">lin</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.25.3">ln</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.25.4">Bantoid</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.25.5">Niger-Congo</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.25.6">Africa</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.25.7">1,650,804</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.26">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.26.1">Luganda</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.26.2">lug</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.26.3">lg</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.26.4">Bantoid</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.26.5">Niger-Congo</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.26.6">Africa</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.26.7">4,568,367</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.27">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.27.1">Malayalam</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.27.2">mal</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.27.3">ml</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.27.4">Southern Dravidian</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.27.5">Dravidian</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.27.6">Eurasia</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.27.7">3,662,571,498</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.28">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.28.1">Marathi</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.28.2">mar</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.28.3">mr</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.28.4">Indic</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.28.5">Indo-European</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.28.6">Eurasia</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.28.7">1,775,483,122</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.29">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.29.1">Nepali</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.29.2">nep</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.29.3">ne</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.29.4">Indic</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.29.5">Indo-European</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.29.6">Eurasia</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.29.7">2,551,307,393</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.30">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.30.1">Northern Sotho</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.30.2">nso</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.30.3">nso</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.30.4">Bantoid</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.30.5">Niger-Congo</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.30.6">Africa</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.30.7">1,764,506</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.31">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.31.1">Odia</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.31.2">ori</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.31.3">or</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.31.4">Indic</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.31.5">Indo-European</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.31.6">Eurasia</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.31.7">1,157,100,133</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.32">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.32.1">Portuguese</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.32.2">por</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.32.3">pt</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.32.4">Romance</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.32.5">Indo-European</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.32.6">Eurasia</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.32.7">79,277,543,375</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.33">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.33.1">Punjabi</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.33.2">pan</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.33.3">pa</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.33.4">Indic</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.33.5">Indo-European</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.33.6">Eurasia</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.33.7">1,572,109,752</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.34">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.34.1">Sesotho</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.34.2">sot</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.34.3">st</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.34.4">Bantoid</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.34.5">Niger-Congo</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.34.6">Africa</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.34.7">751,034</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.35">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.35.1">Setswana</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.35.2">tsn</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.35.3">tn</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.35.4">Bantoid</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.35.5">Niger-Congo</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.35.6">Africa</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.35.7">1,502,200</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.36">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.36.1">Simplified Chinese</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.36.2">â€”</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.36.3">zhs</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.36.4">Chinese</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.36.5">Sino-Tibetan</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.36.6">Eurasia</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.36.7">261,019,433,892</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.37">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.37.1">Spanish</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.37.2">spa</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.37.3">es</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.37.4">Romance</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.37.5">Indo-European</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.37.6">Eurasia</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.37.7">175,098,365,045</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.38">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.38.1">Swahili</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.38.2">swh</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.38.3">sw</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.38.4">Bantoid</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.38.5">Niger-Congo</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.38.6">Africa</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.38.7">236,482,543</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.39">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.39.1">Tamil</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.39.2">tam</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.39.3">ta</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.39.4">Southern Dravidian</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.39.5">Dravidian</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.39.6">Eurasia</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.39.7">7,989,206,220</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.40">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.40.1">Telugu</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.40.2">tel</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.40.3">te</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.40.4">South-Central Dravidian</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.40.5">Dravidian</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.40.6">Eurasia</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.40.7">2993407,159</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.41">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.41.1">Traditional Chinese</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.41.2">â€”</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.41.3">zht</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.41.4">Chinese</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.41.5">Sino-Tibetan</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.41.6">Eurasia</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.41.7">762,489,150</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.42">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.42.1">Twi</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.42.2">twi</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.42.3">tw</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.42.4">Kwa</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.42.5">Niger-Congo</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.42.6">Africa</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.42.7">1,265,041</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.43">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.43.1">Urdu</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.43.2">urd</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.43.3">ur</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.43.4">Indic</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.43.5">Indo-European</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.43.6">Eurasia</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.43.7">2,781,329,959</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.44">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.44.1">Vietnamese</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.44.2">vie</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.44.3">vi</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.44.4">Viet-Muong</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.44.5">Austro-Asiatic</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.44.6">Eurasia</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.44.7">43,709,279,959</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.45">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.45.1">Wolof</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.45.2">wol</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.45.3">wo</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.45.4">Wolof</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.45.5">Niger-Congo</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.45.6">Africa</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.45.7">3,606,973</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.46">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.46.1">Xitsonga</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.46.2">tso</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.46.3">ts</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.46.4">Bantoid</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.46.5">Niger-Congo</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.46.6">Africa</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.46.7">707,634</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.47">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.47.1">Yoruba</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.47.2">yor</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.47.3">yo</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.47.4">Defoid</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.47.5">Niger-Congo</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.47.6">Africa</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.1.47.7">89,695,835</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.48">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.2.1.48.1">Programming Languages</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.1.48.2">â€”</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.1.48.3">â€”</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.2.1.48.4">â€”</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.2.1.48.5">â€”</td>
<td class="ltx_td ltx_border_bb" id="S3.T1.2.1.48.6"></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T1.2.1.48.7">174,700,245,772</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S3.T1.4.2" style="font-size:90%;">Linguistic makeup of the ROOTS corpus.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Beyond the corpus itself, the process resulted in the development and release of a number of organizational and technical tools, including those illustrated in <a class="ltx_ref" href="#S3.F2" title="Figure 2 â€£ 3.1.3 Data Preprocessing â€£ 3.1 Training Dataset â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>.
The rest of this section will contextualize these efforts by providing a brief summary of the steps taken to compile the corpus. For more detailed documentation of the overall dataset curation process and its outcomes, we refer the reader toÂ <cite class="ltx_cite ltx_citemacro_citet">LaurenÃ§on etÂ al. (<a class="ltx_ref" href="#bib.bib74" title="">2022</a>)</cite>.</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Motivation</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">The disconnect between developers and (in)voluntary users of the technology mentioned in <a class="ltx_ref" href="#S2" title="2 Background â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">2</span></a> is particularly apparent in the curation of the datasets that have supported recent large-scale machine learning projects, where intentional â€œData workâ€ is generally under-valuedÂ <cite class="ltx_cite ltx_citemacro_citep">(Sambasivan etÂ al., <a class="ltx_ref" href="#bib.bib127" title="">2021</a>)</cite>. In the context of LLMs, this tendency is exemplified by a range of heuristics-based filtering approaches that prioritize getting as much â€œhigh-qualityâ€ data for as little cost as possible over engaging with the needsâ€”and rightsâ€”of data subjects, where quality is commonly defined as maximizing performance on downstream tasks while occasionally removing content deemed offensive by the developers.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p2.1">While these approaches do yield terabytes of data with comparatively little human effort, compounding biases of the source material (such as CommonCrawl dumps) with those of the filtering method often leads to negative outcomes for marginalized populations. In one case, the use of a block list to remove â€œpornographicâ€ text was shown to also suppress LGBTQ+ and African American EnglishÂ (AAE) text from a corpusÂ <cite class="ltx_cite ltx_citemacro_citep">(Dodge etÂ al., <a class="ltx_ref" href="#bib.bib40" title="">2021</a>)</cite>. In another, using Reddit outgoing links as an indicator of quality for a seed corpusÂ <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a class="ltx_ref" href="#bib.bib117" title="">2019</a>)</cite> leads to trained models that implicitly prioritize US-centric views in their outputsÂ <cite class="ltx_cite ltx_citemacro_citep">(Johnson etÂ al., <a class="ltx_ref" href="#bib.bib65" title="">2022</a>)</cite>. In yet another project, a filtering approach that relied on a machine learning image-text alignment model was shown to exacerbate its biases in the created multimodal datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(Birhane etÂ al., <a class="ltx_ref" href="#bib.bib19" title="">2021</a>)</cite>. In addition, this <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS0.Px1.p2.1.1">abstractive</span> approach to data curation leads to corpora that are difficult to meaningfully document and govern after the fact, as the provenance and authorship of individual items is usually lost in the process (although works such asÂ <cite class="ltx_cite ltx_citemacro_citet">Gao etÂ al. (<a class="ltx_ref" href="#bib.bib50" title="">2020</a>)</cite> that prioritize compilations of previously documented individual sources over crawled data provide a step towards addressing these issuesÂ <cite class="ltx_cite ltx_citemacro_citep">(Biderman etÂ al., <a class="ltx_ref" href="#bib.bib17" title="">2022</a>)</cite>).</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p3">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p3.1">In the context of the BigScience workshop, and in accordance with its Ethical Charter,<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://bigscience.huggingface.co/blog/bigscience-ethical-charter" title="">bigscience.huggingface.co/blog/bigscience-ethical-charter</a></span></span></span> we aimed to prioritize human involvement, local expertise, and language expertise in our data curation and documentation process, as outlined in the following sections.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Data Governance</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">Large text corpora comprise text about and created by people: the data subjects. Different people and institutions might legally â€œownâ€ that data, making them data rights-holders. As machine learning developers gather and collate that data into ever-larger datasets to support training larger models, it becomes increasingly important to develop new ways of accounting for the interests of all parties involved â€“ developers, data subjects, and rights-holders alike.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.1">The BigScience effort aimed to address these needs through a multidisciplinary lens combining technical, legal, and sociological expertise.
The group focused on two main interrelated goals at two different time scales: the design of a structure for long-term international data governance that prioritizes the agency of the data rights-holders, and concrete recommendations for handling the data used directly in the BigScience project.
Progress on the first goal is presented in the work ofÂ <cite class="ltx_cite ltx_citemacro_citet">Jernite etÂ al. (<a class="ltx_ref" href="#bib.bib64" title="">2022</a>)</cite>, which further motivates the needs and requirements of data governance, and outlines the structure needed for a network of data custodians, rights-holders, and other parties to appropriately govern shared data.
The interactions between these actors are designed to account for the privacy, intellectual property, and user rights of the data and algorithm subjects in a way that aims to prioritize local knowledge and expression of guiding values.
In particular, this approach relies on structured agreements between data providers and data hosts<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://hf.co/spaces/bigscience/data_host_provider_agreement" title="">hf.co/spaces/bigscience/data_host_provider_agreement</a></span></span></span> that specify what the data may be used for.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p3">
<p class="ltx_p" id="S3.SS1.SSS1.p3.1">While we were not able to fully establish an international organization in the comparatively short time between the project start and model training, we worked on integrating lessons from this effort (and conversely adapting it to the practical concerns we were experiencing) in the following main ways: (i)Â we sought explicit permission to use the data from specific providers <span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.p3.1.1">within the context of BigScience</span> whenever possible (such as for the AI2<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://allenai.org/" title="">allenai.org</a></span></span></span>-managed S2ORC corpus ofÂ <cite class="ltx_cite ltx_citemacro_citet">Lo etÂ al. (<a class="ltx_ref" href="#bib.bib83" title="">2020</a>)</cite> or articles from the French newspaper Le Monde<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://www.lemonde.fr/" title="">lemonde.fr</a></span></span></span>); (ii)Â we kept individual sources separate until the final stages of preprocessing to maintain traceability and handle each according to the needs of its specific context; and (iii)Â we adopted a composite release approach for the various data sources that make up the overall corpus to foster reproducibility and follow-up research while respecting these source-dependent needs.
Resources to visualize and access the ROOTS corpus can be found on the Hugging Face Hub organization â€œBigScience Dataâ€.<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://hf.co/bigscience-data" title="">hf.co/bigscience-data</a></span></span></span> The organization hosts several demos (or â€œSpacesâ€) that can be used to gain insights into the full corpus, as well as direct access to the 223 (out of 498) components that we are able to distribute taking into account their licensing status, privacy risks, and agreements with their original custodians. Finally, since we understand that future investigation into the BLOOM models may require full access to the entire corpus, we are also inviting researchers with a relevant research project in mind to join ongoing efforts to analyze the data through a sign-up form.<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://forms.gle/qyYswbEL5kA23Wu99" title="">forms.gle/qyYswbEL5kA23Wu99</a></span></span></span></p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Data Sources</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">Given a strategy for data governance, the next step was to determine the composition of the training corpus. This stage was driven by several goals, which sometimes had inherent tensions.
Some of those tensions included building a language model that was accessible to as many people as possible around the world while only including languages for which we had enough expertise to curate a dataset of comparable scale (and to a lesser extent composition) to previous efforts while improving the standards of documentation and respect for data and algorithm subject rights.</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">Language Choices</h5>
<div class="ltx_para" id="S3.SS1.SSS2.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS2.Px1.p1.1">These considerations led us to an incremental process for choosing which languages were to be included in the corpus. We started with a list of eight of the worldâ€™s largest languages by number of speakers for which we did active outreach in the early stages of the project to invite fluent speakers to join the data efforts. Then, on the recommendation of language communitiesÂ <cite class="ltx_cite ltx_citemacro_citep">(Nekoto etÂ al., <a class="ltx_ref" href="#bib.bib104" title="">2020</a>)</cite> we expanded Swahili in the original selection to the category of Niger-Congo languages, and Hindi and Urdu to Indic languagesÂ <cite class="ltx_cite ltx_citemacro_citep">(Kunchukuttan etÂ al., <a class="ltx_ref" href="#bib.bib71" title="">2020</a>)</cite>. Finally, we proposed that any group of 3 or more participants fluent in an additional language could add it to the supported list if they would commit to selecting sources and guiding processing choices in the language in order to avoid common issues with corpora selected through automatic language identification without specific language expertiseÂ <cite class="ltx_cite ltx_citemacro_citep">(Caswell etÂ al., <a class="ltx_ref" href="#bib.bib28" title="">2022</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">Source Selection</h5>
<div class="ltx_para" id="S3.SS1.SSS2.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.Px2.p1.1">The biggest part of the corpus was curated by workshop participants and research collectives who collectively compiled the â€œBigScience Catalogueâ€: a large list of processed and non-processed sources covering a wide range of languages. This took the form of hackathons that were co-organized by communities such as Machine Learning Tokyo, Masakhane, and LatinX in AIÂ <cite class="ltx_cite ltx_citemacro_citep">(McMillan-Major etÂ al., <a class="ltx_ref" href="#bib.bib89" title="">2022</a>)</cite>. Complementary to those efforts, other working group participants compiled language-specific resources such as the Arabic-focused Masader repositoryÂ <cite class="ltx_cite ltx_citemacro_citep">(Alyafeai etÂ al., <a class="ltx_ref" href="#bib.bib7" title="">2021</a>; Altaher etÂ al., <a class="ltx_ref" href="#bib.bib6" title="">2022</a>)</cite>. A total of 252 sources were identified through this bottom-up approach, with at least 21 sources per language category. Additionally, in order to increase the geographic coverage of some of our Spanish, Chinese, French, and English sources, participants identified locally relevant websites in their language to be added to the corpus via pseudocrawl, a method to obtain those websites from a Common Crawl snapshot.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS2.Px3">
<h5 class="ltx_title ltx_title_paragraph">GitHub Code</h5>
<div class="ltx_para" id="S3.SS1.SSS2.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS2.Px3.p1.1">The catalogue was further complemented with a dataset of programming languages collected from the GitHub data collection on Googleâ€™s BigQuery,<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://cloud.google.com/blog/topics/public-datasets/github-on-bigquery-analyze-all-the-open-source-code" title="">cloud.google.com/blog/topics/public-datasets/github-on-bigquery-analyze-all-the-open-
<br class="ltx_break"/>source-code</a></span></span></span> which was then deduplicated of exact matches. The choice of languages to include mirrored the design choices introduced byÂ <cite class="ltx_cite ltx_citemacro_citet">Li etÂ al. (<a class="ltx_ref" href="#bib.bib78" title="">2022</a>)</cite> to train the AlphaCode model.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS2.Px4">
<h5 class="ltx_title ltx_title_paragraph">OSCAR</h5>
<div class="ltx_para" id="S3.SS1.SSS2.Px4.p1">
<p class="ltx_p" id="S3.SS1.SSS2.Px4.p1.1">Both in an effort not to diverge from the standard research practice of using the Web as a source of pretraining data <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a class="ltx_ref" href="#bib.bib116" title="">2018</a>; Raffel etÂ al., <a class="ltx_ref" href="#bib.bib119" title="">2020</a>)</cite>, and also to satisfy the data volume needs of our compute budget given the size of BLOOM, we further sourced data from OSCAR version 21.09, corresponding to the FebruaryÂ 2021 snapshot of the Common CrawlÂ <cite class="ltx_cite ltx_citemacro_citep">(Ortiz SuÃ¡rez etÂ al., <a class="ltx_ref" href="#bib.bib108" title="">2019</a>; Abadji etÂ al., <a class="ltx_ref" href="#bib.bib1" title="">2021</a>)</cite>, which ended up constituting 38% of the corpus.
</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Data Preprocessing</h4>
<div class="ltx_para" id="S3.SS1.SSS3.p1">
<p class="ltx_p" id="S3.SS1.SSS3.p1.1">After the sources had been identified, data processing involved several steps to handle multiple aspects of data curation.
An overarching view of and processing pipeline to build ROOTS can be seen in <a class="ltx_ref" href="#S3.F2" title="Figure 2 â€£ 3.1.3 Data Preprocessing â€£ 3.1 Training Dataset â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>. All tools developed in the process are available on GitHub.<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://github.com/bigscience-workshop/data-preparation" title="">github.com/bigscience-workshop/data-preparation</a></span></span></span></p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="455" id="S3.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">Creation Pipeline of the ROOTS Corpus. The purple-colored sourcing stage of the pipeline and the yellow-colored processing stage are described respectively in <a class="ltx_ref" href="#S3.SS1.SSS2" title="3.1.2 Data Sources â€£ 3.1 Training Dataset â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">3.1.2</span></a> and <a class="ltx_ref" href="#S3.SS1.SSS3" title="3.1.3 Data Preprocessing â€£ 3.1 Training Dataset â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">3.1.3</span></a>.</span></figcaption>
</figure>
<section class="ltx_paragraph" id="S3.SS1.SSS3.Px1">
<h5 class="ltx_title ltx_title_paragraph">Obtaining the Source Data</h5>
<div class="ltx_para" id="S3.SS1.SSS3.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS3.Px1.p1.1">The first step involved obtaining the data for all of the text data sources identified in <a class="ltx_ref" href="#S3.SS1.SSS2" title="3.1.2 Data Sources â€£ 3.1 Training Dataset â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">3.1.2</span></a>, which consisted of a combination of downloading and extracting the text field from a variety of NLP datasets in various formats (including e.g.Â question answering, summarization, or dialogue datasets), scraping and processing large amounts of PDF files from archivesÂ (e.g.Â the French repository of scientific articles<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://hal.archives-ouvertes.fr/" title="">hal.archives-ouvertes.fr</a></span></span></span>), and extracting and preprocessing text from 192 website entries from the catalogue and another geographically diverse set of 456 websites selected by data working group members. The latter required the development of new tools to extract text from the HTML in the Common Crawl WARC files, which we made available on the main data preparation repository.<span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span> <a class="ltx_ref ltx_href ltx_font_typewriter" href="https://github.com/bigscience-workshop/data-preparation/tree/main/sourcing/cc_pseudo_crawl" title="">github.com/bigscience-workshop/data-preparation/tree/main/sourcing/cc_pseudo_crawl</a></span></span></span> We were able to find and extract usable text data from all URLs present in 539 of the websites.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS3.Px2">
<h5 class="ltx_title ltx_title_paragraph">â€œQualityâ€ filtering: Text Produced by Humans for Humans</h5>
<div class="ltx_para" id="S3.SS1.SSS3.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS3.Px2.p1.1">After obtaining the text, we found that most of the sources contained some amount of text that was not natural language, for example preprocessing errors, SEO pages, or spam (including pornographic spam). In order to filter non-natural language, we defined a set of quality indicators, where high-quality text is defined as â€œwritten by humans for humansâ€, without distinction of content (as we wanted content selection to exclusively be the domain of the more accountable human source selection) or <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS3.Px2.p1.1.1">a priori</span> judgments of grammaticality. The full list of indicators are described in <cite class="ltx_cite ltx_citemacro_citep">(LaurenÃ§on etÂ al., <a class="ltx_ref" href="#bib.bib74" title="">2022</a>)</cite>. Importantly, the indicators were adapted to the needs of each of the sources in two main ways. First, their parameters such as the thresholds and supporting term lists were selected individually for each language by fluent speakers. Second, we manually went through each individual source to identify which indicators were most likely to identify non-natural language. Both processes were supported by tools to visualize their impact.<span class="ltx_note ltx_role_footnote" id="footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://hf.co/spaces/huggingface/text-data-filtering" title="">hf.co/spaces/huggingface/text-data-filtering</a></span></span></span><sup class="ltx_sup" id="S3.SS1.SSS3.Px2.p1.1.2">,</sup><span class="ltx_note ltx_role_footnote" id="footnote16"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://hf.co/spaces/bigscience-data/process-pipeline-visualizer" title="">hf.co/spaces/bigscience-data/process-pipeline-visualizer</a></span></span></span></p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS3.Px3">
<h5 class="ltx_title ltx_title_paragraph">Deduplication and Privacy Redaction</h5>
<div class="ltx_para" id="S3.SS1.SSS3.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS3.Px3.p1.1">Finally, we removed near-duplicate documents with two deduplication steps and redacted Personal Identifiable Information (such as social security numbers) that we could identify from the OSCAR version of the corpusâ€”as it was deemed to be the source that presented the highest privacy risks, prompting us to apply regex-based redaction even in cases where the expressions had some false positives. </p>
</div>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2 ltx_img_landscape" height="291" id="S3.F3.g1" src="x3.png" width="581"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2 ltx_img_portrait" height="290" id="S3.F3.g2" src="x4.png" width="174"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.4.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.5.2" style="font-size:90%;">
Graphical overview of the ROOTS corpus.
<span class="ltx_text ltx_font_bold" id="S3.F3.5.2.1">Left:</span> A treemap plot of the language families of all 46 natural languages where surface is proportional to the number of bytes. Indo-European and Sino-Tibetan families overwhelm the plot with a combined total of 1321.89 GB. The thin orange surface represents 18GB of Indonesian data and the green rectangle 0.4GB constituting the Niger-Congo language family subset.
<span class="ltx_text ltx_font_bold" id="S3.F3.5.2.2">Right:</span> A waffle plot of the distribution of the 13 programming languages by size, where one square represents approximately 200MB.
</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.4 </span>Prompted Datasets</h4>
<figure class="ltx_figure" id="S3.F4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;">Language distribution of the prompted dataset xP3 closely follows ROOTS.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="144" id="S3.F4.g1" src="x5.png" width="830"/></div>
<div class="ltx_flex_cell">
<p class="ltx_p ltx_align_center" id="S3.F4.4">.

</p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;">Language distribution of the prompted dataset xP3 closely follows ROOTS.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.SSS4.p1">
<p class="ltx_p" id="S3.SS1.SSS4.p1.1">Multitask prompted finetuning (also referred to as instruction tuning) involves finetuning a pretrained language model on a training mixture composed of a large set of different tasks specified through natural language prompts.
T0Â <cite class="ltx_cite ltx_citemacro_citep">(Sanh etÂ al., <a class="ltx_ref" href="#bib.bib128" title="">2022</a>)</cite> (developed as part of BigScience) demonstrated that language models finetuned on a multitask mixture of prompted datasets have strong zero-shot task generalization abilities.
Moreover, T0 was shown to outperform language models that are an order of magnitude larger but did not undergo such finetuning. Motivated by these results, we explored using existing natural language datasets to carry out multitask prompted finetuning.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS4.p2">
<p class="ltx_p" id="S3.SS1.SSS4.p2.1">T0 was trained on a subset of the Public Pool of Prompts (P3), a collection of prompts for various existing and open-source English natural language datasets. This collection of prompts was created through a series of hackathons involving BigScience collaborators and where hackathon participants wrote a total of of 2000+ prompts for 170+ datasets.
Datasets in P3 cover a variety of natural language task including sentiment analysis, question answering, and natural language inference and exclude harmful content or non-natural language such as programming languages. PromptSourceÂ <cite class="ltx_cite ltx_citemacro_citep">(Bach etÂ al., <a class="ltx_ref" href="#bib.bib8" title="">2022</a>)</cite>,<span class="ltx_note ltx_role_footnote" id="footnote17"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup><span class="ltx_tag ltx_tag_note">17</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://github.com/bigscience-workshop/promptsource" title="">github.com/bigscience-workshop/promptsource</a></span></span></span> an open-source toolkit (also developed as part of BigScience) facilitated creating, sharing and using natural language prompts. Full details of the collection process are given inÂ <cite class="ltx_cite ltx_citemacro_citep">(Sanh etÂ al., <a class="ltx_ref" href="#bib.bib128" title="">2022</a>; Bach etÂ al., <a class="ltx_ref" href="#bib.bib8" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS4.p3">
<p class="ltx_p" id="S3.SS1.SSS4.p3.1">After pretraining BLOOM, we applied the same massively multitask finetuning recipe to equip BLOOM with multilingual zero-shot task generalization abilities. We refer to the resulting models as BLOOMZ. To train BLOOMZ, we extended P3 to include new datasets in languages other than English and new tasks, such as translation. This resulted in xP3, a collection of prompts for 83 datasets covering 46 languages and 16 tasks. As highlighted in <a class="ltx_ref" href="#S3.F4" title="Figure 4 â€£ 3.1.4 Prompted Datasets â€£ 3.1 Training Dataset â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">4</span></a>, xP3 mirrors the language distribution of ROOTS. Tasks in xP3 are both cross-lingual (e.g. translation) and monolingual (e.g. summarization, question answering). We used PromptSource to collect these prompts, adding additional metadata to the prompts, such as input and target languages. To study the importance of multilingual prompts, we also machine-translated English prompts in xP3 to the respective dataset languages to produce a collection called xP3mt. Further details on the prompt collection for xP3 and xP3mt are given inÂ <cite class="ltx_cite ltx_citemacro_cite">Muennighoff etÂ al. (<a class="ltx_ref" href="#bib.bib100" title="">2022b</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Model Architecture</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">This section discusses our design methodology and the architecture of the BLOOM model.
In-depth studies and experiments can be found in <cite class="ltx_cite ltx_citemacro_citet">Le Scao etÂ al. (<a class="ltx_ref" href="#bib.bib75" title="">2022</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Wang etÂ al. (<a class="ltx_ref" href="#bib.bib156" title="">2022a</a>)</cite>. We first review our design methodology, then motivate our choice of training a causal decoder-only model. Finally, we justify the ways that our model architecture deviates from standard practice.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Design Methodology</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">The design space of possible architectures is immense, making exhaustive exploration impossible.
One option would be to exactly replicate the architecture of an existing large language model.
On the other hand, a great deal of work on improving existing architectures has seen relatively little adoption <cite class="ltx_cite ltx_citemacro_citep">(Narang etÂ al., <a class="ltx_ref" href="#bib.bib102" title="">2021</a>)</cite>; adopting some of these recommended practices could yield a significantly better model.
We take a middle ground and focus on model families that have been shown to scale well, and that have reasonable support in publicly available tools and codebases. We ablate components and hyperparameters of the models, seeking to make the best use of our final compute budget.</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Experimental Design for Ablations</h5>
<div class="ltx_para" id="S3.SS2.SSS1.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.Px1.p1.1">One of the main draws of LLMs has been their ability to perform tasks in a â€œzero/few-shotâ€ way: large enough models can perform novel tasks simply from in-context instructions and examples <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a class="ltx_ref" href="#bib.bib117" title="">2019</a>)</cite>, without dedicated training on supervised samples. Accordingly, and because finetuning a 100B+ model is unwieldy, we focused our evaluation of architectural decisions on zero-shot generalization, and do not consider transfer learning. Specifically, we measured zero-shot performance on diverse aggregates of tasks: 29 tasks from the EleutherAI Language Model Evaluation Harness (EAI-Eval, <cite class="ltx_cite ltx_citemacro_cite">Gao etÂ al. (<a class="ltx_ref" href="#bib.bib51" title="">2021</a>)</cite>), and 9 tasks from the evaluation set of T0 (T0-Eval, <cite class="ltx_cite ltx_citemacro_cite">Sanh etÂ al. (<a class="ltx_ref" href="#bib.bib128" title="">2022</a>)</cite>). There is significant overlap between the two: only one task from T0-Eval (StoryCloze) is not in EAI-Eval, although all prompts between the two are different. See <cite class="ltx_cite ltx_citemacro_citet">Le Scao etÂ al. (<a class="ltx_ref" href="#bib.bib75" title="">2022</a>)</cite> for a detailed list of tasks and baselines.
We also note that our tasks aggregates share 17 of the 31 tasks of the evaluation of GPT-3 <cite class="ltx_cite ltx_citemacro_citep">(Brown etÂ al., <a class="ltx_ref" href="#bib.bib27" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.Px1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.Px1.p2.1">We conducted our ablation experiments using smaller models. We used the 6.7B parameter scale for the pretraining objective ablationsÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="#bib.bib156" title="">2022a</a>)</cite> and the 1.3B scale for the rest including position embeddings, activations, and layer normalizationÂ <cite class="ltx_cite ltx_citemacro_citep">(Le Scao etÂ al., <a class="ltx_ref" href="#bib.bib75" title="">2022</a>)</cite>.
Recently, <cite class="ltx_cite ltx_citemacro_citet">Dettmers etÂ al. (<a class="ltx_ref" href="#bib.bib38" title="">2022</a>)</cite> identified a phase transition for models larger than 6.7B, in which the emergence of â€œoutliers featuresâ€ is observed. This questions whether results obtained at the 1.3B scale should be assumed to extrapolate to our final model size.
</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Out-of-scope Architectures</h5>
<div class="ltx_para" id="S3.SS2.SSS1.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS1.Px2.p1.1">We did not consider mixture-of-experts (MoE) <cite class="ltx_cite ltx_citemacro_citep">(Shazeer etÂ al., <a class="ltx_ref" href="#bib.bib134" title="">2017</a>)</cite>, due to a lack of widely used GPU-based codebases suitable for training them at scale. Similarly, we also did not consider state-space models <cite class="ltx_cite ltx_citemacro_citep">(Gu etÂ al., <a class="ltx_ref" href="#bib.bib57" title="">2020</a>)</cite>. At the time of the design of BLOOM, they consistently underperformed in natural language tasks <cite class="ltx_cite ltx_citemacro_citep">(Gu etÂ al., <a class="ltx_ref" href="#bib.bib58" title="">2021</a>)</cite>. Both of these approaches are promising, and have now demonstrated competitive resultsâ€“at large scales for MoEÂ <cite class="ltx_cite ltx_citemacro_citep">(Fedus etÂ al., <a class="ltx_ref" href="#bib.bib43" title="">2022</a>; Srivastava etÂ al., <a class="ltx_ref" href="#bib.bib140" title="">2022</a>)</cite>, and at smaller scale for state-space models with H3Â <cite class="ltx_cite ltx_citemacro_citep">(Fu etÂ al., <a class="ltx_ref" href="#bib.bib48" title="">2023</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Architecture and Pretraining Objective</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">Although most modern language models are based on the Transformer architecture, there are significant deviations between architectural implementations. Notably, while the original Transformer is based on an encoder-decoder architecture, many popular models have opted for encoder-only (e.g. BERT, <cite class="ltx_cite ltx_citemacro_citep">(Devlin etÂ al., <a class="ltx_ref" href="#bib.bib39" title="">2019</a>)</cite>) or decoder-only (e.g. GPT, <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a class="ltx_ref" href="#bib.bib116" title="">2018</a>)</cite>) approaches. Currently, all state-of-the-art language models over 100 billion parameters are causal decoder-only models <cite class="ltx_cite ltx_citemacro_citep">(Brown etÂ al., <a class="ltx_ref" href="#bib.bib27" title="">2020</a>; Rae etÂ al., <a class="ltx_ref" href="#bib.bib118" title="">2021</a>; Chowdhery etÂ al., <a class="ltx_ref" href="#bib.bib31" title="">2022</a>)</cite>.
This is in opposition to the findings of <cite class="ltx_cite ltx_citemacro_citet">Raffel etÂ al. (<a class="ltx_ref" href="#bib.bib119" title="">2020</a>)</cite>, in which encoder-decoder models significantly outperform decoder-only models for transfer learning.
</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1">Prior to our work, the literature was lacking a systematic evaluation of the zero-shot generalization capabilities of different architectures and pretraining objectives.
We explored this question inÂ <cite class="ltx_cite ltx_citemacro_citet">Wang etÂ al. (<a class="ltx_ref" href="#bib.bib156" title="">2022a</a>)</cite> where we evaluated encoder-decoder and
decoder-only architectures and their interactions with causal, prefix, and masked language modeling pretraining objectives.
Our results show that immediately after pretraining, causal decoder-only models performed best â€“ validating the choice of state-of-the-art LLMs. Furthermore, they can be more efficiently adapted after pretraining to a non-causal architecture and objectiveâ€“an approach which has been further explored and confirmed byÂ <cite class="ltx_cite ltx_citemacro_citet">Tay etÂ al. (<a class="ltx_ref" href="#bib.bib145" title="">2022</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Modeling Details</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">Beyond choosing an architecture and pretraining objective, a number of changes to the original Transformer architecture have been proposed. For example, alternative positional embedding schemesÂ <cite class="ltx_cite ltx_citemacro_citep">(Su etÂ al., <a class="ltx_ref" href="#bib.bib142" title="">2021</a>; Press etÂ al., <a class="ltx_ref" href="#bib.bib115" title="">2021</a>)</cite> or novel activation functionsÂ <cite class="ltx_cite ltx_citemacro_citep">(Shazeer, <a class="ltx_ref" href="#bib.bib133" title="">2020</a>)</cite>. We thus performed a series of experiments to evaluate the benefit of each of these modifications for a causal decoder-only model inÂ <cite class="ltx_cite ltx_citemacro_citet">Le Scao etÂ al. (<a class="ltx_ref" href="#bib.bib75" title="">2022</a>)</cite>. We adopted two architectural deviations in BLOOM:</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS3.Px1">
<h5 class="ltx_title ltx_title_paragraph">ALiBi Positional Embeddings</h5>
<div class="ltx_para" id="S3.SS2.SSS3.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS3.Px1.p1.1">Instead of adding positional information to the embedding layer, ALiBi directly attenuates the attention scores based on how far away the keys and queries are <cite class="ltx_cite ltx_citemacro_citep">(Press etÂ al., <a class="ltx_ref" href="#bib.bib115" title="">2021</a>)</cite>. Although ALiBi was initially motivated by its ability to extrapolate to longer sequences, we found it also led to smoother training and better downstream performance even at the original sequence length â€“ outperforming both learned <cite class="ltx_cite ltx_citemacro_citep">(Vaswani etÂ al., <a class="ltx_ref" href="#bib.bib148" title="">2017</a>)</cite> and rotary <cite class="ltx_cite ltx_citemacro_citep">(Su etÂ al., <a class="ltx_ref" href="#bib.bib142" title="">2021</a>)</cite> embeddings.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS3.Px2">
<h5 class="ltx_title ltx_title_paragraph">Embedding LayerNorm</h5>
<div class="ltx_para" id="S3.SS2.SSS3.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS3.Px2.p1.1">In preliminary experiments training a 104B parameters model, we experimented with an additional layer normalization immediately after the embedding layer â€“ as recommended by the <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS3.Px2.p1.1.1">bitsandbytes<span class="ltx_note ltx_role_footnote" id="footnote18"><sup class="ltx_note_mark">18</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">18</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif" id="footnote18.1.1.1">18</span></span><a class="ltx_ref ltx_href" href="https://github.com/TimDettmers/bitsandbytes" title="">github.com/TimDettmers/bitsandbytes</a></span></span></span></span> libraryÂ <cite class="ltx_cite ltx_citemacro_citep">(Dettmers etÂ al., <a class="ltx_ref" href="#bib.bib38" title="">2022</a>)</cite> with its <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS3.Px2.p1.1.2">StableEmbedding</span> layer. We found this significantly improved training stability. Even though we also found it penalizes zero-shot generalization inÂ <cite class="ltx_cite ltx_citemacro_citet">Le Scao etÂ al. (<a class="ltx_ref" href="#bib.bib75" title="">2022</a>)</cite>, we train BLOOM with an additional layer normalization after the first embedding layer to avoid training instabilities.
Note the preliminary 104B experiments were conducted in <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS3.Px2.p1.1.3">float16</span>, while the final training was in <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS3.Px2.p1.1.4">bfloat16</span>. Since then, <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS3.Px2.p1.1.5">float16</span> has been attributed as being responsible for many of the observed instabilities in training LLMs <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="#bib.bib169" title="">2022</a>; Zeng etÂ al., <a class="ltx_ref" href="#bib.bib167" title="">2022</a>)</cite>. It is possible that <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS3.Px2.p1.1.6">bfloat16</span> alleviates the need for the embedding LayerNorm.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.Px2.p2">
<p class="ltx_p" id="S3.SS2.SSS3.Px2.p2.1">We represent the full architecture of BLOOM in figure <a class="ltx_ref" href="#S3.F5" title="Figure 5 â€£ Embedding LayerNorm â€£ 3.2.3 Modeling Details â€£ 3.2 Model Architecture â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">5</span></a> for reference.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="362" id="S3.F5.g1" src="x6.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.10.5.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S3.F5.8.4" style="font-size:90%;">The BLOOM architecture. The <math alttext="k_{head}" class="ltx_Math" display="inline" id="S3.F5.5.1.m1.1"><semantics id="S3.F5.5.1.m1.1b"><msub id="S3.F5.5.1.m1.1.1" xref="S3.F5.5.1.m1.1.1.cmml"><mi id="S3.F5.5.1.m1.1.1.2" xref="S3.F5.5.1.m1.1.1.2.cmml">k</mi><mrow id="S3.F5.5.1.m1.1.1.3" xref="S3.F5.5.1.m1.1.1.3.cmml"><mi id="S3.F5.5.1.m1.1.1.3.2" xref="S3.F5.5.1.m1.1.1.3.2.cmml">h</mi><mo id="S3.F5.5.1.m1.1.1.3.1" xref="S3.F5.5.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.F5.5.1.m1.1.1.3.3" xref="S3.F5.5.1.m1.1.1.3.3.cmml">e</mi><mo id="S3.F5.5.1.m1.1.1.3.1b" xref="S3.F5.5.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.F5.5.1.m1.1.1.3.4" xref="S3.F5.5.1.m1.1.1.3.4.cmml">a</mi><mo id="S3.F5.5.1.m1.1.1.3.1c" xref="S3.F5.5.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.F5.5.1.m1.1.1.3.5" xref="S3.F5.5.1.m1.1.1.3.5.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.F5.5.1.m1.1c"><apply id="S3.F5.5.1.m1.1.1.cmml" xref="S3.F5.5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F5.5.1.m1.1.1.1.cmml" xref="S3.F5.5.1.m1.1.1">subscript</csymbol><ci id="S3.F5.5.1.m1.1.1.2.cmml" xref="S3.F5.5.1.m1.1.1.2">ğ‘˜</ci><apply id="S3.F5.5.1.m1.1.1.3.cmml" xref="S3.F5.5.1.m1.1.1.3"><times id="S3.F5.5.1.m1.1.1.3.1.cmml" xref="S3.F5.5.1.m1.1.1.3.1"></times><ci id="S3.F5.5.1.m1.1.1.3.2.cmml" xref="S3.F5.5.1.m1.1.1.3.2">â„</ci><ci id="S3.F5.5.1.m1.1.1.3.3.cmml" xref="S3.F5.5.1.m1.1.1.3.3">ğ‘’</ci><ci id="S3.F5.5.1.m1.1.1.3.4.cmml" xref="S3.F5.5.1.m1.1.1.3.4">ğ‘</ci><ci id="S3.F5.5.1.m1.1.1.3.5.cmml" xref="S3.F5.5.1.m1.1.1.3.5">ğ‘‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.5.1.m1.1d">k_{head}</annotation><annotation encoding="application/x-llamapun" id="S3.F5.5.1.m1.1e">italic_k start_POSTSUBSCRIPT italic_h italic_e italic_a italic_d end_POSTSUBSCRIPT</annotation></semantics></math> slope parameters for ALIBI are taken as <math alttext="2^{\frac{-8i}{n}}" class="ltx_Math" display="inline" id="S3.F5.6.2.m2.1"><semantics id="S3.F5.6.2.m2.1b"><msup id="S3.F5.6.2.m2.1.1" xref="S3.F5.6.2.m2.1.1.cmml"><mn id="S3.F5.6.2.m2.1.1.2" xref="S3.F5.6.2.m2.1.1.2.cmml">2</mn><mfrac id="S3.F5.6.2.m2.1.1.3" xref="S3.F5.6.2.m2.1.1.3.cmml"><mrow id="S3.F5.6.2.m2.1.1.3.2" xref="S3.F5.6.2.m2.1.1.3.2.cmml"><mo id="S3.F5.6.2.m2.1.1.3.2b" xref="S3.F5.6.2.m2.1.1.3.2.cmml">âˆ’</mo><mrow id="S3.F5.6.2.m2.1.1.3.2.2" xref="S3.F5.6.2.m2.1.1.3.2.2.cmml"><mn id="S3.F5.6.2.m2.1.1.3.2.2.2" xref="S3.F5.6.2.m2.1.1.3.2.2.2.cmml">8</mn><mo id="S3.F5.6.2.m2.1.1.3.2.2.1" xref="S3.F5.6.2.m2.1.1.3.2.2.1.cmml">â¢</mo><mi id="S3.F5.6.2.m2.1.1.3.2.2.3" xref="S3.F5.6.2.m2.1.1.3.2.2.3.cmml">i</mi></mrow></mrow><mi id="S3.F5.6.2.m2.1.1.3.3" xref="S3.F5.6.2.m2.1.1.3.3.cmml">n</mi></mfrac></msup><annotation-xml encoding="MathML-Content" id="S3.F5.6.2.m2.1c"><apply id="S3.F5.6.2.m2.1.1.cmml" xref="S3.F5.6.2.m2.1.1"><csymbol cd="ambiguous" id="S3.F5.6.2.m2.1.1.1.cmml" xref="S3.F5.6.2.m2.1.1">superscript</csymbol><cn id="S3.F5.6.2.m2.1.1.2.cmml" type="integer" xref="S3.F5.6.2.m2.1.1.2">2</cn><apply id="S3.F5.6.2.m2.1.1.3.cmml" xref="S3.F5.6.2.m2.1.1.3"><divide id="S3.F5.6.2.m2.1.1.3.1.cmml" xref="S3.F5.6.2.m2.1.1.3"></divide><apply id="S3.F5.6.2.m2.1.1.3.2.cmml" xref="S3.F5.6.2.m2.1.1.3.2"><minus id="S3.F5.6.2.m2.1.1.3.2.1.cmml" xref="S3.F5.6.2.m2.1.1.3.2"></minus><apply id="S3.F5.6.2.m2.1.1.3.2.2.cmml" xref="S3.F5.6.2.m2.1.1.3.2.2"><times id="S3.F5.6.2.m2.1.1.3.2.2.1.cmml" xref="S3.F5.6.2.m2.1.1.3.2.2.1"></times><cn id="S3.F5.6.2.m2.1.1.3.2.2.2.cmml" type="integer" xref="S3.F5.6.2.m2.1.1.3.2.2.2">8</cn><ci id="S3.F5.6.2.m2.1.1.3.2.2.3.cmml" xref="S3.F5.6.2.m2.1.1.3.2.2.3">ğ‘–</ci></apply></apply><ci id="S3.F5.6.2.m2.1.1.3.3.cmml" xref="S3.F5.6.2.m2.1.1.3.3">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.6.2.m2.1d">2^{\frac{-8i}{n}}</annotation><annotation encoding="application/x-llamapun" id="S3.F5.6.2.m2.1e">2 start_POSTSUPERSCRIPT divide start_ARG - 8 italic_i end_ARG start_ARG italic_n end_ARG end_POSTSUPERSCRIPT</annotation></semantics></math> with <math alttext="n" class="ltx_Math" display="inline" id="S3.F5.7.3.m3.1"><semantics id="S3.F5.7.3.m3.1b"><mi id="S3.F5.7.3.m3.1.1" xref="S3.F5.7.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.F5.7.3.m3.1c"><ci id="S3.F5.7.3.m3.1.1.cmml" xref="S3.F5.7.3.m3.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.7.3.m3.1d">n</annotation><annotation encoding="application/x-llamapun" id="S3.F5.7.3.m3.1e">italic_n</annotation></semantics></math> the number of heads and <math alttext="i\in{1,2,...,n}" class="ltx_Math" display="inline" id="S3.F5.8.4.m4.4"><semantics id="S3.F5.8.4.m4.4b"><mrow id="S3.F5.8.4.m4.4.5" xref="S3.F5.8.4.m4.4.5.cmml"><mi id="S3.F5.8.4.m4.4.5.2" xref="S3.F5.8.4.m4.4.5.2.cmml">i</mi><mo id="S3.F5.8.4.m4.4.5.1" xref="S3.F5.8.4.m4.4.5.1.cmml">âˆˆ</mo><mrow id="S3.F5.8.4.m4.4.5.3.2" xref="S3.F5.8.4.m4.4.5.3.1.cmml"><mn id="S3.F5.8.4.m4.1.1" xref="S3.F5.8.4.m4.1.1.cmml">1</mn><mo id="S3.F5.8.4.m4.4.5.3.2.1" xref="S3.F5.8.4.m4.4.5.3.1.cmml">,</mo><mn id="S3.F5.8.4.m4.2.2" xref="S3.F5.8.4.m4.2.2.cmml">2</mn><mo id="S3.F5.8.4.m4.4.5.3.2.2" xref="S3.F5.8.4.m4.4.5.3.1.cmml">,</mo><mi id="S3.F5.8.4.m4.3.3" mathvariant="normal" xref="S3.F5.8.4.m4.3.3.cmml">â€¦</mi><mo id="S3.F5.8.4.m4.4.5.3.2.3" xref="S3.F5.8.4.m4.4.5.3.1.cmml">,</mo><mi id="S3.F5.8.4.m4.4.4" xref="S3.F5.8.4.m4.4.4.cmml">n</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.F5.8.4.m4.4c"><apply id="S3.F5.8.4.m4.4.5.cmml" xref="S3.F5.8.4.m4.4.5"><in id="S3.F5.8.4.m4.4.5.1.cmml" xref="S3.F5.8.4.m4.4.5.1"></in><ci id="S3.F5.8.4.m4.4.5.2.cmml" xref="S3.F5.8.4.m4.4.5.2">ğ‘–</ci><list id="S3.F5.8.4.m4.4.5.3.1.cmml" xref="S3.F5.8.4.m4.4.5.3.2"><cn id="S3.F5.8.4.m4.1.1.cmml" type="integer" xref="S3.F5.8.4.m4.1.1">1</cn><cn id="S3.F5.8.4.m4.2.2.cmml" type="integer" xref="S3.F5.8.4.m4.2.2">2</cn><ci id="S3.F5.8.4.m4.3.3.cmml" xref="S3.F5.8.4.m4.3.3">â€¦</ci><ci id="S3.F5.8.4.m4.4.4.cmml" xref="S3.F5.8.4.m4.4.4">ğ‘›</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.8.4.m4.4d">i\in{1,2,...,n}</annotation><annotation encoding="application/x-llamapun" id="S3.F5.8.4.m4.4e">italic_i âˆˆ 1 , 2 , â€¦ , italic_n</annotation></semantics></math>.</span></figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Tokenization</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The design decisions when training a tokenizer are often neglected in favour of â€œdefaultâ€ settings <cite class="ltx_cite ltx_citemacro_citep">(Mielke etÂ al., <a class="ltx_ref" href="#bib.bib91" title="">2021</a>)</cite>. For instance, OPTÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="#bib.bib169" title="">2022</a>)</cite> and GPT-3Â <cite class="ltx_cite ltx_citemacro_citep">(Brown etÂ al., <a class="ltx_ref" href="#bib.bib27" title="">2020</a>)</cite> both use GPT-2â€™s tokenizer, trained for English. This can be justified by the fact that evaluating the impact of a particular choice on the downstream performance of the model is constrained by the large computational costs of training. However, the diverse nature of BLOOMâ€™s training data requires careful design choices to ensure that the tokenizer encodes sentences in a lossless manner.</p>
</div>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Validation</h5>
<div class="ltx_para" id="S3.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.1">We use the fertilityÂ <cite class="ltx_cite ltx_citemacro_citep">(Ãcs, <a class="ltx_ref" href="#bib.bib2" title="">2019</a>)</cite> of our tokenizer compared to existing monolingual tokenizers as a metric for sanity checks. Fertility is defined as the number of subwords created per word or per dataset by the tokenizer, which we measured using subsets of Universal Dependencies 2.9Â <cite class="ltx_cite ltx_citemacro_citep">(Nivre etÂ al., <a class="ltx_ref" href="#bib.bib107" title="">2017</a>)</cite> and OSCARÂ <cite class="ltx_cite ltx_citemacro_citep">(Ortiz SuÃ¡rez etÂ al., <a class="ltx_ref" href="#bib.bib108" title="">2019</a>)</cite> in the languages of interest. A very high fertility on a language compared to a monolingual tokenizer may indicate a degradation on the downstream multilingual performance of the modelÂ <cite class="ltx_cite ltx_citemacro_citep">(Rust etÂ al., <a class="ltx_ref" href="#bib.bib124" title="">2021</a>)</cite>. Our goal was to not degrade the fertility on each language by more than 10 percentage points when comparing our multilingual tokenizer with monolingual tokenizers in corresponding languages. For all experiments, the HuggingÂ FaceÂ Tokenizers libraryÂ <cite class="ltx_cite ltx_citemacro_citep">(Moi etÂ al., <a class="ltx_ref" href="#bib.bib96" title="">2019</a>)</cite> was used to design and train the tested tokenizers.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T2.2">
<tr class="ltx_tr" id="S3.T2.2.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.2.1.1"><span class="ltx_text ltx_font_bold" id="S3.T2.2.1.1.1">Tokenizer</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.2.1.2"><span class="ltx_text ltx_font_bold" id="S3.T2.2.1.2.1">fr</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.2.1.3"><span class="ltx_text ltx_font_bold" id="S3.T2.2.1.3.1">en</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.2.1.4"><span class="ltx_text ltx_font_bold" id="S3.T2.2.1.4.1">es</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.2.1.5"><span class="ltx_text ltx_font_bold" id="S3.T2.2.1.5.1">zh</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.2.1.6"><span class="ltx_text ltx_font_bold" id="S3.T2.2.1.6.1">hi</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.2.1.7"><span class="ltx_text ltx_font_bold" id="S3.T2.2.1.7.1">ar</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.2.2.1">Monolingual</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.2.2.2">1.30</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.2.2.3">1.15</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.2.2.4">1.12</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.2.2.5">1.50</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.2.2.6">1.07</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.2.2.7">1.16</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.3">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.2.3.1">BLOOM</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.2.3.2">1.17 <span class="ltx_text" id="S3.T2.2.3.2.1" style="font-size:70%;">(-11%)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.2.3.3">1.15 <span class="ltx_text" id="S3.T2.2.3.3.1" style="font-size:70%;">(+0%)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.2.3.4">1.16 <span class="ltx_text" id="S3.T2.2.3.4.1" style="font-size:70%;">(+3%)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.2.3.5">1.58 <span class="ltx_text" id="S3.T2.2.3.5.1" style="font-size:70%;">(+5%)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.2.3.6">1.18 <span class="ltx_text" id="S3.T2.2.3.6.1" style="font-size:70%;">(+9%)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.2.3.7">1.34 <span class="ltx_text" id="S3.T2.2.3.7.1" style="font-size:70%;">(+13%)</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T2.6.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S3.T2.7.2" style="font-size:90%;">Fertilities obtained on Universal Dependencies treebanks on languages with existing monolingual tokenizers. The monolingual tokenizers we used were the ones from CamemBERTÂ <cite class="ltx_cite ltx_citemacro_citep">(Martin etÂ al., <a class="ltx_ref" href="#bib.bib88" title="">2020</a>)</cite>, GPT-2Â <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a class="ltx_ref" href="#bib.bib117" title="">2019</a>)</cite>, <span class="ltx_text ltx_font_typewriter" id="S3.T2.7.2.1">DeepESP/gpt2-spanish</span>, <span class="ltx_text ltx_font_typewriter" id="S3.T2.7.2.2">bert-base-chinese</span>, <span class="ltx_text ltx_font_typewriter" id="S3.T2.7.2.3">monsoon-nlp/hindi-bert</span> and Arabic BERTÂ <cite class="ltx_cite ltx_citemacro_citep">(Safaya etÂ al., <a class="ltx_ref" href="#bib.bib125" title="">2020</a>)</cite>, all available on the HuggingFace Hub.</span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Tokenizer Training Data</h5>
<div class="ltx_para" id="S3.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px2.p1.1">We initially used a non-deduplicated subset of ROOTS. However, a qualitative study on the vocabulary of the tokenizer revealed issues in its training data. For instance, in earlier versions of the tokenizer, we found entire URLs stored as tokens caused by several documents containing a high number of duplicates. These issues motivated us to remove duplicated lines in the tokenizer training training data. We then applied the same sampling ratios per language as for the training data.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Vocabulary Size</h5>
<div class="ltx_para" id="S3.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px3.p1.1">A large vocabulary size reduces the risk of over-segmenting some sentences, especially for low-resource languages. We conducted validation experiments using 150k and 250k vocabulary sizes to make comparisons with existing multilingual modeling literature easierÂ <cite class="ltx_cite ltx_citemacro_citep">(Conneau etÂ al., <a class="ltx_ref" href="#bib.bib35" title="">2020</a>; Xue etÂ al., <a class="ltx_ref" href="#bib.bib165" title="">2021</a>)</cite>. We ultimately settled for a vocabulary of 250k tokens to reach our initial fertility objective compared to monolingual tokenizers. Since the vocabulary size determines the embedding matrix size, it also had to be divisible by 128 for GPU efficiency reasons and by 4 to be able to use Tensor Parallelism. We used a final size of 250,680Â vocabulary items with 200 tokens reserved for possible future applications such as removing private information using placeholder tokens.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Byte-level BPE</h5>
<div class="ltx_para" id="S3.SS3.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px4.p1.1">The tokenizer is a learned subword tokenizer trained using the Byte Pair EncodingÂ (BPE) algorithm introduced by <cite class="ltx_cite ltx_citemacro_citet">Gage (<a class="ltx_ref" href="#bib.bib49" title="">1994</a>)</cite>. In order not to lose information during tokenization, the tokenizer creates merges starting from bytes as the smallest units instead of characters <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a class="ltx_ref" href="#bib.bib117" title="">2019</a>)</cite>. This way, tokenization never results in unknown tokens because all 256 bytes can be contained in the vocabulary of the tokenizer. In addition, Byte-levelÂ BPE maximizes vocabulary sharing between languagesÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="#bib.bib153" title="">2020</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Normalization</h5>
<div class="ltx_para" id="S3.SS3.SSS0.Px5.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px5.p1.2">Upstream of the BPE tokenization algorithm, no normalization of the text was performed in order to have the most general model possible. In all cases, we observed that adding unicode normalization such as NFKC did not reduce the fertility by more than 0.8% on all the languages considered but came at the cost of making the model less general; for example, causing <math alttext="2^{2}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px5.p1.1.m1.1"><semantics id="S3.SS3.SSS0.Px5.p1.1.m1.1a"><msup id="S3.SS3.SSS0.Px5.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px5.p1.1.m1.1.1.cmml"><mn id="S3.SS3.SSS0.Px5.p1.1.m1.1.1.2" xref="S3.SS3.SSS0.Px5.p1.1.m1.1.1.2.cmml">2</mn><mn id="S3.SS3.SSS0.Px5.p1.1.m1.1.1.3" xref="S3.SS3.SSS0.Px5.p1.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px5.p1.1.m1.1b"><apply id="S3.SS3.SSS0.Px5.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px5.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS0.Px5.p1.1.m1.1.1">superscript</csymbol><cn id="S3.SS3.SSS0.Px5.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS3.SSS0.Px5.p1.1.m1.1.1.2">2</cn><cn id="S3.SS3.SSS0.Px5.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS3.SSS0.Px5.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px5.p1.1.m1.1c">2^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px5.p1.1.m1.1d">2 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="22" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px5.p1.2.m2.1"><semantics id="S3.SS3.SSS0.Px5.p1.2.m2.1a"><mn id="S3.SS3.SSS0.Px5.p1.2.m2.1.1" xref="S3.SS3.SSS0.Px5.p1.2.m2.1.1.cmml">22</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px5.p1.2.m2.1b"><cn id="S3.SS3.SSS0.Px5.p1.2.m2.1.1.cmml" type="integer" xref="S3.SS3.SSS0.Px5.p1.2.m2.1.1">22</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px5.p1.2.m2.1c">22</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px5.p1.2.m2.1d">22</annotation></semantics></math> to be encoded in the same way.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px6">
<h5 class="ltx_title ltx_title_paragraph">Pre-tokenizer</h5>
<div class="ltx_para" id="S3.SS3.SSS0.Px6.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px6.p1.1">Our pre-tokenization has two goals: producing a first division of the text (usually using whitespaces and punctuation) and restricting the maximum length of sequences of tokens produced by the BPE algorithm. The pre-tokenization rule used was the following regex: â€œ<span class="ltx_ERROR undefined" id="S3.SS3.SSS0.Px6.p1.1.1">\scalerel</span>*<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="97" id="S3.SS3.SSS0.Px6.p1.1.g1" src="x7.png" width="1096"/>â—‹â€ <span class="ltx_note ltx_role_footnote" id="footnote19"><sup class="ltx_note_mark">19</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">19</sup><span class="ltx_tag ltx_tag_note">19</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://github.com/bigscience-workshop/bs-tokenizers/blob/c510647dda77a37d83c756bc5a5ce85048ab66c0/tokenizers/train_tokenizer_v3_on_subset.py#L124" title="">github.com/bigscience-workshop/bs-tokenizers</a></span></span></span>
which splits words apart while preserving all the characters and in particular the sequences of spaces and line breaks that are crucial for programming languages. We do not use English-centric splits common in other tokenizers (e.g. splitting around <span class="ltx_text ltx_font_typewriter" id="S3.SS3.SSS0.Px6.p1.1.2">â€™nt</span> or <span class="ltx_text ltx_font_typewriter" id="S3.SS3.SSS0.Px6.p1.1.3">â€™ll</span>). We also didnâ€™t use splits on numbers and digits, which caused issues in Arabic and code.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Engineering</h3>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Hardware</h4>
<div class="ltx_para" id="S3.SS4.SSS1.p1">
<p class="ltx_p" id="S3.SS4.SSS1.p1.1">The model was trained on Jean Zay,<span class="ltx_note ltx_role_footnote" id="footnote20"><sup class="ltx_note_mark">20</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">20</sup><span class="ltx_tag ltx_tag_note">20</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://www.idris.fr/eng/jean-zay/jean-zay-presentation-eng.html" title="">idris.fr/eng/jean-zay/jean-zay-presentation-eng.html</a></span></span></span> a French government-funded supercomputer owned by GENCI and operated at IDRIS, the national computing center for the French National Center for Scientific Research (CNRS).
Training BLOOM took about 3.5 months to complete and consumed 1,082,990 compute hours. Training was conducted on 48 nodes, each having 8 NVIDIA A100 80GB GPUs (a total of 384 GPUs); due to possible hardware failures during training, we also maintained a reserve of 4 spare nodes. The nodes were equipped with 2x AMD EPYC 7543 32-Core CPUs and 512 GB of RAM, while the storage was handled by mix of full flash and hard disk drives using a SpectrumScale (GPFS) parallel file system shared between all nodes and users of the supercomputer. 4 NVLink GPU-to-GPU interconnects per node enabled intra-node communications while 4 Omni-Path 100 Gbps links per node, arranged in an enhanced hypercube 8D global topology, were used for inter-node communications.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Framework</h4>
<div class="ltx_para" id="S3.SS4.SSS2.p1">
<p class="ltx_p" id="S3.SS4.SSS2.p1.1">BLOOM was trained using Megatron-DeepSpeed<span class="ltx_note ltx_role_footnote" id="footnote21"><sup class="ltx_note_mark">21</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">21</sup><span class="ltx_tag ltx_tag_note">21</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://github.com/bigscience-workshop/Megatron-DeepSpeed" title="">github.com/bigscience-workshop/Megatron-DeepSpeed</a></span></span></span>Â <cite class="ltx_cite ltx_citemacro_citep">(Smith etÂ al., <a class="ltx_ref" href="#bib.bib138" title="">2022</a>)</cite>, a framework for large-scale distributed training. It consists of two parts: Megatron-LM<span class="ltx_note ltx_role_footnote" id="footnote22"><sup class="ltx_note_mark">22</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">22</sup><span class="ltx_tag ltx_tag_note">22</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://github.com/NVIDIA/Megatron-LM" title="">github.com/NVIDIA/Megatron-LM</a></span></span></span>Â <cite class="ltx_cite ltx_citemacro_citep">(Shoeybi etÂ al., <a class="ltx_ref" href="#bib.bib136" title="">2019</a>)</cite> provides the Transformer implementation, tensor parallelism, and data loading primitives, whereas DeepSpeed<span class="ltx_note ltx_role_footnote" id="footnote23"><sup class="ltx_note_mark">23</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">23</sup><span class="ltx_tag ltx_tag_note">23</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://github.com/microsoft/DeepSpeed" title="">github.com/microsoft/DeepSpeed</a></span></span></span>Â <cite class="ltx_cite ltx_citemacro_citep">(Rasley etÂ al., <a class="ltx_ref" href="#bib.bib123" title="">2020</a>)</cite> provides the ZeRO optimizer, model pipelining, and general distributed training components. This framework allows us to train efficiently with <span class="ltx_text ltx_font_italic" id="S3.SS4.SSS2.p1.1.1">3D parallelism</span> (<cite class="ltx_cite ltx_citemacro_citep">Narayanan etÂ al., <a class="ltx_ref" href="#bib.bib103" title="">2021</a></cite>, shown inÂ <a class="ltx_ref ltx_refmacro_autoref" href="#S3.F6" title="Figure 6 â€£ 3.4.2 Framework â€£ 3.4 Engineering â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">FigureÂ 6</span></a>), a fusion of three complementary approaches to distributed training. These approaches are described below:</p>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="S3.F6.g1" src="parallelism.pdf"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S3.F6.3.2" style="font-size:90%;">DP+PP+TP combination leads to 3D parallelism.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS4.SSS2.p2">
<dl class="ltx_description" id="S3.I1">
<dt class="ltx_item" id="S3.I1.ix1"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S3.I1.ix1.1.1.1">Data parallelism (DP)</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S3.I1.ix1.p1">
<p class="ltx_p" id="S3.I1.ix1.p1.1">replicates the model multiple times, with each replica placed on a different device and fed a slice of the data. The processing is done in parallel and all model replicas are synchronized at the end of each training step.</p>
</div>
</dd>
<dt class="ltx_item" id="S3.I1.ix2"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S3.I1.ix2.1.1.1">Tensor parallelism (TP)</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S3.I1.ix2.p1">
<p class="ltx_p" id="S3.I1.ix2.p1.1">partitions individual layers of the model across multiple devices. This way, instead of having the whole activation or gradient tensor reside on a single GPU, we place shards of this tensor on separate GPUs. This technique is sometimes called horizontal parallelism or intra-layer model parallelism.</p>
</div>
</dd>
<dt class="ltx_item" id="S3.I1.ix3"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S3.I1.ix3.1.1.1">Pipeline parallelism (PP)</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S3.I1.ix3.p1">
<p class="ltx_p" id="S3.I1.ix3.p1.1">splits up the modelâ€™s layers across multiple GPUs, so that only a fraction of the layers of the model are placed on each GPU. This is sometimes called vertical parallelism.</p>
</div>
</dd>
</dl>
<p class="ltx_p" id="S3.SS4.SSS2.p2.1">Finally, the Zero Redundancy Optimizer (ZeRO; <cite class="ltx_cite ltx_citemacro_citep">Rajbhandari etÂ al., <a class="ltx_ref" href="#bib.bib120" title="">2020</a></cite>) allows different processes to only hold a fraction of data (parameters, gradients, and optimizer states) required for a training step. We used ZeRO stage 1, meaning that only the optimizer states are sharded in this manner.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p3">
<p class="ltx_p" id="S3.SS4.SSS2.p3.1">The four components described above are combined together to allow scaling to hundreds of GPUs with extremely high GPU utilization. We were able to achieve 156 TFLOPs in our fastest configuration with A100 GPUs, attaining our objective of half of the theoretical peak performance of 312 TFLOPs (in <span class="ltx_text ltx_font_typewriter" id="S3.SS4.SSS2.p3.1.1">float32</span> or <span class="ltx_text ltx_font_typewriter" id="S3.SS4.SSS2.p3.1.2">bfloat16</span>).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.3 </span>Floating Point Format</h4>
<div class="ltx_para" id="S3.SS4.SSS3.p1">
<p class="ltx_p" id="S3.SS4.SSS3.p1.1">In earlier experiments with 104B-parameter models on NVIDIA V100 GPUs, we observed numerical instabilities that caused irreversible training divergences.
We hypothesize that these instabilities stem from our initial use of IEEE <span class="ltx_text ltx_font_typewriter" id="S3.SS4.SSS3.p1.1.1">float16</span> â€” a 16-bit floating point format with a very limited dynamic range that can cause overflows.
The NVIDIA A100 GPUs that we ultimately had access to support the <span class="ltx_text ltx_font_typewriter" id="S3.SS4.SSS3.p1.1.2">bfloat16</span> formatÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang and Kanwar, <a class="ltx_ref" href="#bib.bib154" title="">2019</a>; Kalamkar etÂ al., <a class="ltx_ref" href="#bib.bib66" title="">2019</a>)</cite>, which has the same dynamic range as <span class="ltx_text ltx_font_typewriter" id="S3.SS4.SSS3.p1.1.3">float32</span>.
On the other hand, <span class="ltx_text ltx_font_typewriter" id="S3.SS4.SSS3.p1.1.4">bfloat16</span> still has much lower precision, which motivated our use of mixed-precision trainingÂ <cite class="ltx_cite ltx_citemacro_citep">(Micikevicius etÂ al., <a class="ltx_ref" href="#bib.bib90" title="">2018</a>)</cite>. This technique performs certain precision-sensitive operations such as gradient accumulation and softmax in <span class="ltx_text ltx_font_typewriter" id="S3.SS4.SSS3.p1.1.5">float32</span> precision and the rest of operations in lower precision, allowing us to achieve a balance of high performance and training stability.
Ultimately, we performed final training in <span class="ltx_text ltx_font_typewriter" id="S3.SS4.SSS3.p1.1.6">bfloat16</span> mixed precision, which proved to solve the instability problem (in line with previous observation by <cite class="ltx_cite ltx_citemacro_citep">Smith etÂ al., <a class="ltx_ref" href="#bib.bib138" title="">2022</a></cite>).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.4 </span>Fused CUDA Kernels</h4>
<div class="ltx_para" id="S3.SS4.SSS4.p1">
<p class="ltx_p" id="S3.SS4.SSS4.p1.1">In general, GPUs cannot retrieve data to perform computations on and perform these computations at the same time. Moreover, the compute performance of modern GPUs is much higher than the speed of memory transfer required for every operation (often called <span class="ltx_text ltx_font_italic" id="S3.SS4.SSS4.p1.1.1">a kernel</span> in GPU programming).
Kernel fusionÂ <cite class="ltx_cite ltx_citemacro_citep">(Wu etÂ al., <a class="ltx_ref" href="#bib.bib164" title="">2012</a>)</cite> is an approach for optimizing GPU-based computations by performing several consecutive operations in only one kernel call. This approach offers a way to minimize data transfers: intermediary results stay in the GPU register instead of being copied into VRAM, saving overhead.
</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS4.p2">
<p class="ltx_p" id="S3.SS4.SSS4.p2.1">We used several custom fused CUDA kernels provided by Megatron-LM. First, we used an optimized kernel to perform LayerNorm, as well as kernels to fuse various combinations of the scaling, masking, and softmax operations. The addition of a bias term is also fused with the GeLU activation using the JIT functionality of PyTorch. As an example consequence of the use of fused kernels, adding the bias term in the GeLU operation adds no additional time, as the operation is memory-bound: the additional computation is negligible compared to data transfers between GPU VRAM and registers, so fusing both operations essentially halves their runtime.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.5 </span>Additional Challenges</h4>
<div class="ltx_para" id="S3.SS4.SSS5.p1">
<p class="ltx_p" id="S3.SS4.SSS5.p1.1">Scaling to 384 GPUs required two final changes: disabling asynchronous CUDA kernel launches (for ease of debugging and to prevent deadlocks) and splitting parameter groups into smaller subgroups (to avoid excessive CPU memory allocations).</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS5.p2">
<p class="ltx_p" id="S3.SS4.SSS5.p2.1">During training, we faced issues with hardware failures: on average, 1â€“2 GPU failures occurred each week. As backup nodes were available and automatically used, and checkpoints were saved every three hours, this did not affect training throughput significantly. A PyTorch deadlock bug in the data loader and disk space issues led to 5â€“10h downtimes. Given the relative sparsity of engineering issues, and since there was only one loss spike, which the model swiftly recovered from, human intervention was less necessary than in comparable projectsÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="#bib.bib169" title="">2022</a>)</cite>.
Full details of our experience with training BLOOM and a detailed report of all issues we faced are publicly available.<span class="ltx_note ltx_role_footnote" id="footnote24"><sup class="ltx_note_mark">24</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">24</sup><span class="ltx_tag ltx_tag_note">24</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://github.com/bigscience-workshop/bigscience/blob/master/train/tr11-176B-ml/chronicles.md" title="">github.com/bigscience-workshop/bigscience/blob/master/train/tr11-176B-ml/chronicles.md</a></span></span></span></p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Training</h3>
<figure class="ltx_table" id="S3.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T3.2" style="width:433.6pt;height:535.4pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-2.3pt,2.8pt) scale(0.98968361852346,0.98968361852346) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.2.2">
<tr class="ltx_tr" id="S3.T3.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.1.1.1.1">Hyperparameter (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T3.1.1.1.1.m1.1"><semantics id="S3.T3.1.1.1.1.m1.1a"><mo id="S3.T3.1.1.1.1.m1.1.1" stretchy="false" xref="S3.T3.1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.1.m1.1b"><ci id="S3.T3.1.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.1.1.1.1.m1.1d">â†“</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.2">BLOOM-560M</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.3">BLOOM-1.1B</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.4">BLOOM-1.7B</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.5">BLOOM-3B</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.1.1.1.6">BLOOM-7.1B</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.7">BLOOM</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.3">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="7" id="S3.T3.2.2.3.1">
<em class="ltx_emph ltx_font_italic" id="S3.T3.2.2.3.1.1">Architecture hyperparameters</em></td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.4">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T3.2.2.4.1">Parameters</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.4.2">559M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.4.3">1,065M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.4.4">1,722M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.4.5">3,003M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.2.2.4.6">7,069M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.4.7">176,247M</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.2.5.1">Precision</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="5" id="S3.T3.2.2.5.2">
<span class="ltx_text ltx_font_typewriter" id="S3.T3.2.2.5.2.1">float16</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.5.3"><span class="ltx_text ltx_font_typewriter" id="S3.T3.2.2.5.3.1">bfloat16</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.2.6.1">Layers</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.6.2">24</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.6.3">24</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.6.4">24</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.6.5">30</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.2.2.6.6">30</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.6.7">70</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.2.7.1">Hidden dim.</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.7.2">1024</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.7.3">1536</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.7.4">2048</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.7.5">2560</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.2.2.7.6">4096</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.7.7">14336</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.2.8.1">Attention heads</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.8.2">16</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.8.3">16</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.8.4">16</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.8.5">32</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.2.2.8.6">32</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.8.7">112</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.2.9.1">Vocab size</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="5" id="S3.T3.2.2.9.2">250,680</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.9.3">250,680</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.2.10.1">Sequence length</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="5" id="S3.T3.2.2.10.2">2048</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.10.3">2048</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.2.11.1">Activation</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="5" id="S3.T3.2.2.11.2">
<span class="ltx_text ltx_font_typewriter" id="S3.T3.2.2.11.2.1">GELU</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.11.3"><span class="ltx_text ltx_font_typewriter" id="S3.T3.2.2.11.3.1">GELU</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.12">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.2.12.1">Position emb.</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="5" id="S3.T3.2.2.12.2">
<span class="ltx_text ltx_font_typewriter" id="S3.T3.2.2.12.2.1">Alibi</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.12.3"><span class="ltx_text ltx_font_typewriter" id="S3.T3.2.2.12.3.1">Alibi</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.13">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.2.13.1">Tied emb.</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="5" id="S3.T3.2.2.13.2">
<span class="ltx_text ltx_font_typewriter" id="S3.T3.2.2.13.2.1">True</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.13.3"><span class="ltx_text ltx_font_typewriter" id="S3.T3.2.2.13.3.1">True</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.14">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="7" id="S3.T3.2.2.14.1">
<em class="ltx_emph ltx_font_italic" id="S3.T3.2.2.14.1.1">Pretraining hyperparameters</em></td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.15">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T3.2.2.15.1">Global Batch Size</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.15.2">256</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.15.3">256</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.15.4">512</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.15.5">512</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.2.2.15.6">512</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.15.7">2048</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.16">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.2.16.1">Learning rate</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.16.2">3.0e-4</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.16.3">2.5e-4</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.16.4">2e-4</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.16.5">1.6e-4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.2.2.16.6">1.2e-4</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.16.7">6e-5</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.17">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.2.17.1">Total tokens</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="5" id="S3.T3.2.2.17.2">341B</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.17.3">366B</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.18">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.2.18.1">Warmup tokens</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="5" id="S3.T3.2.2.18.2">375M</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.18.3">375M</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.19">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.2.19.1">Decay tokens</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="5" id="S3.T3.2.2.19.2">410B</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.19.3">410B</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.20">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.2.20.1">Decay style</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="5" id="S3.T3.2.2.20.2">
<span class="ltx_text ltx_font_typewriter" id="S3.T3.2.2.20.2.1">cosine</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.20.3"><span class="ltx_text ltx_font_typewriter" id="S3.T3.2.2.20.3.1">cosine</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.21">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.2.21.1">Min. learning rate</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="5" id="S3.T3.2.2.21.2">1e-5</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.21.3">6e-6</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.2.2.1">Adam <math alttext="(\beta_{1},\beta_{2})" class="ltx_Math" display="inline" id="S3.T3.2.2.2.1.m1.2"><semantics id="S3.T3.2.2.2.1.m1.2a"><mrow id="S3.T3.2.2.2.1.m1.2.2.2" xref="S3.T3.2.2.2.1.m1.2.2.3.cmml"><mo id="S3.T3.2.2.2.1.m1.2.2.2.3" stretchy="false" xref="S3.T3.2.2.2.1.m1.2.2.3.cmml">(</mo><msub id="S3.T3.2.2.2.1.m1.1.1.1.1" xref="S3.T3.2.2.2.1.m1.1.1.1.1.cmml"><mi id="S3.T3.2.2.2.1.m1.1.1.1.1.2" xref="S3.T3.2.2.2.1.m1.1.1.1.1.2.cmml">Î²</mi><mn id="S3.T3.2.2.2.1.m1.1.1.1.1.3" xref="S3.T3.2.2.2.1.m1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.T3.2.2.2.1.m1.2.2.2.4" xref="S3.T3.2.2.2.1.m1.2.2.3.cmml">,</mo><msub id="S3.T3.2.2.2.1.m1.2.2.2.2" xref="S3.T3.2.2.2.1.m1.2.2.2.2.cmml"><mi id="S3.T3.2.2.2.1.m1.2.2.2.2.2" xref="S3.T3.2.2.2.1.m1.2.2.2.2.2.cmml">Î²</mi><mn id="S3.T3.2.2.2.1.m1.2.2.2.2.3" xref="S3.T3.2.2.2.1.m1.2.2.2.2.3.cmml">2</mn></msub><mo id="S3.T3.2.2.2.1.m1.2.2.2.5" stretchy="false" xref="S3.T3.2.2.2.1.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.2.1.m1.2b"><interval closure="open" id="S3.T3.2.2.2.1.m1.2.2.3.cmml" xref="S3.T3.2.2.2.1.m1.2.2.2"><apply id="S3.T3.2.2.2.1.m1.1.1.1.1.cmml" xref="S3.T3.2.2.2.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.T3.2.2.2.1.m1.1.1.1.1.1.cmml" xref="S3.T3.2.2.2.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.T3.2.2.2.1.m1.1.1.1.1.2.cmml" xref="S3.T3.2.2.2.1.m1.1.1.1.1.2">ğ›½</ci><cn id="S3.T3.2.2.2.1.m1.1.1.1.1.3.cmml" type="integer" xref="S3.T3.2.2.2.1.m1.1.1.1.1.3">1</cn></apply><apply id="S3.T3.2.2.2.1.m1.2.2.2.2.cmml" xref="S3.T3.2.2.2.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.T3.2.2.2.1.m1.2.2.2.2.1.cmml" xref="S3.T3.2.2.2.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.T3.2.2.2.1.m1.2.2.2.2.2.cmml" xref="S3.T3.2.2.2.1.m1.2.2.2.2.2">ğ›½</ci><cn id="S3.T3.2.2.2.1.m1.2.2.2.2.3.cmml" type="integer" xref="S3.T3.2.2.2.1.m1.2.2.2.2.3">2</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.2.1.m1.2c">(\beta_{1},\beta_{2})</annotation><annotation encoding="application/x-llamapun" id="S3.T3.2.2.2.1.m1.2d">( italic_Î² start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_Î² start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="5" id="S3.T3.2.2.2.2">(0.9, 0.95)</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.2.3">(0.9, 0.95)</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.22">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.2.22.1">Weight decay</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="5" id="S3.T3.2.2.22.2">1e-1</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.22.3">1e-1</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.23">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.2.23.1">Gradient clipping</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="5" id="S3.T3.2.2.23.2">1.0</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.23.3">1.0</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.24">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="7" id="S3.T3.2.2.24.1">
<em class="ltx_emph ltx_font_italic" id="S3.T3.2.2.24.1.1">Multitask finetuning hyperparameters</em></td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.25">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T3.2.2.25.1">Global Batch Size</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.25.2">1024</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.25.3">1024</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.25.4">2048</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.25.5">2048</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.2.2.25.6">2048</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.25.7">2048</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.26">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.2.26.1">Learning rate</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.26.2">2.0e-5</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.26.3">2.0e-5</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.26.4">2.0e-5</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.26.5">2.0e-5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.2.2.26.6">2.0e-5</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.26.7">2.0e-5</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.27">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.2.27.1">Total tokens</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="5" id="S3.T3.2.2.27.2">13B</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.27.3">13B</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.28">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.2.28.1">Warmup tokens</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="5" id="S3.T3.2.2.28.2">0</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.28.3">0</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.29">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.2.29.1">Decay style</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="5" id="S3.T3.2.2.29.2">
<span class="ltx_text ltx_font_typewriter" id="S3.T3.2.2.29.2.1">constant</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.29.3"><span class="ltx_text ltx_font_typewriter" id="S3.T3.2.2.29.3.1">constant</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.30">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.2.2.30.1">Weight decay</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="5" id="S3.T3.2.2.30.2">1e-4</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.30.3">1e-4</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T3.4.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S3.T3.5.2" style="font-size:90%;">BLOOM &amp; BLOOMZ Training Hyperparameters.</span></figcaption>
</figure>
<section class="ltx_paragraph" id="S3.SS5.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Pretrained Models</h5>
<div class="ltx_para" id="S3.SS5.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS5.SSS0.Px1.p1.1">We train six size variants of BLOOM with respective hyperparameters detailed in Table <a class="ltx_ref" href="#S3.T3" title="Table 3 â€£ 3.5 Training â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">3</span></a>. Architecture and training hyperparameters come from our experimental results <cite class="ltx_cite ltx_citemacro_citep">(Le Scao etÂ al., <a class="ltx_ref" href="#bib.bib75" title="">2022</a>)</cite> and prior work on training large language models <cite class="ltx_cite ltx_citemacro_citep">(Brown etÂ al., <a class="ltx_ref" href="#bib.bib27" title="">2020</a>; Kaplan etÂ al., <a class="ltx_ref" href="#bib.bib67" title="">2020</a>)</cite>. Model depth and width for the non-176B models roughly follow previous literature <cite class="ltx_cite ltx_citemacro_citep">(Brown etÂ al., <a class="ltx_ref" href="#bib.bib27" title="">2020</a>; Zhang etÂ al., <a class="ltx_ref" href="#bib.bib169" title="">2022</a>)</cite>, deviating for 3B and 7.1B in order only to fit the models more easily on our training setup. Embedding parameter sizes are larger for BLOOM owing to the larger multilingual vocabulary, but scaling literature discounts embedding operations <cite class="ltx_cite ltx_citemacro_citep">(Kaplan etÂ al., <a class="ltx_ref" href="#bib.bib67" title="">2020</a>)</cite>. During the development process at the 104B parameters scale, we experimented with different values of Adam <math alttext="\beta" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS5.SSS0.Px1.p1.1.m1.1a"><mi id="S3.SS5.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS5.SSS0.Px1.p1.1.m1.1.1.cmml">Î²</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS5.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS5.SSS0.Px1.p1.1.m1.1.1">ğ›½</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS0.Px1.p1.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS0.Px1.p1.1.m1.1d">italic_Î²</annotation></semantics></math> parameters, weight decay and gradient clipping to target stability, but did not find it helpful. For all models, we use a cosine learning rate decay schedule <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov and Hutter, <a class="ltx_ref" href="#bib.bib84" title="">2016</a>)</cite> over 410B tokens, taken as an upper bound for the length of training if compute permitted, and warmup for 375M tokens. We use weight decay, gradient clipping, and no dropout. The ROOTS dataset contains around 341 billion tokens of text, so we aimed to train all models for the equivalent amount of tokens. However, in light of revised scaling laws published during training <cite class="ltx_cite ltx_citemacro_citep">(Hoffmann etÂ al., <a class="ltx_ref" href="#bib.bib61" title="">2022</a>)</cite>, we decided to train the large models for an additional 25 billion tokens on repeated data. As warmup tokens + decay tokens were larger than the total number of tokens, the end of learning rate decay was never reached.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS5.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Multitask Finetuning</h5>
<div class="ltx_para" id="S3.SS5.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS5.SSS0.Px2.p1.1">Finetuned BLOOMZ modelsÂ <cite class="ltx_cite ltx_citemacro_citep">(Muennighoff etÂ al., <a class="ltx_ref" href="#bib.bib100" title="">2022b</a>)</cite> maintain the same architecture hyperparameters as BLOOM models. The finetuning hyperparameters are loosely based on T0 <cite class="ltx_cite ltx_citemacro_citep">(Sanh etÂ al., <a class="ltx_ref" href="#bib.bib128" title="">2022</a>)</cite> and FLAN <cite class="ltx_cite ltx_citemacro_citep">(Wei etÂ al., <a class="ltx_ref" href="#bib.bib158" title="">2021</a>)</cite>. Learning rates are determined by doubling the minimum learning rate of the respective pretrained model and then rounding. Global batch sizes are multiplied by four for small variants to increase throughput. While the models are finetuned for 13 billion tokens, the best checkpoint is chosen according to a separate validation set. We found performance to plateau after 1 â€“ 6 billion tokens of finetuning.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS5.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Contrastive Finetuning</h5>
<div class="ltx_para" id="S3.SS5.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS5.SSS0.Px3.p1.1">We also perform contrastive finetuning of the 1.3 and 7.1 billion parameter BLOOM models using the SGPT Bi-Encoder recipeÂ <cite class="ltx_cite ltx_citemacro_citep">(Muennighoff, <a class="ltx_ref" href="#bib.bib98" title="">2022</a>)</cite> to train models that produce high-quality text embeddings. We created SGPT-BLOOM-7.1B-msmarco<span class="ltx_note ltx_role_footnote" id="footnote25"><sup class="ltx_note_mark">25</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">25</sup><span class="ltx_tag ltx_tag_note">25</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://hf.co/bigscience/sgpt-bloom-7b1-msmarco" title="">hf.co/bigscience/sgpt-bloom-7b1-msmarco</a></span></span></span> geared towards multilingual information retrieval and SGPT-BLOOM-1.7B-nli<span class="ltx_note ltx_role_footnote" id="footnote26"><sup class="ltx_note_mark">26</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">26</sup><span class="ltx_tag ltx_tag_note">26</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://hf.co/bigscience-data/sgpt-bloom-1b7-nli" title="">hf.co/bigscience-data/sgpt-bloom-1b7-nli</a></span></span></span> for multilingual semantic textual similarity (STS). However, recent benchmarking has found these models to also generalize to various other embedding tasks, such as bitext mining, reranking or feature extraction for downstream classificationÂ <cite class="ltx_cite ltx_citemacro_citep">(Muennighoff etÂ al., <a class="ltx_ref" href="#bib.bib99" title="">2022a</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.1 </span>Carbon Footprint</h4>
<div class="ltx_para" id="S3.SS5.SSS1.p1">
<p class="ltx_p" id="S3.SS5.SSS1.p1.1">While most attempts to estimate the carbon footprint of language models have shed light on the emissions produced due to energy consumed during model training (e.g.Â <cite class="ltx_cite ltx_citemacro_citep">Patterson etÂ al., <a class="ltx_ref" href="#bib.bib110" title="">2021</a>; Strubell etÂ al., <a class="ltx_ref" href="#bib.bib141" title="">2019</a></cite>), other sources of emissions are also important to consider. In our efforts to estimate the carbon emissions of BLOOM, we were inspired by the Life Cycle Assessment (LCA) approachÂ <cite class="ltx_cite ltx_citemacro_citep">(KlÃ¶pffer, <a class="ltx_ref" href="#bib.bib69" title="">1997</a>)</cite> and aimed to consider aspects such as the emissions of equipment manufacturing, intermediate model training, and deployment. According to our estimates, the carbon emissions from BLOOM training add up to approximately 81 tons of CO<sub class="ltx_sub" id="S3.SS5.SSS1.p1.1.1">2</sub>eq, of which 14% were generated by the equipment manufacturing process (11 tons), 30% by the energy consumed during training (25 tons) and 55% by idle consumption of the equipment and computing cluster used for training (45 tons).</p>
</div>
<figure class="ltx_table" id="S3.T4">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T4.2">
<tr class="ltx_tr" id="S3.T4.2.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T4.2.1.1">
<table class="ltx_tabular ltx_align_middle" id="S3.T4.2.1.1.1">
<tr class="ltx_tr" id="S3.T4.2.1.1.1.1">
<td class="ltx_td ltx_align_center" id="S3.T4.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.1.1.1.1" style="font-size:90%;">Model</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.1.1.1.2">
<td class="ltx_td ltx_align_center" id="S3.T4.2.1.1.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.1.2.1.1" style="font-size:90%;">name</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T4.2.1.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T4.2.1.2.1">
<tr class="ltx_tr" id="S3.T4.2.1.2.1.1">
<td class="ltx_td ltx_align_center" id="S3.T4.2.1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.2.1.1.1.1" style="font-size:90%;">Number of</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.1.2.1.2">
<td class="ltx_td ltx_align_center" id="S3.T4.2.1.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.2.1.2.1.1" style="font-size:90%;">parameters</span></td>
</tr>
</table>
<span class="ltx_text" id="S3.T4.2.1.2.2" style="font-size:90%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.2.1.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T4.2.1.3.1">
<tr class="ltx_tr" id="S3.T4.2.1.3.1.1">
<td class="ltx_td ltx_align_center" id="S3.T4.2.1.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.3.1.1.1.1" style="font-size:90%;">Power</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.1.3.1.2">
<td class="ltx_td ltx_align_center" id="S3.T4.2.1.3.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.3.1.2.1.1" style="font-size:90%;">consumption</span></td>
</tr>
</table>
<span class="ltx_text" id="S3.T4.2.1.3.2" style="font-size:90%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.2.1.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T4.2.1.4.1">
<tr class="ltx_tr" id="S3.T4.2.1.4.1.1">
<td class="ltx_td ltx_align_center" id="S3.T4.2.1.4.1.1.1">
<span class="ltx_text ltx_font_bold" id="S3.T4.2.1.4.1.1.1.1" style="font-size:90%;">CO</span><sub class="ltx_sub" id="S3.T4.2.1.4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.4.1.1.1.2.1" style="font-size:90%;">2</span></sub><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.4.1.1.1.3" style="font-size:90%;">eq</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.1.4.1.2">
<td class="ltx_td ltx_align_center" id="S3.T4.2.1.4.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.4.1.2.1.1" style="font-size:90%;">emissions</span></td>
</tr>
</table>
<span class="ltx_text" id="S3.T4.2.1.4.2" style="font-size:90%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T4.2.2.1"><span class="ltx_text" id="S3.T4.2.2.1.1" style="font-size:90%;">GPT-3</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T4.2.2.2"><span class="ltx_text" id="S3.T4.2.2.2.1" style="font-size:90%;">175B</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T4.2.2.3"><span class="ltx_text" id="S3.T4.2.2.3.1" style="font-size:90%;">1,287 MWh</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T4.2.2.4"><span class="ltx_text ltx_font_italic" id="S3.T4.2.2.4.1" style="font-size:90%;">502 tons</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.3">
<td class="ltx_td ltx_align_left" id="S3.T4.2.3.1"><span class="ltx_text" id="S3.T4.2.3.1.1" style="font-size:90%;">Gopher</span></td>
<td class="ltx_td ltx_align_right" id="S3.T4.2.3.2"><span class="ltx_text" id="S3.T4.2.3.2.1" style="font-size:90%;">280B</span></td>
<td class="ltx_td ltx_align_right" id="S3.T4.2.3.3"><span class="ltx_text ltx_font_italic" id="S3.T4.2.3.3.1" style="font-size:90%;">1,066 MWh</span></td>
<td class="ltx_td ltx_align_right" id="S3.T4.2.3.4"><span class="ltx_text ltx_font_italic" id="S3.T4.2.3.4.1" style="font-size:90%;">352 tons</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.4">
<td class="ltx_td ltx_align_left" id="S3.T4.2.4.1"><span class="ltx_text" id="S3.T4.2.4.1.1" style="font-size:90%;">OPT</span></td>
<td class="ltx_td ltx_align_right" id="S3.T4.2.4.2"><span class="ltx_text" id="S3.T4.2.4.2.1" style="font-size:90%;">175B</span></td>
<td class="ltx_td ltx_align_right" id="S3.T4.2.4.3"><span class="ltx_text ltx_font_italic" id="S3.T4.2.4.3.1" style="font-size:90%;">324 MWh</span></td>
<td class="ltx_td ltx_align_right" id="S3.T4.2.4.4"><span class="ltx_text" id="S3.T4.2.4.4.1" style="font-size:90%;">70 tons</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.5">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T4.2.5.1"><span class="ltx_text" id="S3.T4.2.5.1.1" style="font-size:90%;">BLOOM</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T4.2.5.2"><span class="ltx_text" id="S3.T4.2.5.2.1" style="font-size:90%;">176B</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T4.2.5.3"><span class="ltx_text" id="S3.T4.2.5.3.1" style="font-size:90%;">433 MWh</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T4.2.5.4"><span class="ltx_text" id="S3.T4.2.5.4.1" style="font-size:90%;">25 tons</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison of carbon emissions between BLOOM and similar LLMs. Numbers in <span class="ltx_text ltx_font_italic" id="S3.T4.7.1">italics</span> have been inferred based on data provided in the papers describing the models.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS5.SSS1.p2">
<p class="ltx_p" id="S3.SS5.SSS1.p2.1">Comparing the carbon emissions of BLOOM training to other similar models (see TableÂ <a class="ltx_ref" href="#S3.T4" title="Table 4 â€£ 3.5.1 Carbon Footprint â€£ 3.5 Training â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">4</span></a>), reveals that while the energy consumption of BLOOM is slightly higher than OPTÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="#bib.bib169" title="">2022</a>)</cite> (433 Mwh compared to OPTâ€™s 324 MWh), its emissions are approximately 2/3 less (25 tons versus 70 tons). This is thanks to the low carbon intensity of the energy grid used for training BLOOM, which emits 57Â gCO<sub class="ltx_sub" id="S3.SS5.SSS1.p2.1.1">2</sub>eq/kWh, compared to 231Â gCO<sub class="ltx_sub" id="S3.SS5.SSS1.p2.1.2">2</sub>eq/kWhÂ  for the grid used for OPT training. Specifically, Franceâ€™s national energy grid (which is used by Jean Zay) is largely powered by nuclear energy, which is low-carbon compared to grids powered by energy sources such as coal and natural gas. While the sustainability of nuclear energy is debated, it is one of the least carbon-intensive sources of energy that is currently available. Both BLOOM and OPT incurred significantly less carbon emissions than GPT-3 (as reported byÂ <cite class="ltx_cite ltx_citemacro_citep">(Patterson etÂ al., <a class="ltx_ref" href="#bib.bib110" title="">2021</a>)</cite>), which can be attributed to several factors including more efficient hardware as well as less carbon-intensive energy sources.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS1.p3">
<p class="ltx_p" id="S3.SS5.SSS1.p3.1">We also pursued further exploration of the carbon footprint of (1) the computation carried out on Jean Zay within the scope of the Big Science workshop, and (2) running the BLOOM model API in real time. In terms of the footprint of the totality of the computation, we estimate that the final BLOOM training represents approximately 37% of the overall emissions, with other processes such as intermediate training runs and model evaluation adding up to the other 63%. This is slightly less than the estimate made by the authors of the OPT paper, who stated that the total carbon footprint of their model is roughly 2 times higher due to experimentation, baselines and ablationÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="#bib.bib169" title="">2022</a>)</cite>. Our ongoing exploration of the carbon emissions of the BLOOM API have estimated that the real-time deployment of the model on a GCP instance with 16 GPUs running in the <span class="ltx_text ltx_font_typewriter" id="S3.SS5.SSS1.p3.1.1">us-central1</span> region results in approximately 20 kg ofÂ CO<sub class="ltx_sub" id="S3.SS5.SSS1.p3.1.2">2</sub>eq emitted per day of deployment (or 0.83 kg per hour). This figure is not representative of all deployment use-cases, and will vary depending on the hardware used as well as the specifics of model implementation (e.g.Â whether batching is used) and the number of requests the model receives. Further information regarding BLOOMâ€™s carbon footprint can be found inÂ <cite class="ltx_cite ltx_citemacro_cite">Luccioni etÂ al. (<a class="ltx_ref" href="#bib.bib85" title="">2022</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Release</h3>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">Openness has been central to the development of BLOOM and we wanted to ensure it is easily available for the community to use. As such, we worked on producing documentation as a Model Card <cite class="ltx_cite ltx_citemacro_citep">(Mitchell etÂ al., <a class="ltx_ref" href="#bib.bib95" title="">2019</a>)</cite> and a new license addressing specific goals of the project.</p>
</div>
<section class="ltx_paragraph" id="S3.SS6.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Model Card</h5>
<div class="ltx_para" id="S3.SS6.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS6.SSS0.Px1.p1.1">Following best practices for releasing machine learning models, the BLOOM model has been released along with a detailed Model Card<span class="ltx_note ltx_role_footnote" id="footnote27"><sup class="ltx_note_mark">27</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">27</sup><span class="ltx_tag ltx_tag_note">27</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://hf.co/bigscience/bloom" title="">hf.co/bigscience/bloom</a></span></span></span>Â <cite class="ltx_cite ltx_citemacro_citep">(Mitchell etÂ al., <a class="ltx_ref" href="#bib.bib95" title="">2019</a>)</cite> describing its technical specifications, details on training, intended-use, out-of-scope uses as well as the modelâ€™s limitations.
Participants across working groups worked together to produce the final Model Card and similar cards for each checkpoint. The work was collaborative, primarily composed â€œliveâ€ by thinking through and discussing each section, then further dividing into subsections based on the categorizations and distinctions participants naturally ended up creating throughout discussions.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS6.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Licensing</h5>
<div class="ltx_para" id="S3.SS6.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS6.SSS0.Px2.p1.1">Being mindful of the potentially harmful use-cases that BLOOM could enable, we chose to strike a balance between unrestricted open-access and responsible-use by including behavioral-use clausesÂ <cite class="ltx_cite ltx_citemacro_citep">(Contractor etÂ al., <a class="ltx_ref" href="#bib.bib36" title="">2022</a>)</cite> to limit the application of the model towards potentially harmful use-cases. Such clauses are routinely being included in a growing class of â€œResponsible AI Licenses (RAIL)â€<span class="ltx_note ltx_role_footnote" id="footnote28"><sup class="ltx_note_mark">28</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">28</sup><span class="ltx_tag ltx_tag_note">28</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://licenses.ai" title="">licenses.ai</a></span></span></span> that the community has been adopting when releasing their models.<span class="ltx_note ltx_role_footnote" id="footnote29"><sup class="ltx_note_mark">29</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">29</sup><span class="ltx_tag ltx_tag_note">29</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://the-turing-way.netlify.app/reproducible-research/licensing/licensing-ml.html" title="">the-turing-way.netlify.app/reproducible-research/licensing/licensing-ml.html</a></span></span></span>
A distinguishing aspect of the RAIL license developed for BLOOM is that it separates licensing of the â€œsource codeâ€ and â€œmodelâ€, as referenced by its trained parameters. It further includes detailed definitions of â€œuseâ€ and â€œderived worksâ€ of the model to ensure that anticipated downstream use by prompting, finetuning, distillation, use of logits and probability distributions are explicitly identified. The license contains <math alttext="13" class="ltx_Math" display="inline" id="S3.SS6.SSS0.Px2.p1.1.m1.1"><semantics id="S3.SS6.SSS0.Px2.p1.1.m1.1a"><mn id="S3.SS6.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS6.SSS0.Px2.p1.1.m1.1.1.cmml">13</mn><annotation-xml encoding="MathML-Content" id="S3.SS6.SSS0.Px2.p1.1.m1.1b"><cn id="S3.SS6.SSS0.Px2.p1.1.m1.1.1.cmml" type="integer" xref="S3.SS6.SSS0.Px2.p1.1.m1.1.1">13</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.SSS0.Px2.p1.1.m1.1c">13</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.SSS0.Px2.p1.1.m1.1d">13</annotation></semantics></math> behavioral-use restrictions that have been identified based on the intended uses and limitations described in the BLOOM Model Card, as well as the BigScience ethical charter. The license offers the model at no charge and users are free to use the model as long as they comply with the terms (including usage restrictions). The source code for BLOOM has been made available under an Apache 2.0 open source license.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Our evaluations focus on zero-shot and few-shot settings. Our goal is to present an accurate picture of how BLOOM compares to existing LLMs in settings that most realistically reflect the way the models are likely to be used in practice. Because of the scale of these models, prompt-based adaptation and few-shot â€œin-context learningâ€ are currently more common than finetuning. Thus, we report results on a range of tasks - SuperGLUE <a class="ltx_ref" href="#S4.SS2" title="4.2 SuperGLUE â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">4.2</span></a>, machine translation <a class="ltx_ref" href="#S4.SS3" title="4.3 Machine Translation â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">4.3</span></a>, summarization <a class="ltx_ref" href="#S4.SS4" title="4.4 Summarization â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">4.4</span></a> - and languages in zero-shot and one-shot prompt-based settings, as well as after multitask finetuning (<a class="ltx_ref" href="#S4.SS7" title="4.7 Multitask Finetuning â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">4.7</span></a>). We also perform code generation <a class="ltx_ref" href="#S4.SS5" title="4.5 Code Generation â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">4.5</span></a>, use BLOOM-derived text embeddings for representation tasks <a class="ltx_ref" href="#S4.SS8" title="4.8 Embeddings â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">4.8</span></a> and interpret BLOOMâ€™s generalization abilities from the perspective of multilingual probing (<a class="ltx_ref" href="#S4.SS9" title="4.9 Multilingual Probing â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">4.9</span></a>).</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Design</h3>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Prompts</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">Based on recent research on the impact of prompting on language model performance, we decided to build a language model evaluation suite that allowed us to vary both the basic task data as well as the prompting that is used to contextualize the task. Our prompts were developed prior to BLOOMâ€™s release, and did not undergo any <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p1.1.1">a priori</span> refinement using models. That is, the prompts we use in our evaluation are ones that humans believed were a reasonable way to solicit the desired task behavior from a language model. Our goal for designing prompts in this way is to simulate realistic zero-shot or one-shot results that a new user could expect from BLOOM. This is in contrast to presenting best-case performances that might result from multiple rounds of trial-and-error on prompt design. We choose to report the former because the latter is harder to reproduce systematically, is arguably a less representative picture of how the model works in the average setting, and is not representative of true zero-shot learning where no labeled data is available.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1">We generate multiple prompts per task using <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS1.p2.1.1">promptsource</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Bach etÂ al., <a class="ltx_ref" href="#bib.bib8" title="">2022</a>)</cite>. We follow the procedure used by <cite class="ltx_cite ltx_citemacro_citet">Sanh etÂ al. (<a class="ltx_ref" href="#bib.bib128" title="">2022</a>)</cite>, in which prompt generation is crowdsourced, and thus we see substantial variety in length and style across prompts. To improve quality and clarity, multiple peer reviews were performed on each prompt for artifacts and consistency.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p3">
<p class="ltx_p" id="S4.SS1.SSS1.p3.1">TableÂ <a class="ltx_ref" href="#S4.T5" title="Table 5 â€£ 4.1.1 Prompts â€£ 4.1 Experimental Design â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">5</span></a> shows examples of the resulting prompts used for the WMTâ€™14 task. We also generate prompts for many tasks that are not included in this paper due to resource constraints. All of our prompts for all tasks (both those analyzed in the paper and those not yet analyzed) are publicly available.<span class="ltx_note ltx_role_footnote" id="footnote30"><sup class="ltx_note_mark">30</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">30</sup><span class="ltx_tag ltx_tag_note">30</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://github.com/bigscience-workshop/promptsource/tree/eval-hackathon" title="">github.com/bigscience-workshop/promptsource/tree/eval-hackathon</a></span></span></span></p>
</div>
<figure class="ltx_table" id="S4.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T5.2" style="width:433.6pt;height:77.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-35.8pt,6.4pt) scale(0.858439569977289,0.858439569977289) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T5.2.1">
<tr class="ltx_tr" id="S4.T5.2.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T5.2.1.1.1"><span class="ltx_text" id="S4.T5.2.1.1.1.1" style="font-size:90%;">Prompt name</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T5.2.1.1.2"><span class="ltx_text" id="S4.T5.2.1.1.2.1" style="font-size:90%;">Prompt</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T5.2.1.1.3"><span class="ltx_text" id="S4.T5.2.1.1.3.1" style="font-size:90%;">Target</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.2.1.2.1"><span class="ltx_text" id="S4.T5.2.1.2.1.1" style="font-size:90%;">a_good_translation-source+target</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.2.1.2.2"><span class="ltx_text" id="S4.T5.2.1.2.2.1" style="font-size:90%;">Given the following source text: [source sentence], a good L2 translation is:</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.2.1.2.3"><span class="ltx_text" id="S4.T5.2.1.2.3.1" style="font-size:90%;">[target sentence]</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.1.3">
<td class="ltx_td ltx_align_left" id="S4.T5.2.1.3.1"><span class="ltx_text" id="S4.T5.2.1.3.1.1" style="font-size:90%;">gpt3-target</span></td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.1.3.2"><span class="ltx_text" id="S4.T5.2.1.3.2.1" style="font-size:90%;">What is the L2 translation of the sentence: [source sentence]?</span></td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.1.3.3"><span class="ltx_text" id="S4.T5.2.1.3.3.1" style="font-size:90%;">[target sentence]</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.1.4">
<td class="ltx_td ltx_align_left" id="S4.T5.2.1.4.1"><span class="ltx_text" id="S4.T5.2.1.4.1.1" style="font-size:90%;">version-target</span></td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.1.4.2"><span class="ltx_text" id="S4.T5.2.1.4.2.1" style="font-size:90%;">if the original version says [source sentence]; then the L2 version should say:</span></td>
<td class="ltx_td ltx_align_left" id="S4.T5.2.1.4.3"><span class="ltx_text" id="S4.T5.2.1.4.3.1" style="font-size:90%;">[target sentence]</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.1.5">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T5.2.1.5.1"><span class="ltx_text" id="S4.T5.2.1.5.1.1" style="font-size:90%;">xglm-source+target</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T5.2.1.5.2"><span class="ltx_text" id="S4.T5.2.1.5.2.1" style="font-size:90%;">L1: [source sentence] = L2:</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T5.2.1.5.3"><span class="ltx_text" id="S4.T5.2.1.5.3.1" style="font-size:90%;">[target sentence]</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5: </span>Four prompts for the WMTâ€™14 dataset <cite class="ltx_cite ltx_citemacro_citep">(Bojar etÂ al., <a class="ltx_ref" href="#bib.bib24" title="">2014</a>)</cite> for MT evaluation. Above, â€œL1â€ and â€œL2â€ areÂ  replaced with language names (e.g.Â â€œBengaliâ€ and â€œRussianâ€).</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Infrastructure</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">Our framework extends EleutherAIâ€™s Language Model Evaluation Harness <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al., <a class="ltx_ref" href="#bib.bib51" title="">2021</a>)</cite> by integrating it with the <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS2.p1.1.1">promptsource</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Bach etÂ al., <a class="ltx_ref" href="#bib.bib8" title="">2022</a>)</cite> library described in <a class="ltx_ref" href="#S3.SS1.SSS4" title="3.1.4 Prompted Datasets â€£ 3.1 Training Dataset â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">3.1.4</span></a>. We release our Prompted Language Model Evaluation Harness as an open source library for people to use.
We use this framework in order to run the experiments and aggregate results.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Datasets</h4>
<section class="ltx_paragraph" id="S4.SS1.SSS3.Px1">
<h5 class="ltx_title ltx_title_paragraph">SuperGLUE</h5>
<div class="ltx_para" id="S4.SS1.SSS3.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS3.Px1.p1.1">We use a subset of the SuperGLUE <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="#bib.bib151" title="">2019</a>)</cite> evaluation suite of classification tasks, specifically: Ax-b, Ax-g, BoolQ, CB, WiC, WSC, and RTE tasks.
We excluded the remaining tasks because they require an order of magntiude more compute to run than all of these tasks we consider combined.
These tasks are English-only, and are thus included to facilitate comparison with prior work, which has primarily focused on English-only models. We also note that performance on these tasks has not yet been widely reported using zero- and one-shot prompt-based setting. T0 <cite class="ltx_cite ltx_citemacro_citep">(Sanh etÂ al., <a class="ltx_ref" href="#bib.bib128" title="">2022</a>)</cite> is the first exception, but that model is instruction-tuned and thus not directly comparable to models like BLOOM and OPT. For each task, we select a random sample of five prompts from <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS3.Px1.p1.1.1">promptsource</span> and evaluate all models on that set of prompts.
As with other prompting tasks in Evaluation HarnessÂ <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al., <a class="ltx_ref" href="#bib.bib51" title="">2021</a>)</cite>, the prediction of a model for a given prompt is measured using the maximum log likelihood among a set of specified candidate label strings associated with the prompt.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS3.Px2">
<h5 class="ltx_title ltx_title_paragraph">Machine Translation (MT)</h5>
<div class="ltx_para" id="S4.SS1.SSS3.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS3.Px2.p1.2">We evaluate BLOOM on three datasets (using ISO-639-1 codes to refer to languages): WMT14 en<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S4.SS1.SSS3.Px2.p1.1.m1.1"><semantics id="S4.SS1.SSS3.Px2.p1.1.m1.1a"><mo id="S4.SS1.SSS3.Px2.p1.1.m1.1.1" stretchy="false" xref="S4.SS1.SSS3.Px2.p1.1.m1.1.1.cmml">â†”</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.Px2.p1.1.m1.1b"><ci id="S4.SS1.SSS3.Px2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS3.Px2.p1.1.m1.1.1">â†”</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.Px2.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS3.Px2.p1.1.m1.1d">â†”</annotation></semantics></math>fr and en<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S4.SS1.SSS3.Px2.p1.2.m2.1"><semantics id="S4.SS1.SSS3.Px2.p1.2.m2.1a"><mo id="S4.SS1.SSS3.Px2.p1.2.m2.1.1" stretchy="false" xref="S4.SS1.SSS3.Px2.p1.2.m2.1.1.cmml">â†”</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.Px2.p1.2.m2.1b"><ci id="S4.SS1.SSS3.Px2.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS3.Px2.p1.2.m2.1.1">â†”</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.Px2.p1.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS3.Px2.p1.2.m2.1d">â†”</annotation></semantics></math>hi <cite class="ltx_cite ltx_citemacro_citep">(Bojar etÂ al., <a class="ltx_ref" href="#bib.bib24" title="">2014</a>)</cite>, Flores-101 <cite class="ltx_cite ltx_citemacro_citep">(Goyal etÂ al., <a class="ltx_ref" href="#bib.bib55" title="">2022</a>)</cite> and DiaBLa <cite class="ltx_cite ltx_citemacro_citep">(Bawden etÂ al., <a class="ltx_ref" href="#bib.bib11" title="">2020</a>)</cite>. We evaluate using the <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS3.Px2.p1.2.1">sacrebleu</span> <cite class="ltx_cite ltx_citemacro_citep">(Post, <a class="ltx_ref" href="#bib.bib114" title="">2018</a>)</cite> implementation of BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni etÂ al., <a class="ltx_ref" href="#bib.bib109" title="">2002</a>)</cite>, using default tokenisation for WMT and DiaBLa and <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS3.Px2.p1.2.2">spm-flores-101</span> for Flores.<span class="ltx_note ltx_role_footnote" id="footnote31"><sup class="ltx_note_mark">31</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">31</sup><span class="ltx_tag ltx_tag_note">31</span>BLEU+case:mixed+numrefs.1+smooth.exp+{13a,tok:spm-flores}+version:2.2.1</span></span></span> We use greedy decoding with generation proceeding until the EOS token, or additionally <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS3.Px2.p1.2.3">\n###\n</span> for the 1-shot case. The maximum generation length was set per dataset to be in line with what is typically used in the literature; specifically, 64 tokens for WMT14 and 512 tokens for Flores-101 and DiaBla.
Task-specific experimental design details are below.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS3.Px3">
<h5 class="ltx_title ltx_title_paragraph">Summarization</h5>
<div class="ltx_para" id="S4.SS1.SSS3.Px3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.Px3.p1.1">We evaluate summarization on the WikiLingua <cite class="ltx_cite ltx_citemacro_citep">(Ladhak etÂ al., <a class="ltx_ref" href="#bib.bib73" title="">2020</a>)</cite> dataset. WikiLingua is a multilingual summarization dataset comprising WikiHow article and step-by-step summary pairs. Pairs are aligned across multiple languages, with translation of source and summary further reviewed by an international translation team. One-shot conditional natural language generation has typically not been reported by models with size comparable to BLOOM. PaLM <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery etÂ al., <a class="ltx_ref" href="#bib.bib31" title="">2022</a>)</cite> is the first exception, and reports scores on WikiLingua; however, only the modelâ€™s ability to summarize in English was examined (-Â¿ en). By contrast, we opted to test BLOOMâ€™s inherent multilingual ability by assessing the abstractive summarization in the source language (e.g.Â vi -Â¿ vi). We focus on the nine languages (Arabic, English, Spanish, French, Hindi, Indonesian, Portuguese, Vietnamese and Chinese) which were amongst those targeted as part of the BigScience effort. </p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.Px3.p2">
<p class="ltx_p" id="S4.SS1.SSS3.Px3.p2.1">Natural language generation is notoriously challenging to evaluate, with multilingual generation compounding this challenge due to a lack of metric support. Following the suggestions by <cite class="ltx_cite ltx_citemacro_citet">Gehrmann etÂ al. (<a class="ltx_ref" href="#bib.bib53" title="">2022b</a>)</cite>, we report ROUGE-2, ROUGE-L <cite class="ltx_cite ltx_citemacro_citep">(Lin, <a class="ltx_ref" href="#bib.bib80" title="">2004</a>)</cite>,<span class="ltx_note ltx_role_footnote" id="footnote32"><sup class="ltx_note_mark">32</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">32</sup><span class="ltx_tag ltx_tag_note">32</span>For ROUGE, we used the Python implementation at
<br class="ltx_break"/><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://github.com/google-research/google-research/tree/master/rouge" title="">github.com/google-research/google-research/rouge</a>, commit <span class="ltx_text ltx_font_typewriter" id="footnote32.1">f935042</span>.</span></span></span> and Levenshtein distance. One important modification to ROUGE is using the SentencePiece tokenizer <cite class="ltx_cite ltx_citemacro_citep">(Kudo and Richardson, <a class="ltx_ref" href="#bib.bib70" title="">2018</a>)</cite> built from the Flores-101 dataset <cite class="ltx_cite ltx_citemacro_citep">(Goyal etÂ al., <a class="ltx_ref" href="#bib.bib55" title="">2022</a>)</cite>.
A naive approach would use a tokenizer based on English, but using a multilingual tokenizer improves the capacity to measure the fidelity of multilingual generations. To minimize inference time of the model we use the subsamples from the updated GEM benchmark <cite class="ltx_cite ltx_citemacro_citep">(Gehrmann etÂ al., <a class="ltx_ref" href="#bib.bib52" title="">2022a</a>)</cite> (3000 uniformly sampled test examples).
The authors note that there is minimal difference when comparing model performance between the subsamples and the full test sets.
For decoding and generation, we use the same procedure as described above for MT.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.4 </span>Baseline Models</h4>
<div class="ltx_para" id="S4.SS1.SSS4.p1">
<p class="ltx_p" id="S4.SS1.SSS4.p1.1">We use the following baseline models where appropriate (e.g.Â in settings where they support the language of the evaluation dataset):</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">mGPT <cite class="ltx_cite ltx_citemacro_citep">(Shliazhko etÂ al., <a class="ltx_ref" href="#bib.bib135" title="">2022</a>)</cite>, GPT-style models trained on 60 languages from Wikipedia and Common Crawl</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">GPT-Neo <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="#bib.bib21" title="">Black etÂ al., </a>)</cite>, GPT-J-6B <cite class="ltx_cite ltx_citemacro_citep">(Wang and Komatsuzaki, <a class="ltx_ref" href="#bib.bib152" title="">2021</a>)</cite>, and GPT-NeoX <cite class="ltx_cite ltx_citemacro_citep">(Black etÂ al., <a class="ltx_ref" href="#bib.bib22" title="">2022</a>)</cite>, a family of GPT-style models trained on The Pile <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al., <a class="ltx_ref" href="#bib.bib50" title="">2020</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">T0 <cite class="ltx_cite ltx_citemacro_citep">(Sanh etÂ al., <a class="ltx_ref" href="#bib.bib128" title="">2022</a>)</cite>, a variant of T5 <cite class="ltx_cite ltx_citemacro_citep">(Raffel etÂ al., <a class="ltx_ref" href="#bib.bib119" title="">2020</a>)</cite> that underwent multitask prompted finetuning on datasets from P3 <cite class="ltx_cite ltx_citemacro_citep">(Bach etÂ al., <a class="ltx_ref" href="#bib.bib8" title="">2022</a>)</cite>
</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1">OPT <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="#bib.bib169" title="">2022</a>)</cite>, a family of GPT-style model trained on a mixture of datasets including those from RoBERTa <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a class="ltx_ref" href="#bib.bib82" title="">2019</a>)</cite> and The Pile <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al., <a class="ltx_ref" href="#bib.bib50" title="">2020</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.1">XGLM <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al., <a class="ltx_ref" href="#bib.bib81" title="">2021</a>)</cite>, a GPT-style multilingual model trained on a variant of CC100 <cite class="ltx_cite ltx_citemacro_citep">(Conneau etÂ al., <a class="ltx_ref" href="#bib.bib35" title="">2020</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i6.p1">
<p class="ltx_p" id="S4.I1.i6.p1.1">M2M <cite class="ltx_cite ltx_citemacro_citep">(Fan etÂ al., <a class="ltx_ref" href="#bib.bib42" title="">2021</a>)</cite>, a massively multilingual model trained to translate between 100 languages</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i7.p1">
<p class="ltx_p" id="S4.I1.i7.p1.1">AlexaTM <cite class="ltx_cite ltx_citemacro_citep">(Soltan etÂ al., <a class="ltx_ref" href="#bib.bib139" title="">2022</a>)</cite>, an encoder-decoder model trained on a mixture of masked and causal language modeling on data from Wikipedia and mC4 <cite class="ltx_cite ltx_citemacro_citep">(Xue etÂ al., <a class="ltx_ref" href="#bib.bib165" title="">2021</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i8.p1">
<p class="ltx_p" id="S4.I1.i8.p1.1">mTk-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="#bib.bib157" title="">2022b</a>)</cite>, a variant of T5 that underwent multitask prompted finetuning on datasets from Super-NaturalInstructions</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i9.p1">
<p class="ltx_p" id="S4.I1.i9.p1.1">Codex <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a class="ltx_ref" href="#bib.bib29" title="">2021</a>)</cite>, a family of GPT models finetuned on code from GitHub</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i10.p1">
<p class="ltx_p" id="S4.I1.i10.p1.1">GPT-fr <cite class="ltx_cite ltx_citemacro_citep">(Simoulin and CrabbÃ©, <a class="ltx_ref" href="#bib.bib137" title="">2021</a>)</cite>, a GPT-style model trained on French text</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>SuperGLUE</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Figure <a class="ltx_ref" href="#S4.F7" title="Figure 7 â€£ 4.2 SuperGLUE â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">7</span></a> shows zero- and one-shot performance on SuperGLUE. In both settings, on entailment tasks (BoolQ and CB), performance is well above random chance for BLOOM, T0, OPT, and GPT-J. On other tasks, while the best prompts do better, the average performance across prompts hovers around chance, suggesting that the success of individual prompts is primarily statistical variation. There is some signal for BLOOM in the diagnostic (Ax-b and Ax-g) datasets. The exception is the T0 model, which shows strong performance. However, this model is finetuned in the multitask setting (similar to BLOOMZ, see <a class="ltx_ref" href="#S4.SS7" title="4.7 Multitask Finetuning â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">4.7</span></a>) in order to improve performance in zero-shot prompting settings, and thus is not directly comparable to the other models shown here.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">As models go from zero-shot to one-shot, variability is reduced across all prompts and models and performance slightly and inconsistently increases. Notably, BLOOM sees more of an increase in performance than comparable models when going from zero-shot to one-shot, as it is generally behind OPT in the zero-shot setting but matches or improves on it in the one-shot setting, even though it has only partly been trained on English. This may be because a multilingual language model gains more certainty in the language of input and output with a longer context.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="415" id="S4.F7.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S4.F7.3.2" style="font-size:90%;">Performance of various LLMs on subset of tasks from SuperGLUE benchmark in zero- and one-shot prompt-based setting.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">We perform an additional analysis comparing BLOOM models across model sizes. As a baseline, we also measure the average one-shot accuracy of OPT models of similar sizes (350M parameters to 175B parameters).<span class="ltx_note ltx_role_footnote" id="footnote33"><sup class="ltx_note_mark">33</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">33</sup><span class="ltx_tag ltx_tag_note">33</span>We do not evaluate OPT-66B because of the lack of a similarly-sized BLOOM model.</span></span></span> Figure <a class="ltx_ref" href="#S4.F8" title="Figure 8 â€£ 4.2 SuperGLUE â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">8</span></a> shows the accuracy of each prompt on each task across model scales. Both OPT and BLOOM model families improve very slightly with scale, with only models over 2 billion parameters showing signal, and there is no consistent difference between families across all tasks. In the 1-shot setting, BLOOM-176B is ahead of OPT-175B on Ax-b, CB, WSC and WiC, and matches it on the other tasks, suggesting that multilinguality does not limit performance of BLOOM on English-only tasks in the zero-shot setting.</p>
</div>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="623" id="S4.F8.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.2.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S4.F8.3.2" style="font-size:90%;">Comparison of the scaling of BLOOM versus OPT on each SuperGLUE one-shot task. Each point represents the average accuracy of a model within the BLOOM or OPT family of models on one of the five task prompts. The number of parameters on the x-axis is presented in log-scale.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Machine Translation</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In addition to the results presented here, a more detailed analysis of BLOOMâ€™s MT quality can be found in <cite class="ltx_cite ltx_citemacro_citep">(Bawden and Yvon, <a class="ltx_ref" href="#bib.bib10" title="">2023</a>)</cite>.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>WMT</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.2">WMT results for BLOOM-176B in the zero-shot and 1-shot setting are given in TableÂ <a class="ltx_ref" href="#S4.T6" title="Table 6 â€£ 4.3.1 WMT â€£ 4.3 Machine Translation â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">6</span></a>.
The best prompts tend to be the more verbose ones; the â€œversion-targetâ€ prompt is consistently better and the â€œgpt3-targetâ€ and â€œxglm-source+targetâ€ prompts have very poor performance, especially for zero-shot. In the one-shot setting, BLOOM can, with the right prompt, perform competent translation, although it is behind dedicated (supervised) models such as M2M-100 (43.8 BLEU for English<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p1.1.m1.1"><semantics id="S4.SS3.SSS1.p1.1.m1.1a"><mo id="S4.SS3.SSS1.p1.1.m1.1.1" stretchy="false" xref="S4.SS3.SSS1.p1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.1.m1.1b"><ci id="S4.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p1.1.m1.1d">â†’</annotation></semantics></math>French and 40.4 for French<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p1.2.m2.1"><semantics id="S4.SS3.SSS1.p1.2.m2.1a"><mo id="S4.SS3.SSS1.p1.2.m2.1.1" stretchy="false" xref="S4.SS3.SSS1.p1.2.m2.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.2.m2.1b"><ci id="S4.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS1.p1.2.m2.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p1.2.m2.1d">â†’</annotation></semantics></math>English, compared to 34.2 and 35.4 BLEU for BLOOM).
The two major problems observed, particularly in the zero-shot setting, are (i)Â over-generation and (ii)Â not producing the correct language (an obvious prerequisite for a good translation). Both of these aspects are greatly improved as the number of few-shot examples is increased. </p>
</div>
<figure class="ltx_table" id="S4.T6">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T6.4">
<tr class="ltx_tr" id="S4.T6.4.4">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T6.4.4.5" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.4.5.1" style="font-size:90%;">Prompt</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T6.1.1.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_text" id="S4.T6.1.1.1.1" style="font-size:90%;">en </span><math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T6.1.1.1.m1.1"><semantics id="S4.T6.1.1.1.m1.1a"><mo id="S4.T6.1.1.1.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T6.1.1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T6.1.1.1.m1.1b"><ci id="S4.T6.1.1.1.m1.1.1.cmml" xref="S4.T6.1.1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.1.1.1.m1.1d">â†’</annotation></semantics></math><span class="ltx_text" id="S4.T6.1.1.1.2" style="font-size:90%;"> fr</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T6.2.2.2" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_text" id="S4.T6.2.2.2.1" style="font-size:90%;">fr </span><math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T6.2.2.2.m1.1"><semantics id="S4.T6.2.2.2.m1.1a"><mo id="S4.T6.2.2.2.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T6.2.2.2.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T6.2.2.2.m1.1b"><ci id="S4.T6.2.2.2.m1.1.1.cmml" xref="S4.T6.2.2.2.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.2.2.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.2.2.2.m1.1d">â†’</annotation></semantics></math><span class="ltx_text" id="S4.T6.2.2.2.2" style="font-size:90%;"> en</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T6.3.3.3" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_text" id="S4.T6.3.3.3.1" style="font-size:90%;">en </span><math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T6.3.3.3.m1.1"><semantics id="S4.T6.3.3.3.m1.1a"><mo id="S4.T6.3.3.3.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T6.3.3.3.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T6.3.3.3.m1.1b"><ci id="S4.T6.3.3.3.m1.1.1.cmml" xref="S4.T6.3.3.3.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.3.3.3.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.3.3.3.m1.1d">â†’</annotation></semantics></math><span class="ltx_text" id="S4.T6.3.3.3.2" style="font-size:90%;"> hi</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T6.4.4.4" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_text" id="S4.T6.4.4.4.1" style="font-size:90%;">hi </span><math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T6.4.4.4.m1.1"><semantics id="S4.T6.4.4.4.m1.1a"><mo id="S4.T6.4.4.4.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T6.4.4.4.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T6.4.4.4.m1.1b"><ci id="S4.T6.4.4.4.m1.1.1.cmml" xref="S4.T6.4.4.4.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.4.4.4.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.4.4.4.m1.1d">â†’</annotation></semantics></math><span class="ltx_text" id="S4.T6.4.4.4.2" style="font-size:90%;"> en</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.4.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.4.5.1" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.5.1.1" style="font-size:90%;">Shots</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T6.4.5.2" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.5.2.1" style="font-size:90%;">0</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T6.4.5.3" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.5.3.1" style="font-size:90%;">1</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T6.4.5.4" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.5.4.1" style="font-size:90%;">0</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T6.4.5.5" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.5.5.1" style="font-size:90%;">1</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T6.4.5.6" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.5.6.1" style="font-size:90%;">0</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T6.4.5.7" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.5.7.1" style="font-size:90%;">1</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T6.4.5.8" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.5.8.1" style="font-size:90%;">0</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T6.4.5.9" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.5.9.1" style="font-size:90%;">1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.4.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.4.6.1" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.6.1.1" style="font-size:90%;">a_good_translation-source+target</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T6.4.6.2" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.6.2.1" style="font-size:90%;">15.38</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T6.4.6.3" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.6.3.1" style="font-size:90%;">36.39</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T6.4.6.4" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.6.4.1" style="font-size:90%;">14.15</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T6.4.6.5" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.6.5.1" style="font-size:90%;">36.56</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T6.4.6.6" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.6.6.1" style="font-size:90%;">1.90</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T6.4.6.7" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.6.7.1" style="font-size:90%;">14.49</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T6.4.6.8" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.6.8.1" style="font-size:90%;">10.19</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T6.4.6.9" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.6.9.1" style="font-size:90%;">24.60</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.4.7">
<td class="ltx_td ltx_align_left" id="S4.T6.4.7.1" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.7.1.1" style="font-size:90%;">gpt3-target</span></td>
<td class="ltx_td ltx_align_right" id="S4.T6.4.7.2" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.7.2.1" style="font-size:90%;">7.90</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T6.4.7.3" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.7.3.1" style="font-size:90%;">32.55</span></td>
<td class="ltx_td ltx_align_right" id="S4.T6.4.7.4" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.7.4.1" style="font-size:90%;">12.73</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T6.4.7.5" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.7.5.1" style="font-size:90%;">33.14</span></td>
<td class="ltx_td ltx_align_right" id="S4.T6.4.7.6" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.7.6.1" style="font-size:90%;">0.26</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T6.4.7.7" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.7.7.1" style="font-size:90%;">6.51</span></td>
<td class="ltx_td ltx_align_right" id="S4.T6.4.7.8" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.7.8.1" style="font-size:90%;">0.66</span></td>
<td class="ltx_td ltx_align_right" id="S4.T6.4.7.9" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.7.9.1" style="font-size:90%;">9.98</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.4.8">
<td class="ltx_td ltx_align_left" id="S4.T6.4.8.1" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.8.1.1" style="font-size:90%;">version-target</span></td>
<td class="ltx_td ltx_align_right" id="S4.T6.4.8.2" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.8.2.1" style="font-size:90%;">21.96</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T6.4.8.3" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.8.3.1" style="font-size:90%;">34.22</span></td>
<td class="ltx_td ltx_align_right" id="S4.T6.4.8.4" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.8.4.1" style="font-size:90%;">26.79</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T6.4.8.5" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.8.5.1" style="font-size:90%;">35.42</span></td>
<td class="ltx_td ltx_align_right" id="S4.T6.4.8.6" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.8.6.1" style="font-size:90%;">1.96</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T6.4.8.7" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.8.7.1" style="font-size:90%;">13.95</span></td>
<td class="ltx_td ltx_align_right" id="S4.T6.4.8.8" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.8.8.1" style="font-size:90%;">11.48</span></td>
<td class="ltx_td ltx_align_right" id="S4.T6.4.8.9" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.8.9.1" style="font-size:90%;">25.80</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.4.9">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T6.4.9.1" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.9.1.1" style="font-size:90%;">xglm-source+target</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T6.4.9.2" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.9.2.1" style="font-size:90%;">14.91</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S4.T6.4.9.3" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.9.3.1" style="font-size:90%;">27.83</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T6.4.9.4" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.9.4.1" style="font-size:90%;">15.52</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S4.T6.4.9.5" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.9.5.1" style="font-size:90%;">34.51</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T6.4.9.6" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.9.6.1" style="font-size:90%;">6.80</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S4.T6.4.9.7" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.9.7.1" style="font-size:90%;">13.62</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T6.4.9.8" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.9.8.1" style="font-size:90%;">12.05</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T6.4.9.9" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S4.T6.4.9.9.1" style="font-size:90%;">25.04</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 6: </span>WMTâ€™14 zero- and one-shot results (BLEU) for BLOOM-176B. The prompts used are described in TableÂ <a class="ltx_ref" href="#S4.T5" title="Table 5 â€£ 4.1.1 Prompts â€£ 4.1 Experimental Design â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">5</span></a>.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>DiaBLa</h4>
<figure class="ltx_table" id="S4.T7">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T7.6">
<tr class="ltx_tr" id="S4.T7.2.2">
<td class="ltx_td ltx_border_tt" id="S4.T7.2.2.3"></td>
<td class="ltx_td ltx_border_tt" id="S4.T7.2.2.4"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T7.1.1.1">
<span class="ltx_text" id="S4.T7.1.1.1.1" style="font-size:90%;">en</span><math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T7.1.1.1.m1.1"><semantics id="S4.T7.1.1.1.m1.1a"><mo id="S4.T7.1.1.1.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T7.1.1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T7.1.1.1.m1.1b"><ci id="S4.T7.1.1.1.m1.1.1.cmml" xref="S4.T7.1.1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T7.1.1.1.m1.1d">â†’</annotation></semantics></math><span class="ltx_text" id="S4.T7.1.1.1.2" style="font-size:90%;">fr</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T7.2.2.2">
<span class="ltx_text" id="S4.T7.2.2.2.1" style="font-size:90%;">fr</span><math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T7.2.2.2.m1.1"><semantics id="S4.T7.2.2.2.m1.1a"><mo id="S4.T7.2.2.2.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T7.2.2.2.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T7.2.2.2.m1.1b"><ci id="S4.T7.2.2.2.m1.1.1.cmml" xref="S4.T7.2.2.2.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.2.2.2.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T7.2.2.2.m1.1d">â†’</annotation></semantics></math><span class="ltx_text" id="S4.T7.2.2.2.2" style="font-size:90%;">en</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T7.6.7">
<td class="ltx_td ltx_align_left" id="S4.T7.6.7.1"><span class="ltx_text" id="S4.T7.6.7.1.1" style="font-size:90%;">1-shot context</span></td>
<td class="ltx_td ltx_align_left" id="S4.T7.6.7.2"><span class="ltx_text" id="S4.T7.6.7.2.1" style="font-size:90%;">Truncate</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.6.7.3"><span class="ltx_text" id="S4.T7.6.7.3.1" style="font-size:90%;">BLEU</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.6.7.4"><span class="ltx_text" id="S4.T7.6.7.4.1" style="font-size:90%;">COMET</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.6.7.5"><span class="ltx_text" id="S4.T7.6.7.5.1" style="font-size:90%;">BLEU</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.6.7.6"><span class="ltx_text" id="S4.T7.6.7.6.1" style="font-size:90%;">COMET</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.3.3">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T7.3.3.2" rowspan="2"><span class="ltx_text" id="S4.T7.3.3.2.1" style="font-size:90%;">Rand.</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T7.3.3.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T7.3.3.1.m1.1"><semantics id="S4.T7.3.3.1.m1.1a"><mo id="S4.T7.3.3.1.m1.1.1" mathsize="90%" xref="S4.T7.3.3.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T7.3.3.1.m1.1b"><times id="S4.T7.3.3.1.m1.1.1.cmml" xref="S4.T7.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.3.3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T7.3.3.1.m1.1d">Ã—</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T7.3.3.3"><span class="ltx_text" id="S4.T7.3.3.3.1" style="font-size:90%;">5.7</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T7.3.3.4"><span class="ltx_text" id="S4.T7.3.3.4.1" style="font-size:90%;">0.342</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T7.3.3.5"><span class="ltx_text" id="S4.T7.3.3.5.1" style="font-size:90%;">12.1</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T7.3.3.6"><span class="ltx_text" id="S4.T7.3.3.6.1" style="font-size:90%;">0.614</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.4.4">
<td class="ltx_td ltx_align_left" id="S4.T7.4.4.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T7.4.4.1.m1.1"><semantics id="S4.T7.4.4.1.m1.1a"><mi id="S4.T7.4.4.1.m1.1.1" mathsize="90%" mathvariant="normal" xref="S4.T7.4.4.1.m1.1.1.cmml">âœ“</mi><annotation-xml encoding="MathML-Content" id="S4.T7.4.4.1.m1.1b"><ci id="S4.T7.4.4.1.m1.1.1.cmml" xref="S4.T7.4.4.1.m1.1.1">âœ“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.4.4.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T7.4.4.1.m1.1d">âœ“</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S4.T7.4.4.2"><span class="ltx_text" id="S4.T7.4.4.2.1" style="font-size:90%;">37.6</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.4.4.3"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.3.1" style="font-size:90%;">0.634</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.4.4.4"><span class="ltx_text" id="S4.T7.4.4.4.1" style="font-size:90%;">41.4</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.4.4.5"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.5.1" style="font-size:90%;">0.757</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.5.5">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T7.5.5.2" rowspan="2"><span class="ltx_text" id="S4.T7.5.5.2.1" style="font-size:90%;">Prev.</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.5.5.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T7.5.5.1.m1.1"><semantics id="S4.T7.5.5.1.m1.1a"><mo id="S4.T7.5.5.1.m1.1.1" mathsize="90%" xref="S4.T7.5.5.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T7.5.5.1.m1.1b"><times id="S4.T7.5.5.1.m1.1.1.cmml" xref="S4.T7.5.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.5.5.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T7.5.5.1.m1.1d">Ã—</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.5.5.3"><span class="ltx_text" id="S4.T7.5.5.3.1" style="font-size:90%;">6.1</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.5.5.4"><span class="ltx_text" id="S4.T7.5.5.4.1" style="font-size:90%;">0.328</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.5.5.5"><span class="ltx_text" id="S4.T7.5.5.5.1" style="font-size:90%;">12.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.5.5.6"><span class="ltx_text" id="S4.T7.5.5.6.1" style="font-size:90%;">0.617</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.6.6">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T7.6.6.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T7.6.6.1.m1.1"><semantics id="S4.T7.6.6.1.m1.1a"><mi id="S4.T7.6.6.1.m1.1.1" mathsize="90%" mathvariant="normal" xref="S4.T7.6.6.1.m1.1.1.cmml">âœ“</mi><annotation-xml encoding="MathML-Content" id="S4.T7.6.6.1.m1.1b"><ci id="S4.T7.6.6.1.m1.1.1.cmml" xref="S4.T7.6.6.1.m1.1.1">âœ“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.6.6.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T7.6.6.1.m1.1d">âœ“</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T7.6.6.2"><span class="ltx_text ltx_font_bold" id="S4.T7.6.6.2.1" style="font-size:90%;">38.5</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T7.6.6.3"><span class="ltx_text" id="S4.T7.6.6.3.1" style="font-size:90%;">0.614</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T7.6.6.4"><span class="ltx_text ltx_font_bold" id="S4.T7.6.6.4.1" style="font-size:90%;">41.6</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T7.6.6.5"><span class="ltx_text" id="S4.T7.6.6.5.1" style="font-size:90%;">0.751</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 7: </span>DiaBLa 1-shot results (BLEU) for the â€œxglm-source+targetâ€ prompt when using the previous or a random sentence as the 1-shot example (with and without truncation of outputs). In bold the best results for each direction.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">TableÂ <a class="ltx_ref" href="#S4.T7" title="Table 7 â€£ 4.3.2 DiaBLa â€£ 4.3 Machine Translation â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">7</span></a> shows results testing the use of linguistic context with DiaBLa, a parallel dataset of informal bilingual dialogues. In a 1-shot context and using the â€œxglm-source+targetâ€ prompt, we compare the effect of using a random test set example as the 1-shot example versus using the previous dialogue utterance. In light of the overgeneration issues seen and in order to evaluate the quality of the prediction independently of overgeneration, we report results for both original outputs and after applying a custom truncation function.<span class="ltx_note ltx_role_footnote" id="footnote34"><sup class="ltx_note_mark">34</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">34</sup><span class="ltx_tag ltx_tag_note">34</span>The truncation rule is specific to the â€œxglm-source+targetâ€ prompt and the fact that overgeneration consists of repeating the prompt pattern. Anything after a first newline or the regular expression pattern <span class="ltx_text ltx_font_typewriter" id="footnote34.1">= .+?:</span> is discarded.</span></span></span> The automatic results are inconclusive, with little difference between scores (BLEU scores are higher for previous context but COMET scores are lower). Despite these results, there is evidence in the predictions themselves that the model is able to use the context of the 1-shot example to make translation choices. See <cite class="ltx_cite ltx_citemacro_citep">(Bawden and Yvon, <a class="ltx_ref" href="#bib.bib10" title="">2023</a>)</cite> for examples and further analysis.
</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Flores</h4>
<figure class="ltx_table" id="S4.T8">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_table ltx_flex_size_2 ltx_align_center" id="S4.T7.st1">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T7.st1.2" style="width:99.2pt;height:170.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-8.1pt,13.9pt) scale(0.86,0.86) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T7.st1.2.2">
<tr class="ltx_tr" id="S4.T7.st1.2.2.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T7.st1.1.1.1.1">
<span class="ltx_text" id="S4.T7.st1.1.1.1.1.1" style="font-size:80%;">Src</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T7.st1.1.1.1.1.m1.1"><semantics id="S4.T7.st1.1.1.1.1.m1.1a"><mo id="S4.T7.st1.1.1.1.1.m1.1.1" mathsize="80%" stretchy="false" xref="S4.T7.st1.1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T7.st1.1.1.1.1.m1.1b"><ci id="S4.T7.st1.1.1.1.1.m1.1.1.cmml" xref="S4.T7.st1.1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.st1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T7.st1.1.1.1.1.m1.1d">â†“</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T7.st1.2.2.2.2">
<span class="ltx_text" id="S4.T7.st1.2.2.2.2.1" style="font-size:80%;">Trg</span><math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T7.st1.2.2.2.2.m1.1"><semantics id="S4.T7.st1.2.2.2.2.m1.1a"><mo id="S4.T7.st1.2.2.2.2.m1.1.1" mathsize="80%" stretchy="false" xref="S4.T7.st1.2.2.2.2.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T7.st1.2.2.2.2.m1.1b"><ci id="S4.T7.st1.2.2.2.2.m1.1.1.cmml" xref="S4.T7.st1.2.2.2.2.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.st1.2.2.2.2.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T7.st1.2.2.2.2.m1.1d">â†’</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T7.st1.2.2.2.3"><span class="ltx_text" id="S4.T7.st1.2.2.2.3.1" style="font-size:80%;">en</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T7.st1.2.2.2.4"><span class="ltx_text" id="S4.T7.st1.2.2.2.4.1" style="font-size:80%;">bn</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T7.st1.2.2.2.5"><span class="ltx_text" id="S4.T7.st1.2.2.2.5.1" style="font-size:80%;">hi</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T7.st1.2.2.2.6"><span class="ltx_text" id="S4.T7.st1.2.2.2.6.1" style="font-size:80%;">sw</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T7.st1.2.2.2.7"><span class="ltx_text" id="S4.T7.st1.2.2.2.7.1" style="font-size:80%;">yo</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st1.2.2.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st1.2.2.3.1" rowspan="2"><span class="ltx_text" id="S4.T7.st1.2.2.3.1.1" style="font-size:80%;">en</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st1.2.2.3.2"><span class="ltx_text" id="S4.T7.st1.2.2.3.2.1" style="font-size:80%;">BLOOM</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st1.2.2.3.3"><span class="ltx_text" id="S4.T7.st1.2.2.3.3.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st1.2.2.3.4"><span class="ltx_text" id="S4.T7.st1.2.2.3.4.1" style="font-size:80%;">24.6</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st1.2.2.3.5"><span class="ltx_text" id="S4.T7.st1.2.2.3.5.1" style="font-size:80%;">27.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st1.2.2.3.6"><span class="ltx_text" id="S4.T7.st1.2.2.3.6.1" style="font-size:80%;">20.5</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st1.2.2.3.7"><span class="ltx_text" id="S4.T7.st1.2.2.3.7.1" style="font-size:80%;">2.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st1.2.2.4">
<td class="ltx_td ltx_align_left" id="S4.T7.st1.2.2.4.1"><span class="ltx_text" id="S4.T7.st1.2.2.4.1.1" style="font-size:80%;">M2M</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st1.2.2.4.2"><span class="ltx_text" id="S4.T7.st1.2.2.4.2.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st1.2.2.4.3"><span class="ltx_text" id="S4.T7.st1.2.2.4.3.1" style="font-size:80%;">23.0</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st1.2.2.4.4"><span class="ltx_text" id="S4.T7.st1.2.2.4.4.1" style="font-size:80%;">28.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st1.2.2.4.5"><span class="ltx_text" id="S4.T7.st1.2.2.4.5.1" style="font-size:80%;">26.9</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st1.2.2.4.6"><span class="ltx_text" id="S4.T7.st1.2.2.4.6.1" style="font-size:80%;">2.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st1.2.2.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st1.2.2.5.1" rowspan="2"><span class="ltx_text" id="S4.T7.st1.2.2.5.1.1" style="font-size:80%;">bn</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st1.2.2.5.2"><span class="ltx_text" id="S4.T7.st1.2.2.5.2.1" style="font-size:80%;">BLOOM</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st1.2.2.5.3"><span class="ltx_text" id="S4.T7.st1.2.2.5.3.1" style="font-size:80%;">29.9</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st1.2.2.5.4"><span class="ltx_text" id="S4.T7.st1.2.2.5.4.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st1.2.2.5.5"><span class="ltx_text" id="S4.T7.st1.2.2.5.5.1" style="font-size:80%;">16.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st1.2.2.5.6"><span class="ltx_text" id="S4.T7.st1.2.2.5.6.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st1.2.2.5.7"><span class="ltx_text" id="S4.T7.st1.2.2.5.7.1" style="font-size:80%;">â€“</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st1.2.2.6">
<td class="ltx_td ltx_align_left" id="S4.T7.st1.2.2.6.1"><span class="ltx_text" id="S4.T7.st1.2.2.6.1.1" style="font-size:80%;">M2M</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st1.2.2.6.2"><span class="ltx_text" id="S4.T7.st1.2.2.6.2.1" style="font-size:80%;">22.9</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st1.2.2.6.3"><span class="ltx_text" id="S4.T7.st1.2.2.6.3.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st1.2.2.6.4"><span class="ltx_text" id="S4.T7.st1.2.2.6.4.1" style="font-size:80%;">21.8</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st1.2.2.6.5"><span class="ltx_text" id="S4.T7.st1.2.2.6.5.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st1.2.2.6.6"><span class="ltx_text" id="S4.T7.st1.2.2.6.6.1" style="font-size:80%;">â€“</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st1.2.2.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st1.2.2.7.1" rowspan="2"><span class="ltx_text" id="S4.T7.st1.2.2.7.1.1" style="font-size:80%;">hi</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st1.2.2.7.2"><span class="ltx_text" id="S4.T7.st1.2.2.7.2.1" style="font-size:80%;">BLOOM</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st1.2.2.7.3"><span class="ltx_text" id="S4.T7.st1.2.2.7.3.1" style="font-size:80%;">35.1</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st1.2.2.7.4"><span class="ltx_text" id="S4.T7.st1.2.2.7.4.1" style="font-size:80%;">23.8</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st1.2.2.7.5"><span class="ltx_text" id="S4.T7.st1.2.2.7.5.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st1.2.2.7.6"><span class="ltx_text" id="S4.T7.st1.2.2.7.6.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st1.2.2.7.7"><span class="ltx_text" id="S4.T7.st1.2.2.7.7.1" style="font-size:80%;">â€“</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st1.2.2.8">
<td class="ltx_td ltx_align_left" id="S4.T7.st1.2.2.8.1"><span class="ltx_text" id="S4.T7.st1.2.2.8.1.1" style="font-size:80%;">M2M</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st1.2.2.8.2"><span class="ltx_text" id="S4.T7.st1.2.2.8.2.1" style="font-size:80%;">27.9</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st1.2.2.8.3"><span class="ltx_text" id="S4.T7.st1.2.2.8.3.1" style="font-size:80%;">21.8</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st1.2.2.8.4"><span class="ltx_text" id="S4.T7.st1.2.2.8.4.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st1.2.2.8.5"><span class="ltx_text" id="S4.T7.st1.2.2.8.5.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st1.2.2.8.6"><span class="ltx_text" id="S4.T7.st1.2.2.8.6.1" style="font-size:80%;">â€“</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st1.2.2.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st1.2.2.9.1" rowspan="2"><span class="ltx_text" id="S4.T7.st1.2.2.9.1.1" style="font-size:80%;">sw</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st1.2.2.9.2"><span class="ltx_text" id="S4.T7.st1.2.2.9.2.1" style="font-size:80%;">BLOOM</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st1.2.2.9.3"><span class="ltx_text" id="S4.T7.st1.2.2.9.3.1" style="font-size:80%;">37.4</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st1.2.2.9.4"><span class="ltx_text" id="S4.T7.st1.2.2.9.4.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st1.2.2.9.5"><span class="ltx_text" id="S4.T7.st1.2.2.9.5.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st1.2.2.9.6"><span class="ltx_text" id="S4.T7.st1.2.2.9.6.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st1.2.2.9.7"><span class="ltx_text" id="S4.T7.st1.2.2.9.7.1" style="font-size:80%;">1.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st1.2.2.10">
<td class="ltx_td ltx_align_left" id="S4.T7.st1.2.2.10.1"><span class="ltx_text" id="S4.T7.st1.2.2.10.1.1" style="font-size:80%;">M2M</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st1.2.2.10.2"><span class="ltx_text" id="S4.T7.st1.2.2.10.2.1" style="font-size:80%;">30.4</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st1.2.2.10.3"><span class="ltx_text" id="S4.T7.st1.2.2.10.3.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st1.2.2.10.4"><span class="ltx_text" id="S4.T7.st1.2.2.10.4.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st1.2.2.10.5"><span class="ltx_text" id="S4.T7.st1.2.2.10.5.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st1.2.2.10.6"><span class="ltx_text" id="S4.T7.st1.2.2.10.6.1" style="font-size:80%;">1.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st1.2.2.11">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T7.st1.2.2.11.1" rowspan="2"><span class="ltx_text" id="S4.T7.st1.2.2.11.1.1" style="font-size:80%;">yo</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st1.2.2.11.2"><span class="ltx_text" id="S4.T7.st1.2.2.11.2.1" style="font-size:80%;">BLOOM</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st1.2.2.11.3"><span class="ltx_text" id="S4.T7.st1.2.2.11.3.1" style="font-size:80%;">4.1</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st1.2.2.11.4"><span class="ltx_text" id="S4.T7.st1.2.2.11.4.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st1.2.2.11.5"><span class="ltx_text" id="S4.T7.st1.2.2.11.5.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st1.2.2.11.6"><span class="ltx_text" id="S4.T7.st1.2.2.11.6.1" style="font-size:80%;">0.9</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st1.2.2.11.7"><span class="ltx_text" id="S4.T7.st1.2.2.11.7.1" style="font-size:80%;">â€“</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st1.2.2.12">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T7.st1.2.2.12.1"><span class="ltx_text" id="S4.T7.st1.2.2.12.1.1" style="font-size:80%;">M2M</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T7.st1.2.2.12.2"><span class="ltx_text" id="S4.T7.st1.2.2.12.2.1" style="font-size:80%;">4.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T7.st1.2.2.12.3"><span class="ltx_text" id="S4.T7.st1.2.2.12.3.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T7.st1.2.2.12.4"><span class="ltx_text" id="S4.T7.st1.2.2.12.4.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T7.st1.2.2.12.5"><span class="ltx_text" id="S4.T7.st1.2.2.12.5.1" style="font-size:80%;">1.9</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T7.st1.2.2.12.6"><span class="ltx_text" id="S4.T7.st1.2.2.12.6.1" style="font-size:80%;">â€“</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T7.st1.6.1.1" style="font-size:113%;">(a)</span> </span><span class="ltx_text" id="S4.T7.st1.7.2" style="font-size:113%;">Low-resource languages</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_table ltx_flex_size_2 ltx_align_center" id="S4.T7.st2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T7.st2.2" style="width:114.9pt;height:201.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-9.4pt,16.4pt) scale(0.86,0.86) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T7.st2.2.2">
<tr class="ltx_tr" id="S4.T7.st2.2.2.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T7.st2.1.1.1.1">
<span class="ltx_text" id="S4.T7.st2.1.1.1.1.1" style="font-size:80%;">Src</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T7.st2.1.1.1.1.m1.1"><semantics id="S4.T7.st2.1.1.1.1.m1.1a"><mo id="S4.T7.st2.1.1.1.1.m1.1.1" mathsize="80%" stretchy="false" xref="S4.T7.st2.1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T7.st2.1.1.1.1.m1.1b"><ci id="S4.T7.st2.1.1.1.1.m1.1.1.cmml" xref="S4.T7.st2.1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.st2.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T7.st2.1.1.1.1.m1.1d">â†“</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T7.st2.2.2.2.2">
<span class="ltx_text" id="S4.T7.st2.2.2.2.2.1" style="font-size:80%;">Trg</span><math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T7.st2.2.2.2.2.m1.1"><semantics id="S4.T7.st2.2.2.2.2.m1.1a"><mo id="S4.T7.st2.2.2.2.2.m1.1.1" mathsize="80%" stretchy="false" xref="S4.T7.st2.2.2.2.2.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T7.st2.2.2.2.2.m1.1b"><ci id="S4.T7.st2.2.2.2.2.m1.1.1.cmml" xref="S4.T7.st2.2.2.2.2.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.st2.2.2.2.2.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T7.st2.2.2.2.2.m1.1d">â†’</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T7.st2.2.2.2.3"><span class="ltx_text" id="S4.T7.st2.2.2.2.3.1" style="font-size:80%;">ca</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T7.st2.2.2.2.4"><span class="ltx_text" id="S4.T7.st2.2.2.2.4.1" style="font-size:80%;">es</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T7.st2.2.2.2.5"><span class="ltx_text" id="S4.T7.st2.2.2.2.5.1" style="font-size:80%;">fr</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T7.st2.2.2.2.6"><span class="ltx_text" id="S4.T7.st2.2.2.2.6.1" style="font-size:80%;">gl</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T7.st2.2.2.2.7"><span class="ltx_text" id="S4.T7.st2.2.2.2.7.1" style="font-size:80%;">it</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T7.st2.2.2.2.8"><span class="ltx_text" id="S4.T7.st2.2.2.2.8.1" style="font-size:80%;">pt</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st2.2.2.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st2.2.2.3.1" rowspan="2"><span class="ltx_text" id="S4.T7.st2.2.2.3.1.1" style="font-size:80%;">ca</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st2.2.2.3.2"><span class="ltx_text" id="S4.T7.st2.2.2.3.2.1" style="font-size:80%;">BLOOM</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.3.3"><span class="ltx_text" id="S4.T7.st2.2.2.3.3.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.3.4"><span class="ltx_text" id="S4.T7.st2.2.2.3.4.1" style="font-size:80%;">28.9</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.3.5"><span class="ltx_text" id="S4.T7.st2.2.2.3.5.1" style="font-size:80%;">33.8</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.3.6"><span class="ltx_text" id="S4.T7.st2.2.2.3.6.1" style="font-size:80%;">19.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.3.7"><span class="ltx_text" id="S4.T7.st2.2.2.3.7.1" style="font-size:80%;">19.8</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.3.8"><span class="ltx_text" id="S4.T7.st2.2.2.3.8.1" style="font-size:80%;">33.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st2.2.2.4">
<td class="ltx_td ltx_align_left" id="S4.T7.st2.2.2.4.1"><span class="ltx_text" id="S4.T7.st2.2.2.4.1.1" style="font-size:80%;">M2M</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.4.2"><span class="ltx_text" id="S4.T7.st2.2.2.4.2.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.4.3"><span class="ltx_text" id="S4.T7.st2.2.2.4.3.1" style="font-size:80%;">25.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.4.4"><span class="ltx_text" id="S4.T7.st2.2.2.4.4.1" style="font-size:80%;">35.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.4.5"><span class="ltx_text" id="S4.T7.st2.2.2.4.5.1" style="font-size:80%;">33.4</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.4.6"><span class="ltx_text" id="S4.T7.st2.2.2.4.6.1" style="font-size:80%;">25.5</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.4.7"><span class="ltx_text" id="S4.T7.st2.2.2.4.7.1" style="font-size:80%;">35.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st2.2.2.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st2.2.2.5.1" rowspan="2"><span class="ltx_text" id="S4.T7.st2.2.2.5.1.1" style="font-size:80%;">es</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st2.2.2.5.2"><span class="ltx_text" id="S4.T7.st2.2.2.5.2.1" style="font-size:80%;">BLOOM</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.5.3"><span class="ltx_text" id="S4.T7.st2.2.2.5.3.1" style="font-size:80%;">31.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.5.4"><span class="ltx_text" id="S4.T7.st2.2.2.5.4.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.5.5"><span class="ltx_text" id="S4.T7.st2.2.2.5.5.1" style="font-size:80%;">24.8</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.5.6"><span class="ltx_text" id="S4.T7.st2.2.2.5.6.1" style="font-size:80%;">23.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.5.7"><span class="ltx_text" id="S4.T7.st2.2.2.5.7.1" style="font-size:80%;">16.5</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.5.8"><span class="ltx_text" id="S4.T7.st2.2.2.5.8.1" style="font-size:80%;">29.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st2.2.2.6">
<td class="ltx_td ltx_align_left" id="S4.T7.st2.2.2.6.1"><span class="ltx_text" id="S4.T7.st2.2.2.6.1.1" style="font-size:80%;">M2M</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.6.2"><span class="ltx_text" id="S4.T7.st2.2.2.6.2.1" style="font-size:80%;">23.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.6.3"><span class="ltx_text" id="S4.T7.st2.2.2.6.3.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.6.4"><span class="ltx_text" id="S4.T7.st2.2.2.6.4.1" style="font-size:80%;">29.3</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.6.5"><span class="ltx_text" id="S4.T7.st2.2.2.6.5.1" style="font-size:80%;">27.5</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.6.6"><span class="ltx_text" id="S4.T7.st2.2.2.6.6.1" style="font-size:80%;">23.9</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.6.7"><span class="ltx_text" id="S4.T7.st2.2.2.6.7.1" style="font-size:80%;">28.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st2.2.2.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st2.2.2.7.1" rowspan="2"><span class="ltx_text" id="S4.T7.st2.2.2.7.1.1" style="font-size:80%;">fr</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st2.2.2.7.2"><span class="ltx_text" id="S4.T7.st2.2.2.7.2.1" style="font-size:80%;">BLOOM</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.7.3"><span class="ltx_text" id="S4.T7.st2.2.2.7.3.1" style="font-size:80%;">37.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.7.4"><span class="ltx_text" id="S4.T7.st2.2.2.7.4.1" style="font-size:80%;">27.5</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.7.5"><span class="ltx_text" id="S4.T7.st2.2.2.7.5.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.7.6"><span class="ltx_text" id="S4.T7.st2.2.2.7.6.1" style="font-size:80%;">24.9</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.7.7"><span class="ltx_text" id="S4.T7.st2.2.2.7.7.1" style="font-size:80%;">24.0</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.7.8"><span class="ltx_text" id="S4.T7.st2.2.2.7.8.1" style="font-size:80%;">38.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st2.2.2.8">
<td class="ltx_td ltx_align_left" id="S4.T7.st2.2.2.8.1"><span class="ltx_text" id="S4.T7.st2.2.2.8.1.1" style="font-size:80%;">M2M</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.8.2"><span class="ltx_text" id="S4.T7.st2.2.2.8.2.1" style="font-size:80%;">28.7</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.8.3"><span class="ltx_text" id="S4.T7.st2.2.2.8.3.1" style="font-size:80%;">25.6</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.8.4"><span class="ltx_text" id="S4.T7.st2.2.2.8.4.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.8.5"><span class="ltx_text" id="S4.T7.st2.2.2.8.5.1" style="font-size:80%;">32.8</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.8.6"><span class="ltx_text" id="S4.T7.st2.2.2.8.6.1" style="font-size:80%;">28.6</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.8.7"><span class="ltx_text" id="S4.T7.st2.2.2.8.7.1" style="font-size:80%;">37.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st2.2.2.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st2.2.2.9.1" rowspan="2"><span class="ltx_text" id="S4.T7.st2.2.2.9.1.1" style="font-size:80%;">gl</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st2.2.2.9.2"><span class="ltx_text" id="S4.T7.st2.2.2.9.2.1" style="font-size:80%;">BLOOM</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.9.3"><span class="ltx_text" id="S4.T7.st2.2.2.9.3.1" style="font-size:80%;">37.5</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.9.4"><span class="ltx_text" id="S4.T7.st2.2.2.9.4.1" style="font-size:80%;">27.1</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.9.5"><span class="ltx_text" id="S4.T7.st2.2.2.9.5.1" style="font-size:80%;">33.8</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.9.6"><span class="ltx_text" id="S4.T7.st2.2.2.9.6.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.9.7"><span class="ltx_text" id="S4.T7.st2.2.2.9.7.1" style="font-size:80%;">18.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.9.8"><span class="ltx_text" id="S4.T7.st2.2.2.9.8.1" style="font-size:80%;">32.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st2.2.2.10">
<td class="ltx_td ltx_align_left" id="S4.T7.st2.2.2.10.1"><span class="ltx_text" id="S4.T7.st2.2.2.10.1.1" style="font-size:80%;">M2M</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.10.2"><span class="ltx_text" id="S4.T7.st2.2.2.10.2.1" style="font-size:80%;">30.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.10.3"><span class="ltx_text" id="S4.T7.st2.2.2.10.3.1" style="font-size:80%;">27.6</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.10.4"><span class="ltx_text" id="S4.T7.st2.2.2.10.4.1" style="font-size:80%;">37.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.10.5"><span class="ltx_text" id="S4.T7.st2.2.2.10.5.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.10.6"><span class="ltx_text" id="S4.T7.st2.2.2.10.6.1" style="font-size:80%;">26.9</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.10.7"><span class="ltx_text" id="S4.T7.st2.2.2.10.7.1" style="font-size:80%;">34.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st2.2.2.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st2.2.2.11.1" rowspan="2"><span class="ltx_text" id="S4.T7.st2.2.2.11.1.1" style="font-size:80%;">it</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st2.2.2.11.2"><span class="ltx_text" id="S4.T7.st2.2.2.11.2.1" style="font-size:80%;">BLOOM</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.11.3"><span class="ltx_text" id="S4.T7.st2.2.2.11.3.1" style="font-size:80%;">31.0</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.11.4"><span class="ltx_text" id="S4.T7.st2.2.2.11.4.1" style="font-size:80%;">25.4</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.11.5"><span class="ltx_text" id="S4.T7.st2.2.2.11.5.1" style="font-size:80%;">31.4</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.11.6"><span class="ltx_text" id="S4.T7.st2.2.2.11.6.1" style="font-size:80%;">20.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.11.7"><span class="ltx_text" id="S4.T7.st2.2.2.11.7.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.11.8"><span class="ltx_text" id="S4.T7.st2.2.2.11.8.1" style="font-size:80%;">29.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st2.2.2.12">
<td class="ltx_td ltx_align_left" id="S4.T7.st2.2.2.12.1"><span class="ltx_text" id="S4.T7.st2.2.2.12.1.1" style="font-size:80%;">M2M</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.12.2"><span class="ltx_text" id="S4.T7.st2.2.2.12.2.1" style="font-size:80%;">25.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.12.3"><span class="ltx_text" id="S4.T7.st2.2.2.12.3.1" style="font-size:80%;">29.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.12.4"><span class="ltx_text" id="S4.T7.st2.2.2.12.4.1" style="font-size:80%;">34.4</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.12.5"><span class="ltx_text" id="S4.T7.st2.2.2.12.5.1" style="font-size:80%;">29.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.12.6"><span class="ltx_text" id="S4.T7.st2.2.2.12.6.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st2.2.2.12.7"><span class="ltx_text" id="S4.T7.st2.2.2.12.7.1" style="font-size:80%;">31.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st2.2.2.13">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T7.st2.2.2.13.1" rowspan="2"><span class="ltx_text" id="S4.T7.st2.2.2.13.1.1" style="font-size:80%;">pt</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st2.2.2.13.2"><span class="ltx_text" id="S4.T7.st2.2.2.13.2.1" style="font-size:80%;">BLOOM</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.13.3"><span class="ltx_text" id="S4.T7.st2.2.2.13.3.1" style="font-size:80%;">39.6</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.13.4"><span class="ltx_text" id="S4.T7.st2.2.2.13.4.1" style="font-size:80%;">28.1</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.13.5"><span class="ltx_text" id="S4.T7.st2.2.2.13.5.1" style="font-size:80%;">40.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.13.6"><span class="ltx_text" id="S4.T7.st2.2.2.13.6.1" style="font-size:80%;">27.1</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.13.7"><span class="ltx_text" id="S4.T7.st2.2.2.13.7.1" style="font-size:80%;">20.1</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st2.2.2.13.8"><span class="ltx_text" id="S4.T7.st2.2.2.13.8.1" style="font-size:80%;">â€“</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st2.2.2.14">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T7.st2.2.2.14.1"><span class="ltx_text" id="S4.T7.st2.2.2.14.1.1" style="font-size:80%;">M2M</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T7.st2.2.2.14.2"><span class="ltx_text" id="S4.T7.st2.2.2.14.2.1" style="font-size:80%;">30.7</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T7.st2.2.2.14.3"><span class="ltx_text" id="S4.T7.st2.2.2.14.3.1" style="font-size:80%;">26.9</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T7.st2.2.2.14.4"><span class="ltx_text" id="S4.T7.st2.2.2.14.4.1" style="font-size:80%;">40.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T7.st2.2.2.14.5"><span class="ltx_text" id="S4.T7.st2.2.2.14.5.1" style="font-size:80%;">33.8</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T7.st2.2.2.14.6"><span class="ltx_text" id="S4.T7.st2.2.2.14.6.1" style="font-size:80%;">28.1</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T7.st2.2.2.14.7"><span class="ltx_text" id="S4.T7.st2.2.2.14.7.1" style="font-size:80%;">â€“</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T7.st2.6.1.1" style="font-size:113%;">(b)</span> </span><span class="ltx_text" id="S4.T7.st2.7.2" style="font-size:113%;">Romance languages</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_table ltx_flex_size_2 ltx_align_center" id="S4.T7.st3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T7.st3.2" style="width:109.4pt;height:244.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-9.7pt,21.6pt) scale(0.85,0.85) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T7.st3.2.2">
<tr class="ltx_tr" id="S4.T7.st3.2.2.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T7.st3.1.1.1.1">
<span class="ltx_text" id="S4.T7.st3.1.1.1.1.1" style="font-size:80%;">Src </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T7.st3.1.1.1.1.m1.1"><semantics id="S4.T7.st3.1.1.1.1.m1.1a"><mo id="S4.T7.st3.1.1.1.1.m1.1.1" mathsize="80%" stretchy="false" xref="S4.T7.st3.1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T7.st3.1.1.1.1.m1.1b"><ci id="S4.T7.st3.1.1.1.1.m1.1.1.cmml" xref="S4.T7.st3.1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.st3.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T7.st3.1.1.1.1.m1.1d">â†“</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T7.st3.2.2.2.2">
<span class="ltx_text" id="S4.T7.st3.2.2.2.2.1" style="font-size:80%;">Trg </span><math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T7.st3.2.2.2.2.m1.1"><semantics id="S4.T7.st3.2.2.2.2.m1.1a"><mo id="S4.T7.st3.2.2.2.2.m1.1.1" mathsize="80%" stretchy="false" xref="S4.T7.st3.2.2.2.2.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T7.st3.2.2.2.2.m1.1b"><ci id="S4.T7.st3.2.2.2.2.m1.1.1.cmml" xref="S4.T7.st3.2.2.2.2.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.st3.2.2.2.2.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T7.st3.2.2.2.2.m1.1d">â†’</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T7.st3.2.2.2.3"><span class="ltx_text" id="S4.T7.st3.2.2.2.3.1" style="font-size:80%;">ar</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T7.st3.2.2.2.4"><span class="ltx_text" id="S4.T7.st3.2.2.2.4.1" style="font-size:80%;">en</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T7.st3.2.2.2.5"><span class="ltx_text" id="S4.T7.st3.2.2.2.5.1" style="font-size:80%;">es</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T7.st3.2.2.2.6"><span class="ltx_text" id="S4.T7.st3.2.2.2.6.1" style="font-size:80%;">fr</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T7.st3.2.2.2.7"><span class="ltx_text" id="S4.T7.st3.2.2.2.7.1" style="font-size:80%;">zh</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st3.2.2.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st3.2.2.3.1" rowspan="3"><span class="ltx_text" id="S4.T7.st3.2.2.3.1.1" style="font-size:80%;">ar</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st3.2.2.3.2"><span class="ltx_text" id="S4.T7.st3.2.2.3.2.1" style="font-size:80%;">BLOOM</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st3.2.2.3.3"><span class="ltx_text" id="S4.T7.st3.2.2.3.3.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st3.2.2.3.4"><span class="ltx_text" id="S4.T7.st3.2.2.3.4.1" style="font-size:80%;">40.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st3.2.2.3.5"><span class="ltx_text" id="S4.T7.st3.2.2.3.5.1" style="font-size:80%;">23.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st3.2.2.3.6"><span class="ltx_text" id="S4.T7.st3.2.2.3.6.1" style="font-size:80%;">33.1</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st3.2.2.3.7"><span class="ltx_text" id="S4.T7.st3.2.2.3.7.1" style="font-size:80%;">17.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st3.2.2.4">
<td class="ltx_td ltx_align_left" id="S4.T7.st3.2.2.4.1"><span class="ltx_text" id="S4.T7.st3.2.2.4.1.1" style="font-size:80%;">M2M</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.4.2"><span class="ltx_text" id="S4.T7.st3.2.2.4.2.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.4.3"><span class="ltx_text" id="S4.T7.st3.2.2.4.3.1" style="font-size:80%;">25.5</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.4.4"><span class="ltx_text" id="S4.T7.st3.2.2.4.4.1" style="font-size:80%;">16.7</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.4.5"><span class="ltx_text" id="S4.T7.st3.2.2.4.5.1" style="font-size:80%;">25.7</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.4.6"><span class="ltx_text" id="S4.T7.st3.2.2.4.6.1" style="font-size:80%;">13.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st3.2.2.5">
<td class="ltx_td ltx_align_left" id="S4.T7.st3.2.2.5.1"><span class="ltx_text" id="S4.T7.st3.2.2.5.1.1" style="font-size:80%;">AlexaTM</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.5.2"><span class="ltx_text" id="S4.T7.st3.2.2.5.2.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.5.3"><span class="ltx_text" id="S4.T7.st3.2.2.5.3.1" style="font-size:80%;">41.8</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.5.4"><span class="ltx_text" id="S4.T7.st3.2.2.5.4.1" style="font-size:80%;">23.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.5.5"><span class="ltx_text" id="S4.T7.st3.2.2.5.5.1" style="font-size:80%;">35.5</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.5.6"><span class="ltx_text" id="S4.T7.st3.2.2.5.6.1" style="font-size:80%;">â€“</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st3.2.2.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st3.2.2.6.1" rowspan="3"><span class="ltx_text" id="S4.T7.st3.2.2.6.1.1" style="font-size:80%;">en</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st3.2.2.6.2"><span class="ltx_text" id="S4.T7.st3.2.2.6.2.1" style="font-size:80%;">BLOOM</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st3.2.2.6.3"><span class="ltx_text" id="S4.T7.st3.2.2.6.3.1" style="font-size:80%;">28.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st3.2.2.6.4"><span class="ltx_text" id="S4.T7.st3.2.2.6.4.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st3.2.2.6.5"><span class="ltx_text" id="S4.T7.st3.2.2.6.5.1" style="font-size:80%;">29.4</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st3.2.2.6.6"><span class="ltx_text" id="S4.T7.st3.2.2.6.6.1" style="font-size:80%;">45.0</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st3.2.2.6.7"><span class="ltx_text" id="S4.T7.st3.2.2.6.7.1" style="font-size:80%;">26.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st3.2.2.7">
<td class="ltx_td ltx_align_left" id="S4.T7.st3.2.2.7.1"><span class="ltx_text" id="S4.T7.st3.2.2.7.1.1" style="font-size:80%;">M2M</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.7.2"><span class="ltx_text" id="S4.T7.st3.2.2.7.2.1" style="font-size:80%;">17.9</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.7.3"><span class="ltx_text" id="S4.T7.st3.2.2.7.3.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.7.4"><span class="ltx_text" id="S4.T7.st3.2.2.7.4.1" style="font-size:80%;">25.6</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.7.5"><span class="ltx_text" id="S4.T7.st3.2.2.7.5.1" style="font-size:80%;">42.0</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.7.6"><span class="ltx_text" id="S4.T7.st3.2.2.7.6.1" style="font-size:80%;">19.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st3.2.2.8">
<td class="ltx_td ltx_align_left" id="S4.T7.st3.2.2.8.1"><span class="ltx_text" id="S4.T7.st3.2.2.8.1.1" style="font-size:80%;">AlexaTM</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.8.2"><span class="ltx_text" id="S4.T7.st3.2.2.8.2.1" style="font-size:80%;">32.0</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.8.3"><span class="ltx_text" id="S4.T7.st3.2.2.8.3.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.8.4"><span class="ltx_text" id="S4.T7.st3.2.2.8.4.1" style="font-size:80%;">31.0</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.8.5"><span class="ltx_text" id="S4.T7.st3.2.2.8.5.1" style="font-size:80%;">50.7</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.8.6"><span class="ltx_text" id="S4.T7.st3.2.2.8.6.1" style="font-size:80%;">â€“</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st3.2.2.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st3.2.2.9.1" rowspan="3"><span class="ltx_text" id="S4.T7.st3.2.2.9.1.1" style="font-size:80%;">es</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st3.2.2.9.2"><span class="ltx_text" id="S4.T7.st3.2.2.9.2.1" style="font-size:80%;">BLOOM</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st3.2.2.9.3"><span class="ltx_text" id="S4.T7.st3.2.2.9.3.1" style="font-size:80%;">18.8</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st3.2.2.9.4"><span class="ltx_text" id="S4.T7.st3.2.2.9.4.1" style="font-size:80%;">32.7</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st3.2.2.9.5"><span class="ltx_text" id="S4.T7.st3.2.2.9.5.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st3.2.2.9.6"><span class="ltx_text" id="S4.T7.st3.2.2.9.6.1" style="font-size:80%;">24.8</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st3.2.2.9.7"><span class="ltx_text" id="S4.T7.st3.2.2.9.7.1" style="font-size:80%;">20.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st3.2.2.10">
<td class="ltx_td ltx_align_left" id="S4.T7.st3.2.2.10.1"><span class="ltx_text" id="S4.T7.st3.2.2.10.1.1" style="font-size:80%;">M2M</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.10.2"><span class="ltx_text" id="S4.T7.st3.2.2.10.2.1" style="font-size:80%;">12.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.10.3"><span class="ltx_text" id="S4.T7.st3.2.2.10.3.1" style="font-size:80%;">25.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.10.4"><span class="ltx_text" id="S4.T7.st3.2.2.10.4.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.10.5"><span class="ltx_text" id="S4.T7.st3.2.2.10.5.1" style="font-size:80%;">29.3</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.10.6"><span class="ltx_text" id="S4.T7.st3.2.2.10.6.1" style="font-size:80%;">14.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st3.2.2.11">
<td class="ltx_td ltx_align_left" id="S4.T7.st3.2.2.11.1"><span class="ltx_text" id="S4.T7.st3.2.2.11.1.1" style="font-size:80%;">AlexaTM</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.11.2"><span class="ltx_text" id="S4.T7.st3.2.2.11.2.1" style="font-size:80%;">20.8</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.11.3"><span class="ltx_text" id="S4.T7.st3.2.2.11.3.1" style="font-size:80%;">34.6</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.11.4"><span class="ltx_text" id="S4.T7.st3.2.2.11.4.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.11.5"><span class="ltx_text" id="S4.T7.st3.2.2.11.5.1" style="font-size:80%;">33.4</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.11.6"><span class="ltx_text" id="S4.T7.st3.2.2.11.6.1" style="font-size:80%;">â€“</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st3.2.2.12">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st3.2.2.12.1" rowspan="3"><span class="ltx_text" id="S4.T7.st3.2.2.12.1.1" style="font-size:80%;">fr</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st3.2.2.12.2"><span class="ltx_text" id="S4.T7.st3.2.2.12.2.1" style="font-size:80%;">BLOOM</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st3.2.2.12.3"><span class="ltx_text" id="S4.T7.st3.2.2.12.3.1" style="font-size:80%;">23.4</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st3.2.2.12.4"><span class="ltx_text" id="S4.T7.st3.2.2.12.4.1" style="font-size:80%;">45.6</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st3.2.2.12.5"><span class="ltx_text" id="S4.T7.st3.2.2.12.5.1" style="font-size:80%;">27.5</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st3.2.2.12.6"><span class="ltx_text" id="S4.T7.st3.2.2.12.6.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st3.2.2.12.7"><span class="ltx_text" id="S4.T7.st3.2.2.12.7.1" style="font-size:80%;">23.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st3.2.2.13">
<td class="ltx_td ltx_align_left" id="S4.T7.st3.2.2.13.1"><span class="ltx_text" id="S4.T7.st3.2.2.13.1.1" style="font-size:80%;">M2M</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.13.2"><span class="ltx_text" id="S4.T7.st3.2.2.13.2.1" style="font-size:80%;">15.4</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.13.3"><span class="ltx_text" id="S4.T7.st3.2.2.13.3.1" style="font-size:80%;">37.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.13.4"><span class="ltx_text" id="S4.T7.st3.2.2.13.4.1" style="font-size:80%;">25.6</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.13.5"><span class="ltx_text" id="S4.T7.st3.2.2.13.5.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.13.6"><span class="ltx_text" id="S4.T7.st3.2.2.13.6.1" style="font-size:80%;">17.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st3.2.2.14">
<td class="ltx_td ltx_align_left" id="S4.T7.st3.2.2.14.1"><span class="ltx_text" id="S4.T7.st3.2.2.14.1.1" style="font-size:80%;">AlexaTM</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.14.2"><span class="ltx_text" id="S4.T7.st3.2.2.14.2.1" style="font-size:80%;">24.7</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.14.3"><span class="ltx_text" id="S4.T7.st3.2.2.14.3.1" style="font-size:80%;">47.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.14.4"><span class="ltx_text" id="S4.T7.st3.2.2.14.4.1" style="font-size:80%;">26.3</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.14.5"><span class="ltx_text" id="S4.T7.st3.2.2.14.5.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.14.6"><span class="ltx_text" id="S4.T7.st3.2.2.14.6.1" style="font-size:80%;">â€“</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st3.2.2.15">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T7.st3.2.2.15.1" rowspan="3"><span class="ltx_text" id="S4.T7.st3.2.2.15.1.1" style="font-size:80%;">zh</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st3.2.2.15.2"><span class="ltx_text" id="S4.T7.st3.2.2.15.2.1" style="font-size:80%;">BLOOM</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st3.2.2.15.3"><span class="ltx_text" id="S4.T7.st3.2.2.15.3.1" style="font-size:80%;">15.0</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st3.2.2.15.4"><span class="ltx_text" id="S4.T7.st3.2.2.15.4.1" style="font-size:80%;">30.5</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st3.2.2.15.5"><span class="ltx_text" id="S4.T7.st3.2.2.15.5.1" style="font-size:80%;">20.5</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st3.2.2.15.6"><span class="ltx_text" id="S4.T7.st3.2.2.15.6.1" style="font-size:80%;">26.0</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st3.2.2.15.7"><span class="ltx_text" id="S4.T7.st3.2.2.15.7.1" style="font-size:80%;">â€“</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st3.2.2.16">
<td class="ltx_td ltx_align_left" id="S4.T7.st3.2.2.16.1"><span class="ltx_text" id="S4.T7.st3.2.2.16.1.1" style="font-size:80%;">M2M</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.16.2"><span class="ltx_text" id="S4.T7.st3.2.2.16.2.1" style="font-size:80%;">11.55</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.16.3"><span class="ltx_text" id="S4.T7.st3.2.2.16.3.1" style="font-size:80%;">20.9</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.16.4"><span class="ltx_text" id="S4.T7.st3.2.2.16.4.1" style="font-size:80%;">16.9</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.16.5"><span class="ltx_text" id="S4.T7.st3.2.2.16.5.1" style="font-size:80%;">24.3</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st3.2.2.16.6"><span class="ltx_text" id="S4.T7.st3.2.2.16.6.1" style="font-size:80%;">â€“</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st3.2.2.17">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T7.st3.2.2.17.1"><span class="ltx_text" id="S4.T7.st3.2.2.17.1.1" style="font-size:80%;">AlexaTM</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T7.st3.2.2.17.2"><span class="ltx_text" id="S4.T7.st3.2.2.17.2.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T7.st3.2.2.17.3"><span class="ltx_text" id="S4.T7.st3.2.2.17.3.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T7.st3.2.2.17.4"><span class="ltx_text" id="S4.T7.st3.2.2.17.4.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T7.st3.2.2.17.5"><span class="ltx_text" id="S4.T7.st3.2.2.17.5.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T7.st3.2.2.17.6"><span class="ltx_text" id="S4.T7.st3.2.2.17.6.1" style="font-size:80%;">â€“</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T7.st3.6.1.1" style="font-size:113%;">(c)</span> </span><span class="ltx_text" id="S4.T7.st3.7.2" style="font-size:113%;">High-resource language pairs.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_table ltx_flex_size_2 ltx_align_center" id="S4.T7.st4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T7.st4.2" style="width:103.7pt;height:168.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-9.2pt,14.9pt) scale(0.85,0.85) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T7.st4.2.2">
<tr class="ltx_tr" id="S4.T7.st4.2.2.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T7.st4.1.1.1.1">
<span class="ltx_text" id="S4.T7.st4.1.1.1.1.1" style="font-size:80%;">Src </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T7.st4.1.1.1.1.m1.1"><semantics id="S4.T7.st4.1.1.1.1.m1.1a"><mo id="S4.T7.st4.1.1.1.1.m1.1.1" mathsize="80%" stretchy="false" xref="S4.T7.st4.1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T7.st4.1.1.1.1.m1.1b"><ci id="S4.T7.st4.1.1.1.1.m1.1.1.cmml" xref="S4.T7.st4.1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.st4.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T7.st4.1.1.1.1.m1.1d">â†“</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T7.st4.2.2.2.2">
<span class="ltx_text" id="S4.T7.st4.2.2.2.2.1" style="font-size:80%;">Trg </span><math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T7.st4.2.2.2.2.m1.1"><semantics id="S4.T7.st4.2.2.2.2.m1.1a"><mo id="S4.T7.st4.2.2.2.2.m1.1.1" mathsize="80%" stretchy="false" xref="S4.T7.st4.2.2.2.2.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T7.st4.2.2.2.2.m1.1b"><ci id="S4.T7.st4.2.2.2.2.m1.1.1.cmml" xref="S4.T7.st4.2.2.2.2.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.st4.2.2.2.2.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T7.st4.2.2.2.2.m1.1d">â†’</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T7.st4.2.2.2.3"><span class="ltx_text" id="S4.T7.st4.2.2.2.3.1" style="font-size:80%;">en</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T7.st4.2.2.2.4"><span class="ltx_text" id="S4.T7.st4.2.2.2.4.1" style="font-size:80%;">fr</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T7.st4.2.2.2.5"><span class="ltx_text" id="S4.T7.st4.2.2.2.5.1" style="font-size:80%;">hi</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T7.st4.2.2.2.6"><span class="ltx_text" id="S4.T7.st4.2.2.2.6.1" style="font-size:80%;">id</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T7.st4.2.2.2.7"><span class="ltx_text" id="S4.T7.st4.2.2.2.7.1" style="font-size:80%;">vi</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st4.2.2.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st4.2.2.3.1" rowspan="2"><span class="ltx_text" id="S4.T7.st4.2.2.3.1.1" style="font-size:80%;">en</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st4.2.2.3.2"><span class="ltx_text" id="S4.T7.st4.2.2.3.2.1" style="font-size:80%;">BLOOM</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st4.2.2.3.3"><span class="ltx_text" id="S4.T7.st4.2.2.3.3.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st4.2.2.3.4"><span class="ltx_text" id="S4.T7.st4.2.2.3.4.1" style="font-size:80%;">45.0</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st4.2.2.3.5"><span class="ltx_text" id="S4.T7.st4.2.2.3.5.1" style="font-size:80%;">27.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st4.2.2.3.6"><span class="ltx_text" id="S4.T7.st4.2.2.3.6.1" style="font-size:80%;">39.0</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st4.2.2.3.7"><span class="ltx_text" id="S4.T7.st4.2.2.3.7.1" style="font-size:80%;">28.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st4.2.2.4">
<td class="ltx_td ltx_align_left" id="S4.T7.st4.2.2.4.1"><span class="ltx_text" id="S4.T7.st4.2.2.4.1.1" style="font-size:80%;">M2M</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st4.2.2.4.2"><span class="ltx_text" id="S4.T7.st4.2.2.4.2.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st4.2.2.4.3"><span class="ltx_text" id="S4.T7.st4.2.2.4.3.1" style="font-size:80%;">42.0</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st4.2.2.4.4"><span class="ltx_text" id="S4.T7.st4.2.2.4.4.1" style="font-size:80%;">28.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st4.2.2.4.5"><span class="ltx_text" id="S4.T7.st4.2.2.4.5.1" style="font-size:80%;">37.3</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st4.2.2.4.6"><span class="ltx_text" id="S4.T7.st4.2.2.4.6.1" style="font-size:80%;">35.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st4.2.2.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st4.2.2.5.1" rowspan="2"><span class="ltx_text" id="S4.T7.st4.2.2.5.1.1" style="font-size:80%;">fr</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st4.2.2.5.2"><span class="ltx_text" id="S4.T7.st4.2.2.5.2.1" style="font-size:80%;">BLOOM</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st4.2.2.5.3"><span class="ltx_text" id="S4.T7.st4.2.2.5.3.1" style="font-size:80%;">45.6</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st4.2.2.5.4"><span class="ltx_text" id="S4.T7.st4.2.2.5.4.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st4.2.2.5.5"><span class="ltx_text" id="S4.T7.st4.2.2.5.5.1" style="font-size:80%;">18.5</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st4.2.2.5.6"><span class="ltx_text" id="S4.T7.st4.2.2.5.6.1" style="font-size:80%;">31.4</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st4.2.2.5.7"><span class="ltx_text" id="S4.T7.st4.2.2.5.7.1" style="font-size:80%;">32.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st4.2.2.6">
<td class="ltx_td ltx_align_left" id="S4.T7.st4.2.2.6.1"><span class="ltx_text" id="S4.T7.st4.2.2.6.1.1" style="font-size:80%;">M2M</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st4.2.2.6.2"><span class="ltx_text" id="S4.T7.st4.2.2.6.2.1" style="font-size:80%;">37.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st4.2.2.6.3"><span class="ltx_text" id="S4.T7.st4.2.2.6.3.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st4.2.2.6.4"><span class="ltx_text" id="S4.T7.st4.2.2.6.4.1" style="font-size:80%;">22.9</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st4.2.2.6.5"><span class="ltx_text" id="S4.T7.st4.2.2.6.5.1" style="font-size:80%;">29.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st4.2.2.6.6"><span class="ltx_text" id="S4.T7.st4.2.2.6.6.1" style="font-size:80%;">30.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st4.2.2.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st4.2.2.7.1" rowspan="2"><span class="ltx_text" id="S4.T7.st4.2.2.7.1.1" style="font-size:80%;">hi</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st4.2.2.7.2"><span class="ltx_text" id="S4.T7.st4.2.2.7.2.1" style="font-size:80%;">BLOOM</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st4.2.2.7.3"><span class="ltx_text" id="S4.T7.st4.2.2.7.3.1" style="font-size:80%;">35.1</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st4.2.2.7.4"><span class="ltx_text" id="S4.T7.st4.2.2.7.4.1" style="font-size:80%;">27.6</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st4.2.2.7.5"><span class="ltx_text" id="S4.T7.st4.2.2.7.5.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st4.2.2.7.6"><span class="ltx_text" id="S4.T7.st4.2.2.7.6.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st4.2.2.7.7"><span class="ltx_text" id="S4.T7.st4.2.2.7.7.1" style="font-size:80%;">â€“</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st4.2.2.8">
<td class="ltx_td ltx_align_left" id="S4.T7.st4.2.2.8.1"><span class="ltx_text" id="S4.T7.st4.2.2.8.1.1" style="font-size:80%;">M2M</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st4.2.2.8.2"><span class="ltx_text" id="S4.T7.st4.2.2.8.2.1" style="font-size:80%;">27.9</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st4.2.2.8.3"><span class="ltx_text" id="S4.T7.st4.2.2.8.3.1" style="font-size:80%;">25.9</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st4.2.2.8.4"><span class="ltx_text" id="S4.T7.st4.2.2.8.4.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st4.2.2.8.5"><span class="ltx_text" id="S4.T7.st4.2.2.8.5.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st4.2.2.8.6"><span class="ltx_text" id="S4.T7.st4.2.2.8.6.1" style="font-size:80%;">â€“</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st4.2.2.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st4.2.2.9.1" rowspan="2"><span class="ltx_text" id="S4.T7.st4.2.2.9.1.1" style="font-size:80%;">id</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st4.2.2.9.2"><span class="ltx_text" id="S4.T7.st4.2.2.9.2.1" style="font-size:80%;">BLOOM</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st4.2.2.9.3"><span class="ltx_text" id="S4.T7.st4.2.2.9.3.1" style="font-size:80%;">43.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st4.2.2.9.4"><span class="ltx_text" id="S4.T7.st4.2.2.9.4.1" style="font-size:80%;">30.4</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st4.2.2.9.5"><span class="ltx_text" id="S4.T7.st4.2.2.9.5.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st4.2.2.9.6"><span class="ltx_text" id="S4.T7.st4.2.2.9.6.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st4.2.2.9.7"><span class="ltx_text" id="S4.T7.st4.2.2.9.7.1" style="font-size:80%;">â€“</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st4.2.2.10">
<td class="ltx_td ltx_align_left" id="S4.T7.st4.2.2.10.1"><span class="ltx_text" id="S4.T7.st4.2.2.10.1.1" style="font-size:80%;">M2M</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st4.2.2.10.2"><span class="ltx_text" id="S4.T7.st4.2.2.10.2.1" style="font-size:80%;">33.7</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st4.2.2.10.3"><span class="ltx_text" id="S4.T7.st4.2.2.10.3.1" style="font-size:80%;">30.8</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st4.2.2.10.4"><span class="ltx_text" id="S4.T7.st4.2.2.10.4.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st4.2.2.10.5"><span class="ltx_text" id="S4.T7.st4.2.2.10.5.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right" id="S4.T7.st4.2.2.10.6"><span class="ltx_text" id="S4.T7.st4.2.2.10.6.1" style="font-size:80%;">â€“</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st4.2.2.11">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T7.st4.2.2.11.1" rowspan="2"><span class="ltx_text" id="S4.T7.st4.2.2.11.1.1" style="font-size:80%;">vi</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.st4.2.2.11.2"><span class="ltx_text" id="S4.T7.st4.2.2.11.2.1" style="font-size:80%;">BLOOM</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st4.2.2.11.3"><span class="ltx_text" id="S4.T7.st4.2.2.11.3.1" style="font-size:80%;">38.7</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st4.2.2.11.4"><span class="ltx_text" id="S4.T7.st4.2.2.11.4.1" style="font-size:80%;">26.8</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st4.2.2.11.5"><span class="ltx_text" id="S4.T7.st4.2.2.11.5.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st4.2.2.11.6"><span class="ltx_text" id="S4.T7.st4.2.2.11.6.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T7.st4.2.2.11.7"><span class="ltx_text" id="S4.T7.st4.2.2.11.7.1" style="font-size:80%;">â€“</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.st4.2.2.12">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T7.st4.2.2.12.1"><span class="ltx_text" id="S4.T7.st4.2.2.12.1.1" style="font-size:80%;">M2M</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T7.st4.2.2.12.2"><span class="ltx_text" id="S4.T7.st4.2.2.12.2.1" style="font-size:80%;">29.5</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T7.st4.2.2.12.3"><span class="ltx_text" id="S4.T7.st4.2.2.12.3.1" style="font-size:80%;">25.8</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T7.st4.2.2.12.4"><span class="ltx_text" id="S4.T7.st4.2.2.12.4.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T7.st4.2.2.12.5"><span class="ltx_text" id="S4.T7.st4.2.2.12.5.1" style="font-size:80%;">â€“</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T7.st4.2.2.12.6"><span class="ltx_text" id="S4.T7.st4.2.2.12.6.1" style="font-size:80%;">â€“</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T7.st4.9.2.1" style="font-size:113%;">(d)</span> </span><span class="ltx_text" id="S4.T7.st4.4.1" style="font-size:113%;">High<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T7.st4.4.1.m1.1"><semantics id="S4.T7.st4.4.1.m1.1b"><mo id="S4.T7.st4.4.1.m1.1.1" stretchy="false" xref="S4.T7.st4.4.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T7.st4.4.1.m1.1c"><ci id="S4.T7.st4.4.1.m1.1.1.cmml" xref="S4.T7.st4.4.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.st4.4.1.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T7.st4.4.1.m1.1e">â†’</annotation></semantics></math>mid-resource language pairs.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T8.4.1.1" style="font-size:113%;">Table 8</span>: </span><span class="ltx_text" id="S4.T8.5.2" style="font-size:113%;">1-shot MT results (spBLEU) on the Flores-101 devtest set. </span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.SSS3.p1">
<p class="ltx_p" id="S4.SS3.SSS3.p1.1">In the 1-shot setting, we test several language directions in the Flores-101 <cite class="ltx_cite ltx_citemacro_citep">(Goyal etÂ al., <a class="ltx_ref" href="#bib.bib55" title="">2022</a>)</cite> devtest set using the â€œxglm-source+targetâ€ prompt <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al., <a class="ltx_ref" href="#bib.bib81" title="">2021</a>)</cite>. The 1-shot example is randomly taken from the dev set.
We separate out results for low-resource language pairs (<a class="ltx_ref" href="#S4.T7.st1" title="7(a) â€£ Table 8 â€£ 4.3.3 Flores â€£ 4.3 Machine Translation â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">7(a)</span></a>), between related languages of the Romance language family (<a class="ltx_ref" href="#S4.T7.st2" title="7(b) â€£ Table 8 â€£ 4.3.3 Flores â€£ 4.3 Machine Translation â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">7(b)</span></a>), high-resource language pairs (<a class="ltx_ref" href="#S4.T7.st3" title="7(c) â€£ Table 8 â€£ 4.3.3 Flores â€£ 4.3 Machine Translation â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">7(c)</span></a>) and high-to-mid-resource language pairs (<a class="ltx_ref" href="#S4.T7.st4" title="7(d) â€£ Table 8 â€£ 4.3.3 Flores â€£ 4.3 Machine Translation â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">7(d)</span></a>). Languages are classified as low-, mid- and high-resource depending on their representation in ROOTS.
We compare to supervised results from the M2M-100 model <cite class="ltx_cite ltx_citemacro_citep">(Fan etÂ al., <a class="ltx_ref" href="#bib.bib42" title="">2021</a>)</cite> with 615M parameters, for which scores are computed by <cite class="ltx_cite ltx_citemacro_cite">Goyal etÂ al. (<a class="ltx_ref" href="#bib.bib55" title="">2022</a>)</cite>. Additionally, we compare to 32-shot AlexaTM results for high-resource language pairs <cite class="ltx_cite ltx_citemacro_citep">(Soltan etÂ al., <a class="ltx_ref" href="#bib.bib139" title="">2022</a>)</cite>. Results are good across the board for both translation between high-resource languages and from high- to mid-resource languages, suggesting BLOOMâ€™s good multilingual capacity, even across scripts (here between Latin (or extended Latin), Chinese, Arabic and Devanagari scripts). Compared to the supervised M2M-100 model, results are often comparable and sometimes better in this 1-shot setting, and results are comparable in many cases to those of AlexaTM (even though AlexTM results are for 32-shot).</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS3.p2">
<p class="ltx_p" id="S4.SS3.SSS3.p2.1">The translation quality for many of the low-resource languages is good, comparable to or even slightly better than the supervised M2M model.
However, results are very poor between Swahili and Yoruba, languages that are present but under-represented in BLOOMâ€™s training data (<math alttext="&lt;" class="ltx_Math" display="inline" id="S4.SS3.SSS3.p2.1.m1.1"><semantics id="S4.SS3.SSS3.p2.1.m1.1a"><mo id="S4.SS3.SSS3.p2.1.m1.1.1" xref="S4.SS3.SSS3.p2.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS3.p2.1.m1.1b"><lt id="S4.SS3.SSS3.p2.1.m1.1.1.cmml" xref="S4.SS3.SSS3.p2.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS3.p2.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS3.p2.1.m1.1d">&lt;</annotation></semantics></math>50k tokens each). This contrasts with the results for translation between Romance (and therefore related) languages, where results are good across-the-board, including for translation from Galician (glg), a language not included in the training data, but which shares many similarities with the other Romance languages, in particular with Portuguese (por). This however does question BLOOMâ€™s quality on those under-represented low-resource languages included in training.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Summarization</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Figure <a class="ltx_ref" href="#S4.F9" title="Figure 9 â€£ 4.4 Summarization â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">9</span></a> shows one-shot results for BLOOM models alongside OPT-175B for comparison. Each point represents a per-prompt score. The key takeaways are that BLOOM attains higher performance on multilingual summarization than OPT and that performance increases as the parameter count of the model increases. We suspect this is due to BLOOMâ€™s multilingual-focused training.</p>
</div>
<figure class="ltx_figure" id="S4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="540" id="S4.F9.g1" src="x10.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F9.2.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="S4.F9.3.2" style="font-size:90%;">WikiLingua One-shot Results. Each plot represents a different language with per-prompt ROUGE-2 F-measure scores.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">As discussed in Section <a class="ltx_ref" href="#S4.SS1" title="4.1 Experimental Design â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">4.1</span></a>, we report ROUGE-2 scores for the sake of comparability with prior work, and because there is a lack of alternatives for generation evaluation.
However, we qualitatively observe that in many cases, the ROUGE-2 score understates the quality of the summaries generated by the systems.
</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Code Generation</h3>
<figure class="ltx_table" id="S4.T9">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T9.4">
<tr class="ltx_tr" id="S4.T9.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T9.1.1.2"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T9.1.1.1">
<span class="ltx_text ltx_font_smallcaps" id="S4.T9.1.1.1.1" style="font-size:90%;">pass@</span><math alttext="k" class="ltx_Math" display="inline" id="S4.T9.1.1.1.m1.1"><semantics id="S4.T9.1.1.1.m1.1a"><mi id="S4.T9.1.1.1.m1.1.1" mathsize="90%" xref="S4.T9.1.1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.T9.1.1.1.m1.1b"><ci id="S4.T9.1.1.1.m1.1.1.cmml" xref="S4.T9.1.1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.1.1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.T9.1.1.1.m1.1d">italic_k</annotation></semantics></math><span class="ltx_text ltx_font_smallcaps" id="S4.T9.1.1.1.2" style="font-size:90%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.4">
<td class="ltx_td" id="S4.T9.4.4.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T9.2.2.1"><math alttext="k=1" class="ltx_Math" display="inline" id="S4.T9.2.2.1.m1.1"><semantics id="S4.T9.2.2.1.m1.1a"><mrow id="S4.T9.2.2.1.m1.1.1" xref="S4.T9.2.2.1.m1.1.1.cmml"><mi id="S4.T9.2.2.1.m1.1.1.2" mathsize="90%" xref="S4.T9.2.2.1.m1.1.1.2.cmml">k</mi><mo id="S4.T9.2.2.1.m1.1.1.1" mathsize="90%" xref="S4.T9.2.2.1.m1.1.1.1.cmml">=</mo><mn id="S4.T9.2.2.1.m1.1.1.3" mathsize="90%" xref="S4.T9.2.2.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.2.2.1.m1.1b"><apply id="S4.T9.2.2.1.m1.1.1.cmml" xref="S4.T9.2.2.1.m1.1.1"><eq id="S4.T9.2.2.1.m1.1.1.1.cmml" xref="S4.T9.2.2.1.m1.1.1.1"></eq><ci id="S4.T9.2.2.1.m1.1.1.2.cmml" xref="S4.T9.2.2.1.m1.1.1.2">ğ‘˜</ci><cn id="S4.T9.2.2.1.m1.1.1.3.cmml" type="integer" xref="S4.T9.2.2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.2.2.1.m1.1c">k=1</annotation><annotation encoding="application/x-llamapun" id="S4.T9.2.2.1.m1.1d">italic_k = 1</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T9.3.3.2"><math alttext="k=10" class="ltx_Math" display="inline" id="S4.T9.3.3.2.m1.1"><semantics id="S4.T9.3.3.2.m1.1a"><mrow id="S4.T9.3.3.2.m1.1.1" xref="S4.T9.3.3.2.m1.1.1.cmml"><mi id="S4.T9.3.3.2.m1.1.1.2" mathsize="90%" xref="S4.T9.3.3.2.m1.1.1.2.cmml">k</mi><mo id="S4.T9.3.3.2.m1.1.1.1" mathsize="90%" xref="S4.T9.3.3.2.m1.1.1.1.cmml">=</mo><mn id="S4.T9.3.3.2.m1.1.1.3" mathsize="90%" xref="S4.T9.3.3.2.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.3.3.2.m1.1b"><apply id="S4.T9.3.3.2.m1.1.1.cmml" xref="S4.T9.3.3.2.m1.1.1"><eq id="S4.T9.3.3.2.m1.1.1.1.cmml" xref="S4.T9.3.3.2.m1.1.1.1"></eq><ci id="S4.T9.3.3.2.m1.1.1.2.cmml" xref="S4.T9.3.3.2.m1.1.1.2">ğ‘˜</ci><cn id="S4.T9.3.3.2.m1.1.1.3.cmml" type="integer" xref="S4.T9.3.3.2.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.3.3.2.m1.1c">k=10</annotation><annotation encoding="application/x-llamapun" id="S4.T9.3.3.2.m1.1d">italic_k = 10</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.4.3"><math alttext="k=100" class="ltx_Math" display="inline" id="S4.T9.4.4.3.m1.1"><semantics id="S4.T9.4.4.3.m1.1a"><mrow id="S4.T9.4.4.3.m1.1.1" xref="S4.T9.4.4.3.m1.1.1.cmml"><mi id="S4.T9.4.4.3.m1.1.1.2" mathsize="90%" xref="S4.T9.4.4.3.m1.1.1.2.cmml">k</mi><mo id="S4.T9.4.4.3.m1.1.1.1" mathsize="90%" xref="S4.T9.4.4.3.m1.1.1.1.cmml">=</mo><mn id="S4.T9.4.4.3.m1.1.1.3" mathsize="90%" xref="S4.T9.4.4.3.m1.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.4.4.3.m1.1b"><apply id="S4.T9.4.4.3.m1.1.1.cmml" xref="S4.T9.4.4.3.m1.1.1"><eq id="S4.T9.4.4.3.m1.1.1.1.cmml" xref="S4.T9.4.4.3.m1.1.1.1"></eq><ci id="S4.T9.4.4.3.m1.1.1.2.cmml" xref="S4.T9.4.4.3.m1.1.1.2">ğ‘˜</ci><cn id="S4.T9.4.4.3.m1.1.1.3.cmml" type="integer" xref="S4.T9.4.4.3.m1.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.4.4.3.m1.1c">k=100</annotation><annotation encoding="application/x-llamapun" id="S4.T9.4.4.3.m1.1d">italic_k = 100</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T9.4.5.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.5.1.1" style="font-size:90%;">GPT-Neo 1.3B</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.4.5.2"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.5.2.1" style="font-size:90%;">4.79%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.4.5.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.5.3.1" style="font-size:90%;">7.47%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.4.5.4"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.5.4.1" style="font-size:90%;">16.30%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.6">
<td class="ltx_td ltx_align_left" id="S4.T9.4.6.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.6.1.1" style="font-size:90%;">GPT-Neo 2.7B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.6.2"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.6.2.1" style="font-size:90%;">6.41%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.6.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.6.3.1" style="font-size:90%;">11.27%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.6.4"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.6.4.1" style="font-size:90%;">21.37%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.7">
<td class="ltx_td ltx_align_left" id="S4.T9.4.7.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.7.1.1" style="font-size:90%;">GPT-J 6B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.7.2"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.7.2.1" style="font-size:90%;">11.62%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.7.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.7.3.1" style="font-size:90%;">15.74%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.7.4"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.7.4.1" style="font-size:90%;">27.74%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.8">
<td class="ltx_td ltx_align_left" id="S4.T9.4.8.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.8.1.1" style="font-size:90%;">GPT-NeoX 20B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.8.2"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.8.2.1" style="font-size:90%;">15.4%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.8.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.8.3.1" style="font-size:90%;">25.6%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.8.4"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.8.4.1" style="font-size:90%;">41.2%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T9.4.9.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.9.1.1" style="font-size:90%;">Codex-300M</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.4.9.2"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.9.2.1" style="font-size:90%;">13.17%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.4.9.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.9.3.1" style="font-size:90%;">20.37%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.4.9.4"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.9.4.1" style="font-size:90%;">36.27%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.10">
<td class="ltx_td ltx_align_left" id="S4.T9.4.10.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.10.1.1" style="font-size:90%;">Codex-679M</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.10.2"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.10.2.1" style="font-size:90%;">16.22%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.10.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.10.3.1" style="font-size:90%;">25.7%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.10.4"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.10.4.1" style="font-size:90%;">40.95%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.11">
<td class="ltx_td ltx_align_left" id="S4.T9.4.11.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.11.1.1" style="font-size:90%;">Codex-2.5B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.11.2"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.11.2.1" style="font-size:90%;">21.36%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.11.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.11.3.1" style="font-size:90%;">35.42%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.11.4"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.11.4.1" style="font-size:90%;">59.5%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.12">
<td class="ltx_td ltx_align_left" id="S4.T9.4.12.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.12.1.1" style="font-size:90%;">Codex-12B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.12.2"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.12.2.1" style="font-size:90%;">28.81%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.12.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.12.3.1" style="font-size:90%;">46.81%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.12.4"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.12.4.1" style="font-size:90%;">72.31%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.13">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T9.4.13.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.13.1.1" style="font-size:90%;">BLOOM-560M</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.4.13.2"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.13.2.1" style="font-size:90%;">0.82%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.4.13.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.13.3.1" style="font-size:90%;">3.02%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.4.13.4"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.13.4.1" style="font-size:90%;">5.91%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.14">
<td class="ltx_td ltx_align_left" id="S4.T9.4.14.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.14.1.1" style="font-size:90%;">BLOOM-1.1B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.14.2"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.14.2.1" style="font-size:90%;">2.48%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.14.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.14.3.1" style="font-size:90%;">5.93%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.14.4"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.14.4.1" style="font-size:90%;">9.62%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.15">
<td class="ltx_td ltx_align_left" id="S4.T9.4.15.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.15.1.1" style="font-size:90%;">BLOOM-1.7B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.15.2"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.15.2.1" style="font-size:90%;">4.03%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.15.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.15.3.1" style="font-size:90%;">7.45%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.15.4"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.15.4.1" style="font-size:90%;">12.75%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.16">
<td class="ltx_td ltx_align_left" id="S4.T9.4.16.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.16.1.1" style="font-size:90%;">BLOOM-3B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.16.2"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.16.2.1" style="font-size:90%;">6.48%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.16.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.16.3.1" style="font-size:90%;">11.35%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.16.4"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.16.4.1" style="font-size:90%;">20.43%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.17">
<td class="ltx_td ltx_align_left" id="S4.T9.4.17.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.17.1.1" style="font-size:90%;">BLOOM-7.1B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.17.2"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.17.2.1" style="font-size:90%;">7.73%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.17.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.17.3.1" style="font-size:90%;">17.38%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.17.4"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.17.4.1" style="font-size:90%;">29.47%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.18">
<td class="ltx_td ltx_align_left" id="S4.T9.4.18.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.18.1.1" style="font-size:90%;">BLOOM</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.18.2"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.18.2.1" style="font-size:90%;">15.52%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.18.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.18.3.1" style="font-size:90%;">32.20%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.18.4"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.18.4.1" style="font-size:90%;">55.45%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.19">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T9.4.19.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.19.1.1" style="font-size:90%;">BLOOMZ-560M</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.4.19.2"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.19.2.1" style="font-size:90%;">2.18 %</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.4.19.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.19.3.1" style="font-size:90%;">4.11%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.4.19.4"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.19.4.1" style="font-size:90%;">9.00%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.20">
<td class="ltx_td ltx_align_left" id="S4.T9.4.20.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.20.1.1" style="font-size:90%;">BLOOMZ-1.1B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.20.2"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.20.2.1" style="font-size:90%;">2.63%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.20.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.20.3.1" style="font-size:90%;">6.22%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.20.4"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.20.4.1" style="font-size:90%;">11.68%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.21">
<td class="ltx_td ltx_align_left" id="S4.T9.4.21.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.21.1.1" style="font-size:90%;">BLOOMZ-1.7B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.21.2"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.21.2.1" style="font-size:90%;">4.38%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.21.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.21.3.1" style="font-size:90%;">8.73%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.21.4"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.21.4.1" style="font-size:90%;">16.09%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.22">
<td class="ltx_td ltx_align_left" id="S4.T9.4.22.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.22.1.1" style="font-size:90%;">BLOOMZ-3B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.22.2"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.22.2.1" style="font-size:90%;">6.29%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.22.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.22.3.1" style="font-size:90%;">11.94%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.22.4"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.22.4.1" style="font-size:90%;">19.06%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.23">
<td class="ltx_td ltx_align_left" id="S4.T9.4.23.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.23.1.1" style="font-size:90%;">BLOOMZ-7.1B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.23.2"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.23.2.1" style="font-size:90%;">8.06%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.23.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.23.3.1" style="font-size:90%;">15.03%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.4.23.4"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.23.4.1" style="font-size:90%;">27.49%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.24">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T9.4.24.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.24.1.1" style="font-size:90%;">BLOOMZ</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T9.4.24.2"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.24.2.1" style="font-size:90%;">12.06%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T9.4.24.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.24.3.1" style="font-size:90%;">26.53%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T9.4.24.4"><span class="ltx_text ltx_font_smallcaps" id="S4.T9.4.24.4.1" style="font-size:90%;">48.44%</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering ltx_font_smallcaps" style="font-size:90%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_upright" id="S4.T9.20.1.1">Table 9</span>: </span>
Performance on HumanEval <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a class="ltx_ref" href="#bib.bib29" title="">2021</a>)</cite>. Non-BLOOM results come from prior work <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a class="ltx_ref" href="#bib.bib29" title="">2021</a>; Fried etÂ al., <a class="ltx_ref" href="#bib.bib45" title="">2022</a>)</cite>. The Codex model is a language model that was finetuned on code, while the GPT models <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="#bib.bib21" title="">Black etÂ al., </a>; Wang and Komatsuzaki, <a class="ltx_ref" href="#bib.bib152" title="">2021</a>; Black etÂ al., <a class="ltx_ref" href="#bib.bib22" title="">2022</a>)</cite> are trained on a mix of code and text like BLOOM.
</figcaption>
</figure>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">The BLOOM pretraining corpus, ROOTS, consists of around 11% of code. In TableÂ <a class="ltx_ref" href="#S4.T9" title="Table 9 â€£ 4.5 Code Generation â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">9</span></a>, we report benchmarking results of BLOOM on HumanEval <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a class="ltx_ref" href="#bib.bib29" title="">2021</a>)</cite>. We find the performance of pretrained BLOOM models to be similar to that of the similar-sized GPT models trained on the Pile <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al., <a class="ltx_ref" href="#bib.bib50" title="">2020</a>)</cite>. The Pile contains English data and around 13% of code (GitHub + StackExchange), which is similar to the code data sources and proportions in ROOTS. The Codex models, which have solely been finetuned on code, are significantly stronger than other models. Multitask finetuned BLOOMZ models do not improve significantly over BLOOM models. We hypothesize this is due to the finetuning dataset, xP3, not containing significant amounts of pure code completion. Rather, xP3 contains code-related tasks, such as estimating the time complexity of a given Python code snippet. Additional analysis is provided inÂ <cite class="ltx_cite ltx_citemacro_cite">Muennighoff etÂ al. (<a class="ltx_ref" href="#bib.bib100" title="">2022b</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>HELM benchmark</h3>
<figure class="ltx_figure" id="S4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="683" id="S4.F10.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F10.2.1.1" style="font-size:90%;">Figure 10</span>: </span><span class="ltx_text" id="S4.F10.3.2" style="font-size:90%;">Results for a wide variety of language models on the 5-shot HELM benchmark. Taken from <cite class="ltx_cite ltx_citemacro_cite">Liang etÂ al. (<a class="ltx_ref" href="#bib.bib79" title="">2022</a>)</cite></span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.1">For completeness, we reproduce here evaluations from the HELM benchmark <cite class="ltx_cite ltx_citemacro_citep">(Liang etÂ al., <a class="ltx_ref" href="#bib.bib79" title="">2022</a>)</cite>, which ran 5-shot evaluations of a variety of language models on English-only tasks. Despite the multilingual training, BLOOM is roughly on par in accuracy with previous-generation English-only models, such as GPT3-davinci v1 and J1-Grande v1, but behind more recent monolingual models such as InstructGPT davinci v2, Turing NLG v2, Anthropic-LM v4-s3, or OPT. Like other large language models of this size, it is not very well calibrated, but quite robust. Finally, on this benchmark, it is one of the best models for fairness, slightly more toxic than average in English, and average for bias.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7 </span>Multitask Finetuning</h3>
<figure class="ltx_figure" id="S4.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="830" id="S4.F11.g1" src="x12.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F11.2.1.1" style="font-size:90%;">Figure 11</span>: </span><span class="ltx_text" id="S4.F11.3.2" style="font-size:90%;">BLOOMZ zero-shot task generalization. Five untuned prompts are evaluated for each dataset and plotted. T0 is monolingual (English) while other models are multilingual. T0 performance may be hurt by its inability to tokenize some non-English texts.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS7.p1">
<p class="ltx_p" id="S4.SS7.p1.1">Building on recent work on multitask finetuning <cite class="ltx_cite ltx_citemacro_citep">(Sanh etÂ al., <a class="ltx_ref" href="#bib.bib128" title="">2022</a>; Wei etÂ al., <a class="ltx_ref" href="#bib.bib158" title="">2021</a>; Wang etÂ al., <a class="ltx_ref" href="#bib.bib156" title="">2022a</a>)</cite> we explore using <span class="ltx_text ltx_font_italic" id="S4.SS7.p1.1.1">multilingual</span> multitask finetuning to improve the zero-shot performance of the BLOOM model. We conducted multilingual multitask finetuning of BLOOM models using the xP3 corpus outlined in Section <a class="ltx_ref" href="#S3.SS1.SSS4" title="3.1.4 Prompted Datasets â€£ 3.1 Training Dataset â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">3.1.4</span></a>. We find that zero-shot performance significantly increases. In FigureÂ <a class="ltx_ref" href="#S4.F11" title="Figure 11 â€£ 4.7 Multitask Finetuning â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">11</span></a>, we compare the zero-shot performance of pretrained BLOOM and XGLM models with multitask finetuned BLOOMZ, T0 and mTk-InstructÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="#bib.bib157" title="">2022b</a>)</cite>. BLOOM and XGLM performances are near the random baselines of 33% for NLI (XNLI) and 50% for coreference resolution (XWinograd) and sentence completion (XCOPA and XStoryCloze). After going through multilingual multitask finetuning (BLOOMZ), zero-shot performance significantly improves on the depicted held-out tasks. Despite also being multitask finetuned, T0 performs badly on the multilingual datasets shown due to it being a monolingual English model. Additional results provided inÂ <cite class="ltx_cite ltx_citemacro_cite">Muennighoff etÂ al. (<a class="ltx_ref" href="#bib.bib100" title="">2022b</a>)</cite>, however, show that models finetuned on xP3 also outperform T0 on English datasets when controlling for size and architecture. This is likely due to T0â€™s finetuning dataset (P3) containing less diverse datasets and prompts than xP3. Multitask finetuning performance has been shown to correlate with the amount of datasets and promptsÂ <cite class="ltx_cite ltx_citemacro_citep">(Chung etÂ al., <a class="ltx_ref" href="#bib.bib32" title="">2022</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS8">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.8 </span>Embeddings</h3>
<figure class="ltx_table" id="S4.T10">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T10.2" style="width:433.6pt;height:581.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(59.8pt,-80.3pt) scale(1.38125966309239,1.38125966309239) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T10.2.1">
<tr class="ltx_tr" id="S4.T10.2.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T10.2.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T10.2.1.1.2"><span class="ltx_text" id="S4.T10.2.1.1.2.1" style="font-size:70%;">ST5-XL</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T10.2.1.1.3"><span class="ltx_text" id="S4.T10.2.1.1.3.1" style="font-size:70%;">LASER2</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T10.2.1.1.4">
<span class="ltx_text" id="S4.T10.2.1.1.4.1" style="font-size:70%;">MiniLM-L12 </span><span class="ltx_note ltx_role_footnote" id="footnote35"><sup class="ltx_note_mark">35</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">35</sup><span class="ltx_tag ltx_tag_note">35</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://hf.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2" title="">https://hf.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2</a></span></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T10.2.1.1.5">
<span class="ltx_text" id="S4.T10.2.1.1.5.1" style="font-size:70%;">MPNet</span><span class="ltx_note ltx_role_footnote" id="footnote36"><sup class="ltx_note_mark">36</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">36</sup><span class="ltx_tag ltx_tag_note">36</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://hf.co/sentence-transformers/https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2" title="">https://hf.co/sentence-transformers/https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2</a></span></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T10.2.1.1.6"><span class="ltx_text" id="S4.T10.2.1.1.6.1" style="font-size:70%;">LaBSE</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T10.2.1.1.7"><span class="ltx_text" id="S4.T10.2.1.1.7.1" style="font-size:70%;">SGPT-BLOOM-1.7B</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T10.2.1.1.8"><span class="ltx_text" id="S4.T10.2.1.1.8.1" style="font-size:70%;">SGPT-BLOOM-7.1B</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.2.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="8" id="S4.T10.2.1.2.1">
<em class="ltx_emph ltx_font_italic" id="S4.T10.2.1.2.1.1" style="font-size:70%;">Embedding classification performance on MASSIVEÂ <cite class="ltx_cite ltx_citemacro_citep">(FitzGerald etÂ al., <a class="ltx_ref" href="#bib.bib44" title="">2022</a>)</cite> scored using accuracy</em><span class="ltx_text" id="S4.T10.2.1.2.1.2" style="font-size:70%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T10.2.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T10.2.1.3.1"><span class="ltx_text" id="S4.T10.2.1.3.1.1" style="font-size:70%;">Arabic (ar)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.2.1.3.2"><span class="ltx_text" id="S4.T10.2.1.3.2.1" style="font-size:70%;">4.18</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.2.1.3.3"><span class="ltx_text" id="S4.T10.2.1.3.3.1" style="font-size:70%;">37.16</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.2.1.3.4"><span class="ltx_text" id="S4.T10.2.1.3.4.1" style="font-size:70%;">51.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.2.1.3.5"><span class="ltx_text" id="S4.T10.2.1.3.5.1" style="font-size:70%;">45.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.2.1.3.6"><span class="ltx_text" id="S4.T10.2.1.3.6.1" style="font-size:70%;">50.86</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.2.1.3.7"><span class="ltx_text" id="S4.T10.2.1.3.7.1" style="font-size:70%;">54.59</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.2.1.3.8"><span class="ltx_text ltx_font_bold" id="S4.T10.2.1.3.8.1" style="font-size:70%;">59.25</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.2.1.4">
<td class="ltx_td ltx_align_left" id="S4.T10.2.1.4.1"><span class="ltx_text" id="S4.T10.2.1.4.1.1" style="font-size:70%;">Bengali (bn)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.4.2"><span class="ltx_text" id="S4.T10.2.1.4.2.1" style="font-size:70%;">2.60</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.4.3"><span class="ltx_text" id="S4.T10.2.1.4.3.1" style="font-size:70%;">42.51</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.4.4"><span class="ltx_text" id="S4.T10.2.1.4.4.1" style="font-size:70%;">48.79</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.4.5"><span class="ltx_text" id="S4.T10.2.1.4.5.1" style="font-size:70%;">35.34</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.4.6"><span class="ltx_text" id="S4.T10.2.1.4.6.1" style="font-size:70%;">58.22</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.4.7"><span class="ltx_text" id="S4.T10.2.1.4.7.1" style="font-size:70%;">57.76</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.4.8"><span class="ltx_text ltx_font_bold" id="S4.T10.2.1.4.8.1" style="font-size:70%;">61.59</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.2.1.5">
<td class="ltx_td ltx_align_left" id="S4.T10.2.1.5.1"><span class="ltx_text" id="S4.T10.2.1.5.1.1" style="font-size:70%;">English (en)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.5.2"><span class="ltx_text ltx_font_bold" id="S4.T10.2.1.5.2.1" style="font-size:70%;">72.09</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.5.3"><span class="ltx_text" id="S4.T10.2.1.5.3.1" style="font-size:70%;">47.91</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.5.4"><span class="ltx_text" id="S4.T10.2.1.5.4.1" style="font-size:70%;">69.32</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.5.5"><span class="ltx_text" id="S4.T10.2.1.5.5.1" style="font-size:70%;">66.84</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.5.6"><span class="ltx_text" id="S4.T10.2.1.5.6.1" style="font-size:70%;">61.46</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.5.7"><span class="ltx_text" id="S4.T10.2.1.5.7.1" style="font-size:70%;">66.69</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.5.8"><span class="ltx_text" id="S4.T10.2.1.5.8.1" style="font-size:70%;">69.67</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.2.1.6">
<td class="ltx_td ltx_align_left" id="S4.T10.2.1.6.1"><span class="ltx_text" id="S4.T10.2.1.6.1.1" style="font-size:70%;">Spanish (es)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.6.2"><span class="ltx_text" id="S4.T10.2.1.6.2.1" style="font-size:70%;">57.97</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.6.3"><span class="ltx_text" id="S4.T10.2.1.6.3.1" style="font-size:70%;">45.44</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.6.4"><span class="ltx_text" id="S4.T10.2.1.6.4.1" style="font-size:70%;">64.43</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.6.5"><span class="ltx_text" id="S4.T10.2.1.6.5.1" style="font-size:70%;">59.66</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.6.6"><span class="ltx_text" id="S4.T10.2.1.6.6.1" style="font-size:70%;">58.32</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.6.7"><span class="ltx_text" id="S4.T10.2.1.6.7.1" style="font-size:70%;">61.77</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.6.8"><span class="ltx_text ltx_font_bold" id="S4.T10.2.1.6.8.1" style="font-size:70%;">66.35</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.2.1.7">
<td class="ltx_td ltx_align_left" id="S4.T10.2.1.7.1"><span class="ltx_text" id="S4.T10.2.1.7.1.1" style="font-size:70%;">French (fr)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.7.2"><span class="ltx_text" id="S4.T10.2.1.7.2.1" style="font-size:70%;">60.99</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.7.3"><span class="ltx_text" id="S4.T10.2.1.7.3.1" style="font-size:70%;">46.13</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.7.4"><span class="ltx_text" id="S4.T10.2.1.7.4.1" style="font-size:70%;">64.82</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.7.5"><span class="ltx_text" id="S4.T10.2.1.7.5.1" style="font-size:70%;">60.25</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.7.6"><span class="ltx_text" id="S4.T10.2.1.7.6.1" style="font-size:70%;">60.47</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.7.7"><span class="ltx_text" id="S4.T10.2.1.7.7.1" style="font-size:70%;">64.58</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.7.8"><span class="ltx_text ltx_font_bold" id="S4.T10.2.1.7.8.1" style="font-size:70%;">66.95</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.2.1.8">
<td class="ltx_td ltx_align_left" id="S4.T10.2.1.8.1"><span class="ltx_text" id="S4.T10.2.1.8.1.1" style="font-size:70%;">Hindi (hi)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.8.2"><span class="ltx_text" id="S4.T10.2.1.8.2.1" style="font-size:70%;">3.02</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.8.3"><span class="ltx_text" id="S4.T10.2.1.8.3.1" style="font-size:70%;">40.20</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.8.4"><span class="ltx_text" id="S4.T10.2.1.8.4.1" style="font-size:70%;">62.77</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.8.5"><span class="ltx_text" id="S4.T10.2.1.8.5.1" style="font-size:70%;">58.37</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.8.6"><span class="ltx_text" id="S4.T10.2.1.8.6.1" style="font-size:70%;">59.40</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.8.7"><span class="ltx_text" id="S4.T10.2.1.8.7.1" style="font-size:70%;">60.74</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.8.8"><span class="ltx_text ltx_font_bold" id="S4.T10.2.1.8.8.1" style="font-size:70%;">63.54</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.2.1.9">
<td class="ltx_td ltx_align_left" id="S4.T10.2.1.9.1"><span class="ltx_text" id="S4.T10.2.1.9.1.1" style="font-size:70%;">Indonesian (id)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.9.2"><span class="ltx_text" id="S4.T10.2.1.9.2.1" style="font-size:70%;">41.53</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.9.3"><span class="ltx_text" id="S4.T10.2.1.9.3.1" style="font-size:70%;">45.81</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.9.4"><span class="ltx_text ltx_font_bold" id="S4.T10.2.1.9.4.1" style="font-size:70%;">65.43</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.9.5"><span class="ltx_text" id="S4.T10.2.1.9.5.1" style="font-size:70%;">59.85</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.9.6"><span class="ltx_text" id="S4.T10.2.1.9.6.1" style="font-size:70%;">61.12</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.9.7"><span class="ltx_text" id="S4.T10.2.1.9.7.1" style="font-size:70%;">60.07</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.9.8"><span class="ltx_text" id="S4.T10.2.1.9.8.1" style="font-size:70%;">64.06</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.2.1.10">
<td class="ltx_td ltx_align_left" id="S4.T10.2.1.10.1"><span class="ltx_text" id="S4.T10.2.1.10.1.1" style="font-size:70%;">Kannada (kn)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.10.2"><span class="ltx_text" id="S4.T10.2.1.10.2.1" style="font-size:70%;">2.79</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.10.3"><span class="ltx_text" id="S4.T10.2.1.10.3.1" style="font-size:70%;">4.32</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.10.4"><span class="ltx_text" id="S4.T10.2.1.10.4.1" style="font-size:70%;">50.63</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.10.5"><span class="ltx_text" id="S4.T10.2.1.10.5.1" style="font-size:70%;">40.98</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.10.6"><span class="ltx_text ltx_font_bold" id="S4.T10.2.1.10.6.1" style="font-size:70%;">56.24</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.10.7"><span class="ltx_text" id="S4.T10.2.1.10.7.1" style="font-size:70%;">48.56</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.10.8"><span class="ltx_text" id="S4.T10.2.1.10.8.1" style="font-size:70%;">53.54</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.2.1.11">
<td class="ltx_td ltx_align_left" id="S4.T10.2.1.11.1"><span class="ltx_text" id="S4.T10.2.1.11.1.1" style="font-size:70%;">Malayalam (ml)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.11.2"><span class="ltx_text" id="S4.T10.2.1.11.2.1" style="font-size:70%;">2.98</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.11.3"><span class="ltx_text" id="S4.T10.2.1.11.3.1" style="font-size:70%;">41.33</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.11.4"><span class="ltx_text" id="S4.T10.2.1.11.4.1" style="font-size:70%;">54.34</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.11.5"><span class="ltx_text" id="S4.T10.2.1.11.5.1" style="font-size:70%;">42.41</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.11.6"><span class="ltx_text" id="S4.T10.2.1.11.6.1" style="font-size:70%;">57.91</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.11.7"><span class="ltx_text" id="S4.T10.2.1.11.7.1" style="font-size:70%;">55.10</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.11.8"><span class="ltx_text ltx_font_bold" id="S4.T10.2.1.11.8.1" style="font-size:70%;">58.27</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.2.1.12">
<td class="ltx_td ltx_align_left" id="S4.T10.2.1.12.1"><span class="ltx_text" id="S4.T10.2.1.12.1.1" style="font-size:70%;">Portuguese (pt)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.12.2"><span class="ltx_text" id="S4.T10.2.1.12.2.1" style="font-size:70%;">57.95</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.12.3"><span class="ltx_text" id="S4.T10.2.1.12.3.1" style="font-size:70%;">48.55</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.12.4"><span class="ltx_text" id="S4.T10.2.1.12.4.1" style="font-size:70%;">64.89</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.12.5"><span class="ltx_text" id="S4.T10.2.1.12.5.1" style="font-size:70%;">61.27</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.12.6"><span class="ltx_text" id="S4.T10.2.1.12.6.1" style="font-size:70%;">60.16</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.12.7"><span class="ltx_text" id="S4.T10.2.1.12.7.1" style="font-size:70%;">62.52</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.12.8"><span class="ltx_text ltx_font_bold" id="S4.T10.2.1.12.8.1" style="font-size:70%;">66.69</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.2.1.13">
<td class="ltx_td ltx_align_left" id="S4.T10.2.1.13.1"><span class="ltx_text" id="S4.T10.2.1.13.1.1" style="font-size:70%;">Swahili (sw)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.13.2"><span class="ltx_text" id="S4.T10.2.1.13.2.1" style="font-size:70%;">30.60</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.13.3"><span class="ltx_text" id="S4.T10.2.1.13.3.1" style="font-size:70%;">31.89</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.13.4"><span class="ltx_text" id="S4.T10.2.1.13.4.1" style="font-size:70%;">31.95</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.13.5"><span class="ltx_text" id="S4.T10.2.1.13.5.1" style="font-size:70%;">29.57</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.13.6"><span class="ltx_text ltx_font_bold" id="S4.T10.2.1.13.6.1" style="font-size:70%;">51.62</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.13.7"><span class="ltx_text" id="S4.T10.2.1.13.7.1" style="font-size:70%;">43.90</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.13.8"><span class="ltx_text" id="S4.T10.2.1.13.8.1" style="font-size:70%;">49.81</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.2.1.14">
<td class="ltx_td ltx_align_left" id="S4.T10.2.1.14.1"><span class="ltx_text" id="S4.T10.2.1.14.1.1" style="font-size:70%;">Tamil (ta)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.14.2"><span class="ltx_text" id="S4.T10.2.1.14.2.1" style="font-size:70%;">1.79</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.14.3"><span class="ltx_text" id="S4.T10.2.1.14.3.1" style="font-size:70%;">29.63</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.14.4"><span class="ltx_text" id="S4.T10.2.1.14.4.1" style="font-size:70%;">50.17</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.14.5"><span class="ltx_text" id="S4.T10.2.1.14.5.1" style="font-size:70%;">36.77</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.14.6"><span class="ltx_text" id="S4.T10.2.1.14.6.1" style="font-size:70%;">55.04</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.14.7"><span class="ltx_text" id="S4.T10.2.1.14.7.1" style="font-size:70%;">52.66</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.14.8"><span class="ltx_text ltx_font_bold" id="S4.T10.2.1.14.8.1" style="font-size:70%;">56.40</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.2.1.15">
<td class="ltx_td ltx_align_left" id="S4.T10.2.1.15.1"><span class="ltx_text" id="S4.T10.2.1.15.1.1" style="font-size:70%;">Telugu (te)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.15.2"><span class="ltx_text" id="S4.T10.2.1.15.2.1" style="font-size:70%;">2.26</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.15.3"><span class="ltx_text" id="S4.T10.2.1.15.3.1" style="font-size:70%;">36.03</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.15.4"><span class="ltx_text" id="S4.T10.2.1.15.4.1" style="font-size:70%;">52.82</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.15.5"><span class="ltx_text" id="S4.T10.2.1.15.5.1" style="font-size:70%;">40.72</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.15.6"><span class="ltx_text ltx_font_bold" id="S4.T10.2.1.15.6.1" style="font-size:70%;">58.32</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.15.7"><span class="ltx_text" id="S4.T10.2.1.15.7.1" style="font-size:70%;">49.32</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.15.8"><span class="ltx_text" id="S4.T10.2.1.15.8.1" style="font-size:70%;">54.71</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.2.1.16">
<td class="ltx_td ltx_align_left" id="S4.T10.2.1.16.1"><span class="ltx_text" id="S4.T10.2.1.16.1.1" style="font-size:70%;">Urdu (ur)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.16.2"><span class="ltx_text" id="S4.T10.2.1.16.2.1" style="font-size:70%;">2.70</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.16.3"><span class="ltx_text" id="S4.T10.2.1.16.3.1" style="font-size:70%;">26.11</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.16.4"><span class="ltx_text" id="S4.T10.2.1.16.4.1" style="font-size:70%;">56.37</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.16.5"><span class="ltx_text" id="S4.T10.2.1.16.5.1" style="font-size:70%;">52.80</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.16.6"><span class="ltx_text" id="S4.T10.2.1.16.6.1" style="font-size:70%;">56.70</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.16.7"><span class="ltx_text" id="S4.T10.2.1.16.7.1" style="font-size:70%;">51.00</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.16.8"><span class="ltx_text ltx_font_bold" id="S4.T10.2.1.16.8.1" style="font-size:70%;">56.75</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.2.1.17">
<td class="ltx_td ltx_align_left" id="S4.T10.2.1.17.1"><span class="ltx_text" id="S4.T10.2.1.17.1.1" style="font-size:70%;">Vietnamese (vi)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.17.2"><span class="ltx_text" id="S4.T10.2.1.17.2.1" style="font-size:70%;">21.47</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.17.3"><span class="ltx_text" id="S4.T10.2.1.17.3.1" style="font-size:70%;">44.33</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.17.4"><span class="ltx_text" id="S4.T10.2.1.17.4.1" style="font-size:70%;">59.68</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.17.5"><span class="ltx_text" id="S4.T10.2.1.17.5.1" style="font-size:70%;">56.61</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.17.6"><span class="ltx_text" id="S4.T10.2.1.17.6.1" style="font-size:70%;">56.67</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.17.7"><span class="ltx_text" id="S4.T10.2.1.17.7.1" style="font-size:70%;">59.85</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.17.8"><span class="ltx_text ltx_font_bold" id="S4.T10.2.1.17.8.1" style="font-size:70%;">64.53</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.2.1.18">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="8" id="S4.T10.2.1.18.1">
<em class="ltx_emph ltx_font_italic" id="S4.T10.2.1.18.1.1" style="font-size:70%;">Semantic textual similarity on STS22Â <cite class="ltx_cite ltx_citemacro_citep">(Madabushi etÂ al., <a class="ltx_ref" href="#bib.bib86" title="">2022</a>)</cite> scored using spearman correlation of cosine similarities</em><span class="ltx_text" id="S4.T10.2.1.18.1.2" style="font-size:70%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T10.2.1.19">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T10.2.1.19.1"><span class="ltx_text" id="S4.T10.2.1.19.1.1" style="font-size:70%;">Arabic (ar)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.2.1.19.2"><span class="ltx_text" id="S4.T10.2.1.19.2.1" style="font-size:70%;">29.60</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.2.1.19.3"><span class="ltx_text" id="S4.T10.2.1.19.3.1" style="font-size:70%;">42.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.2.1.19.4"><span class="ltx_text" id="S4.T10.2.1.19.4.1" style="font-size:70%;">52.19</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.2.1.19.5"><span class="ltx_text" id="S4.T10.2.1.19.5.1" style="font-size:70%;">46.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.2.1.19.6"><span class="ltx_text" id="S4.T10.2.1.19.6.1" style="font-size:70%;">57.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.2.1.19.7"><span class="ltx_text" id="S4.T10.2.1.19.7.1" style="font-size:70%;">48.64</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.2.1.19.8"><span class="ltx_text ltx_font_bold" id="S4.T10.2.1.19.8.1" style="font-size:70%;">58.67</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.2.1.20">
<td class="ltx_td ltx_align_left" id="S4.T10.2.1.20.1"><span class="ltx_text" id="S4.T10.2.1.20.1.1" style="font-size:70%;">English (en)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.20.2"><span class="ltx_text" id="S4.T10.2.1.20.2.1" style="font-size:70%;">64.32</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.20.3"><span class="ltx_text" id="S4.T10.2.1.20.3.1" style="font-size:70%;">39.76</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.20.4"><span class="ltx_text" id="S4.T10.2.1.20.4.1" style="font-size:70%;">63.06</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.20.5"><span class="ltx_text" id="S4.T10.2.1.20.5.1" style="font-size:70%;">61.72</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.20.6"><span class="ltx_text" id="S4.T10.2.1.20.6.1" style="font-size:70%;">60.97</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.20.7"><span class="ltx_text" id="S4.T10.2.1.20.7.1" style="font-size:70%;">61.45</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.20.8"><span class="ltx_text ltx_font_bold" id="S4.T10.2.1.20.8.1" style="font-size:70%;">66.13</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.2.1.21">
<td class="ltx_td ltx_align_left" id="S4.T10.2.1.21.1"><span class="ltx_text" id="S4.T10.2.1.21.1.1" style="font-size:70%;">Spanish (es)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.21.2"><span class="ltx_text" id="S4.T10.2.1.21.2.1" style="font-size:70%;">58.16</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.21.3"><span class="ltx_text" id="S4.T10.2.1.21.3.1" style="font-size:70%;">54.92</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.21.4"><span class="ltx_text" id="S4.T10.2.1.21.4.1" style="font-size:70%;">59.91</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.21.5"><span class="ltx_text" id="S4.T10.2.1.21.5.1" style="font-size:70%;">56.56</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.21.6"><span class="ltx_text" id="S4.T10.2.1.21.6.1" style="font-size:70%;">63.18</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.21.7"><span class="ltx_text" id="S4.T10.2.1.21.7.1" style="font-size:70%;">61.81</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.21.8"><span class="ltx_text ltx_font_bold" id="S4.T10.2.1.21.8.1" style="font-size:70%;">65.41</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.2.1.22">
<td class="ltx_td ltx_align_left" id="S4.T10.2.1.22.1"><span class="ltx_text" id="S4.T10.2.1.22.1.1" style="font-size:70%;">French (fr)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.22.2"><span class="ltx_text" id="S4.T10.2.1.22.2.1" style="font-size:70%;">77.49</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.22.3"><span class="ltx_text" id="S4.T10.2.1.22.3.1" style="font-size:70%;">58.61</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.22.4"><span class="ltx_text" id="S4.T10.2.1.22.4.1" style="font-size:70%;">74.30</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.22.5"><span class="ltx_text" id="S4.T10.2.1.22.5.1" style="font-size:70%;">70.55</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.22.6"><span class="ltx_text" id="S4.T10.2.1.22.6.1" style="font-size:70%;">77.95</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.22.7"><span class="ltx_text" id="S4.T10.2.1.22.7.1" style="font-size:70%;">73.18</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.2.1.22.8"><span class="ltx_text ltx_font_bold" id="S4.T10.2.1.22.8.1" style="font-size:70%;">80.38</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.2.1.23">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T10.2.1.23.1"><span class="ltx_text" id="S4.T10.2.1.23.1.1" style="font-size:70%;">Chinese (zh)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T10.2.1.23.2"><span class="ltx_text" id="S4.T10.2.1.23.2.1" style="font-size:70%;">33.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T10.2.1.23.3"><span class="ltx_text" id="S4.T10.2.1.23.3.1" style="font-size:70%;">49.41</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T10.2.1.23.4"><span class="ltx_text" id="S4.T10.2.1.23.4.1" style="font-size:70%;">61.75</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T10.2.1.23.5"><span class="ltx_text" id="S4.T10.2.1.23.5.1" style="font-size:70%;">58.75</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T10.2.1.23.6"><span class="ltx_text" id="S4.T10.2.1.23.6.1" style="font-size:70%;">63.02</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T10.2.1.23.7"><span class="ltx_text" id="S4.T10.2.1.23.7.1" style="font-size:70%;">58.53</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T10.2.1.23.8"><span class="ltx_text ltx_font_bold" id="S4.T10.2.1.23.8.1" style="font-size:70%;">66.78</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T10.9.1.1" style="font-size:129%;">Table 10</span>: </span><span class="ltx_text" id="S4.T10.10.2" style="font-size:129%;">Performance of BLOOM models finetuned for sentence embeddings on classification and STS datasets from MTEBÂ <cite class="ltx_cite ltx_citemacro_citep">(Muennighoff etÂ al., <a class="ltx_ref" href="#bib.bib100" title="">2022b</a>)</cite>.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS8.p1">
<p class="ltx_p" id="S4.SS8.p1.1">In Section <a class="ltx_ref" href="#S3.SS5" title="3.5 Training â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">3.5</span></a>, we have outlined the contrastive finetuning procedure for creating SGPT-BLOOM text embedding models. In Table <a class="ltx_ref" href="#S4.T10" title="Table 10 â€£ 4.8 Embeddings â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">10</span></a>, we report benchmarking results on two multilingual datasets from the Massive Text Embedding Benchmark (MTEB,Â <cite class="ltx_cite ltx_citemacro_citep">Muennighoff etÂ al., <a class="ltx_ref" href="#bib.bib99" title="">2022a</a></cite>). We find that SGPT-BLOOM-7.1B-msmarco<span class="ltx_note ltx_role_footnote" id="footnote37"><sup class="ltx_note_mark">37</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">37</sup><span class="ltx_tag ltx_tag_note">37</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://hf.co/bigscience/sgpt-bloom-7b1-msmarco" title="">hf.co/bigscience/sgpt-bloom-7b1-msmarco</a></span></span></span> provides state-of-the-art performance on several classification and semantic textual similarity splits. However, with 7.1 billion parameters it is an order of magnitude larger than models like the displayed multilingual MiniLM<span class="ltx_note ltx_role_footnote" id="footnote38"><sup class="ltx_note_mark">38</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">38</sup><span class="ltx_tag ltx_tag_note">38</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://hf.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2" title="">hf.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2</a></span></span></span> and MPNet<span class="ltx_note ltx_role_footnote" id="footnote39"><sup class="ltx_note_mark">39</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">39</sup><span class="ltx_tag ltx_tag_note">39</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://hf.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2" title="">hf.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2</a></span></span></span>. SGPT-BLOOM-1.7B-nli<span class="ltx_note ltx_role_footnote" id="footnote40"><sup class="ltx_note_mark">40</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">40</sup><span class="ltx_tag ltx_tag_note">40</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://hf.co/bigscience-data/sgpt-bloom-1b7-nli" title="">hf.co/bigscience/sgpt-bloom-1b7-nli</a></span></span></span> performs significantly worse, likely due to less parameters and its finetuning being shorter (NLI is a much smaller dataset than MS-MARCO). Apart from the BLOOM models, ST5-XL<span class="ltx_note ltx_role_footnote" id="footnote41"><sup class="ltx_note_mark">41</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">41</sup><span class="ltx_tag ltx_tag_note">41</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://hf.co/sentence-transformers/sentence-t5-xl" title="">hf.co/sentence-transformers/sentence-t5-xl</a></span></span></span> is the largest model with 1.2 billion parameters. However, as an English-only model its performance on non-English languages is poor. The languages displayed are part of the BLOOM pretraining corpus. Performance on more languages and datasets can be inspected on the MTEB leaderboard<span class="ltx_note ltx_role_footnote" id="footnote42"><sup class="ltx_note_mark">42</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">42</sup><span class="ltx_tag ltx_tag_note">42</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://hf.co/spaces/mteb/leaderboard" title="">hf.co/spaces/mteb/leaderboard</a></span></span></span>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS9">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.9 </span>Multilingual Probing</h3>
<div class="ltx_para" id="S4.SS9.p1">
<p class="ltx_p" id="S4.SS9.p1.1">Probing has emerged as a significant evaluation paradigm to analyze and interpret the inner workings of LLMsÂ <cite class="ltx_cite ltx_citemacro_citep">(Ettinger etÂ al., <a class="ltx_ref" href="#bib.bib41" title="">2016</a>; Adi etÂ al., <a class="ltx_ref" href="#bib.bib3" title="">2017</a>; Belinkov etÂ al., <a class="ltx_ref" href="#bib.bib14" title="">2017</a>; Hupkes etÂ al., <a class="ltx_ref" href="#bib.bib63" title="">2018</a>; Tenney etÂ al., <a class="ltx_ref" href="#bib.bib147" title="">2018</a>; Belinkov and Glass, <a class="ltx_ref" href="#bib.bib13" title="">2019</a>; Teehan etÂ al., <a class="ltx_ref" href="#bib.bib146" title="">2022</a>)</cite>, although it comes with certain shortcomings <cite class="ltx_cite ltx_citemacro_citep">(Belinkov, <a class="ltx_ref" href="#bib.bib12" title="">2022</a>)</cite>. Examination of the LLM embeddings can help shed light on the generalizing abilities of the model apart from its training objective loss or downstream task evaluation, which is especially beneficial for examining languages lacking annotated datasets or benchmarks.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS9.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.9.1 </span>Method</h4>
<div class="ltx_para" id="S4.SS9.SSS1.p1">
<p class="ltx_p" id="S4.SS9.SSS1.p1.5">For interpreting BLOOMâ€™s multilingual generalizing abilities, we utilize the â€œUniversal Probingâ€ framework<span class="ltx_note ltx_role_footnote" id="footnote43"><sup class="ltx_note_mark">43</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">43</sup><span class="ltx_tag ltx_tag_note">43</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://github.com/bigscience-workshop/massive-probing-framework" title="">github.com/bigscience-workshop/massive-probing-framework</a></span></span></span> for systematic probing analysis in <math alttext="104" class="ltx_Math" display="inline" id="S4.SS9.SSS1.p1.1.m1.1"><semantics id="S4.SS9.SSS1.p1.1.m1.1a"><mn id="S4.SS9.SSS1.p1.1.m1.1.1" xref="S4.SS9.SSS1.p1.1.m1.1.1.cmml">104</mn><annotation-xml encoding="MathML-Content" id="S4.SS9.SSS1.p1.1.m1.1b"><cn id="S4.SS9.SSS1.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS9.SSS1.p1.1.m1.1.1">104</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS9.SSS1.p1.1.m1.1c">104</annotation><annotation encoding="application/x-llamapun" id="S4.SS9.SSS1.p1.1.m1.1d">104</annotation></semantics></math> languages and <math alttext="80" class="ltx_Math" display="inline" id="S4.SS9.SSS1.p1.2.m2.1"><semantics id="S4.SS9.SSS1.p1.2.m2.1a"><mn id="S4.SS9.SSS1.p1.2.m2.1.1" xref="S4.SS9.SSS1.p1.2.m2.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S4.SS9.SSS1.p1.2.m2.1b"><cn id="S4.SS9.SSS1.p1.2.m2.1.1.cmml" type="integer" xref="S4.SS9.SSS1.p1.2.m2.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS9.SSS1.p1.2.m2.1c">80</annotation><annotation encoding="application/x-llamapun" id="S4.SS9.SSS1.p1.2.m2.1d">80</annotation></semantics></math> morphosyntactic featuresÂ <cite class="ltx_cite ltx_citemacro_citep">(Serikov etÂ al., <a class="ltx_ref" href="#bib.bib131" title="">2022</a>)</cite>. The framework provides SentEval-styleÂ <cite class="ltx_cite ltx_citemacro_citep">(Conneau etÂ al., <a class="ltx_ref" href="#bib.bib34" title="">2018</a>)</cite> probing setup and datasets for each language available in Universal Dependencies (UD;Â <cite class="ltx_cite ltx_citemacro_citep">Nivre etÂ al., <a class="ltx_ref" href="#bib.bib106" title="">2016</a></cite>). We consider the following <math alttext="17" class="ltx_Math" display="inline" id="S4.SS9.SSS1.p1.3.m3.1"><semantics id="S4.SS9.SSS1.p1.3.m3.1a"><mn id="S4.SS9.SSS1.p1.3.m3.1.1" xref="S4.SS9.SSS1.p1.3.m3.1.1.cmml">17</mn><annotation-xml encoding="MathML-Content" id="S4.SS9.SSS1.p1.3.m3.1b"><cn id="S4.SS9.SSS1.p1.3.m3.1.1.cmml" type="integer" xref="S4.SS9.SSS1.p1.3.m3.1.1">17</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS9.SSS1.p1.3.m3.1c">17</annotation><annotation encoding="application/x-llamapun" id="S4.SS9.SSS1.p1.3.m3.1d">17</annotation></semantics></math> languages from <math alttext="7" class="ltx_Math" display="inline" id="S4.SS9.SSS1.p1.4.m4.1"><semantics id="S4.SS9.SSS1.p1.4.m4.1a"><mn id="S4.SS9.SSS1.p1.4.m4.1.1" xref="S4.SS9.SSS1.p1.4.m4.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S4.SS9.SSS1.p1.4.m4.1b"><cn id="S4.SS9.SSS1.p1.4.m4.1.1.cmml" type="integer" xref="S4.SS9.SSS1.p1.4.m4.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS9.SSS1.p1.4.m4.1c">7</annotation><annotation encoding="application/x-llamapun" id="S4.SS9.SSS1.p1.4.m4.1d">7</annotation></semantics></math> language families present in BLOOMâ€™s pretraining corpus (<a class="ltx_ref" href="#S3.SS1" title="3.1 Training Dataset â€£ 3 BLOOM â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">3.1</span></a>) and UD treebanks: Arabic (Afro-Asiatic), Bambara (Mande), Basque (language isolate), Bengali, Catalan, English, French, Hindi, Marathi, Portuguese, Spanish, Urdu (Indo-European), Chinese (Sino-Tibetan), Indonesian (Austronesian), Tamil (Dravidian), Wolof, Yoruba (Niger-Congo). Our setup covers <math alttext="38" class="ltx_Math" display="inline" id="S4.SS9.SSS1.p1.5.m5.1"><semantics id="S4.SS9.SSS1.p1.5.m5.1a"><mn id="S4.SS9.SSS1.p1.5.m5.1.1" xref="S4.SS9.SSS1.p1.5.m5.1.1.cmml">38</mn><annotation-xml encoding="MathML-Content" id="S4.SS9.SSS1.p1.5.m5.1b"><cn id="S4.SS9.SSS1.p1.5.m5.1.1.cmml" type="integer" xref="S4.SS9.SSS1.p1.5.m5.1.1">38</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS9.SSS1.p1.5.m5.1c">38</annotation><annotation encoding="application/x-llamapun" id="S4.SS9.SSS1.p1.5.m5.1d">38</annotation></semantics></math> morphosyntactic features in total, which represent language-specific linguistic information. We provide a dataset sample inÂ <a class="ltx_ref ltx_refmacro_autoref" href="#S4.T11" title="Table 11 â€£ 4.9.1 Method â€£ 4.9 Multilingual Probing â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">TableÂ 11</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T11">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T11.2" style="width:433.6pt;height:104.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(30.8pt,-7.4pt) scale(1.16529531186063,1.16529531186063) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T11.2.1">
<tr class="ltx_tr" id="S4.T11.2.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T11.2.1.1.1">Language</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T11.2.1.1.2">Label</td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S4.T11.2.1.1.3" style="width:346.9pt;">
<p class="ltx_p ltx_align_top" id="S4.T11.2.1.1.3.1">Sentence</p>
</td>
</tr>
<tr class="ltx_tr" id="S4.T11.2.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T11.2.1.2.1">English</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T11.2.1.2.2">Sing</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S4.T11.2.1.2.3" style="width:346.9pt;">
<p class="ltx_p ltx_align_top" id="S4.T11.2.1.2.3.1">The <span class="ltx_text ltx_font_bold" id="S4.T11.2.1.2.3.1.1">scheme</span> makes money through sponsorship and advertising .</p>
</td>
</tr>
<tr class="ltx_tr" id="S4.T11.2.1.3">
<td class="ltx_td" id="S4.T11.2.1.3.1"></td>
<td class="ltx_td ltx_align_left" id="S4.T11.2.1.3.2">Plur</td>
<td class="ltx_td ltx_align_justify" id="S4.T11.2.1.3.3" style="width:346.9pt;">
<p class="ltx_p ltx_align_top" id="S4.T11.2.1.3.3.1">Still , there are <span class="ltx_text ltx_font_bold" id="S4.T11.2.1.3.3.1.1">questions</span> left unanswered .</p>
</td>
</tr>
<tr class="ltx_tr" id="S4.T11.2.1.4">
<td class="ltx_td ltx_align_left" id="S4.T11.2.1.4.1">Spanish</td>
<td class="ltx_td ltx_align_left" id="S4.T11.2.1.4.2">Sing</td>
<td class="ltx_td ltx_align_justify" id="S4.T11.2.1.4.3" style="width:346.9pt;">
<p class="ltx_p ltx_align_top" id="S4.T11.2.1.4.3.1">Eligi<span class="ltx_text ltx_font_bold" id="S4.T11.2.1.4.3.1.1">o</span> no ir tras un tercer perÃ­odo en el siguiente ciclo de elecciones .</p>
</td>
</tr>
<tr class="ltx_tr" id="S4.T11.2.1.5">
<td class="ltx_td ltx_border_bb" id="S4.T11.2.1.5.1"></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T11.2.1.5.2">Plur</td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S4.T11.2.1.5.3" style="width:346.9pt;">
<p class="ltx_p ltx_align_top" id="S4.T11.2.1.5.3.1">TodavÃ­a quedan <span class="ltx_text ltx_font_bold" id="S4.T11.2.1.5.3.1.1">preguntas</span> sin responder .</p>
</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T11.4.1.1" style="font-size:90%;">Table 11</span>: </span><span class="ltx_text" id="S4.T11.5.2" style="font-size:90%;">Examples of the Number task in English and Spanish. The subject number indicator is highlighted in <span class="ltx_text ltx_font_bold" id="S4.T11.5.2.1">bold</span>. The task is to predict if the sentence includes a singular subject number (upper sentence) and a plural subject number (bottom sentence).</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS9.SSS1.p2">
<p class="ltx_p" id="S4.SS9.SSS1.p2.1">The probing procedure is conducted as follows. First, we compute <span class="ltx_text ltx_font_typewriter" id="S4.SS9.SSS1.p2.1.1">&lt;s&gt;</span>-pooled representations of the input sentence at each layer of the 1.7B-parameter BLOOM variant (â€œBLOOM 1B7â€) and BLOOM (with 176B parameters). Second, we train a binary logistic regression classifier to predict a presence of a morphosyntactic feature in the sentence. Logistic regression is chosen due to its higher selectivity as opposed to non-linear probing classifiersÂ <cite class="ltx_cite ltx_citemacro_citep">(Hewitt and Liang, <a class="ltx_ref" href="#bib.bib60" title="">2019</a>)</cite>. We use the original UD training, validation, and test splits here. Third, the probing performance is evaluated by <math alttext="F_{1}" class="ltx_Math" display="inline" id="S4.SS9.SSS1.p2.1.m1.1"><semantics id="S4.SS9.SSS1.p2.1.m1.1a"><msub id="S4.SS9.SSS1.p2.1.m1.1.1" xref="S4.SS9.SSS1.p2.1.m1.1.1.cmml"><mi id="S4.SS9.SSS1.p2.1.m1.1.1.2" xref="S4.SS9.SSS1.p2.1.m1.1.1.2.cmml">F</mi><mn id="S4.SS9.SSS1.p2.1.m1.1.1.3" xref="S4.SS9.SSS1.p2.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS9.SSS1.p2.1.m1.1b"><apply id="S4.SS9.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS9.SSS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS9.SSS1.p2.1.m1.1.1.1.cmml" xref="S4.SS9.SSS1.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS9.SSS1.p2.1.m1.1.1.2.cmml" xref="S4.SS9.SSS1.p2.1.m1.1.1.2">ğ¹</ci><cn id="S4.SS9.SSS1.p2.1.m1.1.1.3.cmml" type="integer" xref="S4.SS9.SSS1.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS9.SSS1.p2.1.m1.1c">F_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS9.SSS1.p2.1.m1.1d">italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> weighted score due to target class imbalance for most probing tasks. The results are averaged across three runs with different random seeds.</p>
</div>
<section class="ltx_paragraph" id="S4.SS9.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Baselines</h5>
<div class="ltx_para" id="S4.SS9.SSS1.Px1.p1">
<p class="ltx_p" id="S4.SS9.SSS1.Px1.p1.2">We compare the probing performance with random guessing and logistic regression classifiers trained on the following TF-IDF featuresÂ <cite class="ltx_cite ltx_citemacro_citep">(Salton and Yang, <a class="ltx_ref" href="#bib.bib126" title="">1973</a>)</cite>: word unigrams, character N-grams, BPE<span class="ltx_note ltx_role_footnote" id="footnote44"><sup class="ltx_note_mark">44</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">44</sup><span class="ltx_tag ltx_tag_note">44</span>BertTokenizer: <a class="ltx_ref ltx_href ltx_font_typewriter" href="https://hf.co/bert-base-multilingual-cased" title="">hf.co/bert-base-multilingual-cased</a></span></span></span> token N-grams, and SentencePiece<span class="ltx_note ltx_role_footnote" id="footnote45"><sup class="ltx_note_mark">45</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">45</sup><span class="ltx_tag ltx_tag_note">45</span>XLMRobertaTokenizer: <a class="ltx_ref ltx_href ltx_font_typewriter" href="https://hf.co/xlm-roberta-base" title="">hf.co/xlm-roberta-base</a></span></span></span> (SP;Â <cite class="ltx_cite ltx_citemacro_citep">Kudo and Richardson, <a class="ltx_ref" href="#bib.bib70" title="">2018</a></cite>) token N-grams. We use the N-gram range <math alttext="\in[1;4]" class="ltx_Math" display="inline" id="S4.SS9.SSS1.Px1.p1.1.m1.2"><semantics id="S4.SS9.SSS1.Px1.p1.1.m1.2a"><mrow id="S4.SS9.SSS1.Px1.p1.1.m1.2.3" xref="S4.SS9.SSS1.Px1.p1.1.m1.2.3.cmml"><mi id="S4.SS9.SSS1.Px1.p1.1.m1.2.3.2" xref="S4.SS9.SSS1.Px1.p1.1.m1.2.3.2.cmml"></mi><mo id="S4.SS9.SSS1.Px1.p1.1.m1.2.3.1" xref="S4.SS9.SSS1.Px1.p1.1.m1.2.3.1.cmml">âˆˆ</mo><mrow id="S4.SS9.SSS1.Px1.p1.1.m1.2.3.3.2" xref="S4.SS9.SSS1.Px1.p1.1.m1.2.3.3.1.cmml"><mo id="S4.SS9.SSS1.Px1.p1.1.m1.2.3.3.2.1" stretchy="false" xref="S4.SS9.SSS1.Px1.p1.1.m1.2.3.3.1.cmml">[</mo><mn id="S4.SS9.SSS1.Px1.p1.1.m1.1.1" xref="S4.SS9.SSS1.Px1.p1.1.m1.1.1.cmml">1</mn><mo id="S4.SS9.SSS1.Px1.p1.1.m1.2.3.3.2.2" xref="S4.SS9.SSS1.Px1.p1.1.m1.2.3.3.1.cmml">;</mo><mn id="S4.SS9.SSS1.Px1.p1.1.m1.2.2" xref="S4.SS9.SSS1.Px1.p1.1.m1.2.2.cmml">4</mn><mo id="S4.SS9.SSS1.Px1.p1.1.m1.2.3.3.2.3" stretchy="false" xref="S4.SS9.SSS1.Px1.p1.1.m1.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS9.SSS1.Px1.p1.1.m1.2b"><apply id="S4.SS9.SSS1.Px1.p1.1.m1.2.3.cmml" xref="S4.SS9.SSS1.Px1.p1.1.m1.2.3"><in id="S4.SS9.SSS1.Px1.p1.1.m1.2.3.1.cmml" xref="S4.SS9.SSS1.Px1.p1.1.m1.2.3.1"></in><csymbol cd="latexml" id="S4.SS9.SSS1.Px1.p1.1.m1.2.3.2.cmml" xref="S4.SS9.SSS1.Px1.p1.1.m1.2.3.2">absent</csymbol><list id="S4.SS9.SSS1.Px1.p1.1.m1.2.3.3.1.cmml" xref="S4.SS9.SSS1.Px1.p1.1.m1.2.3.3.2"><cn id="S4.SS9.SSS1.Px1.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS9.SSS1.Px1.p1.1.m1.1.1">1</cn><cn id="S4.SS9.SSS1.Px1.p1.1.m1.2.2.cmml" type="integer" xref="S4.SS9.SSS1.Px1.p1.1.m1.2.2">4</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS9.SSS1.Px1.p1.1.m1.2c">\in[1;4]</annotation><annotation encoding="application/x-llamapun" id="S4.SS9.SSS1.Px1.p1.1.m1.2d">âˆˆ [ 1 ; 4 ]</annotation></semantics></math> and limit the TF-IDF vocabularies to top-<math alttext="250" class="ltx_Math" display="inline" id="S4.SS9.SSS1.Px1.p1.2.m2.1"><semantics id="S4.SS9.SSS1.Px1.p1.2.m2.1a"><mn id="S4.SS9.SSS1.Px1.p1.2.m2.1.1" xref="S4.SS9.SSS1.Px1.p1.2.m2.1.1.cmml">250</mn><annotation-xml encoding="MathML-Content" id="S4.SS9.SSS1.Px1.p1.2.m2.1b"><cn id="S4.SS9.SSS1.Px1.p1.2.m2.1.1.cmml" type="integer" xref="S4.SS9.SSS1.Px1.p1.2.m2.1.1">250</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS9.SSS1.Px1.p1.2.m2.1c">250</annotation><annotation encoding="application/x-llamapun" id="S4.SS9.SSS1.Px1.p1.2.m2.1d">250</annotation></semantics></math>k features.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS9.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Correlation</h5>
<div class="ltx_para" id="S4.SS9.SSS1.Px2.p1">
<p class="ltx_p" id="S4.SS9.SSS1.Px2.p1.1">We run statistical tests to analyze correlations between the probing performance and linguistic, dataset, and model configuration criteria:</p>
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1">Language script: the results are divided into two groups by the language script â€“ Latin and others (Devanagari, Tamil, and Arabic). Here, we use the non-parametric test Mann-Whitney UÂ <cite class="ltx_cite ltx_citemacro_citep">(Mann and Whitney, <a class="ltx_ref" href="#bib.bib87" title="">1947</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1">Language family: the results are divided into <math alttext="7" class="ltx_Math" display="inline" id="S4.I2.i2.p1.1.m1.1"><semantics id="S4.I2.i2.p1.1.m1.1a"><mn id="S4.I2.i2.p1.1.m1.1.1" xref="S4.I2.i2.p1.1.m1.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S4.I2.i2.p1.1.m1.1b"><cn id="S4.I2.i2.p1.1.m1.1.1.cmml" type="integer" xref="S4.I2.i2.p1.1.m1.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i2.p1.1.m1.1c">7</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i2.p1.1.m1.1d">7</annotation></semantics></math> groups by the language family. We apply the ANOVA to analyze the variance between the groups.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1">Probing and pretraining dataset size: we run the Pearson correlation coefficient test <cite class="ltx_cite ltx_citemacro_citep">(Pearson, <a class="ltx_ref" href="#bib.bib111" title="">1895</a>)</cite> to compute correlation between the probing performance and these data configuration criteria.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I2.i4.p1">
<p class="ltx_p" id="S4.I2.i4.p1.1">Effect of the model size: the results are divided into two groups by the BLOOM version. Here, we use the Mann-Whitney U test to see if there is a correlation between the number of parameters and the probing results.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="S4.T12">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T12.119" style="width:433.6pt;height:445.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(59.2pt,-60.8pt) scale(1.37556325412158,1.37556325412158) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T12.119.119">
<tr class="ltx_tr" id="S4.T12.119.119.120">
<td class="ltx_td ltx_border_tt" id="S4.T12.119.119.120.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T12.119.119.120.2"><span class="ltx_text" id="S4.T12.119.119.120.2.1" style="font-size:70%;">BLOOM-1B7</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T12.119.119.120.3"><span class="ltx_text" id="S4.T12.119.119.120.3.1" style="font-size:70%;">BLOOM</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T12.119.119.120.4"><span class="ltx_text" id="S4.T12.119.119.120.4.1" style="font-size:70%;">Random</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T12.119.119.120.5"><span class="ltx_text" id="S4.T12.119.119.120.5.1" style="font-size:70%;">TF-IDF (Char)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T12.119.119.120.6"><span class="ltx_text" id="S4.T12.119.119.120.6.1" style="font-size:70%;">TF-IDF (Word)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T12.119.119.120.7"><span class="ltx_text" id="S4.T12.119.119.120.7.1" style="font-size:70%;">TF-IDF (BPE)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T12.119.119.120.8"><span class="ltx_text" id="S4.T12.119.119.120.8.1" style="font-size:70%;">TF-IDF (SP)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T12.7.7.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T12.7.7.7.8"><span class="ltx_text" id="S4.T12.7.7.7.8.1" style="font-size:70%;">Arabic</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T12.1.1.1.1">
<span class="ltx_text ltx_font_bold" id="S4.T12.1.1.1.1.1" style="font-size:70%;">0.66</span><span class="ltx_text" id="S4.T12.1.1.1.1.2" style="font-size:70%;"> </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.1.1.1.1.m1.1"><semantics id="S4.T12.1.1.1.1.m1.1a"><mo id="S4.T12.1.1.1.1.m1.1.1" mathsize="50%" xref="S4.T12.1.1.1.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T12.1.1.1.1.m1.1.1.cmml" xref="S4.T12.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.1.1.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.1.1.1.1.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.1.1.1.1.3" style="font-size:50%;">0.27</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T12.2.2.2.2">
<span class="ltx_text" id="S4.T12.2.2.2.2.1" style="font-size:70%;">0.64 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.2.2.2.2.m1.1"><semantics id="S4.T12.2.2.2.2.m1.1a"><mo id="S4.T12.2.2.2.2.m1.1.1" mathsize="50%" xref="S4.T12.2.2.2.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.2.2.2.2.m1.1b"><csymbol cd="latexml" id="S4.T12.2.2.2.2.m1.1.1.cmml" xref="S4.T12.2.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.2.2.2.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.2.2.2.2.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.2.2.2.2.2" style="font-size:50%;">0.27</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T12.3.3.3.3">
<span class="ltx_text" id="S4.T12.3.3.3.3.1" style="font-size:70%;">0.49 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.3.3.3.3.m1.1"><semantics id="S4.T12.3.3.3.3.m1.1a"><mo id="S4.T12.3.3.3.3.m1.1.1" mathsize="50%" xref="S4.T12.3.3.3.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.3.3.3.3.m1.1b"><csymbol cd="latexml" id="S4.T12.3.3.3.3.m1.1.1.cmml" xref="S4.T12.3.3.3.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.3.3.3.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.3.3.3.3.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.3.3.3.3.2" style="font-size:50%;">0.013</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T12.4.4.4.4">
<span class="ltx_text" id="S4.T12.4.4.4.4.1" style="font-size:70%;">0.41 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.4.4.4.4.m1.1"><semantics id="S4.T12.4.4.4.4.m1.1a"><mo id="S4.T12.4.4.4.4.m1.1.1" mathsize="50%" xref="S4.T12.4.4.4.4.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.4.4.4.4.m1.1b"><csymbol cd="latexml" id="S4.T12.4.4.4.4.m1.1.1.cmml" xref="S4.T12.4.4.4.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.4.4.4.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.4.4.4.4.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.4.4.4.4.2" style="font-size:50%;">0.44</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T12.5.5.5.5">
<span class="ltx_text" id="S4.T12.5.5.5.5.1" style="font-size:70%;">0.4 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.5.5.5.5.m1.1"><semantics id="S4.T12.5.5.5.5.m1.1a"><mo id="S4.T12.5.5.5.5.m1.1.1" mathsize="50%" xref="S4.T12.5.5.5.5.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.5.5.5.5.m1.1b"><csymbol cd="latexml" id="S4.T12.5.5.5.5.m1.1.1.cmml" xref="S4.T12.5.5.5.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.5.5.5.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.5.5.5.5.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.5.5.5.5.2" style="font-size:50%;">0.44</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T12.6.6.6.6">
<span class="ltx_text" id="S4.T12.6.6.6.6.1" style="font-size:70%;">0.41 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.6.6.6.6.m1.1"><semantics id="S4.T12.6.6.6.6.m1.1a"><mo id="S4.T12.6.6.6.6.m1.1.1" mathsize="50%" xref="S4.T12.6.6.6.6.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.6.6.6.6.m1.1b"><csymbol cd="latexml" id="S4.T12.6.6.6.6.m1.1.1.cmml" xref="S4.T12.6.6.6.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.6.6.6.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.6.6.6.6.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.6.6.6.6.2" style="font-size:50%;">0.44</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T12.7.7.7.7">
<span class="ltx_text" id="S4.T12.7.7.7.7.1" style="font-size:70%;">0.41 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.7.7.7.7.m1.1"><semantics id="S4.T12.7.7.7.7.m1.1a"><mo id="S4.T12.7.7.7.7.m1.1.1" mathsize="50%" xref="S4.T12.7.7.7.7.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.7.7.7.7.m1.1b"><csymbol cd="latexml" id="S4.T12.7.7.7.7.m1.1.1.cmml" xref="S4.T12.7.7.7.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.7.7.7.7.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.7.7.7.7.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.7.7.7.7.2" style="font-size:50%;">0.44</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T12.14.14.14">
<td class="ltx_td ltx_align_left" id="S4.T12.14.14.14.8"><span class="ltx_text" id="S4.T12.14.14.14.8.1" style="font-size:70%;">Bambara</span></td>
<td class="ltx_td ltx_align_center" id="S4.T12.8.8.8.1">
<span class="ltx_text ltx_font_bold" id="S4.T12.8.8.8.1.1" style="font-size:70%;">0.64</span><span class="ltx_text" id="S4.T12.8.8.8.1.2" style="font-size:70%;"> </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.8.8.8.1.m1.1"><semantics id="S4.T12.8.8.8.1.m1.1a"><mo id="S4.T12.8.8.8.1.m1.1.1" mathsize="50%" xref="S4.T12.8.8.8.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.8.8.8.1.m1.1b"><csymbol cd="latexml" id="S4.T12.8.8.8.1.m1.1.1.cmml" xref="S4.T12.8.8.8.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.8.8.8.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.8.8.8.1.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.8.8.8.1.3" style="font-size:50%;">0.16</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.9.9.9.2">
<span class="ltx_text" id="S4.T12.9.9.9.2.1" style="font-size:70%;">0.59 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.9.9.9.2.m1.1"><semantics id="S4.T12.9.9.9.2.m1.1a"><mo id="S4.T12.9.9.9.2.m1.1.1" mathsize="50%" xref="S4.T12.9.9.9.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.9.9.9.2.m1.1b"><csymbol cd="latexml" id="S4.T12.9.9.9.2.m1.1.1.cmml" xref="S4.T12.9.9.9.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.9.9.9.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.9.9.9.2.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.9.9.9.2.2" style="font-size:50%;">0.16</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.10.10.10.3">
<span class="ltx_text" id="S4.T12.10.10.10.3.1" style="font-size:70%;">0.45 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.10.10.10.3.m1.1"><semantics id="S4.T12.10.10.10.3.m1.1a"><mo id="S4.T12.10.10.10.3.m1.1.1" mathsize="50%" xref="S4.T12.10.10.10.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.10.10.10.3.m1.1b"><csymbol cd="latexml" id="S4.T12.10.10.10.3.m1.1.1.cmml" xref="S4.T12.10.10.10.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.10.10.10.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.10.10.10.3.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.10.10.10.3.2" style="font-size:50%;">0.1</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.11.11.11.4">
<span class="ltx_text" id="S4.T12.11.11.11.4.1" style="font-size:70%;">0.52 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.11.11.11.4.m1.1"><semantics id="S4.T12.11.11.11.4.m1.1a"><mo id="S4.T12.11.11.11.4.m1.1.1" mathsize="50%" xref="S4.T12.11.11.11.4.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.11.11.11.4.m1.1b"><csymbol cd="latexml" id="S4.T12.11.11.11.4.m1.1.1.cmml" xref="S4.T12.11.11.11.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.11.11.11.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.11.11.11.4.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.11.11.11.4.2" style="font-size:50%;">0.46</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.12.12.12.5">
<span class="ltx_text" id="S4.T12.12.12.12.5.1" style="font-size:70%;">0.45 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.12.12.12.5.m1.1"><semantics id="S4.T12.12.12.12.5.m1.1a"><mo id="S4.T12.12.12.12.5.m1.1.1" mathsize="50%" xref="S4.T12.12.12.12.5.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.12.12.12.5.m1.1b"><csymbol cd="latexml" id="S4.T12.12.12.12.5.m1.1.1.cmml" xref="S4.T12.12.12.12.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.12.12.12.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.12.12.12.5.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.12.12.12.5.2" style="font-size:50%;">0.47</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.13.13.13.6">
<span class="ltx_text" id="S4.T12.13.13.13.6.1" style="font-size:70%;">0.48 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.13.13.13.6.m1.1"><semantics id="S4.T12.13.13.13.6.m1.1a"><mo id="S4.T12.13.13.13.6.m1.1.1" mathsize="50%" xref="S4.T12.13.13.13.6.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.13.13.13.6.m1.1b"><csymbol cd="latexml" id="S4.T12.13.13.13.6.m1.1.1.cmml" xref="S4.T12.13.13.13.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.13.13.13.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.13.13.13.6.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.13.13.13.6.2" style="font-size:50%;">0.49</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.14.14.14.7">
<span class="ltx_text" id="S4.T12.14.14.14.7.1" style="font-size:70%;">0.49 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.14.14.14.7.m1.1"><semantics id="S4.T12.14.14.14.7.m1.1a"><mo id="S4.T12.14.14.14.7.m1.1.1" mathsize="50%" xref="S4.T12.14.14.14.7.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.14.14.14.7.m1.1b"><csymbol cd="latexml" id="S4.T12.14.14.14.7.m1.1.1.cmml" xref="S4.T12.14.14.14.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.14.14.14.7.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.14.14.14.7.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.14.14.14.7.2" style="font-size:50%;">0.49</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T12.21.21.21">
<td class="ltx_td ltx_align_left" id="S4.T12.21.21.21.8"><span class="ltx_text" id="S4.T12.21.21.21.8.1" style="font-size:70%;">Basque</span></td>
<td class="ltx_td ltx_align_center" id="S4.T12.15.15.15.1">
<span class="ltx_text ltx_font_bold" id="S4.T12.15.15.15.1.1" style="font-size:70%;">0.68</span><span class="ltx_text" id="S4.T12.15.15.15.1.2" style="font-size:70%;"> </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.15.15.15.1.m1.1"><semantics id="S4.T12.15.15.15.1.m1.1a"><mo id="S4.T12.15.15.15.1.m1.1.1" mathsize="50%" xref="S4.T12.15.15.15.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.15.15.15.1.m1.1b"><csymbol cd="latexml" id="S4.T12.15.15.15.1.m1.1.1.cmml" xref="S4.T12.15.15.15.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.15.15.15.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.15.15.15.1.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.15.15.15.1.3" style="font-size:50%;">0.19</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.16.16.16.2">
<span class="ltx_text" id="S4.T12.16.16.16.2.1" style="font-size:70%;">0.62 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.16.16.16.2.m1.1"><semantics id="S4.T12.16.16.16.2.m1.1a"><mo id="S4.T12.16.16.16.2.m1.1.1" mathsize="50%" xref="S4.T12.16.16.16.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.16.16.16.2.m1.1b"><csymbol cd="latexml" id="S4.T12.16.16.16.2.m1.1.1.cmml" xref="S4.T12.16.16.16.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.16.16.16.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.16.16.16.2.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.16.16.16.2.2" style="font-size:50%;">0.19</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.17.17.17.3">
<span class="ltx_text" id="S4.T12.17.17.17.3.1" style="font-size:70%;">0.49 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.17.17.17.3.m1.1"><semantics id="S4.T12.17.17.17.3.m1.1a"><mo id="S4.T12.17.17.17.3.m1.1.1" mathsize="50%" xref="S4.T12.17.17.17.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.17.17.17.3.m1.1b"><csymbol cd="latexml" id="S4.T12.17.17.17.3.m1.1.1.cmml" xref="S4.T12.17.17.17.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.17.17.17.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.17.17.17.3.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.17.17.17.3.2" style="font-size:50%;">0.03</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.18.18.18.4">
<span class="ltx_text" id="S4.T12.18.18.18.4.1" style="font-size:70%;">0.41 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.18.18.18.4.m1.1"><semantics id="S4.T12.18.18.18.4.m1.1a"><mo id="S4.T12.18.18.18.4.m1.1.1" mathsize="50%" xref="S4.T12.18.18.18.4.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.18.18.18.4.m1.1b"><csymbol cd="latexml" id="S4.T12.18.18.18.4.m1.1.1.cmml" xref="S4.T12.18.18.18.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.18.18.18.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.18.18.18.4.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.18.18.18.4.2" style="font-size:50%;">0.43</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.19.19.19.5">
<span class="ltx_text" id="S4.T12.19.19.19.5.1" style="font-size:70%;">0.44 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.19.19.19.5.m1.1"><semantics id="S4.T12.19.19.19.5.m1.1a"><mo id="S4.T12.19.19.19.5.m1.1.1" mathsize="50%" xref="S4.T12.19.19.19.5.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.19.19.19.5.m1.1b"><csymbol cd="latexml" id="S4.T12.19.19.19.5.m1.1.1.cmml" xref="S4.T12.19.19.19.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.19.19.19.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.19.19.19.5.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.19.19.19.5.2" style="font-size:50%;">0.46</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.20.20.20.6">
<span class="ltx_text" id="S4.T12.20.20.20.6.1" style="font-size:70%;">0.48 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.20.20.20.6.m1.1"><semantics id="S4.T12.20.20.20.6.m1.1a"><mo id="S4.T12.20.20.20.6.m1.1.1" mathsize="50%" xref="S4.T12.20.20.20.6.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.20.20.20.6.m1.1b"><csymbol cd="latexml" id="S4.T12.20.20.20.6.m1.1.1.cmml" xref="S4.T12.20.20.20.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.20.20.20.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.20.20.20.6.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.20.20.20.6.2" style="font-size:50%;">0.44</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.21.21.21.7">
<span class="ltx_text" id="S4.T12.21.21.21.7.1" style="font-size:70%;">0.41 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.21.21.21.7.m1.1"><semantics id="S4.T12.21.21.21.7.m1.1a"><mo id="S4.T12.21.21.21.7.m1.1.1" mathsize="50%" xref="S4.T12.21.21.21.7.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.21.21.21.7.m1.1b"><csymbol cd="latexml" id="S4.T12.21.21.21.7.m1.1.1.cmml" xref="S4.T12.21.21.21.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.21.21.21.7.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.21.21.21.7.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.21.21.21.7.2" style="font-size:50%;">0.46</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T12.28.28.28">
<td class="ltx_td ltx_align_left" id="S4.T12.28.28.28.8"><span class="ltx_text" id="S4.T12.28.28.28.8.1" style="font-size:70%;">Bengali</span></td>
<td class="ltx_td ltx_align_center" id="S4.T12.22.22.22.1">
<span class="ltx_text" id="S4.T12.22.22.22.1.1" style="font-size:70%;">0.42 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.22.22.22.1.m1.1"><semantics id="S4.T12.22.22.22.1.m1.1a"><mo id="S4.T12.22.22.22.1.m1.1.1" mathsize="50%" xref="S4.T12.22.22.22.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.22.22.22.1.m1.1b"><csymbol cd="latexml" id="S4.T12.22.22.22.1.m1.1.1.cmml" xref="S4.T12.22.22.22.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.22.22.22.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.22.22.22.1.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.22.22.22.1.2" style="font-size:50%;">0.15</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.23.23.23.2">
<span class="ltx_text" id="S4.T12.23.23.23.2.1" style="font-size:70%;">0.45 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.23.23.23.2.m1.1"><semantics id="S4.T12.23.23.23.2.m1.1a"><mo id="S4.T12.23.23.23.2.m1.1.1" mathsize="50%" xref="S4.T12.23.23.23.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.23.23.23.2.m1.1b"><csymbol cd="latexml" id="S4.T12.23.23.23.2.m1.1.1.cmml" xref="S4.T12.23.23.23.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.23.23.23.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.23.23.23.2.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.23.23.23.2.2" style="font-size:50%;">0.12</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.24.24.24.3">
<span class="ltx_text" id="S4.T12.24.24.24.3.1" style="font-size:70%;">0.35 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.24.24.24.3.m1.1"><semantics id="S4.T12.24.24.24.3.m1.1a"><mo id="S4.T12.24.24.24.3.m1.1.1" mathsize="50%" xref="S4.T12.24.24.24.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.24.24.24.3.m1.1b"><csymbol cd="latexml" id="S4.T12.24.24.24.3.m1.1.1.cmml" xref="S4.T12.24.24.24.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.24.24.24.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.24.24.24.3.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.24.24.24.3.2" style="font-size:50%;">0.23</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.25.25.25.4">
<span class="ltx_text" id="S4.T12.25.25.25.4.1" style="font-size:70%;">0.63 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.25.25.25.4.m1.1"><semantics id="S4.T12.25.25.25.4.m1.1a"><mo id="S4.T12.25.25.25.4.m1.1.1" mathsize="50%" xref="S4.T12.25.25.25.4.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.25.25.25.4.m1.1b"><csymbol cd="latexml" id="S4.T12.25.25.25.4.m1.1.1.cmml" xref="S4.T12.25.25.25.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.25.25.25.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.25.25.25.4.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.25.25.25.4.2" style="font-size:50%;">0.48</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.26.26.26.5">
<span class="ltx_text" id="S4.T12.26.26.26.5.1" style="font-size:70%;">0.37 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.26.26.26.5.m1.1"><semantics id="S4.T12.26.26.26.5.m1.1a"><mo id="S4.T12.26.26.26.5.m1.1.1" mathsize="50%" xref="S4.T12.26.26.26.5.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.26.26.26.5.m1.1b"><csymbol cd="latexml" id="S4.T12.26.26.26.5.m1.1.1.cmml" xref="S4.T12.26.26.26.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.26.26.26.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.26.26.26.5.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.26.26.26.5.2" style="font-size:50%;">0.44</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.27.27.27.6">
<span class="ltx_text" id="S4.T12.27.27.27.6.1" style="font-size:70%;">0.41 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.27.27.27.6.m1.1"><semantics id="S4.T12.27.27.27.6.m1.1a"><mo id="S4.T12.27.27.27.6.m1.1.1" mathsize="50%" xref="S4.T12.27.27.27.6.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.27.27.27.6.m1.1b"><csymbol cd="latexml" id="S4.T12.27.27.27.6.m1.1.1.cmml" xref="S4.T12.27.27.27.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.27.27.27.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.27.27.27.6.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.27.27.27.6.2" style="font-size:50%;">0.32</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.28.28.28.7">
<span class="ltx_text ltx_font_bold" id="S4.T12.28.28.28.7.1" style="font-size:70%;">0.76</span><span class="ltx_text" id="S4.T12.28.28.28.7.2" style="font-size:70%;"> </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.28.28.28.7.m1.1"><semantics id="S4.T12.28.28.28.7.m1.1a"><mo id="S4.T12.28.28.28.7.m1.1.1" mathsize="50%" xref="S4.T12.28.28.28.7.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.28.28.28.7.m1.1b"><csymbol cd="latexml" id="S4.T12.28.28.28.7.m1.1.1.cmml" xref="S4.T12.28.28.28.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.28.28.28.7.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.28.28.28.7.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.28.28.28.7.3" style="font-size:50%;">0.28</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T12.35.35.35">
<td class="ltx_td ltx_align_left" id="S4.T12.35.35.35.8"><span class="ltx_text" id="S4.T12.35.35.35.8.1" style="font-size:70%;">Catalan</span></td>
<td class="ltx_td ltx_align_center" id="S4.T12.29.29.29.1">
<span class="ltx_text ltx_font_bold" id="S4.T12.29.29.29.1.1" style="font-size:70%;">0.65</span><span class="ltx_text" id="S4.T12.29.29.29.1.2" style="font-size:70%;"> </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.29.29.29.1.m1.1"><semantics id="S4.T12.29.29.29.1.m1.1a"><mo id="S4.T12.29.29.29.1.m1.1.1" mathsize="50%" xref="S4.T12.29.29.29.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.29.29.29.1.m1.1b"><csymbol cd="latexml" id="S4.T12.29.29.29.1.m1.1.1.cmml" xref="S4.T12.29.29.29.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.29.29.29.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.29.29.29.1.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.29.29.29.1.3" style="font-size:50%;">0.25</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.30.30.30.2">
<span class="ltx_text" id="S4.T12.30.30.30.2.1" style="font-size:70%;">0.61 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.30.30.30.2.m1.1"><semantics id="S4.T12.30.30.30.2.m1.1a"><mo id="S4.T12.30.30.30.2.m1.1.1" mathsize="50%" xref="S4.T12.30.30.30.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.30.30.30.2.m1.1b"><csymbol cd="latexml" id="S4.T12.30.30.30.2.m1.1.1.cmml" xref="S4.T12.30.30.30.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.30.30.30.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.30.30.30.2.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.30.30.30.2.2" style="font-size:50%;">0.26</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.31.31.31.3">
<span class="ltx_text" id="S4.T12.31.31.31.3.1" style="font-size:70%;">0.34 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.31.31.31.3.m1.1"><semantics id="S4.T12.31.31.31.3.m1.1a"><mo id="S4.T12.31.31.31.3.m1.1.1" mathsize="50%" xref="S4.T12.31.31.31.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.31.31.31.3.m1.1b"><csymbol cd="latexml" id="S4.T12.31.31.31.3.m1.1.1.cmml" xref="S4.T12.31.31.31.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.31.31.31.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.31.31.31.3.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.31.31.31.3.2" style="font-size:50%;">0.01</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.32.32.32.4">
<span class="ltx_text" id="S4.T12.32.32.32.4.1" style="font-size:70%;">0.24 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.32.32.32.4.m1.1"><semantics id="S4.T12.32.32.32.4.m1.1a"><mo id="S4.T12.32.32.32.4.m1.1.1" mathsize="50%" xref="S4.T12.32.32.32.4.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.32.32.32.4.m1.1b"><csymbol cd="latexml" id="S4.T12.32.32.32.4.m1.1.1.cmml" xref="S4.T12.32.32.32.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.32.32.32.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.32.32.32.4.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.32.32.32.4.2" style="font-size:50%;">0.38</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.33.33.33.5">
<span class="ltx_text" id="S4.T12.33.33.33.5.1" style="font-size:70%;">0.24 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.33.33.33.5.m1.1"><semantics id="S4.T12.33.33.33.5.m1.1a"><mo id="S4.T12.33.33.33.5.m1.1.1" mathsize="50%" xref="S4.T12.33.33.33.5.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.33.33.33.5.m1.1b"><csymbol cd="latexml" id="S4.T12.33.33.33.5.m1.1.1.cmml" xref="S4.T12.33.33.33.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.33.33.33.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.33.33.33.5.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.33.33.33.5.2" style="font-size:50%;">0.39</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.34.34.34.6">
<span class="ltx_text" id="S4.T12.34.34.34.6.1" style="font-size:70%;">0.24 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.34.34.34.6.m1.1"><semantics id="S4.T12.34.34.34.6.m1.1a"><mo id="S4.T12.34.34.34.6.m1.1.1" mathsize="50%" xref="S4.T12.34.34.34.6.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.34.34.34.6.m1.1b"><csymbol cd="latexml" id="S4.T12.34.34.34.6.m1.1.1.cmml" xref="S4.T12.34.34.34.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.34.34.34.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.34.34.34.6.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.34.34.34.6.2" style="font-size:50%;">0.39</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.35.35.35.7">
<span class="ltx_text" id="S4.T12.35.35.35.7.1" style="font-size:70%;">0.24 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.35.35.35.7.m1.1"><semantics id="S4.T12.35.35.35.7.m1.1a"><mo id="S4.T12.35.35.35.7.m1.1.1" mathsize="50%" xref="S4.T12.35.35.35.7.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.35.35.35.7.m1.1b"><csymbol cd="latexml" id="S4.T12.35.35.35.7.m1.1.1.cmml" xref="S4.T12.35.35.35.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.35.35.35.7.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.35.35.35.7.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.35.35.35.7.2" style="font-size:50%;">0.39</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T12.42.42.42">
<td class="ltx_td ltx_align_left" id="S4.T12.42.42.42.8"><span class="ltx_text" id="S4.T12.42.42.42.8.1" style="font-size:70%;">Chinese</span></td>
<td class="ltx_td ltx_align_center" id="S4.T12.36.36.36.1">
<span class="ltx_text ltx_font_bold" id="S4.T12.36.36.36.1.1" style="font-size:70%;">0.66</span><span class="ltx_text" id="S4.T12.36.36.36.1.2" style="font-size:70%;"> </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.36.36.36.1.m1.1"><semantics id="S4.T12.36.36.36.1.m1.1a"><mo id="S4.T12.36.36.36.1.m1.1.1" mathsize="50%" xref="S4.T12.36.36.36.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.36.36.36.1.m1.1b"><csymbol cd="latexml" id="S4.T12.36.36.36.1.m1.1.1.cmml" xref="S4.T12.36.36.36.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.36.36.36.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.36.36.36.1.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.36.36.36.1.3" style="font-size:50%;">0.25</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.37.37.37.2">
<span class="ltx_text" id="S4.T12.37.37.37.2.1" style="font-size:70%;">0.50 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.37.37.37.2.m1.1"><semantics id="S4.T12.37.37.37.2.m1.1a"><mo id="S4.T12.37.37.37.2.m1.1.1" mathsize="50%" xref="S4.T12.37.37.37.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.37.37.37.2.m1.1b"><csymbol cd="latexml" id="S4.T12.37.37.37.2.m1.1.1.cmml" xref="S4.T12.37.37.37.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.37.37.37.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.37.37.37.2.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.37.37.37.2.2" style="font-size:50%;">0.21</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.38.38.38.3">
<span class="ltx_text" id="S4.T12.38.38.38.3.1" style="font-size:70%;">0.55 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.38.38.38.3.m1.1"><semantics id="S4.T12.38.38.38.3.m1.1a"><mo id="S4.T12.38.38.38.3.m1.1.1" mathsize="50%" xref="S4.T12.38.38.38.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.38.38.38.3.m1.1b"><csymbol cd="latexml" id="S4.T12.38.38.38.3.m1.1.1.cmml" xref="S4.T12.38.38.38.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.38.38.38.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.38.38.38.3.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.38.38.38.3.2" style="font-size:50%;">0.03</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.39.39.39.4">
<span class="ltx_text" id="S4.T12.39.39.39.4.1" style="font-size:70%;">0.03 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.39.39.39.4.m1.1"><semantics id="S4.T12.39.39.39.4.m1.1a"><mo id="S4.T12.39.39.39.4.m1.1.1" mathsize="50%" xref="S4.T12.39.39.39.4.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.39.39.39.4.m1.1b"><csymbol cd="latexml" id="S4.T12.39.39.39.4.m1.1.1.cmml" xref="S4.T12.39.39.39.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.39.39.39.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.39.39.39.4.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.39.39.39.4.2" style="font-size:50%;">0.05</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.40.40.40.5">
<span class="ltx_text" id="S4.T12.40.40.40.5.1" style="font-size:70%;">0.11 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.40.40.40.5.m1.1"><semantics id="S4.T12.40.40.40.5.m1.1a"><mo id="S4.T12.40.40.40.5.m1.1.1" mathsize="50%" xref="S4.T12.40.40.40.5.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.40.40.40.5.m1.1b"><csymbol cd="latexml" id="S4.T12.40.40.40.5.m1.1.1.cmml" xref="S4.T12.40.40.40.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.40.40.40.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.40.40.40.5.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.40.40.40.5.2" style="font-size:50%;">0.28</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.41.41.41.6">
<span class="ltx_text" id="S4.T12.41.41.41.6.1" style="font-size:70%;">0.04 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.41.41.41.6.m1.1"><semantics id="S4.T12.41.41.41.6.m1.1a"><mo id="S4.T12.41.41.41.6.m1.1.1" mathsize="50%" xref="S4.T12.41.41.41.6.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.41.41.41.6.m1.1b"><csymbol cd="latexml" id="S4.T12.41.41.41.6.m1.1.1.cmml" xref="S4.T12.41.41.41.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.41.41.41.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.41.41.41.6.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.41.41.41.6.2" style="font-size:50%;">0.06</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.42.42.42.7">
<span class="ltx_text" id="S4.T12.42.42.42.7.1" style="font-size:70%;">0.03 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.42.42.42.7.m1.1"><semantics id="S4.T12.42.42.42.7.m1.1a"><mo id="S4.T12.42.42.42.7.m1.1.1" mathsize="50%" xref="S4.T12.42.42.42.7.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.42.42.42.7.m1.1b"><csymbol cd="latexml" id="S4.T12.42.42.42.7.m1.1.1.cmml" xref="S4.T12.42.42.42.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.42.42.42.7.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.42.42.42.7.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.42.42.42.7.2" style="font-size:50%;">0.05</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T12.49.49.49">
<td class="ltx_td ltx_align_left" id="S4.T12.49.49.49.8"><span class="ltx_text" id="S4.T12.49.49.49.8.1" style="font-size:70%;">English</span></td>
<td class="ltx_td ltx_align_center" id="S4.T12.43.43.43.1">
<span class="ltx_text ltx_font_bold" id="S4.T12.43.43.43.1.1" style="font-size:70%;">0.57</span><span class="ltx_text" id="S4.T12.43.43.43.1.2" style="font-size:70%;"> </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.43.43.43.1.m1.1"><semantics id="S4.T12.43.43.43.1.m1.1a"><mo id="S4.T12.43.43.43.1.m1.1.1" mathsize="50%" xref="S4.T12.43.43.43.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.43.43.43.1.m1.1b"><csymbol cd="latexml" id="S4.T12.43.43.43.1.m1.1.1.cmml" xref="S4.T12.43.43.43.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.43.43.43.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.43.43.43.1.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.43.43.43.1.3" style="font-size:50%;">0.24</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.44.44.44.2">
<span class="ltx_text ltx_font_bold" id="S4.T12.44.44.44.2.1" style="font-size:70%;">0.57</span><span class="ltx_text" id="S4.T12.44.44.44.2.2" style="font-size:70%;"> </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.44.44.44.2.m1.1"><semantics id="S4.T12.44.44.44.2.m1.1a"><mo id="S4.T12.44.44.44.2.m1.1.1" mathsize="50%" xref="S4.T12.44.44.44.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.44.44.44.2.m1.1b"><csymbol cd="latexml" id="S4.T12.44.44.44.2.m1.1.1.cmml" xref="S4.T12.44.44.44.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.44.44.44.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.44.44.44.2.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.44.44.44.2.3" style="font-size:50%;">0.24</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.45.45.45.3">
<span class="ltx_text" id="S4.T12.45.45.45.3.1" style="font-size:70%;">0.43 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.45.45.45.3.m1.1"><semantics id="S4.T12.45.45.45.3.m1.1a"><mo id="S4.T12.45.45.45.3.m1.1.1" mathsize="50%" xref="S4.T12.45.45.45.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.45.45.45.3.m1.1b"><csymbol cd="latexml" id="S4.T12.45.45.45.3.m1.1.1.cmml" xref="S4.T12.45.45.45.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.45.45.45.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.45.45.45.3.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.45.45.45.3.2" style="font-size:50%;">0.03</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.46.46.46.4">
<span class="ltx_text" id="S4.T12.46.46.46.4.1" style="font-size:70%;">0.45 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.46.46.46.4.m1.1"><semantics id="S4.T12.46.46.46.4.m1.1a"><mo id="S4.T12.46.46.46.4.m1.1.1" mathsize="50%" xref="S4.T12.46.46.46.4.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.46.46.46.4.m1.1b"><csymbol cd="latexml" id="S4.T12.46.46.46.4.m1.1.1.cmml" xref="S4.T12.46.46.46.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.46.46.46.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.46.46.46.4.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.46.46.46.4.2" style="font-size:50%;">0.43</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.47.47.47.5">
<span class="ltx_text" id="S4.T12.47.47.47.5.1" style="font-size:70%;">0.46 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.47.47.47.5.m1.1"><semantics id="S4.T12.47.47.47.5.m1.1a"><mo id="S4.T12.47.47.47.5.m1.1.1" mathsize="50%" xref="S4.T12.47.47.47.5.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.47.47.47.5.m1.1b"><csymbol cd="latexml" id="S4.T12.47.47.47.5.m1.1.1.cmml" xref="S4.T12.47.47.47.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.47.47.47.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.47.47.47.5.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.47.47.47.5.2" style="font-size:50%;">0.43</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.48.48.48.6">
<span class="ltx_text" id="S4.T12.48.48.48.6.1" style="font-size:70%;">0.45 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.48.48.48.6.m1.1"><semantics id="S4.T12.48.48.48.6.m1.1a"><mo id="S4.T12.48.48.48.6.m1.1.1" mathsize="50%" xref="S4.T12.48.48.48.6.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.48.48.48.6.m1.1b"><csymbol cd="latexml" id="S4.T12.48.48.48.6.m1.1.1.cmml" xref="S4.T12.48.48.48.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.48.48.48.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.48.48.48.6.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.48.48.48.6.2" style="font-size:50%;">0.43</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.49.49.49.7">
<span class="ltx_text" id="S4.T12.49.49.49.7.1" style="font-size:70%;">0.44 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.49.49.49.7.m1.1"><semantics id="S4.T12.49.49.49.7.m1.1a"><mo id="S4.T12.49.49.49.7.m1.1.1" mathsize="50%" xref="S4.T12.49.49.49.7.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.49.49.49.7.m1.1b"><csymbol cd="latexml" id="S4.T12.49.49.49.7.m1.1.1.cmml" xref="S4.T12.49.49.49.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.49.49.49.7.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.49.49.49.7.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.49.49.49.7.2" style="font-size:50%;">0.44</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T12.56.56.56">
<td class="ltx_td ltx_align_left" id="S4.T12.56.56.56.8"><span class="ltx_text" id="S4.T12.56.56.56.8.1" style="font-size:70%;">French</span></td>
<td class="ltx_td ltx_align_center" id="S4.T12.50.50.50.1">
<span class="ltx_text ltx_font_bold" id="S4.T12.50.50.50.1.1" style="font-size:70%;">0.61</span><span class="ltx_text" id="S4.T12.50.50.50.1.2" style="font-size:70%;"> </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.50.50.50.1.m1.1"><semantics id="S4.T12.50.50.50.1.m1.1a"><mo id="S4.T12.50.50.50.1.m1.1.1" mathsize="50%" xref="S4.T12.50.50.50.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.50.50.50.1.m1.1b"><csymbol cd="latexml" id="S4.T12.50.50.50.1.m1.1.1.cmml" xref="S4.T12.50.50.50.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.50.50.50.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.50.50.50.1.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.50.50.50.1.3" style="font-size:50%;">0.23</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.51.51.51.2">
<span class="ltx_text" id="S4.T12.51.51.51.2.1" style="font-size:70%;">0.57 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.51.51.51.2.m1.1"><semantics id="S4.T12.51.51.51.2.m1.1a"><mo id="S4.T12.51.51.51.2.m1.1.1" mathsize="50%" xref="S4.T12.51.51.51.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.51.51.51.2.m1.1b"><csymbol cd="latexml" id="S4.T12.51.51.51.2.m1.1.1.cmml" xref="S4.T12.51.51.51.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.51.51.51.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.51.51.51.2.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.51.51.51.2.2" style="font-size:50%;">0.22</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.52.52.52.3">
<span class="ltx_text" id="S4.T12.52.52.52.3.1" style="font-size:70%;">0.44 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.52.52.52.3.m1.1"><semantics id="S4.T12.52.52.52.3.m1.1a"><mo id="S4.T12.52.52.52.3.m1.1.1" mathsize="50%" xref="S4.T12.52.52.52.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.52.52.52.3.m1.1b"><csymbol cd="latexml" id="S4.T12.52.52.52.3.m1.1.1.cmml" xref="S4.T12.52.52.52.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.52.52.52.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.52.52.52.3.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.52.52.52.3.2" style="font-size:50%;">0.02</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.53.53.53.4">
<span class="ltx_text" id="S4.T12.53.53.53.4.1" style="font-size:70%;">0.32 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.53.53.53.4.m1.1"><semantics id="S4.T12.53.53.53.4.m1.1a"><mo id="S4.T12.53.53.53.4.m1.1.1" mathsize="50%" xref="S4.T12.53.53.53.4.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.53.53.53.4.m1.1b"><csymbol cd="latexml" id="S4.T12.53.53.53.4.m1.1.1.cmml" xref="S4.T12.53.53.53.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.53.53.53.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.53.53.53.4.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.53.53.53.4.2" style="font-size:50%;">0.43</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.54.54.54.5">
<span class="ltx_text" id="S4.T12.54.54.54.5.1" style="font-size:70%;">0.32 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.54.54.54.5.m1.1"><semantics id="S4.T12.54.54.54.5.m1.1a"><mo id="S4.T12.54.54.54.5.m1.1.1" mathsize="50%" xref="S4.T12.54.54.54.5.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.54.54.54.5.m1.1b"><csymbol cd="latexml" id="S4.T12.54.54.54.5.m1.1.1.cmml" xref="S4.T12.54.54.54.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.54.54.54.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.54.54.54.5.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.54.54.54.5.2" style="font-size:50%;">0.43</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.55.55.55.6">
<span class="ltx_text" id="S4.T12.55.55.55.6.1" style="font-size:70%;">0.32 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.55.55.55.6.m1.1"><semantics id="S4.T12.55.55.55.6.m1.1a"><mo id="S4.T12.55.55.55.6.m1.1.1" mathsize="50%" xref="S4.T12.55.55.55.6.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.55.55.55.6.m1.1b"><csymbol cd="latexml" id="S4.T12.55.55.55.6.m1.1.1.cmml" xref="S4.T12.55.55.55.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.55.55.55.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.55.55.55.6.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.55.55.55.6.2" style="font-size:50%;">0.43</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.56.56.56.7">
<span class="ltx_text" id="S4.T12.56.56.56.7.1" style="font-size:70%;">0.33 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.56.56.56.7.m1.1"><semantics id="S4.T12.56.56.56.7.m1.1a"><mo id="S4.T12.56.56.56.7.m1.1.1" mathsize="50%" xref="S4.T12.56.56.56.7.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.56.56.56.7.m1.1b"><csymbol cd="latexml" id="S4.T12.56.56.56.7.m1.1.1.cmml" xref="S4.T12.56.56.56.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.56.56.56.7.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.56.56.56.7.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.56.56.56.7.2" style="font-size:50%;">0.44</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T12.63.63.63">
<td class="ltx_td ltx_align_left" id="S4.T12.63.63.63.8"><span class="ltx_text" id="S4.T12.63.63.63.8.1" style="font-size:70%;">Hindi</span></td>
<td class="ltx_td ltx_align_center" id="S4.T12.57.57.57.1">
<span class="ltx_text ltx_font_bold" id="S4.T12.57.57.57.1.1" style="font-size:70%;">0.63</span><span class="ltx_text" id="S4.T12.57.57.57.1.2" style="font-size:70%;"> </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.57.57.57.1.m1.1"><semantics id="S4.T12.57.57.57.1.m1.1a"><mo id="S4.T12.57.57.57.1.m1.1.1" mathsize="50%" xref="S4.T12.57.57.57.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.57.57.57.1.m1.1b"><csymbol cd="latexml" id="S4.T12.57.57.57.1.m1.1.1.cmml" xref="S4.T12.57.57.57.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.57.57.57.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.57.57.57.1.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.57.57.57.1.3" style="font-size:50%;">0.23</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.58.58.58.2">
<span class="ltx_text" id="S4.T12.58.58.58.2.1" style="font-size:70%;">0.6 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.58.58.58.2.m1.1"><semantics id="S4.T12.58.58.58.2.m1.1a"><mo id="S4.T12.58.58.58.2.m1.1.1" mathsize="50%" xref="S4.T12.58.58.58.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.58.58.58.2.m1.1b"><csymbol cd="latexml" id="S4.T12.58.58.58.2.m1.1.1.cmml" xref="S4.T12.58.58.58.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.58.58.58.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.58.58.58.2.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.58.58.58.2.2" style="font-size:50%;">0.25</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.59.59.59.3">
<span class="ltx_text" id="S4.T12.59.59.59.3.1" style="font-size:70%;">0.48 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.59.59.59.3.m1.1"><semantics id="S4.T12.59.59.59.3.m1.1a"><mo id="S4.T12.59.59.59.3.m1.1.1" mathsize="50%" xref="S4.T12.59.59.59.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.59.59.59.3.m1.1b"><csymbol cd="latexml" id="S4.T12.59.59.59.3.m1.1.1.cmml" xref="S4.T12.59.59.59.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.59.59.59.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.59.59.59.3.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.59.59.59.3.2" style="font-size:50%;">0.03</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.60.60.60.4">
<span class="ltx_text" id="S4.T12.60.60.60.4.1" style="font-size:70%;">0.53 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.60.60.60.4.m1.1"><semantics id="S4.T12.60.60.60.4.m1.1a"><mo id="S4.T12.60.60.60.4.m1.1.1" mathsize="50%" xref="S4.T12.60.60.60.4.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.60.60.60.4.m1.1b"><csymbol cd="latexml" id="S4.T12.60.60.60.4.m1.1.1.cmml" xref="S4.T12.60.60.60.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.60.60.60.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.60.60.60.4.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.60.60.60.4.2" style="font-size:50%;">0.46</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.61.61.61.5">
<span class="ltx_text" id="S4.T12.61.61.61.5.1" style="font-size:70%;">0.55 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.61.61.61.5.m1.1"><semantics id="S4.T12.61.61.61.5.m1.1a"><mo id="S4.T12.61.61.61.5.m1.1.1" mathsize="50%" xref="S4.T12.61.61.61.5.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.61.61.61.5.m1.1b"><csymbol cd="latexml" id="S4.T12.61.61.61.5.m1.1.1.cmml" xref="S4.T12.61.61.61.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.61.61.61.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.61.61.61.5.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.61.61.61.5.2" style="font-size:50%;">0.47</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.62.62.62.6">
<span class="ltx_text" id="S4.T12.62.62.62.6.1" style="font-size:70%;">0.53 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.62.62.62.6.m1.1"><semantics id="S4.T12.62.62.62.6.m1.1a"><mo id="S4.T12.62.62.62.6.m1.1.1" mathsize="50%" xref="S4.T12.62.62.62.6.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.62.62.62.6.m1.1b"><csymbol cd="latexml" id="S4.T12.62.62.62.6.m1.1.1.cmml" xref="S4.T12.62.62.62.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.62.62.62.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.62.62.62.6.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.62.62.62.6.2" style="font-size:50%;">0.46</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.63.63.63.7">
<span class="ltx_text" id="S4.T12.63.63.63.7.1" style="font-size:70%;">0.53 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.63.63.63.7.m1.1"><semantics id="S4.T12.63.63.63.7.m1.1a"><mo id="S4.T12.63.63.63.7.m1.1.1" mathsize="50%" xref="S4.T12.63.63.63.7.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.63.63.63.7.m1.1b"><csymbol cd="latexml" id="S4.T12.63.63.63.7.m1.1.1.cmml" xref="S4.T12.63.63.63.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.63.63.63.7.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.63.63.63.7.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.63.63.63.7.2" style="font-size:50%;">0.46</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T12.70.70.70">
<td class="ltx_td ltx_align_left" id="S4.T12.70.70.70.8"><span class="ltx_text" id="S4.T12.70.70.70.8.1" style="font-size:70%;">Indonesian</span></td>
<td class="ltx_td ltx_align_center" id="S4.T12.64.64.64.1">
<span class="ltx_text ltx_font_bold" id="S4.T12.64.64.64.1.1" style="font-size:70%;">0.65</span><span class="ltx_text" id="S4.T12.64.64.64.1.2" style="font-size:70%;"> </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.64.64.64.1.m1.1"><semantics id="S4.T12.64.64.64.1.m1.1a"><mo id="S4.T12.64.64.64.1.m1.1.1" mathsize="50%" xref="S4.T12.64.64.64.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.64.64.64.1.m1.1b"><csymbol cd="latexml" id="S4.T12.64.64.64.1.m1.1.1.cmml" xref="S4.T12.64.64.64.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.64.64.64.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.64.64.64.1.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.64.64.64.1.3" style="font-size:50%;">0.27</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.65.65.65.2">
<span class="ltx_text" id="S4.T12.65.65.65.2.1" style="font-size:70%;">0.6 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.65.65.65.2.m1.1"><semantics id="S4.T12.65.65.65.2.m1.1a"><mo id="S4.T12.65.65.65.2.m1.1.1" mathsize="50%" xref="S4.T12.65.65.65.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.65.65.65.2.m1.1b"><csymbol cd="latexml" id="S4.T12.65.65.65.2.m1.1.1.cmml" xref="S4.T12.65.65.65.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.65.65.65.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.65.65.65.2.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.65.65.65.2.2" style="font-size:50%;">0.27</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.66.66.66.3">
<span class="ltx_text" id="S4.T12.66.66.66.3.1" style="font-size:70%;">0.48 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.66.66.66.3.m1.1"><semantics id="S4.T12.66.66.66.3.m1.1a"><mo id="S4.T12.66.66.66.3.m1.1.1" mathsize="50%" xref="S4.T12.66.66.66.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.66.66.66.3.m1.1b"><csymbol cd="latexml" id="S4.T12.66.66.66.3.m1.1.1.cmml" xref="S4.T12.66.66.66.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.66.66.66.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.66.66.66.3.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.66.66.66.3.2" style="font-size:50%;">0.05</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.67.67.67.4">
<span class="ltx_text" id="S4.T12.67.67.67.4.1" style="font-size:70%;">0.41 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.67.67.67.4.m1.1"><semantics id="S4.T12.67.67.67.4.m1.1a"><mo id="S4.T12.67.67.67.4.m1.1.1" mathsize="50%" xref="S4.T12.67.67.67.4.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.67.67.67.4.m1.1b"><csymbol cd="latexml" id="S4.T12.67.67.67.4.m1.1.1.cmml" xref="S4.T12.67.67.67.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.67.67.67.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.67.67.67.4.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.67.67.67.4.2" style="font-size:50%;">0.46</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.68.68.68.5">
<span class="ltx_text" id="S4.T12.68.68.68.5.1" style="font-size:70%;">0.43 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.68.68.68.5.m1.1"><semantics id="S4.T12.68.68.68.5.m1.1a"><mo id="S4.T12.68.68.68.5.m1.1.1" mathsize="50%" xref="S4.T12.68.68.68.5.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.68.68.68.5.m1.1b"><csymbol cd="latexml" id="S4.T12.68.68.68.5.m1.1.1.cmml" xref="S4.T12.68.68.68.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.68.68.68.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.68.68.68.5.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.68.68.68.5.2" style="font-size:50%;">0.45</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.69.69.69.6">
<span class="ltx_text" id="S4.T12.69.69.69.6.1" style="font-size:70%;">0.41 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.69.69.69.6.m1.1"><semantics id="S4.T12.69.69.69.6.m1.1a"><mo id="S4.T12.69.69.69.6.m1.1.1" mathsize="50%" xref="S4.T12.69.69.69.6.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.69.69.69.6.m1.1b"><csymbol cd="latexml" id="S4.T12.69.69.69.6.m1.1.1.cmml" xref="S4.T12.69.69.69.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.69.69.69.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.69.69.69.6.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.69.69.69.6.2" style="font-size:50%;">0.46</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.70.70.70.7">
<span class="ltx_text" id="S4.T12.70.70.70.7.1" style="font-size:70%;">0.45 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.70.70.70.7.m1.1"><semantics id="S4.T12.70.70.70.7.m1.1a"><mo id="S4.T12.70.70.70.7.m1.1.1" mathsize="50%" xref="S4.T12.70.70.70.7.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.70.70.70.7.m1.1b"><csymbol cd="latexml" id="S4.T12.70.70.70.7.m1.1.1.cmml" xref="S4.T12.70.70.70.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.70.70.70.7.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.70.70.70.7.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.70.70.70.7.2" style="font-size:50%;">0.45</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T12.77.77.77">
<td class="ltx_td ltx_align_left" id="S4.T12.77.77.77.8"><span class="ltx_text" id="S4.T12.77.77.77.8.1" style="font-size:70%;">Marathi</span></td>
<td class="ltx_td ltx_align_center" id="S4.T12.71.71.71.1">
<span class="ltx_text ltx_font_bold" id="S4.T12.71.71.71.1.1" style="font-size:70%;">0.57</span><span class="ltx_text" id="S4.T12.71.71.71.1.2" style="font-size:70%;"> </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.71.71.71.1.m1.1"><semantics id="S4.T12.71.71.71.1.m1.1a"><mo id="S4.T12.71.71.71.1.m1.1.1" mathsize="50%" xref="S4.T12.71.71.71.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.71.71.71.1.m1.1b"><csymbol cd="latexml" id="S4.T12.71.71.71.1.m1.1.1.cmml" xref="S4.T12.71.71.71.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.71.71.71.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.71.71.71.1.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.71.71.71.1.3" style="font-size:50%;">0.25</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.72.72.72.2">
<span class="ltx_text" id="S4.T12.72.72.72.2.1" style="font-size:70%;">0.48 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.72.72.72.2.m1.1"><semantics id="S4.T12.72.72.72.2.m1.1a"><mo id="S4.T12.72.72.72.2.m1.1.1" mathsize="50%" xref="S4.T12.72.72.72.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.72.72.72.2.m1.1b"><csymbol cd="latexml" id="S4.T12.72.72.72.2.m1.1.1.cmml" xref="S4.T12.72.72.72.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.72.72.72.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.72.72.72.2.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.72.72.72.2.2" style="font-size:50%;">0.24</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.73.73.73.3">
<span class="ltx_text" id="S4.T12.73.73.73.3.1" style="font-size:70%;">0.32 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.73.73.73.3.m1.1"><semantics id="S4.T12.73.73.73.3.m1.1a"><mo id="S4.T12.73.73.73.3.m1.1.1" mathsize="50%" xref="S4.T12.73.73.73.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.73.73.73.3.m1.1b"><csymbol cd="latexml" id="S4.T12.73.73.73.3.m1.1.1.cmml" xref="S4.T12.73.73.73.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.73.73.73.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.73.73.73.3.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.73.73.73.3.2" style="font-size:50%;">0.09</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.74.74.74.4">
<span class="ltx_text" id="S4.T12.74.74.74.4.1" style="font-size:70%;">0.44 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.74.74.74.4.m1.1"><semantics id="S4.T12.74.74.74.4.m1.1a"><mo id="S4.T12.74.74.74.4.m1.1.1" mathsize="50%" xref="S4.T12.74.74.74.4.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.74.74.74.4.m1.1b"><csymbol cd="latexml" id="S4.T12.74.74.74.4.m1.1.1.cmml" xref="S4.T12.74.74.74.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.74.74.74.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.74.74.74.4.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.74.74.74.4.2" style="font-size:50%;">0.47</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.75.75.75.5">
<span class="ltx_text" id="S4.T12.75.75.75.5.1" style="font-size:70%;">0.46 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.75.75.75.5.m1.1"><semantics id="S4.T12.75.75.75.5.m1.1a"><mo id="S4.T12.75.75.75.5.m1.1.1" mathsize="50%" xref="S4.T12.75.75.75.5.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.75.75.75.5.m1.1b"><csymbol cd="latexml" id="S4.T12.75.75.75.5.m1.1.1.cmml" xref="S4.T12.75.75.75.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.75.75.75.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.75.75.75.5.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.75.75.75.5.2" style="font-size:50%;">0.46</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.76.76.76.6">
<span class="ltx_text" id="S4.T12.76.76.76.6.1" style="font-size:70%;">0.44 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.76.76.76.6.m1.1"><semantics id="S4.T12.76.76.76.6.m1.1a"><mo id="S4.T12.76.76.76.6.m1.1.1" mathsize="50%" xref="S4.T12.76.76.76.6.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.76.76.76.6.m1.1b"><csymbol cd="latexml" id="S4.T12.76.76.76.6.m1.1.1.cmml" xref="S4.T12.76.76.76.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.76.76.76.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.76.76.76.6.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.76.76.76.6.2" style="font-size:50%;">0.47</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.77.77.77.7">
<span class="ltx_text" id="S4.T12.77.77.77.7.1" style="font-size:70%;">0.44 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.77.77.77.7.m1.1"><semantics id="S4.T12.77.77.77.7.m1.1a"><mo id="S4.T12.77.77.77.7.m1.1.1" mathsize="50%" xref="S4.T12.77.77.77.7.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.77.77.77.7.m1.1b"><csymbol cd="latexml" id="S4.T12.77.77.77.7.m1.1.1.cmml" xref="S4.T12.77.77.77.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.77.77.77.7.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.77.77.77.7.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.77.77.77.7.2" style="font-size:50%;">0.47</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T12.84.84.84">
<td class="ltx_td ltx_align_left" id="S4.T12.84.84.84.8"><span class="ltx_text" id="S4.T12.84.84.84.8.1" style="font-size:70%;">Portugese</span></td>
<td class="ltx_td ltx_align_center" id="S4.T12.78.78.78.1">
<span class="ltx_text ltx_font_bold" id="S4.T12.78.78.78.1.1" style="font-size:70%;">0.67</span><span class="ltx_text" id="S4.T12.78.78.78.1.2" style="font-size:70%;"> </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.78.78.78.1.m1.1"><semantics id="S4.T12.78.78.78.1.m1.1a"><mo id="S4.T12.78.78.78.1.m1.1.1" mathsize="50%" xref="S4.T12.78.78.78.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.78.78.78.1.m1.1b"><csymbol cd="latexml" id="S4.T12.78.78.78.1.m1.1.1.cmml" xref="S4.T12.78.78.78.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.78.78.78.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.78.78.78.1.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.78.78.78.1.3" style="font-size:50%;">0.23</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.79.79.79.2">
<span class="ltx_text" id="S4.T12.79.79.79.2.1" style="font-size:70%;">0.63 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.79.79.79.2.m1.1"><semantics id="S4.T12.79.79.79.2.m1.1a"><mo id="S4.T12.79.79.79.2.m1.1.1" mathsize="50%" xref="S4.T12.79.79.79.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.79.79.79.2.m1.1b"><csymbol cd="latexml" id="S4.T12.79.79.79.2.m1.1.1.cmml" xref="S4.T12.79.79.79.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.79.79.79.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.79.79.79.2.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.79.79.79.2.2" style="font-size:50%;">0.26</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.80.80.80.3">
<span class="ltx_text" id="S4.T12.80.80.80.3.1" style="font-size:70%;">0.4 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.80.80.80.3.m1.1"><semantics id="S4.T12.80.80.80.3.m1.1a"><mo id="S4.T12.80.80.80.3.m1.1.1" mathsize="50%" xref="S4.T12.80.80.80.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.80.80.80.3.m1.1b"><csymbol cd="latexml" id="S4.T12.80.80.80.3.m1.1.1.cmml" xref="S4.T12.80.80.80.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.80.80.80.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.80.80.80.3.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.80.80.80.3.2" style="font-size:50%;">0.03</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.81.81.81.4">
<span class="ltx_text" id="S4.T12.81.81.81.4.1" style="font-size:70%;">0.48 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.81.81.81.4.m1.1"><semantics id="S4.T12.81.81.81.4.m1.1a"><mo id="S4.T12.81.81.81.4.m1.1.1" mathsize="50%" xref="S4.T12.81.81.81.4.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.81.81.81.4.m1.1b"><csymbol cd="latexml" id="S4.T12.81.81.81.4.m1.1.1.cmml" xref="S4.T12.81.81.81.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.81.81.81.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.81.81.81.4.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.81.81.81.4.2" style="font-size:50%;">0.48</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.82.82.82.5">
<span class="ltx_text" id="S4.T12.82.82.82.5.1" style="font-size:70%;">0.49 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.82.82.82.5.m1.1"><semantics id="S4.T12.82.82.82.5.m1.1a"><mo id="S4.T12.82.82.82.5.m1.1.1" mathsize="50%" xref="S4.T12.82.82.82.5.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.82.82.82.5.m1.1b"><csymbol cd="latexml" id="S4.T12.82.82.82.5.m1.1.1.cmml" xref="S4.T12.82.82.82.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.82.82.82.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.82.82.82.5.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.82.82.82.5.2" style="font-size:50%;">0.48</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.83.83.83.6">
<span class="ltx_text" id="S4.T12.83.83.83.6.1" style="font-size:70%;">0.48 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.83.83.83.6.m1.1"><semantics id="S4.T12.83.83.83.6.m1.1a"><mo id="S4.T12.83.83.83.6.m1.1.1" mathsize="50%" xref="S4.T12.83.83.83.6.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.83.83.83.6.m1.1b"><csymbol cd="latexml" id="S4.T12.83.83.83.6.m1.1.1.cmml" xref="S4.T12.83.83.83.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.83.83.83.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.83.83.83.6.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.83.83.83.6.2" style="font-size:50%;">0.48</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.84.84.84.7">
<span class="ltx_text" id="S4.T12.84.84.84.7.1" style="font-size:70%;">0.48 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.84.84.84.7.m1.1"><semantics id="S4.T12.84.84.84.7.m1.1a"><mo id="S4.T12.84.84.84.7.m1.1.1" mathsize="50%" xref="S4.T12.84.84.84.7.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.84.84.84.7.m1.1b"><csymbol cd="latexml" id="S4.T12.84.84.84.7.m1.1.1.cmml" xref="S4.T12.84.84.84.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.84.84.84.7.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.84.84.84.7.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.84.84.84.7.2" style="font-size:50%;">0.48</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T12.91.91.91">
<td class="ltx_td ltx_align_left" id="S4.T12.91.91.91.8"><span class="ltx_text" id="S4.T12.91.91.91.8.1" style="font-size:70%;">Spanish</span></td>
<td class="ltx_td ltx_align_center" id="S4.T12.85.85.85.1">
<span class="ltx_text ltx_font_bold" id="S4.T12.85.85.85.1.1" style="font-size:70%;">0.66</span><span class="ltx_text" id="S4.T12.85.85.85.1.2" style="font-size:70%;"> </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.85.85.85.1.m1.1"><semantics id="S4.T12.85.85.85.1.m1.1a"><mo id="S4.T12.85.85.85.1.m1.1.1" mathsize="50%" xref="S4.T12.85.85.85.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.85.85.85.1.m1.1b"><csymbol cd="latexml" id="S4.T12.85.85.85.1.m1.1.1.cmml" xref="S4.T12.85.85.85.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.85.85.85.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.85.85.85.1.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.85.85.85.1.3" style="font-size:50%;">0.24</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.86.86.86.2">
<span class="ltx_text" id="S4.T12.86.86.86.2.1" style="font-size:70%;">0.65 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.86.86.86.2.m1.1"><semantics id="S4.T12.86.86.86.2.m1.1a"><mo id="S4.T12.86.86.86.2.m1.1.1" mathsize="50%" xref="S4.T12.86.86.86.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.86.86.86.2.m1.1b"><csymbol cd="latexml" id="S4.T12.86.86.86.2.m1.1.1.cmml" xref="S4.T12.86.86.86.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.86.86.86.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.86.86.86.2.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.86.86.86.2.2" style="font-size:50%;">0.24</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.87.87.87.3">
<span class="ltx_text" id="S4.T12.87.87.87.3.1" style="font-size:70%;">0.42 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.87.87.87.3.m1.1"><semantics id="S4.T12.87.87.87.3.m1.1a"><mo id="S4.T12.87.87.87.3.m1.1.1" mathsize="50%" xref="S4.T12.87.87.87.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.87.87.87.3.m1.1b"><csymbol cd="latexml" id="S4.T12.87.87.87.3.m1.1.1.cmml" xref="S4.T12.87.87.87.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.87.87.87.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.87.87.87.3.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.87.87.87.3.2" style="font-size:50%;">0.02</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.88.88.88.4">
<span class="ltx_text" id="S4.T12.88.88.88.4.1" style="font-size:70%;">0.35 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.88.88.88.4.m1.1"><semantics id="S4.T12.88.88.88.4.m1.1a"><mo id="S4.T12.88.88.88.4.m1.1.1" mathsize="50%" xref="S4.T12.88.88.88.4.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.88.88.88.4.m1.1b"><csymbol cd="latexml" id="S4.T12.88.88.88.4.m1.1.1.cmml" xref="S4.T12.88.88.88.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.88.88.88.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.88.88.88.4.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.88.88.88.4.2" style="font-size:50%;">0.42</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.89.89.89.5">
<span class="ltx_text" id="S4.T12.89.89.89.5.1" style="font-size:70%;">0.35 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.89.89.89.5.m1.1"><semantics id="S4.T12.89.89.89.5.m1.1a"><mo id="S4.T12.89.89.89.5.m1.1.1" mathsize="50%" xref="S4.T12.89.89.89.5.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.89.89.89.5.m1.1b"><csymbol cd="latexml" id="S4.T12.89.89.89.5.m1.1.1.cmml" xref="S4.T12.89.89.89.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.89.89.89.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.89.89.89.5.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.89.89.89.5.2" style="font-size:50%;">0.44</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.90.90.90.6">
<span class="ltx_text" id="S4.T12.90.90.90.6.1" style="font-size:70%;">0.36 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.90.90.90.6.m1.1"><semantics id="S4.T12.90.90.90.6.m1.1a"><mo id="S4.T12.90.90.90.6.m1.1.1" mathsize="50%" xref="S4.T12.90.90.90.6.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.90.90.90.6.m1.1b"><csymbol cd="latexml" id="S4.T12.90.90.90.6.m1.1.1.cmml" xref="S4.T12.90.90.90.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.90.90.90.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.90.90.90.6.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.90.90.90.6.2" style="font-size:50%;">0.44</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.91.91.91.7">
<span class="ltx_text" id="S4.T12.91.91.91.7.1" style="font-size:70%;">0.36 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.91.91.91.7.m1.1"><semantics id="S4.T12.91.91.91.7.m1.1a"><mo id="S4.T12.91.91.91.7.m1.1.1" mathsize="50%" xref="S4.T12.91.91.91.7.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.91.91.91.7.m1.1b"><csymbol cd="latexml" id="S4.T12.91.91.91.7.m1.1.1.cmml" xref="S4.T12.91.91.91.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.91.91.91.7.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.91.91.91.7.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.91.91.91.7.2" style="font-size:50%;">0.43</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T12.98.98.98">
<td class="ltx_td ltx_align_left" id="S4.T12.98.98.98.8"><span class="ltx_text" id="S4.T12.98.98.98.8.1" style="font-size:70%;">Tamil</span></td>
<td class="ltx_td ltx_align_center" id="S4.T12.92.92.92.1">
<span class="ltx_text ltx_font_bold" id="S4.T12.92.92.92.1.1" style="font-size:70%;">0.57</span><span class="ltx_text" id="S4.T12.92.92.92.1.2" style="font-size:70%;"> </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.92.92.92.1.m1.1"><semantics id="S4.T12.92.92.92.1.m1.1a"><mo id="S4.T12.92.92.92.1.m1.1.1" mathsize="50%" xref="S4.T12.92.92.92.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.92.92.92.1.m1.1b"><csymbol cd="latexml" id="S4.T12.92.92.92.1.m1.1.1.cmml" xref="S4.T12.92.92.92.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.92.92.92.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.92.92.92.1.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.92.92.92.1.3" style="font-size:50%;">0.25</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.93.93.93.2">
<span class="ltx_text" id="S4.T12.93.93.93.2.1" style="font-size:70%;">0.51 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.93.93.93.2.m1.1"><semantics id="S4.T12.93.93.93.2.m1.1a"><mo id="S4.T12.93.93.93.2.m1.1.1" mathsize="50%" xref="S4.T12.93.93.93.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.93.93.93.2.m1.1b"><csymbol cd="latexml" id="S4.T12.93.93.93.2.m1.1.1.cmml" xref="S4.T12.93.93.93.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.93.93.93.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.93.93.93.2.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.93.93.93.2.2" style="font-size:50%;">0.27</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.94.94.94.3">
<span class="ltx_text" id="S4.T12.94.94.94.3.1" style="font-size:70%;">0.43 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.94.94.94.3.m1.1"><semantics id="S4.T12.94.94.94.3.m1.1a"><mo id="S4.T12.94.94.94.3.m1.1.1" mathsize="50%" xref="S4.T12.94.94.94.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.94.94.94.3.m1.1b"><csymbol cd="latexml" id="S4.T12.94.94.94.3.m1.1.1.cmml" xref="S4.T12.94.94.94.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.94.94.94.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.94.94.94.3.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.94.94.94.3.2" style="font-size:50%;">0.05</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.95.95.95.4">
<span class="ltx_text" id="S4.T12.95.95.95.4.1" style="font-size:70%;">0.51 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.95.95.95.4.m1.1"><semantics id="S4.T12.95.95.95.4.m1.1a"><mo id="S4.T12.95.95.95.4.m1.1.1" mathsize="50%" xref="S4.T12.95.95.95.4.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.95.95.95.4.m1.1b"><csymbol cd="latexml" id="S4.T12.95.95.95.4.m1.1.1.cmml" xref="S4.T12.95.95.95.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.95.95.95.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.95.95.95.4.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.95.95.95.4.2" style="font-size:50%;">0.44</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.96.96.96.5">
<span class="ltx_text" id="S4.T12.96.96.96.5.1" style="font-size:70%;">0.53 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.96.96.96.5.m1.1"><semantics id="S4.T12.96.96.96.5.m1.1a"><mo id="S4.T12.96.96.96.5.m1.1.1" mathsize="50%" xref="S4.T12.96.96.96.5.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.96.96.96.5.m1.1b"><csymbol cd="latexml" id="S4.T12.96.96.96.5.m1.1.1.cmml" xref="S4.T12.96.96.96.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.96.96.96.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.96.96.96.5.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.96.96.96.5.2" style="font-size:50%;">0.44</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.97.97.97.6">
<span class="ltx_text" id="S4.T12.97.97.97.6.1" style="font-size:70%;">0.5 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.97.97.97.6.m1.1"><semantics id="S4.T12.97.97.97.6.m1.1a"><mo id="S4.T12.97.97.97.6.m1.1.1" mathsize="50%" xref="S4.T12.97.97.97.6.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.97.97.97.6.m1.1b"><csymbol cd="latexml" id="S4.T12.97.97.97.6.m1.1.1.cmml" xref="S4.T12.97.97.97.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.97.97.97.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.97.97.97.6.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.97.97.97.6.2" style="font-size:50%;">0.44</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.98.98.98.7">
<span class="ltx_text" id="S4.T12.98.98.98.7.1" style="font-size:70%;">0.5 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.98.98.98.7.m1.1"><semantics id="S4.T12.98.98.98.7.m1.1a"><mo id="S4.T12.98.98.98.7.m1.1.1" mathsize="50%" xref="S4.T12.98.98.98.7.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.98.98.98.7.m1.1b"><csymbol cd="latexml" id="S4.T12.98.98.98.7.m1.1.1.cmml" xref="S4.T12.98.98.98.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.98.98.98.7.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.98.98.98.7.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.98.98.98.7.2" style="font-size:50%;">0.44</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T12.105.105.105">
<td class="ltx_td ltx_align_left" id="S4.T12.105.105.105.8"><span class="ltx_text" id="S4.T12.105.105.105.8.1" style="font-size:70%;">Urdu</span></td>
<td class="ltx_td ltx_align_center" id="S4.T12.99.99.99.1">
<span class="ltx_text ltx_font_bold" id="S4.T12.99.99.99.1.1" style="font-size:70%;">0.75</span><span class="ltx_text" id="S4.T12.99.99.99.1.2" style="font-size:70%;"> </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.99.99.99.1.m1.1"><semantics id="S4.T12.99.99.99.1.m1.1a"><mo id="S4.T12.99.99.99.1.m1.1.1" mathsize="50%" xref="S4.T12.99.99.99.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.99.99.99.1.m1.1b"><csymbol cd="latexml" id="S4.T12.99.99.99.1.m1.1.1.cmml" xref="S4.T12.99.99.99.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.99.99.99.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.99.99.99.1.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.99.99.99.1.3" style="font-size:50%;">0.21</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.100.100.100.2">
<span class="ltx_text" id="S4.T12.100.100.100.2.1" style="font-size:70%;">0.70 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.100.100.100.2.m1.1"><semantics id="S4.T12.100.100.100.2.m1.1a"><mo id="S4.T12.100.100.100.2.m1.1.1" mathsize="50%" xref="S4.T12.100.100.100.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.100.100.100.2.m1.1b"><csymbol cd="latexml" id="S4.T12.100.100.100.2.m1.1.1.cmml" xref="S4.T12.100.100.100.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.100.100.100.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.100.100.100.2.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.100.100.100.2.2" style="font-size:50%;">0.24</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.101.101.101.3">
<span class="ltx_text" id="S4.T12.101.101.101.3.1" style="font-size:70%;">0.43 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.101.101.101.3.m1.1"><semantics id="S4.T12.101.101.101.3.m1.1a"><mo id="S4.T12.101.101.101.3.m1.1.1" mathsize="50%" xref="S4.T12.101.101.101.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.101.101.101.3.m1.1b"><csymbol cd="latexml" id="S4.T12.101.101.101.3.m1.1.1.cmml" xref="S4.T12.101.101.101.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.101.101.101.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.101.101.101.3.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.101.101.101.3.2" style="font-size:50%;">0.02</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.102.102.102.4">
<span class="ltx_text" id="S4.T12.102.102.102.4.1" style="font-size:70%;">0.39 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.102.102.102.4.m1.1"><semantics id="S4.T12.102.102.102.4.m1.1a"><mo id="S4.T12.102.102.102.4.m1.1.1" mathsize="50%" xref="S4.T12.102.102.102.4.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.102.102.102.4.m1.1b"><csymbol cd="latexml" id="S4.T12.102.102.102.4.m1.1.1.cmml" xref="S4.T12.102.102.102.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.102.102.102.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.102.102.102.4.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.102.102.102.4.2" style="font-size:50%;">0.48</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.103.103.103.5">
<span class="ltx_text" id="S4.T12.103.103.103.5.1" style="font-size:70%;">0.39 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.103.103.103.5.m1.1"><semantics id="S4.T12.103.103.103.5.m1.1a"><mo id="S4.T12.103.103.103.5.m1.1.1" mathsize="50%" xref="S4.T12.103.103.103.5.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.103.103.103.5.m1.1b"><csymbol cd="latexml" id="S4.T12.103.103.103.5.m1.1.1.cmml" xref="S4.T12.103.103.103.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.103.103.103.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.103.103.103.5.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.103.103.103.5.2" style="font-size:50%;">0.47</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.104.104.104.6">
<span class="ltx_text" id="S4.T12.104.104.104.6.1" style="font-size:70%;">0.39 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.104.104.104.6.m1.1"><semantics id="S4.T12.104.104.104.6.m1.1a"><mo id="S4.T12.104.104.104.6.m1.1.1" mathsize="50%" xref="S4.T12.104.104.104.6.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.104.104.104.6.m1.1b"><csymbol cd="latexml" id="S4.T12.104.104.104.6.m1.1.1.cmml" xref="S4.T12.104.104.104.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.104.104.104.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.104.104.104.6.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.104.104.104.6.2" style="font-size:50%;">0.48</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.105.105.105.7">
<span class="ltx_text" id="S4.T12.105.105.105.7.1" style="font-size:70%;">0.39 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.105.105.105.7.m1.1"><semantics id="S4.T12.105.105.105.7.m1.1a"><mo id="S4.T12.105.105.105.7.m1.1.1" mathsize="50%" xref="S4.T12.105.105.105.7.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.105.105.105.7.m1.1b"><csymbol cd="latexml" id="S4.T12.105.105.105.7.m1.1.1.cmml" xref="S4.T12.105.105.105.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.105.105.105.7.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.105.105.105.7.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.105.105.105.7.2" style="font-size:50%;">0.48</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T12.112.112.112">
<td class="ltx_td ltx_align_left" id="S4.T12.112.112.112.8"><span class="ltx_text" id="S4.T12.112.112.112.8.1" style="font-size:70%;">Wolof</span></td>
<td class="ltx_td ltx_align_center" id="S4.T12.106.106.106.1">
<span class="ltx_text ltx_font_bold" id="S4.T12.106.106.106.1.1" style="font-size:70%;">0.51</span><span class="ltx_text" id="S4.T12.106.106.106.1.2" style="font-size:70%;"> </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.106.106.106.1.m1.1"><semantics id="S4.T12.106.106.106.1.m1.1a"><mo id="S4.T12.106.106.106.1.m1.1.1" mathsize="50%" xref="S4.T12.106.106.106.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.106.106.106.1.m1.1b"><csymbol cd="latexml" id="S4.T12.106.106.106.1.m1.1.1.cmml" xref="S4.T12.106.106.106.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.106.106.106.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.106.106.106.1.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.106.106.106.1.3" style="font-size:50%;">0.32</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.107.107.107.2">
<span class="ltx_text" id="S4.T12.107.107.107.2.1" style="font-size:70%;">0.47 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.107.107.107.2.m1.1"><semantics id="S4.T12.107.107.107.2.m1.1a"><mo id="S4.T12.107.107.107.2.m1.1.1" mathsize="50%" xref="S4.T12.107.107.107.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.107.107.107.2.m1.1b"><csymbol cd="latexml" id="S4.T12.107.107.107.2.m1.1.1.cmml" xref="S4.T12.107.107.107.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.107.107.107.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.107.107.107.2.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.107.107.107.2.2" style="font-size:50%;">0.32</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.108.108.108.3">
<span class="ltx_text" id="S4.T12.108.108.108.3.1" style="font-size:70%;">0.41 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.108.108.108.3.m1.1"><semantics id="S4.T12.108.108.108.3.m1.1a"><mo id="S4.T12.108.108.108.3.m1.1.1" mathsize="50%" xref="S4.T12.108.108.108.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.108.108.108.3.m1.1b"><csymbol cd="latexml" id="S4.T12.108.108.108.3.m1.1.1.cmml" xref="S4.T12.108.108.108.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.108.108.108.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.108.108.108.3.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.108.108.108.3.2" style="font-size:50%;">0.02</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.109.109.109.4">
<span class="ltx_text" id="S4.T12.109.109.109.4.1" style="font-size:70%;">0.26 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.109.109.109.4.m1.1"><semantics id="S4.T12.109.109.109.4.m1.1a"><mo id="S4.T12.109.109.109.4.m1.1.1" mathsize="50%" xref="S4.T12.109.109.109.4.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.109.109.109.4.m1.1b"><csymbol cd="latexml" id="S4.T12.109.109.109.4.m1.1.1.cmml" xref="S4.T12.109.109.109.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.109.109.109.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.109.109.109.4.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.109.109.109.4.2" style="font-size:50%;">0.39</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.110.110.110.5">
<span class="ltx_text" id="S4.T12.110.110.110.5.1" style="font-size:70%;">0.25 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.110.110.110.5.m1.1"><semantics id="S4.T12.110.110.110.5.m1.1a"><mo id="S4.T12.110.110.110.5.m1.1.1" mathsize="50%" xref="S4.T12.110.110.110.5.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.110.110.110.5.m1.1b"><csymbol cd="latexml" id="S4.T12.110.110.110.5.m1.1.1.cmml" xref="S4.T12.110.110.110.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.110.110.110.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.110.110.110.5.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.110.110.110.5.2" style="font-size:50%;">0.39</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.111.111.111.6">
<span class="ltx_text" id="S4.T12.111.111.111.6.1" style="font-size:70%;">0.3 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.111.111.111.6.m1.1"><semantics id="S4.T12.111.111.111.6.m1.1a"><mo id="S4.T12.111.111.111.6.m1.1.1" mathsize="50%" xref="S4.T12.111.111.111.6.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.111.111.111.6.m1.1b"><csymbol cd="latexml" id="S4.T12.111.111.111.6.m1.1.1.cmml" xref="S4.T12.111.111.111.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.111.111.111.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.111.111.111.6.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.111.111.111.6.2" style="font-size:50%;">0.43</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T12.112.112.112.7">
<span class="ltx_text" id="S4.T12.112.112.112.7.1" style="font-size:70%;">0.27 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.112.112.112.7.m1.1"><semantics id="S4.T12.112.112.112.7.m1.1a"><mo id="S4.T12.112.112.112.7.m1.1.1" mathsize="50%" xref="S4.T12.112.112.112.7.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.112.112.112.7.m1.1b"><csymbol cd="latexml" id="S4.T12.112.112.112.7.m1.1.1.cmml" xref="S4.T12.112.112.112.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.112.112.112.7.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.112.112.112.7.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.112.112.112.7.2" style="font-size:50%;">0.39</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T12.119.119.119">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T12.119.119.119.8"><span class="ltx_text" id="S4.T12.119.119.119.8.1" style="font-size:70%;">Yoruba</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T12.113.113.113.1">
<span class="ltx_text ltx_font_bold" id="S4.T12.113.113.113.1.1" style="font-size:70%;">0.48</span><span class="ltx_text" id="S4.T12.113.113.113.1.2" style="font-size:70%;"> </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.113.113.113.1.m1.1"><semantics id="S4.T12.113.113.113.1.m1.1a"><mo id="S4.T12.113.113.113.1.m1.1.1" mathsize="50%" xref="S4.T12.113.113.113.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.113.113.113.1.m1.1b"><csymbol cd="latexml" id="S4.T12.113.113.113.1.m1.1.1.cmml" xref="S4.T12.113.113.113.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.113.113.113.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.113.113.113.1.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.113.113.113.1.3" style="font-size:50%;">0.07</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T12.114.114.114.2">
<span class="ltx_text" id="S4.T12.114.114.114.2.1" style="font-size:70%;">0.36 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.114.114.114.2.m1.1"><semantics id="S4.T12.114.114.114.2.m1.1a"><mo id="S4.T12.114.114.114.2.m1.1.1" mathsize="50%" xref="S4.T12.114.114.114.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.114.114.114.2.m1.1b"><csymbol cd="latexml" id="S4.T12.114.114.114.2.m1.1.1.cmml" xref="S4.T12.114.114.114.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.114.114.114.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.114.114.114.2.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.114.114.114.2.2" style="font-size:50%;">0.07</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T12.115.115.115.3">
<span class="ltx_text" id="S4.T12.115.115.115.3.1" style="font-size:70%;">0.43 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.115.115.115.3.m1.1"><semantics id="S4.T12.115.115.115.3.m1.1a"><mo id="S4.T12.115.115.115.3.m1.1.1" mathsize="50%" xref="S4.T12.115.115.115.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.115.115.115.3.m1.1b"><csymbol cd="latexml" id="S4.T12.115.115.115.3.m1.1.1.cmml" xref="S4.T12.115.115.115.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.115.115.115.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.115.115.115.3.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.115.115.115.3.2" style="font-size:50%;">0.06</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T12.116.116.116.4">
<span class="ltx_text" id="S4.T12.116.116.116.4.1" style="font-size:70%;">0.33 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.116.116.116.4.m1.1"><semantics id="S4.T12.116.116.116.4.m1.1a"><mo id="S4.T12.116.116.116.4.m1.1.1" mathsize="50%" xref="S4.T12.116.116.116.4.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.116.116.116.4.m1.1b"><csymbol cd="latexml" id="S4.T12.116.116.116.4.m1.1.1.cmml" xref="S4.T12.116.116.116.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.116.116.116.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.116.116.116.4.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.116.116.116.4.2" style="font-size:50%;">0.45</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T12.117.117.117.5">
<span class="ltx_text" id="S4.T12.117.117.117.5.1" style="font-size:70%;">0.09 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.117.117.117.5.m1.1"><semantics id="S4.T12.117.117.117.5.m1.1a"><mo id="S4.T12.117.117.117.5.m1.1.1" mathsize="50%" xref="S4.T12.117.117.117.5.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.117.117.117.5.m1.1b"><csymbol cd="latexml" id="S4.T12.117.117.117.5.m1.1.1.cmml" xref="S4.T12.117.117.117.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.117.117.117.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.117.117.117.5.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.117.117.117.5.2" style="font-size:50%;">0.05</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T12.118.118.118.6">
<span class="ltx_text" id="S4.T12.118.118.118.6.1" style="font-size:70%;">0.16 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.118.118.118.6.m1.1"><semantics id="S4.T12.118.118.118.6.m1.1a"><mo id="S4.T12.118.118.118.6.m1.1.1" mathsize="50%" xref="S4.T12.118.118.118.6.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.118.118.118.6.m1.1b"><csymbol cd="latexml" id="S4.T12.118.118.118.6.m1.1.1.cmml" xref="S4.T12.118.118.118.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.118.118.118.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.118.118.118.6.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.118.118.118.6.2" style="font-size:50%;">0.11</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T12.119.119.119.7">
<span class="ltx_text" id="S4.T12.119.119.119.7.1" style="font-size:70%;">0.09 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S4.T12.119.119.119.7.m1.1"><semantics id="S4.T12.119.119.119.7.m1.1a"><mo id="S4.T12.119.119.119.7.m1.1.1" mathsize="50%" xref="S4.T12.119.119.119.7.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.119.119.119.7.m1.1b"><csymbol cd="latexml" id="S4.T12.119.119.119.7.m1.1.1.cmml" xref="S4.T12.119.119.119.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.119.119.119.7.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T12.119.119.119.7.m1.1d">Â±</annotation></semantics></math><span class="ltx_text" id="S4.T12.119.119.119.7.2" style="font-size:50%;">0.05</span>
</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T12.126.2.1" style="font-size:129%;">Table 12</span>: </span><span class="ltx_text" id="S4.T12.121.1" style="font-size:129%;">Probing performance (<math alttext="F_{1}" class="ltx_Math" display="inline" id="S4.T12.121.1.m1.1"><semantics id="S4.T12.121.1.m1.1b"><msub id="S4.T12.121.1.m1.1.1" xref="S4.T12.121.1.m1.1.1.cmml"><mi id="S4.T12.121.1.m1.1.1.2" xref="S4.T12.121.1.m1.1.1.2.cmml">F</mi><mn id="S4.T12.121.1.m1.1.1.3" xref="S4.T12.121.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T12.121.1.m1.1c"><apply id="S4.T12.121.1.m1.1.1.cmml" xref="S4.T12.121.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T12.121.1.m1.1.1.1.cmml" xref="S4.T12.121.1.m1.1.1">subscript</csymbol><ci id="S4.T12.121.1.m1.1.1.2.cmml" xref="S4.T12.121.1.m1.1.1.2">ğ¹</ci><cn id="S4.T12.121.1.m1.1.1.3.cmml" type="integer" xref="S4.T12.121.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.121.1.m1.1d">F_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.T12.121.1.m1.1e">italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> averaged by layers) of the BLOOM-based classifiers and count-based baselines. The results are averaged over probing tasks, and three experiment runs within each language. Standard deviation is determined by the results along the language tasks.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsubsection" id="S4.SS9.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.9.2 </span>Results</h4>
<figure class="ltx_figure" id="S4.F12">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S4.F11.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="590" id="S4.F11.sf1.g1" src="x13.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F11.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S4.F11.sf1.3.2" style="font-size:90%;">BLOOM-1B7</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S4.F11.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="590" id="S4.F11.sf2.g1" src="x14.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F11.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.F11.sf2.3.2" style="font-size:90%;">BLOOM</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F12.2.1.1" style="font-size:90%;">Figure 12</span>: </span><span class="ltx_text" id="S4.F12.3.2" style="font-size:90%;">Probing classifiersâ€™ results by language and task category. White squares denote that the morphosyntactic category is not represented in the language.
</span></figcaption>
</figure>
<section class="ltx_paragraph" id="S4.SS9.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">Probing</h5>
<div class="ltx_para" id="S4.SS9.SSS2.Px1.p1">
<p class="ltx_p" id="S4.SS9.SSS2.Px1.p1.1"><a class="ltx_ref ltx_refmacro_autoref" href="#S4.T12" title="Table 12 â€£ Correlation â€£ 4.9.1 Method â€£ 4.9 Multilingual Probing â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">TableÂ 12</span></a> presents the results of probing experiments averaged over the probing tasks and experiment runs within each language. The overall pattern is that BLOOM-1B7 performs on par or better than BLOOM, and both LLMs outperform the count-based baselines. In particular, the LLMs achieve more robust performance on Arabic, Basque, and Indo-European languages (e.g., Catalan, French, Hindi, Portuguese, Spanish, and Urdu), while Bengali, Wolof, and Yoruba receive the lowest scores. We attribute this behavior to the transfer abilities: BLOOM infers linguistic properties better for the closely related languages that comprise a significant amount of data. For example, the performance on any Romance language is better than in English, and the results in Indic languages are close to those in high-resource languages.</p>
</div>
<div class="ltx_para" id="S4.SS9.SSS2.Px1.p2">
<p class="ltx_p" id="S4.SS9.SSS2.Px1.p2.1"><a class="ltx_ref ltx_refmacro_autoref" href="#S4.F12" title="Figure 12 â€£ 4.9.2 Results â€£ 4.9 Multilingual Probing â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">FigureÂ 12</span></a> presents the language-wise probing performance results for morphosyntactic features represented at least in <math alttext="5" class="ltx_Math" display="inline" id="S4.SS9.SSS2.Px1.p2.1.m1.1"><semantics id="S4.SS9.SSS2.Px1.p2.1.m1.1a"><mn id="S4.SS9.SSS2.Px1.p2.1.m1.1.1" xref="S4.SS9.SSS2.Px1.p2.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS9.SSS2.Px1.p2.1.m1.1b"><cn id="S4.SS9.SSS2.Px1.p2.1.m1.1.1.cmml" type="integer" xref="S4.SS9.SSS2.Px1.p2.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS9.SSS2.Px1.p2.1.m1.1c">5</annotation><annotation encoding="application/x-llamapun" id="S4.SS9.SSS2.Px1.p2.1.m1.1d">5</annotation></semantics></math> languages. The probing performance of both LLMs is similar despite the difference in size. We find that the LLMs infer Mood and Person well with no regard for language. Number, NumType (numeral type), and Voice are moderately inferred in most languages. The models generally show worse qualities in the other categories, indicating that they do not encode such morphological information. The possible explanation of such difference in performance may be the diversity of possible values of these categories. For example, Mood and Person share similar values across the presented languages, while the set of Case values is highly dependent on the language.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS9.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">Correlation</h5>
<figure class="ltx_table" id="S4.T13">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T13.2">
<tr class="ltx_tr" id="S4.T13.2.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T13.2.1.1"><span class="ltx_text" id="S4.T13.2.1.1.1" style="font-size:90%;">Criterion</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T13.2.1.2"><span class="ltx_text" id="S4.T13.2.1.2.1" style="font-size:90%;">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T13.2.1.3"><span class="ltx_text" id="S4.T13.2.1.3.1" style="font-size:90%;">Test</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T13.2.1.4"><span class="ltx_text" id="S4.T13.2.1.4.1" style="font-size:90%;">p-value</span></td>
</tr>
<tr class="ltx_tr" id="S4.T13.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T13.2.2.1"><span class="ltx_text" id="S4.T13.2.2.1.1" style="font-size:90%;">Language script</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T13.2.2.2">
<table class="ltx_tabular ltx_align_middle" id="S4.T13.2.2.2.1">
<tr class="ltx_tr" id="S4.T13.2.2.2.1.1">
<td class="ltx_td ltx_align_center" id="S4.T13.2.2.2.1.1.1"><span class="ltx_text" id="S4.T13.2.2.2.1.1.1.1" style="font-size:90%;">BLOOM</span></td>
</tr>
<tr class="ltx_tr" id="S4.T13.2.2.2.1.2">
<td class="ltx_td ltx_align_center" id="S4.T13.2.2.2.1.2.1"><span class="ltx_text" id="S4.T13.2.2.2.1.2.1.1" style="font-size:90%;">BLOOM-1B7</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T13.2.2.3"><span class="ltx_text" id="S4.T13.2.2.3.1" style="font-size:90%;">Mann-Whitney U</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T13.2.2.4">
<table class="ltx_tabular ltx_align_middle" id="S4.T13.2.2.4.1">
<tr class="ltx_tr" id="S4.T13.2.2.4.1.1">
<td class="ltx_td ltx_align_center" id="S4.T13.2.2.4.1.1.1"><span class="ltx_text" id="S4.T13.2.2.4.1.1.1.1" style="font-size:90%;">0.72</span></td>
</tr>
<tr class="ltx_tr" id="S4.T13.2.2.4.1.2">
<td class="ltx_td ltx_align_center" id="S4.T13.2.2.4.1.2.1"><span class="ltx_text" id="S4.T13.2.2.4.1.2.1.1" style="font-size:90%;">0.13</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T13.2.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T13.2.3.1"><span class="ltx_text" id="S4.T13.2.3.1.1" style="font-size:90%;">Language family</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T13.2.3.2">
<table class="ltx_tabular ltx_align_middle" id="S4.T13.2.3.2.1">
<tr class="ltx_tr" id="S4.T13.2.3.2.1.1">
<td class="ltx_td ltx_align_center" id="S4.T13.2.3.2.1.1.1"><span class="ltx_text" id="S4.T13.2.3.2.1.1.1.1" style="font-size:90%;">BLOOM</span></td>
</tr>
<tr class="ltx_tr" id="S4.T13.2.3.2.1.2">
<td class="ltx_td ltx_align_center" id="S4.T13.2.3.2.1.2.1"><span class="ltx_text" id="S4.T13.2.3.2.1.2.1.1" style="font-size:90%;">BLOOM-1B7</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T13.2.3.3"><span class="ltx_text" id="S4.T13.2.3.3.1" style="font-size:90%;">ANOVA</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T13.2.3.4">
<table class="ltx_tabular ltx_align_middle" id="S4.T13.2.3.4.1">
<tr class="ltx_tr" id="S4.T13.2.3.4.1.1">
<td class="ltx_td ltx_align_center" id="S4.T13.2.3.4.1.1.1"><span class="ltx_text" id="S4.T13.2.3.4.1.1.1.1" style="font-size:90%;">Â¡0.01</span></td>
</tr>
<tr class="ltx_tr" id="S4.T13.2.3.4.1.2">
<td class="ltx_td ltx_align_center" id="S4.T13.2.3.4.1.2.1"><span class="ltx_text" id="S4.T13.2.3.4.1.2.1.1" style="font-size:90%;">Â¡0.01</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T13.2.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T13.2.4.1"><span class="ltx_text" id="S4.T13.2.4.1.1" style="font-size:90%;">Probing dataset size</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T13.2.4.2">
<table class="ltx_tabular ltx_align_middle" id="S4.T13.2.4.2.1">
<tr class="ltx_tr" id="S4.T13.2.4.2.1.1">
<td class="ltx_td ltx_align_center" id="S4.T13.2.4.2.1.1.1"><span class="ltx_text" id="S4.T13.2.4.2.1.1.1.1" style="font-size:90%;">BLOOM</span></td>
</tr>
<tr class="ltx_tr" id="S4.T13.2.4.2.1.2">
<td class="ltx_td ltx_align_center" id="S4.T13.2.4.2.1.2.1"><span class="ltx_text" id="S4.T13.2.4.2.1.2.1.1" style="font-size:90%;">BLOOM-1B7</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T13.2.4.3"><span class="ltx_text" id="S4.T13.2.4.3.1" style="font-size:90%;">Pearson</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T13.2.4.4">
<table class="ltx_tabular ltx_align_middle" id="S4.T13.2.4.4.1">
<tr class="ltx_tr" id="S4.T13.2.4.4.1.1">
<td class="ltx_td ltx_align_center" id="S4.T13.2.4.4.1.1.1"><span class="ltx_text" id="S4.T13.2.4.4.1.1.1.1" style="font-size:90%;">0.63</span></td>
</tr>
<tr class="ltx_tr" id="S4.T13.2.4.4.1.2">
<td class="ltx_td ltx_align_center" id="S4.T13.2.4.4.1.2.1"><span class="ltx_text" id="S4.T13.2.4.4.1.2.1.1" style="font-size:90%;">0.02</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T13.2.5">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T13.2.5.1"><span class="ltx_text" id="S4.T13.2.5.1.1" style="font-size:90%;">Pretraining dataset size</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T13.2.5.2">
<table class="ltx_tabular ltx_align_middle" id="S4.T13.2.5.2.1">
<tr class="ltx_tr" id="S4.T13.2.5.2.1.1">
<td class="ltx_td ltx_align_center" id="S4.T13.2.5.2.1.1.1"><span class="ltx_text" id="S4.T13.2.5.2.1.1.1.1" style="font-size:90%;">BLOOM</span></td>
</tr>
<tr class="ltx_tr" id="S4.T13.2.5.2.1.2">
<td class="ltx_td ltx_align_center" id="S4.T13.2.5.2.1.2.1"><span class="ltx_text" id="S4.T13.2.5.2.1.2.1.1" style="font-size:90%;">BLOOM-1B7</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T13.2.5.3"><span class="ltx_text" id="S4.T13.2.5.3.1" style="font-size:90%;">Pearson</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T13.2.5.4">
<table class="ltx_tabular ltx_align_middle" id="S4.T13.2.5.4.1">
<tr class="ltx_tr" id="S4.T13.2.5.4.1.1">
<td class="ltx_td ltx_align_center" id="S4.T13.2.5.4.1.1.1"><span class="ltx_text" id="S4.T13.2.5.4.1.1.1.1" style="font-size:90%;">0.46</span></td>
</tr>
<tr class="ltx_tr" id="S4.T13.2.5.4.1.2">
<td class="ltx_td ltx_align_center" id="S4.T13.2.5.4.1.2.1"><span class="ltx_text" id="S4.T13.2.5.4.1.2.1.1" style="font-size:90%;">Â¡0.01</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T13.2.6">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T13.2.6.1"><span class="ltx_text" id="S4.T13.2.6.1.1" style="font-size:90%;">Difference between versions</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T13.2.6.2"><span class="ltx_text" id="S4.T13.2.6.2.1" style="font-size:90%;">BLOOM &amp; BLOOM-1B7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T13.2.6.3"><span class="ltx_text" id="S4.T13.2.6.3.1" style="font-size:90%;">Mann-Whitney U</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T13.2.6.4"><span class="ltx_text" id="S4.T13.2.6.4.1" style="font-size:90%;">Â¡0.01</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 13: </span>Results of statistical tests and correlation analysis between probing performance and linguistic, dataset, and model configuration criteria.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS9.SSS2.Px2.p1">
<p class="ltx_p" id="S4.SS9.SSS2.Px2.p1.1">The correlation analysis results support conclusions on the probing performance and reveals contributing factors (seeÂ <a class="ltx_ref ltx_refmacro_autoref" href="#S4.T13" title="Table 13 â€£ Correlation â€£ 4.9.2 Results â€£ 4.9 Multilingual Probing â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">TableÂ 13</span></a>). Both models show similar results on the languages with different language scripts. Results of BLOOM-1B7 are highly correlated with language family, probing dataset size, and pretraining dataset size. According to the results of Mann-Whithey U test, BLOOM-1B7 shows significantly better results (<math alttext="p&lt;0.01" class="ltx_Math" display="inline" id="S4.SS9.SSS2.Px2.p1.1.m1.1"><semantics id="S4.SS9.SSS2.Px2.p1.1.m1.1a"><mrow id="S4.SS9.SSS2.Px2.p1.1.m1.1.1" xref="S4.SS9.SSS2.Px2.p1.1.m1.1.1.cmml"><mi id="S4.SS9.SSS2.Px2.p1.1.m1.1.1.2" xref="S4.SS9.SSS2.Px2.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS9.SSS2.Px2.p1.1.m1.1.1.1" xref="S4.SS9.SSS2.Px2.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.SS9.SSS2.Px2.p1.1.m1.1.1.3" xref="S4.SS9.SSS2.Px2.p1.1.m1.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS9.SSS2.Px2.p1.1.m1.1b"><apply id="S4.SS9.SSS2.Px2.p1.1.m1.1.1.cmml" xref="S4.SS9.SSS2.Px2.p1.1.m1.1.1"><lt id="S4.SS9.SSS2.Px2.p1.1.m1.1.1.1.cmml" xref="S4.SS9.SSS2.Px2.p1.1.m1.1.1.1"></lt><ci id="S4.SS9.SSS2.Px2.p1.1.m1.1.1.2.cmml" xref="S4.SS9.SSS2.Px2.p1.1.m1.1.1.2">ğ‘</ci><cn id="S4.SS9.SSS2.Px2.p1.1.m1.1.1.3.cmml" type="float" xref="S4.SS9.SSS2.Px2.p1.1.m1.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS9.SSS2.Px2.p1.1.m1.1c">p&lt;0.01</annotation><annotation encoding="application/x-llamapun" id="S4.SS9.SSS2.Px2.p1.1.m1.1d">italic_p &lt; 0.01</annotation></semantics></math>) than BLOOM. However, BLOOM shows more stable performance on different languages in spite of the amount of data it has seen during pretraining. This might indicate the better generalization abilities of the model with more parameters.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS9.SSS2.Px3">
<h5 class="ltx_title ltx_title_paragraph">Discussion</h5>
<div class="ltx_para" id="S4.SS9.SSS2.Px3.p1">
<p class="ltx_p" id="S4.SS9.SSS2.Px3.p1.1">It should be noted that the following questions remain for further research:</p>
<ol class="ltx_enumerate" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p" id="S4.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I3.i1.p1.1.1">Generalizing abilities.</span> BLOOM-1B7 is leading in the average performance of morphosyntactic feature classification for the languages inÂ <a class="ltx_ref ltx_refmacro_autoref" href="#S4.T12" title="Table 12 â€£ Correlation â€£ 4.9.1 Method â€£ 4.9 Multilingual Probing â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">TableÂ 12</span></a>. The BLOOM results are lower, which can be interpreted as a worse grammatical generalization over the aforecited languages. However, the BLOOM-1B7â€™s probing correlation results with factors like pretraining dataset size are more prominent, which makes it potentially less generalizing on the under-resourced languages than the bigger version.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I3.i2.p1">
<p class="ltx_p" id="S4.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I3.i2.p1.1.1">Multilingual abilities.</span>
A separate research interest implies considering languages that are not explicitly included in the pretraining corpus of the models. Expanding the set of languages for probing will allow for a typological interpretation and a deeper analysis of the most learnable and hard-to-learn linguistic features on a more considerable scope.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I3.i3.p1">
<p class="ltx_p" id="S4.I3.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I3.i3.p1.1.1">Under-resourced language evaluation.</span> The under-resourced languages of the Indic and Niger-Congo families included in the pretraining corpus in smaller shares represent a separate subject for future probing. We also plan to investigate the results of high-resourced and under-resourced languages to reveal possible linguistic insights in these two groups.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S4.I3.i4.p1">
<p class="ltx_p" id="S4.I3.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I3.i4.p1.1.1">Different layers and training dynamics.</span> The analysis has focused on averaged representations of all layers and at the end of training. Analyzing different layers may reveal how morpho-syntactic representations are built during processing. Similarly, investigating how properties are acquired over the course of pre-training <cite class="ltx_cite ltx_citemacro_citep">(Choshen etÂ al., <a class="ltx_ref" href="#bib.bib30" title="">2022</a>; Zhang etÂ al., <a class="ltx_ref" href="#bib.bib170" title="">2021</a>; Voloshina etÂ al., <a class="ltx_ref" href="#bib.bib150" title="">2022</a>)</cite> is a viable direction for research.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
</section>
<section class="ltx_subsection" id="S4.SS10">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.10 </span>Bias</h3>
<div class="ltx_para" id="S4.SS10.p1">
<p class="ltx_p" id="S4.SS10.p1.1">As a preliminary study into the biases learned by BLOOM, we present evaluation on the <span class="ltx_text ltx_font_typewriter" id="S4.SS10.p1.1.1">multilingual CrowS-Pairs</span> dataset, which combines a revised version of the CrowS-Pairs dataset developed byÂ <cite class="ltx_cite ltx_citemacro_cite">Nangia etÂ al. (<a class="ltx_ref" href="#bib.bib101" title="">2020</a>)</cite> together with the French version of CrowS-Pairs introduced byÂ <cite class="ltx_cite ltx_citemacro_cite">NÃ©vÃ©ol etÂ al. (<a class="ltx_ref" href="#bib.bib105" title="">2022</a>)</cite>.
One challenge of this evaluation was to adapt a dataset originally intended for masked language models to autoregressive language models such as BLOOM. CrowS-Pairs relies on minimal pairs to compare a stereotyped statement and a non-stereotyped statement (e.g. â€œ<span class="ltx_text ltx_font_italic" id="S4.SS10.p1.1.2">Women</span> canâ€™t drive.â€ is a gender stereotype while â€œ<span class="ltx_text ltx_font_italic" id="S4.SS10.p1.1.3">Men</span> canâ€™t driveâ€ is not). The two statements differ only by the social category targeted by the stereotype and that social category is present in the stereotyped statement and not in the non-stereotyped statement. The evaluation aims at assessing systematic preference of models for stereotyped statements. The original â€œmetric scoreâ€ compared pseudo-log-likelihood of sentences in a pair to determine which sentence received a higher score from a masked language model. Prompts were designed to require the model to select one of the statements based on the â€œlikelyâ€ and â€œrealisticâ€ nature of the situations described.
</p>
</div>
<div class="ltx_para" id="S4.SS10.p2">
<p class="ltx_p" id="S4.SS10.p2.1">Figure <a class="ltx_ref" href="#S4.F13" title="Figure 13 â€£ 4.10 Bias â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">13</span></a> shows that BLOOMâ€™s overall prompt accuracy was close to .50, which suggests an overall absence of bias. We note that the scores in English and French are very close, suggesting similar overall behavior of the model on both languages.
We also show results on mono-lingual autoregressive models â€” GPT-NeoÂ <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="#bib.bib21" title="">Black etÂ al., </a>)</cite> and GPT-FRÂ <cite class="ltx_cite ltx_citemacro_citep">(Simoulin and CrabbÃ©, <a class="ltx_ref" href="#bib.bib137" title="">2021</a>)</cite> for English and French, respectively.</p>
</div>
<figure class="ltx_figure" id="S4.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="291" id="S4.F13.g1" src="x15.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F13.3.1.1" style="font-size:90%;">Figure 13</span>: </span><span class="ltx_text" id="S4.F13.4.2" style="font-size:90%;">Overall accuracy of BLOOM on <span class="ltx_text ltx_font_typewriter" id="S4.F13.4.2.1">crowS-Pairs</span> per prompt for English and French. Results on the two smallest BLOOM models and monolingual GPT models of comparable size are also shown.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS10.p3">
<p class="ltx_p" id="S4.SS10.p3.1">Table <a class="ltx_ref" href="#S4.T14" title="Table 14 â€£ 4.10 Bias â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">14</span></a> presents the results per bias type in the <span class="ltx_text ltx_font_typewriter" id="S4.SS10.p3.1.1">CrowS-Pairs</span> dataset. The results are quite homogeneous over the categories, which contrasts with previous studies on masked language models, which suggested models were prone to bias in specific categories, which differed between models tested. Nonetheless, accuracy significantly differs from 50 (T-test, p Â¡ .05) overall for both languages, as well as for a number of bias categories, as shown per asterisks in the table.</p>
</div>
<figure class="ltx_table" id="S4.T14">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T14.4">
<tr class="ltx_tr" id="S4.T14.4.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T14.4.1.1">Bias type</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T14.4.1.2">support</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T14.4.1.3">English</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T14.4.1.4">French</td>
</tr>
<tr class="ltx_tr" id="S4.T14.4.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T14.4.2.1">ethnicity  color</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T14.4.2.2">460</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T14.4.2.3">50.05</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T14.4.2.4">50.48*</td>
</tr>
<tr class="ltx_tr" id="S4.T14.4.3">
<td class="ltx_td ltx_align_left" id="S4.T14.4.3.1">gender</td>
<td class="ltx_td ltx_align_left" id="S4.T14.4.3.2">321</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T14.4.3.3">51.17*</td>
<td class="ltx_td ltx_align_left" id="S4.T14.4.3.4">51.24*</td>
</tr>
<tr class="ltx_tr" id="S4.T14.4.4">
<td class="ltx_td ltx_align_left" id="S4.T14.4.4.1">socioeconomic status</td>
<td class="ltx_td ltx_align_left" id="S4.T14.4.4.2">196</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T14.4.4.3">51.05*</td>
<td class="ltx_td ltx_align_left" id="S4.T14.4.4.4">52.22*</td>
</tr>
<tr class="ltx_tr" id="S4.T14.4.5">
<td class="ltx_td ltx_align_left" id="S4.T14.4.5.1">nationality</td>
<td class="ltx_td ltx_align_left" id="S4.T14.4.5.2">253</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T14.4.5.3">49.25*</td>
<td class="ltx_td ltx_align_left" id="S4.T14.4.5.4">48.49*</td>
</tr>
<tr class="ltx_tr" id="S4.T14.4.6">
<td class="ltx_td ltx_align_left" id="S4.T14.4.6.1">religion</td>
<td class="ltx_td ltx_align_left" id="S4.T14.4.6.2">115</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T14.4.6.3">53.82*</td>
<td class="ltx_td ltx_align_left" id="S4.T14.4.6.4">53.01*</td>
</tr>
<tr class="ltx_tr" id="S4.T14.4.7">
<td class="ltx_td ltx_align_left" id="S4.T14.4.7.1">age</td>
<td class="ltx_td ltx_align_left" id="S4.T14.4.7.2">90</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T14.4.7.3">49.35</td>
<td class="ltx_td ltx_align_left" id="S4.T14.4.7.4">50.13</td>
</tr>
<tr class="ltx_tr" id="S4.T14.4.8">
<td class="ltx_td ltx_align_left" id="S4.T14.4.8.1">sexual orientation</td>
<td class="ltx_td ltx_align_left" id="S4.T14.4.8.2">91</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T14.4.8.3">50.00</td>
<td class="ltx_td ltx_align_left" id="S4.T14.4.8.4">49.9</td>
</tr>
<tr class="ltx_tr" id="S4.T14.4.9">
<td class="ltx_td ltx_align_left" id="S4.T14.4.9.1">physical appearance</td>
<td class="ltx_td ltx_align_left" id="S4.T14.4.9.2">72</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T14.4.9.3">48.20</td>
<td class="ltx_td ltx_align_left" id="S4.T14.4.9.4">49.67</td>
</tr>
<tr class="ltx_tr" id="S4.T14.4.10">
<td class="ltx_td ltx_align_left" id="S4.T14.4.10.1">disability</td>
<td class="ltx_td ltx_align_left" id="S4.T14.4.10.2">66</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T14.4.10.3">48.49*</td>
<td class="ltx_td ltx_align_left" id="S4.T14.4.10.4">49.16*</td>
</tr>
<tr class="ltx_tr" id="S4.T14.4.11">
<td class="ltx_td ltx_align_left" id="S4.T14.4.11.1">other</td>
<td class="ltx_td ltx_align_left" id="S4.T14.4.11.2">13</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T14.4.11.3">50.18</td>
<td class="ltx_td ltx_align_left" id="S4.T14.4.11.4">42.1*</td>
</tr>
<tr class="ltx_tr" id="S4.T14.4.12">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T14.4.12.1">All</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T14.4.12.2">1,677</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S4.T14.4.12.3">49.78*</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T14.4.12.4">50.61*</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T14.6.2.1" style="font-size:90%;">Table 14</span>: </span><span class="ltx_text" id="S4.T14.2.1" style="font-size:90%;">BLOOM accuracy results on <span class="ltx_text ltx_font_typewriter" id="S4.T14.2.1.1">crowS-Pairs</span> bias categories averaged over eight runs for English and French. Significance for the one sample T-test (<math alttext="p&lt;.05" class="ltx_Math" display="inline" id="S4.T14.2.1.m1.1"><semantics id="S4.T14.2.1.m1.1b"><mrow id="S4.T14.2.1.m1.1.1" xref="S4.T14.2.1.m1.1.1.cmml"><mi id="S4.T14.2.1.m1.1.1.2" xref="S4.T14.2.1.m1.1.1.2.cmml">p</mi><mo id="S4.T14.2.1.m1.1.1.1" xref="S4.T14.2.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.T14.2.1.m1.1.1.3" xref="S4.T14.2.1.m1.1.1.3.cmml">.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T14.2.1.m1.1c"><apply id="S4.T14.2.1.m1.1.1.cmml" xref="S4.T14.2.1.m1.1.1"><lt id="S4.T14.2.1.m1.1.1.1.cmml" xref="S4.T14.2.1.m1.1.1.1"></lt><ci id="S4.T14.2.1.m1.1.1.2.cmml" xref="S4.T14.2.1.m1.1.1.2">ğ‘</ci><cn id="S4.T14.2.1.m1.1.1.3.cmml" type="float" xref="S4.T14.2.1.m1.1.1.3">.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T14.2.1.m1.1d">p&lt;.05</annotation><annotation encoding="application/x-llamapun" id="S4.T14.2.1.m1.1e">italic_p &lt; .05</annotation></semantics></math>) is indicated with *.</span></figcaption>
</figure>
<section class="ltx_paragraph" id="S4.SS10.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Limitations</h5>
<div class="ltx_para" id="S4.SS10.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS10.SSS0.Px1.p1.1"><cite class="ltx_cite ltx_citemacro_cite">Blodgett etÂ al. (<a class="ltx_ref" href="#bib.bib23" title="">2021</a>)</cite> discuss validity issues with the original CrowS-Pairs corpus. The CrowS-Pairs version used here differs from the original by addressing some of the issues pointed out by <cite class="ltx_cite ltx_citemacro_citet">Blodgett etÂ al. (<a class="ltx_ref" href="#bib.bib23" title="">2021</a>)</cite> and by constructing <math alttext="~{}200" class="ltx_Math" display="inline" id="S4.SS10.SSS0.Px1.p1.1.m1.1"><semantics id="S4.SS10.SSS0.Px1.p1.1.m1.1a"><mn id="S4.SS10.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS10.SSS0.Px1.p1.1.m1.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="S4.SS10.SSS0.Px1.p1.1.m1.1b"><cn id="S4.SS10.SSS0.Px1.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS10.SSS0.Px1.p1.1.m1.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS10.SSS0.Px1.p1.1.m1.1c">~{}200</annotation><annotation encoding="application/x-llamapun" id="S4.SS10.SSS0.Px1.p1.1.m1.1d">200</annotation></semantics></math> additional sentence pairs based on stereotypes collected from French speakers.
In a recent evaluation of bias in masked language models in English and French, results obtained on the revised dataset were not significantly different from those obtained on the original datasetÂ <cite class="ltx_cite ltx_citemacro_cite">NÃ©vÃ©ol etÂ al. (<a class="ltx_ref" href="#bib.bib105" title="">2022</a>)</cite>. However, its original validation does not naturally apply here, and comparison to other CrowS-Pairs results is more difficult.
For a stronger assessment of bias, results obtained with CrowS-Pairs should be compared with other measures of bias, and also assessed for all languages in the model. However, as noted by <cite class="ltx_cite ltx_citemacro_cite">Talat etÂ al. (<a class="ltx_ref" href="#bib.bib144" title="">2022</a>)</cite>, very little material (corpora, measures) is available for multilingual bias assessment.</p>
</div>
<div class="ltx_para" id="S4.SS10.SSS0.Px1.p2">
<p class="ltx_p" id="S4.SS10.SSS0.Px1.p2.1">Although our examinations suggest a limited presence of bias in the model, they cannot cover the breadth of possible usage scenarios. One such scenario where models may have a larger impact is on linguistic diversity and language variation encountered.
As the training resources for BLOOM are carefully curated, they may also capture some language variations to a larger degree than other models. This also impacts the ability of trained models to equitably represent different variations. Such differences can aid in the propagation and legitimization of some language variants over others.
Our evaluation of biases in the model are further limited to the situations, languages and language variants that are covered by <span class="ltx_text ltx_font_typewriter" id="S4.SS10.SSS0.Px1.p2.1.1">multilingual CrowS-Pairs</span>.
We therefore expect a distinction between our findings using CrowS-Pairs and wider model use <cite class="ltx_cite ltx_citemacro_citep">(for a more detailed exploration on such differences, see Raji etÂ al., <a class="ltx_ref" href="#bib.bib121" title="">2021</a>)</cite>.
</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this work, we present BLOOM, a 176B-parameter open-access multilingual language model.
BLOOM was created by BigScience, a collaboration of hundreds of researchers, and was trained on the French government-funded Jean Zay supercomputer for 3.5 months.
In this paper, we chronicled the development of BLOOM, from the creation of its training dataset ROOTS to the design of its architecture and tokenizer.
We also discuss evaluation results of BLOOM and other large language models, finding it has competitive performance that improves after multitask finetuning.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">We hope that the release of a powerful multilingual language model unlocks new applications and research directions for large language models.
Further, we hope that documenting our experience will help the machine learning research community organize new large-scale collaborative projects similar to BigScience. Besides enabling results that are impossible for any individual research group to achieve, this form of organization will also allow more people with different backgrounds to share their ideas and participate in the development of major advances in the field.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Contributions</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Authors are assigned to each authorship category according to which aspects of the project they contributed to.
Many authors appear under multiple categories because they contributed to the project in more than one way.
Author order in all categories is alphabetical by first name, except for â€œMajor Contributorsâ€ where authors are shuffled randomly apart from Teven Le Scao, who is intentionally listed first and â€œOrganizationâ€ where Thomas Wolf is intentionally listed last.
A description of each category follows.
For finer-grained contribution details, please see the papers mentioned under each category.</p>
<dl class="ltx_description" id="S6.I1">
<dt class="ltx_item" id="S6.I1.ix1"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S6.I1.ix1.1.1.1">Major Contributors</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S6.I1.ix1.p1">
<p class="ltx_p" id="S6.I1.ix1.p1.1">lists individuals without whom BLOOM would not have happened and/or who spent more than 20% of their time on the BigScience effort as a whole.</p>
</div>
</dd>
<dt class="ltx_item" id="S6.I1.ix2"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S6.I1.ix2.1.1.1">Dataset</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S6.I1.ix2.p1">
<p class="ltx_p" id="S6.I1.ix2.p1.1">lists individuals who contributed to data sourcing, organization, and processing efforts, including the authors of <cite class="ltx_cite ltx_citemacro_citet">LaurenÃ§on etÂ al. (<a class="ltx_ref" href="#bib.bib74" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">McMillan-Major etÂ al. (<a class="ltx_ref" href="#bib.bib89" title="">2022</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citet">Jernite etÂ al. (<a class="ltx_ref" href="#bib.bib64" title="">2022</a>)</cite>.</p>
</div>
</dd>
<dt class="ltx_item" id="S6.I1.ix3"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S6.I1.ix3.1.1.1">Tokenization</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S6.I1.ix3.p1">
<p class="ltx_p" id="S6.I1.ix3.p1.1">lists individuals who built the BLOOM tokenizer and authors of <cite class="ltx_cite ltx_citemacro_citet">Mielke etÂ al. (<a class="ltx_ref" href="#bib.bib91" title="">2021</a>)</cite>.</p>
</div>
</dd>
<dt class="ltx_item" id="S6.I1.ix4"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S6.I1.ix4.1.1.1">Prompt Engineering</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S6.I1.ix4.p1">
<p class="ltx_p" id="S6.I1.ix4.p1.1">lists individuals who wrote, edited, and reviewed prompt templates for the datasets we consider as well as authors of <cite class="ltx_cite ltx_citemacro_citet">Sanh etÂ al. (<a class="ltx_ref" href="#bib.bib128" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Bach etÂ al. (<a class="ltx_ref" href="#bib.bib8" title="">2022</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citet">Muennighoff etÂ al. (<a class="ltx_ref" href="#bib.bib100" title="">2022b</a>)</cite>.</p>
</div>
</dd>
<dt class="ltx_item" id="S6.I1.ix5"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S6.I1.ix5.1.1.1">Architecture and Objective</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S6.I1.ix5.p1">
<p class="ltx_p" id="S6.I1.ix5.p1.1">lists individuals who ran experiments to help determine BLOOMâ€™s model architecture and training objective, including authors of <cite class="ltx_cite ltx_citemacro_citet">Wang etÂ al. (<a class="ltx_ref" href="#bib.bib156" title="">2022a</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Le Scao etÂ al. (<a class="ltx_ref" href="#bib.bib75" title="">2022</a>)</cite>.</p>
</div>
</dd>
<dt class="ltx_item" id="S6.I1.ix6"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S6.I1.ix6.1.1.1">Engineering</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S6.I1.ix6.p1">
<p class="ltx_p" id="S6.I1.ix6.p1.1">lists individuals who contributed to code and infrastructure to train BLOOM on the Jean Zay supercomputer.
</p>
</div>
</dd>
<dt class="ltx_item" id="S6.I1.ix7"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S6.I1.ix7.1.1.1">Evaluation and interpretability</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S6.I1.ix7.p1">
<p class="ltx_p" id="S6.I1.ix7.p1.1">lists individuals who helped evaluate the BLOOM model as well as authors of <cite class="ltx_cite ltx_citemacro_citet">Talat etÂ al. (<a class="ltx_ref" href="#bib.bib144" title="">2022</a>)</cite>.</p>
</div>
</dd>
<dt class="ltx_item" id="S6.I1.ix8"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S6.I1.ix8.1.1.1">Broader Impacts</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S6.I1.ix8.p1">
<p class="ltx_p" id="S6.I1.ix8.p1.1">lists authors of the ethical charter, license, and model card, in addition to individuals who studied privacy issues, social impacts, and BLOOMâ€™s carbon footprint.</p>
</div>
</dd>
<dt class="ltx_item" id="S6.I1.ix9"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S6.I1.ix9.1.1.1">Applications</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S6.I1.ix9.p1">
<p class="ltx_p" id="S6.I1.ix9.p1.1">lists members of working groups focused on applications of BLOOM, including authors of <cite class="ltx_cite ltx_citemacro_citet">Fries etÂ al. (<a class="ltx_ref" href="#bib.bib47" title="">2022b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Fries etÂ al. (<a class="ltx_ref" href="#bib.bib46" title="">2022a</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citet">DeÂ Toni etÂ al. (<a class="ltx_ref" href="#bib.bib37" title="">2022</a>)</cite>.</p>
</div>
</dd>
<dt class="ltx_item" id="S6.I1.ix10"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S6.I1.ix10.1.1.1">Organization</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S6.I1.ix10.p1">
<p class="ltx_p" id="S6.I1.ix10.p1.1">lists individuals who coordinated the BigScience effort and authors of <cite class="ltx_cite ltx_citemacro_citet">Akiki etÂ al. (<a class="ltx_ref" href="#bib.bib4" title="">2022</a>)</cite>.</p>
</div>
</dd>
</dl>
</div>
<div class="ltx_para ltx_noindent" id="S6.p2">
<br class="ltx_break"/>
<p class="ltx_p" id="S6.p2.1"><span class="ltx_text ltx_font_bold" id="S6.p2.1.1" style="font-size:120%;">Acknowledgments</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">The BigScience Workshop was granted access to the HPC resources of the Institut du dÃ©veloppement et des ressources en informatique scientifique (IDRIS) du Centre national de la recherche scientifique (CNRS) under the allocation 2021-A0101012475 made by the Grand Ã©quipement national de calcul intensif (GENCI). Model training ran on the Jean-Zay supercomputer of GENCI at IDRIS, and we thank the IDRIS team for their responsive support throughout the project, in particular RÃ©mi Lacroix.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">Roman CastagnÃ©, Thomas Wang, BenoÃ®t Sagot and Rachel Bawdenâ€™s contributions were funded by BenoÃ®t Sagotâ€™s and Rachel Bawdenâ€™s chairs in the PRAIRIE institute funded by the French national agency ANR as part of the â€œInvestissements dâ€™avenirâ€ programme under the reference ANR-19-P3IA-0001.
AurÃ©lie NÃ©vÃ©olâ€™s contribution was supported by ANR under grant GEM
ANR-19-CE38-0012.
Oskar van der Walâ€™s contributions were financed by the Dutch Research Council (NWO) as part of Open Competition Digitalisation-SSH with project number 406.DI.19.059.</p>
</div>
<div class="ltx_para" id="S6.p5">
<p class="ltx_p" id="S6.p5.1">The BigScience Workshop would also like to acknowledge the support and financing of the following organizations, organization members and affiliations of some of the participants: ESPCI and LAMSADE (Dauphine UniversitÃ©, PSL, CNRS) for Alexandre Allauzen; MELODI team at IRIT/University of Toulouse for Farah Benamara, ChloÃ© Braud, Philippe Muller, and VÃ©ronique Moriceau; IRISA LinkMedia team IMATAG/CNRS for Vincent Claveau and Antoine Chaffin; UniversitÃ© de Lorraine ATILF UMR 7118 CNRS / UL for Mathieu Constant; University of Paris for BenoÃ®t CrabbÃ©, Marie Candito and Antoine Simoulin; GdR TAL (CNRS) for BÃ©atrice Daille; CNRS DR1 INSERM UMR1093 UBFC Dijon for Peter Ford Dominey; Aix-Marseille University UTLN CNRS LIS/UMR7220 for BenoÃ®t Favre and FrÃ©dÃ©ric BÃ©chet; CEA LASTI for Bertrand Delezoide, Olivier Ferret, Adrian Popescu and Julien Tourille; Sorbonne UniversitÃ© LORIA for Karen Fort; CNRS DR1 LORIA UMR7503 Nancy for Claire Gardent and Christophe Cerisara; MAS Laboratory of Ecole Centrale Paris for CÃ©line Hudelot, RCLN/LIPN UMR 7030 University Sorbonne-Paris-Nord/CNRS for Joseph Le Roux and Nadi Tomeh, UniversitÃ© de Paris and Necker - Enfants Malades hospital for Antoine Neuraz and Ivan Lerner, UniversitÃ© Paris Saclay LISN CNRS UMR9105 for AurÃ©lie NÃ©vÃ©ol, Anne-Laure Ligozat, Caio Corro, Francois Yvon; Inria, Univ. Bordeaux and Ensta ParisTech for Pierre-Yves Oudeyer, CÃ©dric Colas, Grgur Kovac, Tristan Karch; Inria Paris for BenoÃ®t Sagot, DjamÃ© Seddah, Pedro Ortiz; University Toulouse CNRS for Ludovic Tanguy, Sorbonne UniversitÃ©, LIMICS (Sorbonne UniversitÃ©, Inserm, Univ. Sorbonne Paris Nord) for Xavier Tannier; I3S Laboratory, CNRS, INRIA, UniversitÃ© Cote dâ€™Azur for Serena Villata and Elena Cabrio; Airbus, Central Research &amp; Technology for Guillaume Alleon, Alexandre Arnold, and Catherine Kobus; Cloud Temple for Jean-Michel Dussoux; Illuin Technology for Robert Vesoul, Gautier Viaud, Martin dâ€™Hoffschmidt, and Wacim Belblidia; Levia.ai for
Romain Riviere; LightOn for Igor Carron, Laurent Daudet, Iacopo Poli, and Julien Launay; Nabla for Alexandre Lebrun, Martin Raison, and Samuel Humeau; Naver Labs Europe for Matthias GallÃ© and Laurent Besacier; Orange Labs for GÃ©raldine Damnati, Johannes Heinecke, and Frederic Herledan; OVHcloud for Jean-Louis Queguiner and Guillaume Salou; ReciTAL for Thomas Scialom, Gilles Moyse, and Jacopo Staiano; Renault Group for Vincent Feuillard, Joan AndrÃ©, Francois-Paul Servant, Raphael Sourty, and Ayhan Uyanik; SYSTRAN for Jean Senellart, Josep Crego, Elise Michon, Guillaume Klein, Dakun Zhang, and Natalia Segal; Ubisoft for Guillaume Gaudron. Leipzig University and the Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI) in Leipzig for Christopher Akiki.</p>
</div>
<div class="ltx_para" id="S6.p6">
<p class="ltx_p" id="S6.p6.1">Hugging Face provided storage for the entirety of the project, as well as compute for development and part of training the smaller BLOOM models. Many of the evaluations in this paper were made possible by compute resources donated by CoreWeave and EleutherAI.</p>
</div>
<div class="ltx_para" id="S6.p7">
<br class="ltx_break"/>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadji etÂ al. (2021)</span>
<span class="ltx_bibblock">
Julien Abadji, Pedro JavierÂ Ortiz SuÃ¡rez, Laurent Romary, and BenoÃ®t
Sagot.

</span>
<span class="ltx_bibblock">Ungoliant: An optimized pipeline for the generation of a very
large-scale multilingual web corpus.

</span>
<span class="ltx_bibblock">In Harald LÃ¼ngen, Marc Kupietz, Piotr BaÅ„ski, Adrien Barbaresi,
Simon Clematide, and Ines Pisetta, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the Workshop
on Challenges in the Management of Large Corpora (CMLC-9)</em>, pages 1â€“9,
Limerick, Ireland, 2021. Leibniz-Institut fÃ¼r Deutsche Sprache.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.14618/ids-pub-10468" title="">10.14618/ids-pub-10468</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://nbn-resolving.org/urn:nbn:de:bsz:mh39-104688" title="">https://nbn-resolving.org/urn:nbn:de:bsz:mh39-104688</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ãcs (2019)</span>
<span class="ltx_bibblock">
Judit Ãcs.

</span>
<span class="ltx_bibblock">Exploring bertâ€™s vocabulary, 2019.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://juditacs.github.io/2019/02/19/bert-tokenization-stats.html" title="">http://juditacs.github.io/2019/02/19/bert-tokenization-stats.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adi etÂ al. (2017)</span>
<span class="ltx_bibblock">
Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg.

</span>
<span class="ltx_bibblock">Fine-grained analysis of sentence embeddings using auxiliary
prediction tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">International Conference on Learning Representations
(ICLR)</em>, April 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Akiki etÂ al. (2022)</span>
<span class="ltx_bibblock">
Christopher Akiki, Giada Pistilli, Margot Mieskes, Matthias GallÃ©, Thomas
Wolf, Suzana IliÄ‡, and Yacine Jernite.

</span>
<span class="ltx_bibblock">BigScience: A Case Study in the Social Construction of a
Multilingual Large Language Model, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2212.04960" title="">https://arxiv.org/abs/2212.04960</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Al-Rfou etÂ al. (2019)</span>
<span class="ltx_bibblock">
Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones.

</span>
<span class="ltx_bibblock">Character-level language modeling with deeper self-attention.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the AAAI conference on artificial
intelligence</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Altaher etÂ al. (2022)</span>
<span class="ltx_bibblock">
Yousef Altaher, Ali Fadel, Mazen Alotaibi, Mazen Alyazidi, Mishari
Al-Mutairi, Mutlaq Aldhbuiub, Abdulrahman Mosaibah, Abdelrahman Rezk,
Abdulrazzaq Alhendi, MazenÂ Abo Shal, EmadÂ A. Alghamdi, MagedÂ Saeed
AlShaibani, Jezia Zakraoui, Wafaa Mohammed, Kamel Gaanoun, KhalidÂ N.
Elmadani, Mustafa Ghaleb, Nouamane Tazi, Raed Alharbi, Maraim Masoud, and
Zaid Alyafeai.

</span>
<span class="ltx_bibblock">Masader plus: A new interface for exploring +500 arabic NLP
datasets.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">CoRR</em>, abs/2208.00932, 2022.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.48550/arXiv.2208.00932" title="">10.48550/arXiv.2208.00932</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2208.00932" title="">https://doi.org/10.48550/arXiv.2208.00932</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alyafeai etÂ al. (2021)</span>
<span class="ltx_bibblock">
Zaid Alyafeai, Maraim Masoud, Mustafa Ghaleb, and MagedÂ Saeed AlShaibani.

</span>
<span class="ltx_bibblock">Masader: Metadata sourcing for arabic text and speech data resources.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">CoRR</em>, abs/2110.06744, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2110.06744" title="">https://arxiv.org/abs/2110.06744</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bach etÂ al. (2022)</span>
<span class="ltx_bibblock">
Stephen Bach, Victor Sanh, ZhengÂ Xin Yong, Albert Webson, Colin Raffel,
NihalÂ V. Nayak, Abheesht Sharma, Taewoon Kim, MÂ Saiful Bari, Thibault Fevry,
Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-david,
Canwen Xu, Gunjan Chhablani, Han Wang, Jason Fries, Maged Al-shaibani, Shanya
Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev, Mike
Tian-jian Jiang, and Alexander Rush.

</span>
<span class="ltx_bibblock">PromptSource: An integrated development environment and
repository for natural language prompts.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics: System Demonstrations</em>, pages 93â€“104, Dublin,
Ireland, May 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/2022.acl-demo.9" title="">10.18653/v1/2022.acl-demo.9</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.acl-demo.9" title="">https://aclanthology.org/2022.acl-demo.9</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bannour etÂ al. (2021)</span>
<span class="ltx_bibblock">
Nesrine Bannour, Sahar Ghannay, AurÃ©lie NÃ©vÃ©ol, and Anne-Laure
Ligozat.

</span>
<span class="ltx_bibblock">Evaluating the carbon footprint of NLP methods: a survey and
analysis of existing tools.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the Second Workshop on Simple and Efficient
Natural Language Processing</em>, pages 11â€“21, Virtual, November 2021.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/2021.sustainlp-1.2" title="">10.18653/v1/2021.sustainlp-1.2</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.sustainlp-1.2" title="">https://aclanthology.org/2021.sustainlp-1.2</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bawden and Yvon (2023)</span>
<span class="ltx_bibblock">
Rachel Bawden and FranÃ§ois Yvon.

</span>
<span class="ltx_bibblock">Investigating the translation performance of a large multilingual
language model: the case of BLOOM.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">CoRR</em>, abs/2303.01911, 2023.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.48550/arXiv.2303.01911" title="">10.48550/arXiv.2303.01911</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2303.01911" title="">https://doi.org/10.48550/arXiv.2303.01911</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bawden etÂ al. (2020)</span>
<span class="ltx_bibblock">
Rachel Bawden, Eric Bilinski, Thomas Lavergne, and Sophie Rosset.

</span>
<span class="ltx_bibblock">DiaBLa: A Corpus of Bilingual Spontaneous Written Dialogues for
Machine Translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Language Resources and Evaluation</em>, pages 635â€“660, 2020.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1007/s10579-020-09514-4" title="">10.1007/s10579-020-09514-4</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s10579-020-09514-4" title="">https://doi.org/10.1007/s10579-020-09514-4</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belinkov (2022)</span>
<span class="ltx_bibblock">
Yonatan Belinkov.

</span>
<span class="ltx_bibblock">Probing classifiers: Promises, shortcomings, and advances.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Computational Linguistics</em>, 48(1):207â€“219,
March 2022.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1162/coli_a_00422" title="">10.1162/coliË™aË™00422</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.cl-1.7" title="">https://aclanthology.org/2022.cl-1.7</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belinkov and Glass (2019)</span>
<span class="ltx_bibblock">
Yonatan Belinkov and James Glass.

</span>
<span class="ltx_bibblock">Analysis methods in neural language processing: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Transactions of the Association for Computational Linguistics</em>,
7:49â€“72, March 2019.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1162/tacl_a_00254" title="">10.1162/taclË™aË™00254</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.aclweb.org/anthology/Q19-1004" title="">https://www.aclweb.org/anthology/Q19-1004</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belinkov etÂ al. (2017)</span>
<span class="ltx_bibblock">
Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James Glass.

</span>
<span class="ltx_bibblock">What do neural machine translation models learn about morphology?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 861â€“872,
Vancouver, Canada, July 2017. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/P17-1080" title="">10.18653/v1/P17-1080</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.aclweb.org/anthology/P17-1080" title="">https://www.aclweb.org/anthology/P17-1080</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bender etÂ al. (2021)</span>
<span class="ltx_bibblock">
EmilyÂ M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret
Shmitchell.

</span>
<span class="ltx_bibblock">On the dangers of stochastic parrots: Can language models be too big?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 2021 ACM Conference on Fairness,
Accountability, and Transparency</em>, pages 610â€“623, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bengio etÂ al. (2000)</span>
<span class="ltx_bibblock">
Yoshua Bengio, RÃ©jean Ducharme, and Pascal Vincent.

</span>
<span class="ltx_bibblock">A neural probabilistic language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Advances in Neural Information Processing Systems</em>, 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biderman etÂ al. (2022)</span>
<span class="ltx_bibblock">
Stella Biderman, Kieran Bicheno, and Leo Gao.

</span>
<span class="ltx_bibblock">Datasheet for the pile.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2201.07311</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BigScience Workshop (2022)</span>
<span class="ltx_bibblock">
BigScience Workshop.

</span>
<span class="ltx_bibblock">BLOOM (revision 4ab0472), 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/bigscience/bloom" title="">https://huggingface.co/bigscience/bloom</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Birhane etÂ al. (2021)</span>
<span class="ltx_bibblock">
Abeba Birhane, VinayÂ Uday Prabhu, and Emmanuel Kahembwe.

</span>
<span class="ltx_bibblock">Multimodal datasets: misogyny, pornography, and malignant
stereotypes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">ArXiv</em>, abs/2110.01963, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Birhane etÂ al. (2022)</span>
<span class="ltx_bibblock">
Abeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, and
Michelle Bao.

</span>
<span class="ltx_bibblock">The values encoded in machine learning research.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">2022 ACM Conference on Fairness, Accountability, and
Transparency</em>, FAccT â€™22, page 173â€“184, New York, NY, USA, 2022.
Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450393522.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1145/3531146.3533083" title="">10.1145/3531146.3533083</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3531146.3533083" title="">https://doi.org/10.1145/3531146.3533083</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(21)</span>
<span class="ltx_bibblock">
Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman.

</span>
<span class="ltx_bibblock">Gpt-neo: Large scale autoregressive language modeling with
mesh-tensorflow, march 2021.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">URL https://doi. org/10.5281/zenodo</em>, 5297715.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Black etÂ al. (2022)</span>
<span class="ltx_bibblock">
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence
Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, etÂ al.

</span>
<span class="ltx_bibblock">GPT-NeoX-20B: An open-source autoregressive language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2204.06745</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blodgett etÂ al. (2021)</span>
<span class="ltx_bibblock">
SuÂ Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu, Robert Sim, and Hanna
Wallach.

</span>
<span class="ltx_bibblock">Stereotyping Norwegian salmon: An inventory of pitfalls in fairness
benchmark datasets.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 1004â€“1015,
Online, August 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/2021.acl-long.81" title="">10.18653/v1/2021.acl-long.81</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.acl-long.81" title="">https://aclanthology.org/2021.acl-long.81</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bojar etÂ al. (2014)</span>
<span class="ltx_bibblock">
OndÅ™ej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp
Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, Lucia Specia, and AleÅ¡ Tamchyna.

</span>
<span class="ltx_bibblock">Findings of the 2014 workshop on statistical machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the Ninth Workshop on Statistical Machine
Translation</em>, pages 12â€“58, Baltimore, Maryland, USA, June 2014. Association
for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.3115/v1/W14-3302" title="">10.3115/v1/W14-3302</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/W14-3302" title="">https://aclanthology.org/W14-3302</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brennen (2018)</span>
<span class="ltx_bibblock">
J.Â Scott Brennen.

</span>
<span class="ltx_bibblock">An industry-led debate: how uk media cover artificial intelligence,
2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brennen etÂ al. (2022)</span>
<span class="ltx_bibblock">
JÂ Scott Brennen, PhilipÂ N Howard, and RasmusÂ K Nielsen.

</span>
<span class="ltx_bibblock">What to expect when youâ€™re expecting robots: Futures, expectations,
and pseudo-artificial general intelligence in uk news.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Journalism</em>, 23(1):22â€“38, 2022.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1177/1464884920947535" title="">10.1177/1464884920947535</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1177/1464884920947535" title="">https://doi.org/10.1177/1464884920947535</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown etÂ al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredÂ D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Advances in Neural Information Processing Systems</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caswell etÂ al. (2022)</span>
<span class="ltx_bibblock">
Isaac Caswell, Julia Kreutzer, Lisa Wang, Ahsan Wahab, Daan van Esch,
Nasanbayar Ulzii-Orshikh, AllahseraÂ Auguste Tapo, Nishant Subramani, Artem
Sokolov, Claytone Sikasote, Monang Setyawan, Supheakmungkol Sarin, Sokhar
Samb, BenoÃ®t Sagot, Clara Rivera, AnnetteÂ Rios Gonzales, Isabel
Papadimitriou, Salomey Osei, PedroÂ Ortiz Suarez, Iroro Orife, Kelechi Ogueji,
RubungoÂ Andre Niyongabo, ToanÂ Q. Nguyen, Mathias Muller, AndreÂ Matthias
Muller, ShamsuddeenÂ Hassan Muhammad, NandaÂ Firdausi Muhammad, Ayanda
Mnyakeni, Jamshidbek Mirzakhalov, Tapiwanashe Matangira, Colin Leong, Nze
Lawson, Sneha Kudugunta, Yacine Jernite, M.Â Jenny, Orhan Firat, Bonaventure
F.Â P. Dossou, Sakhile Dlamini, Nisansa deÂ Silva, Sakine cCabuk Balli,
StellaÂ Rose Biderman, Alessia Battisti, Ahmed Baruwa, Ankur Bapna, PallaviÂ N.
Baljekar, IsraelÂ Abebe Azime, Ayodele Awokoya, Duygu Ataman, Orevaoghene
Ahia, Oghenefego Ahia, Sweta Agrawal, and Mofetoluwa Adeyemi.

</span>
<span class="ltx_bibblock">Quality at a glance: An audit of web-crawled multilingual datasets.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Transactions of the Association for Computational Linguistics</em>,
10:50â€“72, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2021)</span>
<span class="ltx_bibblock">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde deÂ Oliveira
Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
Brockman, etÂ al.

</span>
<span class="ltx_bibblock">Evaluating large language models trained on code.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2107.03374</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choshen etÂ al. (2022)</span>
<span class="ltx_bibblock">
Leshem Choshen, Guy Hacohen, Daphna Weinshall, and Omri Abend.

</span>
<span class="ltx_bibblock">The grammar-learning trajectories of neural language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 8281â€“8297,
Dublin, Ireland, May 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/2022.acl-long.568" title="">10.18653/v1/2022.acl-long.568</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.acl-long.568" title="">https://aclanthology.org/2022.acl-long.568</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery etÂ al. (2022)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
Adam Roberts, Paul Barham, HyungÂ Won Chung, Charles Sutton, Sebastian
Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
ParkerÂ Barnes AbhishekÂ Rao, YiÂ Tay, Noam Shazeer, Vinodkumar Prabhakaran,
Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David
Dohan, Shivani Agrawal, Mark Omernick, AndrewÂ M. Dai,
ThanumalayanÂ Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,
Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2204.02311</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung etÂ al. (2022)</span>
<span class="ltx_bibblock">
HyungÂ Won Chung, LeÂ Hou, Shayne Longpre, Barret Zoph, YiÂ Tay, William Fedus,
Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, etÂ al.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2210.11416</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Collobert etÂ al. (2011)</span>
<span class="ltx_bibblock">
Ronan Collobert, Jason Weston, LÃ©on Bottou, Michael Karlen, Koray
Kavukcuoglu, and Pavel Kuksa.

</span>
<span class="ltx_bibblock">Natural language processing (almost) from scratch.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Journal of machine learning research</em>, 12, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau etÂ al. (2018)</span>
<span class="ltx_bibblock">
Alexis Conneau, German Kruszewski, Guillaume Lample, LoÃ¯c Barrault, and
Marco Baroni.

</span>
<span class="ltx_bibblock">What you can cram into a single $&amp;!#* vector: Probing
sentence embeddings for linguistic properties.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 2126â€“2136,
Melbourne, Australia, July 2018. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/P18-1198" title="">10.18653/v1/P18-1198</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P18-1198" title="">https://aclanthology.org/P18-1198</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau etÂ al. (2020)</span>
<span class="ltx_bibblock">
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
Wenzek, Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and
Veselin Stoyanov.

</span>
<span class="ltx_bibblock">Unsupervised cross-lingual representation learning at scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 8440â€“8451, Online, July 2020.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/2020.acl-main.747" title="">10.18653/v1/2020.acl-main.747</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.acl-main.747" title="">https://aclanthology.org/2020.acl-main.747</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Contractor etÂ al. (2022)</span>
<span class="ltx_bibblock">
Danish Contractor, Daniel McDuff, JuliaÂ Katherine Haines, Jenny Lee,
Christopher Hines, Brent Hecht, Nicholas Vincent, and Hanlin Li.

</span>
<span class="ltx_bibblock">Behavioral use licensing for responsible ai.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">2022 ACM Conference on Fairness, Accountability, and
Transparency</em>, FAccT â€™22, page 778â€“788, New York, NY, USA, 2022.
Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450393522.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1145/3531146.3533143" title="">10.1145/3531146.3533143</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3531146.3533143" title="">https://doi.org/10.1145/3531146.3533143</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeÂ Toni etÂ al. (2022)</span>
<span class="ltx_bibblock">
Francesco DeÂ Toni, Christopher Akiki, Javier DeÂ LaÂ Rosa, ClÃ©mentine
Fourrier, Enrique Manjavacas, Stefan Schweter, and Daniel VanÂ Strien.

</span>
<span class="ltx_bibblock">Entities, dates, and languages: Zero-shot on historical texts with
t0.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of BigScience Episode #5 â€“ Workshop on
Challenges &amp; Perspectives in Creating Large Language Models</em>, pages
75â€“83, virtual+Dublin, May 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/2022.bigscience-1.7" title="">10.18653/v1/2022.bigscience-1.7</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.bigscience-1.7" title="">https://aclanthology.org/2022.bigscience-1.7</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers etÂ al. (2022)</span>
<span class="ltx_bibblock">
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">LLM.int8(): 8-bit matrix multiplication for transformers at scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2208.07339</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin etÂ al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">BERT: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Conference of the North American Chapter of the Association
for Computational Linguistics</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dodge etÂ al. (2021)</span>
<span class="ltx_bibblock">
Jesse Dodge, Maarten Sap, Ana MarasoviÄ‡, William Agnew, Gabriel Ilharco,
Dirk Groeneveld, Margaret Mitchell, and Matt Gardner.

</span>
<span class="ltx_bibblock">Documenting large webtext corpora: A case study on the colossal clean
crawled corpus.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Conference on Empirical Methods in Natural Language
Processing</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ettinger etÂ al. (2016)</span>
<span class="ltx_bibblock">
Allyson Ettinger, Ahmed Elgohary, and Philip Resnik.

</span>
<span class="ltx_bibblock">Probing for semantic evidence of composition by means of simple
classification tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the 1st Workshop on Evaluating Vector-Space
Representations for NLP</em>, pages 134â€“139, Berlin, Germany, August 2016.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/W16-2524" title="">10.18653/v1/W16-2524</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.aclweb.org/anthology/W16-2524" title="">https://www.aclweb.org/anthology/W16-2524</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan etÂ al. (2021)</span>
<span class="ltx_bibblock">
Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky,
Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav
Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov,
Michael Auli, and Armand Joulin.

</span>
<span class="ltx_bibblock">Beyond English-Centric multilingual machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Journal of Machine Learning Research</em>, 22(107):1â€“48, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://jmlr.org/papers/v22/20-1307.html" title="">http://jmlr.org/papers/v22/20-1307.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fedus etÂ al. (2022)</span>
<span class="ltx_bibblock">
William Fedus, Barret Zoph, and Noam Shazeer.

</span>
<span class="ltx_bibblock">Switch transformers: Scaling to trillion parameter models with simple
and efficient sparsity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Journal of Machine Learning Research</em>, 23(120):1â€“39, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">FitzGerald etÂ al. (2022)</span>
<span class="ltx_bibblock">
Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mackie, Kay Rottmann,
Ana Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Singh, Swetha
Ranganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tur, and Prem
Natarajan.

</span>
<span class="ltx_bibblock">Massive: A 1m-example multilingual natural language understanding
dataset with 51 typologically-diverse languages, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2204.08582" title="">https://arxiv.org/abs/2204.08582</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fried etÂ al. (2022)</span>
<span class="ltx_bibblock">
Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi,
Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis.

</span>
<span class="ltx_bibblock">Incoder: A generative model for code infilling and synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">arXiv preprint arXiv:2204.05999</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fries etÂ al. (2022a)</span>
<span class="ltx_bibblock">
JasonÂ Alan Fries, Natasha Seelam, Gabriel Altay, Leon Weber, Myungsun Kang,
Debajyoti Datta, Ruisi Su, Samuele Garda, BoÂ Wang, Simon Ott, Matthias
Samwald, and Wojciech Kusa.

</span>
<span class="ltx_bibblock">Dataset debt in biomedical language modeling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Challenges &amp; Perspectives in Creating Large Language
Models</em>, 2022a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=HRfzInfr8Z9" title="">https://openreview.net/forum?id=HRfzInfr8Z9</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fries etÂ al. (2022b)</span>
<span class="ltx_bibblock">
JasonÂ Alan Fries, Leon Weber, Natasha Seelam, Gabriel Altay, Debajyoti Datta,
Samuele Garda, Myungsun Kang, Ruisi Su, Wojciech Kusa, Samuel Cahyawijaya,
Fabio Barth, Simon Ott, Matthias Samwald, Stephen Bach, Stella Biderman,
Mario SÃ¤nger, BoÂ Wang, Alison Callahan, DanielÂ LeÃ³n PeriÃ±Ã¡n,
ThÃ©o Gigant, Patrick Haller, Jenny Chim, JoseÂ David Posada, JohnÂ Michael
Giorgi, KarthikÂ Rangasai Sivaraman, Marc PÃ mies, Marianna Nezhurina,
Robert Martin, Michael Cullan, Moritz Freidank, Nathan Dahlberg, Shubhanshu
Mishra, Shamik Bose, NicholasÂ Michio Broad, Yanis Labrak, ShlokÂ S Deshmukh,
Sid Kiblawi, Ayush Singh, MinhÂ Chien Vu, Trishala Neeraj, Jonas Golde,
AlbertÂ Villanova del Moral, and Benjamin Beilharz.

</span>
<span class="ltx_bibblock">BigBio: A framework for data-centric biomedical natural language
processing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Thirty-sixth Conference on Neural Information Processing
Systems Datasets and Benchmarks Track</em>, 2022b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=8lQDn9zTQlW" title="">https://openreview.net/forum?id=8lQDn9zTQlW</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu etÂ al. (2023)</span>
<span class="ltx_bibblock">
DanielÂ Y Fu, Tri Dao, KhaledÂ Kamal Saab, ArminÂ W Thomas, Atri Rudra, and
Christopher Re.

</span>
<span class="ltx_bibblock">Hungry hungry hippos: Towards language modeling with state space
models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">The Eleventh International Conference on Learning
Representations</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=COZDy0WYGg" title="">https://openreview.net/forum?id=COZDy0WYGg</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gage (1994)</span>
<span class="ltx_bibblock">
Philip Gage.

</span>
<span class="ltx_bibblock">A new algorithm for data compression.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">C Users J.</em>, 12(2):23â€“38, feb 1994.

</span>
<span class="ltx_bibblock">ISSN 0898-9788.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2020)</span>
<span class="ltx_bibblock">
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser,
and Connor Leahy.

</span>
<span class="ltx_bibblock">The pile: An 800gb dataset of diverse text for language modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">arXiv preprint arXiv:2101.00027</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2021)</span>
<span class="ltx_bibblock">
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles
Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang,
and Andy Zou.

</span>
<span class="ltx_bibblock">A framework for few-shot language model evaluation, September 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.5281/zenodo.5371628" title="">https://doi.org/10.5281/zenodo.5371628</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gehrmann etÂ al. (2022a)</span>
<span class="ltx_bibblock">
Sebastian Gehrmann, Abhik Bhattacharjee, Abinaya Mahendiran, Alex Wang,
Alexandros Papangelis, Aman Madaan, Angelina McMillan-Major, Anna Shvets,
Ashish Upadhyay, Bingsheng Yao, Bryan Wilie, Chandra Bhagavatula, Chaobin
You, Craig Thomson, Cristina Garbacea, Dakuo Wang, Daniel Deutsch, Deyi
Xiong, DiÂ Jin, Dimitra Gkatzia, Dragomir Radev, Elizabeth Clark, Esin Durmus,
Faisal Ladhak, Filip Ginter, GentaÂ Indra Winata, Hendrik Strobelt, Hiroaki
Hayashi, Jekaterina Novikova, Jenna Kanerva, Jenny Chim, Jiawei Zhou, Jordan
Clive, Joshua Maynez, JoÃ£o Sedoc, Juraj Juraska, Kaustubh Dhole,
KhyathiÂ Raghavi Chandu, Laura Perez-Beltrachini, Leonardo F.Â R. Ribeiro,
Lewis Tunstall, LiÂ Zhang, Mahima Pushkarna, Mathias Creutz, Michael White,
MihirÂ Sanjay Kale, MoussaÂ Kamal Eddine, Nico Daheim, Nishant Subramani,
Ondrej Dusek, PaulÂ Pu Liang, PawanÂ Sasanka Ammanamanchi, QiÂ Zhu, Ratish
Puduppully, Reno Kriz, Rifat Shahriyar, Ronald Cardenas, Saad Mahamood,
Salomey Osei, Samuel Cahyawijaya, Sanja Å tajner, Sebastien Montella,
Shailza, Shailza Jolly, Simon Mille, Tahmid Hasan, Tianhao Shen, Tosin
Adewumi, Vikas Raunak, Vipul Raheja, Vitaly Nikolaev, Vivian Tsai, Yacine
Jernite, Ying Xu, Yisi Sang, Yixin Liu, and Yufang Hou.

</span>
<span class="ltx_bibblock">Gemv2: Multilingual nlg benchmarking in a single line of code,
2022a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2206.11249" title="">https://arxiv.org/abs/2206.11249</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gehrmann etÂ al. (2022b)</span>
<span class="ltx_bibblock">
Sebastian Gehrmann, Elizabeth Clark, and Thibault Sellam.

</span>
<span class="ltx_bibblock">Repairing the cracked foundation: A survey of obstacles in evaluation
practices for generated text, 2022b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2202.06935" title="">https://arxiv.org/abs/2202.06935</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodman (2001)</span>
<span class="ltx_bibblock">
JoshuaÂ T. Goodman.

</span>
<span class="ltx_bibblock">A bit of progress in language modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Computer Speech &amp; Language</em>, 15(4), 2001.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal etÂ al. (2022)</span>
<span class="ltx_bibblock">
Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek,
DaÂ Ju, Sanjana Krishnan, Marcâ€™Aurelio Ranzato, Francisco GuzmÃ¡n, and
Angela Fan.

</span>
<span class="ltx_bibblock">The Flores-101 evaluation benchmark for low-resource and
multilingual machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Transactions of the Association for Computational Linguistics</em>,
10:522â€“538, 2022.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1162/tacl_a_00474" title="">10.1162/taclË™aË™00474</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.tacl-1.30" title="">https://aclanthology.org/2022.tacl-1.30</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graves (2013)</span>
<span class="ltx_bibblock">
Alex Graves.

</span>
<span class="ltx_bibblock">Generating sequences with recurrent neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">arXiv preprint arXiv:1308.0850</em>, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu etÂ al. (2020)</span>
<span class="ltx_bibblock">
Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher RÃ©.

</span>
<span class="ltx_bibblock">Hippo: Recurrent memory with optimal polynomial projections.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Advances in Neural Information Processing Systems</em>,
33:1474â€“1487, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu etÂ al. (2021)</span>
<span class="ltx_bibblock">
Albert Gu, Karan Goel, and Christopher Re.

</span>
<span class="ltx_bibblock">Efficiently modeling long sequences with structured state spaces.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">International Conference on Learning Representations</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hestness etÂ al. (2017)</span>
<span class="ltx_bibblock">
Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun,
Hassan Kianinejad, MdÂ Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou.

</span>
<span class="ltx_bibblock">Deep learning scaling is predictable, empirically.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">arXiv preprint arXiv:1712.00409</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hewitt and Liang (2019)</span>
<span class="ltx_bibblock">
John Hewitt and Percy Liang.

</span>
<span class="ltx_bibblock">Designing and interpreting probes with control tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pages 2733â€“2743, Hong Kong,
China, November 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/D19-1275" title="">10.18653/v1/D19-1275</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D19-1275" title="">https://aclanthology.org/D19-1275</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann etÂ al. (2022)</span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
Cai, Eliza Rutherford, Diego deÂ LasÂ Casas, LisaÂ Anne Hendricks, Johannes
Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George vanÂ den
Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich
Elsen, JackÂ W. Rae, Oriol Vinyals, and Laurent Sifre.

</span>
<span class="ltx_bibblock">Training compute-optimal large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">arXiv preprint arXiv:2203.15556</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Howard and Ruder (2018)</span>
<span class="ltx_bibblock">
Jeremy Howard and Sebastian Ruder.

</span>
<span class="ltx_bibblock">Universal language model fine-tuning for text classification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Annual Meeting of the Association for Computational
Linguistics</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hupkes etÂ al. (2018)</span>
<span class="ltx_bibblock">
Dieuwke Hupkes, Sara Veldhoen, and Willem Zuidema.

</span>
<span class="ltx_bibblock">Visualisation and â€™diagnostic classifiersâ€™ reveal how recurrent and
recursive neural networks process hierarchical structure.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">Journal of Artificial Intelligence Research</em>, 61:907â€“926, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jernite etÂ al. (2022)</span>
<span class="ltx_bibblock">
Yacine Jernite, Huu Nguyen, Stella Biderman, Anna Rogers, Maraim Masoud,
Valentin Danchev, Samson Tan, AlexandraÂ Sasha Luccioni, Nishant Subramani,
Isaac Johnson, Gerard Dupont, Jesse Dodge, Kyle Lo, Zeerak Talat, Dragomir
Radev, Aaron Gokaslan, Somaieh Nikpoor, Peter Henderson, Rishi Bommasani, and
Margaret Mitchell.

</span>
<span class="ltx_bibblock">Data governance in the age of large-scale data-driven language
technology.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">2022 ACM Conference on Fairness, Accountability, and
Transparency</em>, FAccT â€™22, page 2206â€“2222, New York, NY, USA, 2022.
Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450393522.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1145/3531146.3534637" title="">10.1145/3531146.3534637</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3531146.3534637" title="">https://doi.org/10.1145/3531146.3534637</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson etÂ al. (2022)</span>
<span class="ltx_bibblock">
RebeccaÂ Lynn Johnson, Giada Pistilli, Natalia Menâ€™edez-Gonzâ€™alez, Leslye
DenisseÂ Dias Duran, Enrico Panai, Julija KalpokienÄ—, and DonaldÂ Jay
Bertulfo.

</span>
<span class="ltx_bibblock">The ghost in the machine has an american accent: value conflict in
gpt-3.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">ArXiv</em>, abs/2203.07785, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kalamkar etÂ al. (2019)</span>
<span class="ltx_bibblock">
Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal
Banerjee, Sasikanth Avancha, DharmaÂ Teja Vooturi, Nataraj Jammalamadaka,
Jianyu Huang, Hector Yuen, Jiyan Yang, Jongsoo Park, Alexander Heinecke,
Evangelos Georganas, Sudarshan Srinivasan, Abhisek Kundu, Misha Smelyanskiy,
Bharat Kaul, and Pradeep Dubey.

</span>
<span class="ltx_bibblock">A study of bfloat16 for deep learning training, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan etÂ al. (2020)</span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, Tom Henighan, TomÂ B Brown, Benjamin Chess, Rewon
Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">arXiv preprint arXiv:2001.08361</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim etÂ al. (2021)</span>
<span class="ltx_bibblock">
Boseop Kim, HyoungSeok Kim, Sang-Woo Lee, Gichang Lee, Donghyun Kwak, Jeon
DongÂ Hyeon, Sunghyun Park, Sungju Kim, Seonhoon Kim, Dongpil Seo, Heungsub
Lee, Minyoung Jeong, Sungjae Lee, Minsub Kim, SukÂ Hyun Ko, Seokhun Kim,
Taeyong Park, Jinuk Kim, Soyoung Kang, Na-Hyeon Ryu, KangÂ Min Yoo, Minsuk
Chang, Soobin Suh, Sookyo In, Jinseong Park, Kyungduk Kim, Hiun Kim, Jisu
Jeong, YongÂ Goo Yeo, Donghoon Ham, Dongju Park, MinÂ Young Lee, Jaewook Kang,
Inho Kang, Jung-Woo Ha, Woomyoung Park, and Nako Sung.

</span>
<span class="ltx_bibblock">What changes can large-scale language models bring? intensive study
on HyperCLOVA: Billions-scale korean generative pretrained transformers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Conference on Empirical Methods in Natural Language
Processing</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">KlÃ¶pffer (1997)</span>
<span class="ltx_bibblock">
Walter KlÃ¶pffer.

</span>
<span class="ltx_bibblock">Life cycle assessment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">Environmental Science and Pollution Research</em>, 4(4):223â€“228, 1997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson (2018)</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson.

</span>
<span class="ltx_bibblock">SentencePiece: A simple and language independent subword
tokenizer and detokenizer for neural text processing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations</em>, pages 66â€“71, Brussels,
Belgium, November 2018. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/D18-2012" title="">10.18653/v1/D18-2012</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D18-2012" title="">https://aclanthology.org/D18-2012</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kunchukuttan etÂ al. (2020)</span>
<span class="ltx_bibblock">
Anoop Kunchukuttan, Divyanshu Kakwani, Satish Golla, C.Â GokulN., Avik
Bhattacharyya, MiteshÂ M. Khapra, and Pratyush Kumar.

</span>
<span class="ltx_bibblock">Ai4bharat-indicnlp corpus: Monolingual corpora and word embeddings
for indic languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">ArXiv</em>, abs/2005.00085, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lacoste etÂ al. (2019)</span>
<span class="ltx_bibblock">
Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres.

</span>
<span class="ltx_bibblock">Quantifying the carbon emissions of machine learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">arXiv preprint arXiv:1910.09700</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ladhak etÂ al. (2020)</span>
<span class="ltx_bibblock">
Faisal Ladhak, Esin Durmus, Claire Cardie, and Kathleen McKeown.

</span>
<span class="ltx_bibblock">WikiLingua: A new benchmark dataset for cross-lingual abstractive
summarization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">Findings of the Association for Computational Linguistics:
EMNLP 2020</em>, pages 4034â€“4048, Online, November 2020. Association for
Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/2020.findings-emnlp.360" title="">10.18653/v1/2020.findings-emnlp.360</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.findings-emnlp.360" title="">https://aclanthology.org/2020.findings-emnlp.360</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LaurenÃ§on etÂ al. (2022)</span>
<span class="ltx_bibblock">
Hugo LaurenÃ§on, Lucile Saulnier, Thomas Wang, Christopher Akiki,
AlbertÂ Villanova del Moral, TevenÂ Le Scao, LeandroÂ Von Werra, Chenghao Mou,
EduardoÂ GonzÃ¡lez Ponferrada, Huu Nguyen, JÃ¶rg Frohberg, Mario
Å aÅ¡ko, Quentin Lhoest, Angelina McMillan-Major, GÃ©rard
Dupont, Stella Biderman, Anna Rogers, LoubnaÂ Ben allal, FrancescoÂ De Toni,
Giada Pistilli, Olivier Nguyen, Somaieh Nikpoor, Maraim Masoud, Pierre
Colombo, Javier deÂ la Rosa, Paulo Villegas, Tristan Thrush, Shayne Longpre,
Sebastian Nagel, Leon Weber, ManuelÂ Romero MuÃ±oz, Jian Zhu, DanielÂ Van
Strien, Zaid Alyafeai, Khalid Almubarak, VuÂ Minh Chien, Itziar Gonzalez-Dios,
Aitor Soroa, Kyle Lo, Manan Dey, PedroÂ Ortiz Suarez, Aaron Gokaslan, Shamik
Bose, DavidÂ Ifeoluwa Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas Pai, Jenny
Chim, Violette Lepercq, Suzana Ilic, Margaret Mitchell, Sasha Luccioni, and
Yacine Jernite.

</span>
<span class="ltx_bibblock">The BigScience ROOTS corpus: A 1.6TB composite multilingual
dataset.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">Thirty-sixth Conference on Neural Information Processing
Systems Datasets and Benchmarks Track</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=UoEw6KigkUn" title="">https://openreview.net/forum?id=UoEw6KigkUn</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le Scao etÂ al. (2022)</span>
<span class="ltx_bibblock">
Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman,
MÂ Saiful Bari, Stella Biderman, Hady Elsahar, Niklas Muennighoff, Jason
Phang, Ofir Press, Colin Raffel, Victor Sanh, Sheng Shen, Lintang Sutawika,
Jaesung Tae, ZhengÂ Xin Yong, Julien Launay, and IzÂ Beltagy.

</span>
<span class="ltx_bibblock">What language model to train if you have one million GPU hours?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">Challenges &amp; Perspectives in Creating Large Language
Models</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=rI7BL3fHIZq" title="">https://openreview.net/forum?id=rI7BL3fHIZq</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis etÂ al. (2020)</span>
<span class="ltx_bibblock">
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">BART: Denoising sequence-to-sequence pre-training for natural
language generation, translation, and comprehension.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">Annual Meeting of the Association for Computational
Linguistics</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lhoest etÂ al. (2021)</span>
<span class="ltx_bibblock">
Quentin Lhoest, Albert VillanovaÂ del Moral, Yacine Jernite, Abhishek Thakur,
Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu,
Lewis Tunstall, Joe Davison, Mario Å aÅ¡ko, Gunjan Chhablani,
Bhavitvya Malik, Simon Brandeis, Teven LeÂ Scao, Victor Sanh, Canwen Xu,
Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger,
ClÃ©ment Delangue, ThÃ©o MatussiÃ¨re, Lysandre Debut, Stas Bekman,
Pierric Cistac, Thibault Goehringer, Victor Mustar, FranÃ§ois Lagunas,
Alexander Rush, and Thomas Wolf.

</span>
<span class="ltx_bibblock">Datasets: A community library for natural language processing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations</em>, pages 175â€“184, Online
and Punta Cana, Dominican Republic, November 2021. Association for
Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/2021.emnlp-demo.21" title="">10.18653/v1/2021.emnlp-demo.21</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.emnlp-demo.21" title="">https://aclanthology.org/2021.emnlp-demo.21</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2022)</span>
<span class="ltx_bibblock">
Yujia Li, DavidÂ H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,
RÃ©mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, AgustinÂ Dal
Lago, Thomas Hubert, Peter Choy, Cyprien deÂ MassonÂ dâ€™Autume, Igor Babuschkin,
Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov,
James Molloy, DanielÂ J. Mankowitz, EsmeÂ Sutherland Robson, Pushmeet Kohli,
Nando deÂ Freitas, Koray Kavukcuoglu, and Oriol Vinyals.

</span>
<span class="ltx_bibblock">Competition-level code generation with AlphaCode.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">CoRR</em>, abs/2203.07814, 2022.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.48550/arXiv.2203.07814" title="">10.48550/arXiv.2203.07814</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2203.07814" title="">https://doi.org/10.48550/arXiv.2203.07814</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,
Benjamin Newman, Binhang Yuan, Bobby Yan, CeÂ Zhang, Christian Cosgrove,
ChristopherÂ D. Manning, Christopher RÃ©, Diana Acosta-Navas, DrewÂ A. Hudson,
Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu
Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul,
Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter
Henderson, Qian Huang, Ryan Chi, SangÂ Michael Xie, Shibani Santurkar, Surya
Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary,
William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda.

</span>
<span class="ltx_bibblock">Holistic evaluation of language models, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2211.09110" title="">https://arxiv.org/abs/2211.09110</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)</span>
<span class="ltx_bibblock">
Chin-Yew Lin.

</span>
<span class="ltx_bibblock">ROUGE: A package for automatic evaluation of summaries.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">Text Summarization Branches Out</em>, pages 74â€“81, Barcelona,
Spain, July 2004. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/W04-1013" title="">https://aclanthology.org/W04-1013</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al. (2021)</span>
<span class="ltx_bibblock">
XiÂ Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen,
Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth
Pasunuru, Sam Shleifer, PunitÂ Singh Koura, Vishrav Chaudhary, Brian Oâ€™Horo,
Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov,
and Xian Li.

</span>
<span class="ltx_bibblock">Few-shot learning with multilingual language models, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2112.10668" title="">https://arxiv.org/abs/2112.10668</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.

</span>
<span class="ltx_bibblock">RoBERTa: A robustly optimized BERT pretraining approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">arXiv preprint arXiv:1907.11692</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lo etÂ al. (2020)</span>
<span class="ltx_bibblock">
Kyle Lo, LucyÂ Lu Wang, Mark Neumann, RodneyÂ Michael Kinney, and DanielÂ S. Weld.

</span>
<span class="ltx_bibblock">S2ORC: The semantic scholar open research corpus.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">ACL</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2016)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter.

</span>
<span class="ltx_bibblock">SGDR: stochastic gradient descent with restarts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">CoRR</em>, abs/1608.03983, 2016.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1608.03983" title="">http://arxiv.org/abs/1608.03983</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luccioni etÂ al. (2022)</span>
<span class="ltx_bibblock">
AlexandraÂ Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat.

</span>
<span class="ltx_bibblock">Estimating the Carbon Footprint of BLOOM, a 176B Parameter
Language Model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">arXiv preprint arXiv:2211.02001</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madabushi etÂ al. (2022)</span>
<span class="ltx_bibblock">
HarishÂ Tayyar Madabushi, Edward Gow-Smith, Marcos Garcia, Carolina Scarton,
Marco Idiart, and Aline Villavicencio.

</span>
<span class="ltx_bibblock">Semeval-2022 task 2: Multilingual idiomaticity detection and sentence
embedding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">arXiv preprint arXiv:2204.10050</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mann and Whitney (1947)</span>
<span class="ltx_bibblock">
HÂ Mann and DÂ Whitney.

</span>
<span class="ltx_bibblock">Controlling the false discovery rate: A practical and powerful
approach to multiple testing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">Ann. Math. Stat</em>, 18(1):50â€“60, 1947.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martin etÂ al. (2020)</span>
<span class="ltx_bibblock">
Louis Martin, Benjamin Muller, PedroÂ Javier OrtizÂ SuÃ¡rez, Yoann Dupont,
Laurent Romary, Ã‰ric deÂ la Clergerie, DjamÃ© Seddah, and BenoÃ®t
Sagot.

</span>
<span class="ltx_bibblock">CamemBERT: a tasty French language model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 7203â€“7219, Online, July 2020.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.aclweb.org/anthology/2020.acl-main.645" title="">https://www.aclweb.org/anthology/2020.acl-main.645</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMillan-Major etÂ al. (2022)</span>
<span class="ltx_bibblock">
Angelina McMillan-Major, Zaid Alyafeai, Stella Biderman, Kimbo Chen, Francesco
DeÂ Toni, GÃ©rard Dupont, Hady Elsahar, Chris Emezue, AlhamÂ Fikri Aji, Suzana
IliÄ‡, Nurulaqilla Khamis, Colin Leong, Maraim Masoud, Aitor Soroa,
PedroÂ Ortiz Suarez, Zeerak Talat, Daniel van Strien, and Yacine Jernite.

</span>
<span class="ltx_bibblock">Documenting geographically and contextually diverse data sources: The
bigscience catalogue of language data and resources, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2201.10066" title="">https://arxiv.org/abs/2201.10066</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Micikevicius etÂ al. (2018)</span>
<span class="ltx_bibblock">
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen,
David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
Venkatesh, and Hao Wu.

</span>
<span class="ltx_bibblock">Mixed precision training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">International Conference on Learning Representations</em>, 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=r1gs9JgRZ" title="">https://openreview.net/forum?id=r1gs9JgRZ</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mielke etÂ al. (2021)</span>
<span class="ltx_bibblock">
SabrinaÂ J. Mielke, Zaid Alyafeai, Elizabeth Salesky, Colin Raffel, Manan Dey,
Matthias GallÃ©, Arun Raja, Chenglei Si, WilsonÂ Y. Lee, BenoÃ®t Sagot, and
Samson Tan.

</span>
<span class="ltx_bibblock">Between words and characters: A brief history of open-vocabulary
modeling and tokenization in nlp, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2112.10508" title="">https://arxiv.org/abs/2112.10508</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miikkulainen and Dyer (1991)</span>
<span class="ltx_bibblock">
Risto Miikkulainen and MichaelÂ G. Dyer.

</span>
<span class="ltx_bibblock">Natural language processing with modular pdp networks and distributed
lexicon.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">Cognitive Science</em>, 15(3), 1991.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mikolov etÂ al. (2010)</span>
<span class="ltx_bibblock">
Tomas Mikolov, Martin KarafiÃ¡t, Lukas Burget, Jan Cernocká»³, and Sanjeev
Khudanpur.

</span>
<span class="ltx_bibblock">Recurrent neural network based language model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">Interspeech</em>, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mikolov etÂ al. (2013)</span>
<span class="ltx_bibblock">
Tomas Mikolov, Ilya Sutskever, Kai Chen, GregÂ S. Corrado, and Jeff Dean.

</span>
<span class="ltx_bibblock">Distributed representations of words and phrases and their
compositionality.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">Advances in neural information processing systems</em>, 26, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mitchell etÂ al. (2019)</span>
<span class="ltx_bibblock">
Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman,
Ben Hutchinson, Elena Spitzer, InioluwaÂ Deborah Raji, and Timnit Gebru.

</span>
<span class="ltx_bibblock">Model cards for model reporting.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">Proceedings of the Conference on Fairness, Accountability,
and Transparency</em>, FAT* â€™19, page 220â€“229, New York, NY, USA, 2019.
Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450361255.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1145/3287560.3287596" title="">10.1145/3287560.3287596</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3287560.3287596" title="">https://doi.org/10.1145/3287560.3287596</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moi etÂ al. (2019)</span>
<span class="ltx_bibblock">
Anthony Moi, Pierric Cistac, Nicolas Patry, EvanÂ P. Walsh, Funtowicz Morgan,
Sebastian PÃ¼tz, Thomas Wolf, Sylvain Gugger, ClÃ©ment Delangue, Julien
Chaumond, Lysandre Debut, and Patrick von Platen.

</span>
<span class="ltx_bibblock">Hugging face tokenizers library.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/huggingface/tokenizers" title="">https://github.com/huggingface/tokenizers</a>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mostafazadeh etÂ al. (2017)</span>
<span class="ltx_bibblock">
Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James
Allen.

</span>
<span class="ltx_bibblock">Lsdsem 2017 shared task: The story cloze test.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">Proceedings of the 2nd Workshop on Linking Models of
Lexical, Sentential and Discourse-level Semantics</em>, pages 46â€“51, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff (2022)</span>
<span class="ltx_bibblock">
Niklas Muennighoff.

</span>
<span class="ltx_bibblock">SGPT: GPT sentence embeddings for semantic search.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">arXiv preprint arXiv:2202.08904</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff etÂ al. (2022a)</span>
<span class="ltx_bibblock">
Niklas Muennighoff, Nouamane Tazi, LoÃ¯c Magne, and Nils Reimers.

</span>
<span class="ltx_bibblock">MTEB: Massive text embedding benchmark.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib99.1.1">arXiv preprint arXiv:2210.07316</em>, 2022a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff etÂ al. (2022b)</span>
<span class="ltx_bibblock">
Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella
Biderman, TevenÂ Le Scao, MÂ Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey
Schoelkopf, etÂ al.

</span>
<span class="ltx_bibblock">Crosslingual generalization through multitask finetuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">arXiv preprint arXiv:2211.01786</em>, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nangia etÂ al. (2020)</span>
<span class="ltx_bibblock">
Nikita Nangia, Clara Vania, Rasika Bhalerao, and SamuelÂ R. Bowman.

</span>
<span class="ltx_bibblock">CrowS-pairs: A challenge dataset for measuring social biases in
masked language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 1953â€“1967, Online, November
2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/2020.emnlp-main.154" title="">10.18653/v1/2020.emnlp-main.154</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.emnlp-main.154" title="">https://aclanthology.org/2020.emnlp-main.154</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Narang etÂ al. (2021)</span>
<span class="ltx_bibblock">
Sharan Narang, HyungÂ Won Chung, YiÂ Tay, William Fedus, Thibault Fevry, Michael
Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, Yanqi
Zhou, Wei Li, Nan Ding, Jake Marcus, Adam Roberts, and Colin Raffel.

</span>
<span class="ltx_bibblock">Do transformer modifications transfer across implementations and
applications?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib102.1.1">Conference on Empirical Methods in Natural Language
Processing</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Narayanan etÂ al. (2021)</span>
<span class="ltx_bibblock">
Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa
Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie
Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia.

</span>
<span class="ltx_bibblock">Efficient Large-Scale Language Model Training on GPU Clusters using
Megatron-LM.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib103.1.1">Proceedings of the International Conference for High
Performance Computing, Networking, Storage and Analysis</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nekoto etÂ al. (2020)</span>
<span class="ltx_bibblock">
Wilhelmina Nekoto, Vukosi Marivate, Tshinondiwa Matsila, TimiÂ E. Fasubaa,
TÂ Kolawole, TaiwoÂ Helen Fagbohungbe, SolomonÂ Oluwole Akinola,
ShamsuddeenÂ Hassan Muhammad, SalomonÂ Kabongo Kabenamualu, Salomey Osei,
Sackey Freshia, RubungoÂ Andre Niyongabo, Ricky Macharm, Perez Ogayo,
Orevaoghene Ahia, Musie Meressa, Mofetoluwa Adeyemi, Masabata
Mokgesi-Selinga, Lawrence Okegbemi, Laura Martinus, Kolawole Tajudeen, Kevin
Degila, Kelechi Ogueji, Kathleen Siminyu, Julia Kreutzer, Jason Webster,
JamiilÂ Toure Ali, JadeÂ Z. Abbott, Iroro Orife, IgnatiusÂ U. Ezeani,
IdrisÂ Abdulkabir Dangana, Herman Kamper, Hady ElSahar, Goodness Duru, Ghollah
Kioko, Espoir Murhabazi, ElanÂ Van Biljon, Daniel Whitenack, Christopher
Onyefuluchi, ChrisÂ C. Emezue, Bonaventure F.Â P. Dossou, BlessingÂ K. Sibanda,
BlessingÂ Itoro Bassey, Ayodele Olabiyi, Arshath Ramkilowan, Alp Oktem,
Adewale Akinfaderin, and AbdallahÂ M. Bashir.

</span>
<span class="ltx_bibblock">Participatory research for low-resourced machine translation: A case
study in African languages.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib104.1.1">ACL Findings</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NÃ©vÃ©ol etÂ al. (2022)</span>
<span class="ltx_bibblock">
AurÃ©lie NÃ©vÃ©ol, Yoann Dupont, Julien BezanÃ§on, and KarÃ«n
Fort.

</span>
<span class="ltx_bibblock">French CrowS-pairs: Extending a challenge dataset for measuring
social bias in masked language models to a language other than English.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 8521â€“8531,
Dublin, Ireland, May 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/2022.acl-long.583" title="">10.18653/v1/2022.acl-long.583</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.acl-long.583" title="">https://aclanthology.org/2022.acl-long.583</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nivre etÂ al. (2016)</span>
<span class="ltx_bibblock">
Joakim Nivre, Marie-Catherine deÂ Marneffe, Filip Ginter, Yoav Goldberg, Jan
HajiÄ, ChristopherÂ D. Manning, Ryan McDonald, Slav Petrov, Sampo
Pyysalo, Natalia Silveira, Reut Tsarfaty, and Daniel Zeman.

</span>
<span class="ltx_bibblock">Universal Dependencies v1: A multilingual treebank collection.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">Proceedings of the Tenth International Conference on
Language Resources and Evaluation (LRECâ€™16)</em>, pages 1659â€“1666,
PortoroÅ¾, Slovenia, May 2016. European Language Resources Association
(ELRA).

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/L16-1262" title="">https://aclanthology.org/L16-1262</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nivre etÂ al. (2017)</span>
<span class="ltx_bibblock">
Joakim Nivre, Daniel Zeman, Filip Ginter, and Francis Tyers.

</span>
<span class="ltx_bibblock">Universal Dependencies.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib107.1.1">Proceedings of the 15th Conference of the European Chapter
of the Association for Computational Linguistics: Tutorial Abstracts</em>,
Valencia, Spain, April 2017. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/E17-5001" title="">https://aclanthology.org/E17-5001</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ortiz SuÃ¡rez etÂ al. (2019)</span>
<span class="ltx_bibblock">
PedroÂ Javier Ortiz SuÃ¡rez, BenoÃ®t Sagot, and Laurent Romary.

</span>
<span class="ltx_bibblock">Asynchronous pipelines for processing huge corpora on medium to low
resource infrastructures.

</span>
<span class="ltx_bibblock">In Piotr BaÅ„ski, Adrien Barbaresi, Hanno Biber, Evelyn Breiteneder,
Simon Clematide, Marc Kupietz, Harald LÃ¼ngen, and Caroline Iliadi,
editors, <em class="ltx_emph ltx_font_italic" id="bib.bib108.1.1">Proceedings of the Workshop on Challenges in the Management of
Large Corpora (CMLC-7)</em>, pages 9 â€“ 16, Cardiff, UK, 2019. Leibniz-Institut
fÃ¼r Deutsche Sprache.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.14618/ids-pub-9021" title="">10.14618/ids-pub-9021</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215" title="">http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni etÂ al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.

</span>
<span class="ltx_bibblock">BLEU: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib109.1.1">Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics</em>, pages 311â€“318, Philadelphia, Pennsylvania,
USA, July 2002. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.3115/1073083.1073135" title="">10.3115/1073083.1073135</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P02-1040" title="">https://aclanthology.org/P02-1040</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patterson etÂ al. (2021)</span>
<span class="ltx_bibblock">
David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia,
Daniel Rothchild, David So, Maud Texier, and Jeff Dean.

</span>
<span class="ltx_bibblock">Carbon emissions and large neural network training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib110.1.1">arXiv preprint arXiv:2104.10350</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pearson (1895)</span>
<span class="ltx_bibblock">
Karl Pearson.

</span>
<span class="ltx_bibblock">Note on regression and inheritance in the case of two parents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib111.1.1">Proceedings of the Royal Society of London</em>, 58(347-352):240â€“242, 1895.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peters etÂ al. (2018)</span>
<span class="ltx_bibblock">
MatthewÂ E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,
Kenton Lee, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Deep contextualized word representations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib112.1.1">Conference of the North American Chapter of the Association
for Computational Linguistics</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Phang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Jason Phang, Herbie Bradley, Leo Gao, LouisÂ J Castricato, and Stella Biderman.

</span>
<span class="ltx_bibblock">EleutherAI: going beyond â€open scienceâ€ to â€science in the openâ€.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib113.1.1">Workshop on Broadening Research Collaborations</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post (2018)</span>
<span class="ltx_bibblock">
Matt Post.

</span>
<span class="ltx_bibblock">A call for clarity in reporting BLEU scores.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib114.1.1">Proceedings of the Third Conference on Machine Translation:
Research Papers</em>, pages 186â€“191, Brussels, Belgium, October 2018.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/W18-6319" title="">10.18653/v1/W18-6319</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/W18-6319" title="">https://aclanthology.org/W18-6319</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Press etÂ al. (2021)</span>
<span class="ltx_bibblock">
Ofir Press, Noah Smith, and Mike Lewis.

</span>
<span class="ltx_bibblock">Train short, test long: Attention with linear biases enables input
length extrapolation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib115.1.1">International Conference on Learning Representations</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2018)</span>
<span class="ltx_bibblock">
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Improving language understanding by generative pre-training, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae etÂ al. (2021)</span>
<span class="ltx_bibblock">
JackÂ W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
etÂ al.

</span>
<span class="ltx_bibblock">Scaling language models: Methods, analysis &amp; insights from training
gopher.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib118.1.1">arXiv preprint arXiv:2112.11446</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel etÂ al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, PeterÂ J Liu, etÂ al.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text
transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib119.1.1">J. Mach. Learn. Res.</em>, 21(140):1â€“67, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajbhandari etÂ al. (2020)</span>
<span class="ltx_bibblock">
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.

</span>
<span class="ltx_bibblock">ZeRO: Memory optimizations toward training trillion parameter
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib120.1.1">SC20: International Conference for High Performance Computing,
Networking, Storage and Analysis</em>, Nov 2020.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1109/sc41405.2020.00024" title="">10.1109/sc41405.2020.00024</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1109/SC41405.2020.00024" title="">http://dx.doi.org/10.1109/SC41405.2020.00024</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raji etÂ al. (2021)</span>
<span class="ltx_bibblock">
Deborah Raji, Emily Denton, EmilyÂ M. Bender, Alex Hanna, and Amandalynne
Paullada.

</span>
<span class="ltx_bibblock">Ai and the everything in the whole wide world benchmark.

</span>
<span class="ltx_bibblock">In J.Â Vanschoren and S.Â Yeung, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib121.1.1">Proceedings of the
Neural Information Processing Systems Track on Datasets and Benchmarks</em>,
volumeÂ 1, 2021.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/084b6fbb10729ed4da8c3d3f5a3ae7c9-Paper-round2.pdf" title="">https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/084b6fbb10729ed4da8c3d3f5a3ae7c9-Paper-round2.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raji etÂ al. (2022)</span>
<span class="ltx_bibblock">
InioluwaÂ Deborah Raji, I.Â Elizabeth Kumar, Aaron Horowitz, and Andrew Selbst.

</span>
<span class="ltx_bibblock">The fallacy of AI functionality.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib122.1.1">2022 ACM Conference on Fairness, Accountability, and
Transparency</em>, FAccT â€™22, page 959â€“972, New York, NY, USA, 2022.
Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450393522.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1145/3531146.3533158" title="">10.1145/3531146.3533158</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3531146.3533158" title="">https://doi.org/10.1145/3531146.3533158</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rasley etÂ al. (2020)</span>
<span class="ltx_bibblock">
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He.

</span>
<span class="ltx_bibblock">DeepSpeed: System optimizations enable training deep learning
models with over 100 billion parameters.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib123.1.1">Proceedings of the 26th ACM SIGKDD International Conference
on Knowledge Discovery &amp; Data Mining</em>, KDD â€™20, page 3505â€“3506, New York,
NY, USA, 2020. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450379984.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1145/3394486.3406703" title="">10.1145/3394486.3406703</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3394486.3406703" title="">https://doi.org/10.1145/3394486.3406703</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rust etÂ al. (2021)</span>
<span class="ltx_bibblock">
Phillip Rust, Jonas Pfeiffer, Ivan VuliÄ‡, Sebastian Ruder, and Iryna
Gurevych.

</span>
<span class="ltx_bibblock">How good is your tokenizer? on the monolingual performance of
multilingual language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib124.1.1">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 3118â€“3135,
Online, August 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/2021.acl-long.243" title="">10.18653/v1/2021.acl-long.243</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.acl-long.243" title="">https://aclanthology.org/2021.acl-long.243</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Safaya etÂ al. (2020)</span>
<span class="ltx_bibblock">
Ali Safaya, Moutasem Abdullatif, and Deniz Yuret.

</span>
<span class="ltx_bibblock">KUISAIL at SemEval-2020 task 12: BERT-CNN for offensive
speech identification in social media.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib125.1.1">Proceedings of the Fourteenth Workshop on Semantic
Evaluation</em>, pages 2054â€“2059, Barcelona (online), December 2020.
International Committee for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.aclweb.org/anthology/2020.semeval-1.271" title="">https://www.aclweb.org/anthology/2020.semeval-1.271</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salton and Yang (1973)</span>
<span class="ltx_bibblock">
Gerard Salton and Chung-Shu Yang.

</span>
<span class="ltx_bibblock">On the specification of term values in automatic indexing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib126.1.1">Journal of documentation</em>, 1973.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sambasivan etÂ al. (2021)</span>
<span class="ltx_bibblock">
Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen
Paritosh, and LoraÂ M Aroyo.

</span>
<span class="ltx_bibblock">â€œeveryone wants to do the model work, not the data workâ€: Data
cascades in high-stakes ai.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib127.1.1">Proceedings of the 2021 CHI Conference on Human Factors in
Computing Systems</em>, CHI â€™21, New York, NY, USA, 2021. Association for
Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450380966.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1145/3411764.3445518" title="">10.1145/3411764.3445518</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3411764.3445518" title="">https://doi.org/10.1145/3411764.3445518</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh etÂ al. (2022)</span>
<span class="ltx_bibblock">
Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid
Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, MÂ Saiful
Bari, Canwen Xu, Urmish Thakker, ShanyaÂ Sharma Sharma, Eliza Szczechla,
Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang,
Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, ZhengÂ Xin Yong,
Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen,
Abheesht Sharma, Andrea Santilli, Thibault Fevry, JasonÂ Alan Fries, Ryan
Teehan, TevenÂ Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and AlexanderÂ M
Rush.

</span>
<span class="ltx_bibblock">Multitask prompted training enables zero-shot task generalization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib128.1.1">International Conference on Learning Representations</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=9Vrb9D0WI4" title="">https://openreview.net/forum?id=9Vrb9D0WI4</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schmidhuber and Heil (1996)</span>
<span class="ltx_bibblock">
JÃ¼rgen Schmidhuber and Stefan Heil.

</span>
<span class="ltx_bibblock">Sequential neural text compression.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib129.1.1">IEEE Transactions on Neural Networks</em>, 7(1), 1996.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwartz etÂ al. (2020)</span>
<span class="ltx_bibblock">
Roy Schwartz, Jesse Dodge, NoahÂ A. Smith, and Oren Etzioni.

</span>
<span class="ltx_bibblock">Green ai.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib130.1.1">Communications of the ACM</em>, 63(12), 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Serikov etÂ al. (2022)</span>
<span class="ltx_bibblock">
Oleg Serikov, Vitaly Protasov, Ekaterina Voloshina, Viktoria Knyazkova, and
Tatiana Shavrina.

</span>
<span class="ltx_bibblock">Universal and independent: Multilingual probing framework for
exhaustive model interpretation and evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib131.1.1">arXiv preprint arXiv:2210.13236</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shannon (1948)</span>
<span class="ltx_bibblock">
ClaudeÂ Elwood Shannon.

</span>
<span class="ltx_bibblock">A mathematical theory of communication.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib132.1.1">The Bell system technical journal</em>, 27(3), 1948.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer (2020)</span>
<span class="ltx_bibblock">
Noam Shazeer.

</span>
<span class="ltx_bibblock">GLU variants improve transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib133.1.1">arXiv preprint arXiv:2002.05202</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer etÂ al. (2017)</span>
<span class="ltx_bibblock">
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,
Geoffrey Hinton, and Jeff Dean.

</span>
<span class="ltx_bibblock">Outrageously large neural networks: The sparsely-gated
mixture-of-experts layer.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib134.1.1">International Conference on Learning Representations</em>, 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=B1ckMDqlg" title="">https://openreview.net/forum?id=B1ckMDqlg</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shliazhko etÂ al. (2022)</span>
<span class="ltx_bibblock">
Oleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov,
Anastasia Kozlova, and Tatiana Shavrina.

</span>
<span class="ltx_bibblock">mgpt: Few-shot learners go multilingual.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib135.1.1">arXiv preprint arXiv:2204.07580</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shoeybi etÂ al. (2019)</span>
<span class="ltx_bibblock">
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Megatron-LM: Training multi-billion parameter language models using
model parallelism.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib136.1.1">arXiv preprint arXiv:1909.08053</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simoulin and CrabbÃ© (2021)</span>
<span class="ltx_bibblock">
Antoine Simoulin and Benoit CrabbÃ©.

</span>
<span class="ltx_bibblock">Un modÃ¨le Transformer GÃ©nÃ©ratif PrÃ©-entrainÃ©
pour le ______ franÃ§ais.

</span>
<span class="ltx_bibblock">In Pascal Denis, Natalia Grabar, Amel Fraisse, RÃ©mi Cardon,
Bernard Jacquemin, Eric Kergosien, and Antonio Balvet, editors,
<em class="ltx_emph ltx_font_italic" id="bib.bib137.1.1">Traitement Automatique des Langues Naturelles</em>, pages 246â€“255,
Lille, France, 2021. ATALA.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://hal.archives-ouvertes.fr/hal-03265900" title="">https://hal.archives-ouvertes.fr/hal-03265900</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith etÂ al. (2022)</span>
<span class="ltx_bibblock">
Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam
Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas,
Vijay Korthikanti, Elton Zhang, Rewon Child, RezaÂ Yazdani Aminabadi, Julie
Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh
Tiwary, and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Using DeepSpeed and Megatron to train Megatron-Turing NLG 530B,
a large-scale generative language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib138.1.1">arXiv preprint arXiv:2201.11990</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Soltan etÂ al. (2022)</span>
<span class="ltx_bibblock">
Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael
Hamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna
Rumshisky, ChandanaÂ Satya Prakash, Mukund Sridhar, Fabian Triefenbach, Apurv
Verma, Gokhan Tur, and Prem Natarajan.

</span>
<span class="ltx_bibblock">Alexatm 20b: Few-shot learning using a large-scale multilingual
seq2seq model, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2208.01448" title="">https://arxiv.org/abs/2208.01448</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava etÂ al. (2022)</span>
<span class="ltx_bibblock">
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu AwalÂ Md Shoeb, Abubakar
Abid, Adam Fisch, AdamÂ R Brown, Adam Santoro, Aditya Gupta, AdriÃ 
Garriga-Alonso, etÂ al.

</span>
<span class="ltx_bibblock">Beyond the imitation game: Quantifying and extrapolating the
capabilities of language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib140.1.1">arXiv preprint arXiv:2206.04615</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Strubell etÂ al. (2019)</span>
<span class="ltx_bibblock">
Emma Strubell, Ananya Ganesh, and Andrew McCallum.

</span>
<span class="ltx_bibblock">Energy and policy considerations for deep learning in nlp.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib141.1.1">Annual Meeting of the Association for Computational
Linguistics</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su etÂ al. (2021)</span>
<span class="ltx_bibblock">
Jianlin Su, YuÂ Lu, Shengfeng Pan, BoÂ Wen, and Yunfeng Liu.

</span>
<span class="ltx_bibblock">RoFormer: Enhanced transformer with rotary position embedding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib142.1.1">arXiv preprint arXiv:2104.09864</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sutskever etÂ al. (2011)</span>
<span class="ltx_bibblock">
Ilya Sutskever, James Martens, and GeoffreyÂ E. Hinton.

</span>
<span class="ltx_bibblock">Generating text with recurrent neural networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib143.1.1">International Conference on Machine Learning</em>, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Talat etÂ al. (2022)</span>
<span class="ltx_bibblock">
Zeerak Talat, AurÃ©lie NÃ©vÃ©ol, Stella Biderman, Miruna Clinciu,
Manan Dey, Shayne Longpre, Sasha Luccioni, Maraim Masoud, Margaret Mitchell,
Dragomir Radev, Shanya Sharma, Arjun Subramonian, Jaesung Tae, Samson Tan,
Deepak Tunuguntla, and Oskar vanÂ der Wal.

</span>
<span class="ltx_bibblock">You reap what you sow: On the challenges of bias evaluation under
multilingual settings.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib144.1.1">Challenges &amp; Perspectives in Creating Large Language
Models</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=rK-7NhfSIW5" title="">https://openreview.net/forum?id=rK-7NhfSIW5</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay etÂ al. (2022)</span>
<span class="ltx_bibblock">
YiÂ Tay, Jason Wei, HyungÂ Won Chung, VinhÂ Q Tran, DavidÂ R So, Siamak Shakeri,
Xavier Garcia, HuaixiuÂ Steven Zheng, Jinfeng Rao, Aakanksha Chowdhery, etÂ al.

</span>
<span class="ltx_bibblock">Transcending scaling laws with 0.1% extra compute.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib145.1.1">arXiv preprint arXiv:2210.11399</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teehan etÂ al. (2022)</span>
<span class="ltx_bibblock">
Ryan Teehan, Miruna Clinciu, Oleg Serikov, Eliza Szczechla, Natasha Seelam,
Shachar Mirkin, and Aaron Gokaslan.

</span>
<span class="ltx_bibblock">Emergent structures and training dynamics in large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib146.1.1">Proceedings of BigScience Episode #5 â€“ Workshop on
Challenges &amp; Perspectives in Creating Large Language Models</em>, pages
146â€“159, virtual+Dublin, May 2022. Association for Computational
Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/2022.bigscience-1.11" title="">10.18653/v1/2022.bigscience-1.11</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.bigscience-1.11" title="">https://aclanthology.org/2022.bigscience-1.11</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tenney etÂ al. (2018)</span>
<span class="ltx_bibblock">
Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, RÂ Thomas McCoy,
Najoung Kim, Benjamin VanÂ Durme, SamuelÂ R Bowman, Dipanjan Das, etÂ al.

</span>
<span class="ltx_bibblock">What do you learn from context? probing for sentence structure in
contextualized word representations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib147.1.1">International Conference on Learning Representations</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani etÂ al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
AidanÂ N Gomez, Åukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib148.1.1">Advances in neural information processing systems</em>, 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vinyals and Le (2015)</span>
<span class="ltx_bibblock">
Oriol Vinyals and QuocÂ V. Le.

</span>
<span class="ltx_bibblock">A neural conversational model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib149.1.1">arXiv preprint arXiv:1506.05869</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voloshina etÂ al. (2022)</span>
<span class="ltx_bibblock">
Ekaterina Voloshina, Oleg Serikov, and Tatiana Shavrina.

</span>
<span class="ltx_bibblock">Is neural language acquisition similar to natural? a chronological
probing study.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib150.1.1">arXiv preprint arXiv:2207.00560</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2019)</span>
<span class="ltx_bibblock">
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,
Felix Hill, Omer Levy, and Samuel Bowman.

</span>
<span class="ltx_bibblock">Superglue: A stickier benchmark for general-purpose language
understanding systems.

</span>
<span class="ltx_bibblock">In H.Â Wallach, H.Â Larochelle, A.Â Beygelzimer, F.Â d'AlchÃ©-Buc, E.Â Fox, and R.Â Garnett, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib151.1.1">Advances in Neural
Information Processing Systems</em>, volumeÂ 32. Curran Associates, Inc., 2019.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2019/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf" title="">https://proceedings.neurips.cc/paper/2019/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Komatsuzaki (2021)</span>
<span class="ltx_bibblock">
Ben Wang and Aran Komatsuzaki.

</span>
<span class="ltx_bibblock">GPT-J-6B: A 6 billion parameter autoregressive language model,
2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2020)</span>
<span class="ltx_bibblock">
Changhan Wang, Kyunghyun Cho, and Jiatao Gu.

</span>
<span class="ltx_bibblock">Neural machine translation with byte-level subwords.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib153.1.1">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Kanwar (2019)</span>
<span class="ltx_bibblock">
Shibo Wang and Pankaj Kanwar.

</span>
<span class="ltx_bibblock">Bfloat16: The secret to high performance on cloud tpus, 2019.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus" title="">https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2021)</span>
<span class="ltx_bibblock">
Shuohuan Wang, YuÂ Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun
Feng, Junyuan Shang, Yanbin Zhao, Chao Pang, Jiaxiang Liu, Xuyi Chen, Yuxiang
Lu, Weixin Liu, XiÂ Wang, Yangfan Bai, Qiuliang Chen, LiÂ Zhao, Shiyong Li,
Peng Sun, Dianhai Yu, Yanjun Ma, Hao Tian, Hua Wu, Tian Wu, Wei Zeng, GeÂ Li,
Wen Gao, and Haifeng Wang.

</span>
<span class="ltx_bibblock">Ernie 3.0 titan: Exploring larger-scale knowledge enhanced
pre-training for language understanding and generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib155.1.1">arXiv preprint arXiv:2112.12731</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2022a)</span>
<span class="ltx_bibblock">
Thomas Wang, Adam Roberts, Daniel Hesslow, TevenÂ Le Scao, HyungÂ Won Chung,
IzÂ Beltagy, Julien Launay, and Colin Raffel.

</span>
<span class="ltx_bibblock">What language model architecture and pretraining objective works best
for zero-shot generalization?

</span>
<span class="ltx_bibblock">In Kamalika Chaudhuri, Stefanie Jegelka, LeÂ Song, Csaba Szepesvari,
Gang Niu, and Sivan Sabato, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib156.1.1">Proceedings of the 39th
International Conference on Machine Learning</em>, volume 162 of
<em class="ltx_emph ltx_font_italic" id="bib.bib156.2.2">Proceedings of Machine Learning Research</em>, pages 22964â€“22984. PMLR,
17â€“23 Jul 2022a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v162/wang22u.html" title="">https://proceedings.mlr.press/v162/wang22u.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2022b)</span>
<span class="ltx_bibblock">
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza
Mirzaei, Anjana Arunkumar, Arjun Ashok, ArutÂ Selvan Dhanasekaran, Atharva
Naik, David Stap, etÂ al.

</span>
<span class="ltx_bibblock">Benchmarking generalization via in-context instructions on 1,600+
language tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib157.1.1">arXiv preprint arXiv:2204.07705</em>, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2021)</span>
<span class="ltx_bibblock">
Jason Wei, Maarten Bosma, VincentÂ Y Zhao, Kelvin Guu, AdamsÂ Wei Yu, Brian
Lester, Nan Du, AndrewÂ M Dai, and QuocÂ V Le.

</span>
<span class="ltx_bibblock">Finetuned language models are zero-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib158.1.1">arXiv preprint arXiv:2109.01652</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, YiÂ Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, EdÂ H.
Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William
Fedus.

</span>
<span class="ltx_bibblock">Emergent abilities of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib159.1.1">Transactions on Machine Learning Research</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Westra and Lawson (2001)</span>
<span class="ltx_bibblock">
LauraÂ S. Westra and BillÂ E. Lawson.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib160.1.1">Faces of Environmental Racism: Confronting Issues of Global
Justice</em>.

</span>
<span class="ltx_bibblock">Rowman &amp; Littlefield Publishers, 2001.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Winner (1977)</span>
<span class="ltx_bibblock">
Langdon Winner.

</span>
<span class="ltx_bibblock">Technology as master. (book reviews: Autonomous technology.
technics-out-of-control as a theme in political thought).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib161.1.1">Science</em>, 1977.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Winner (2017)</span>
<span class="ltx_bibblock">
Langdon Winner.

</span>
<span class="ltx_bibblock">Do artifacts have politics?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib162.1.1">Computer Ethics</em>, pages 177â€“192. Routledge, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wong etÂ al. (2021)</span>
<span class="ltx_bibblock">
Andrew Wong, Erkin Otles, JohnÂ P. Donnelly, Andrew Krumm, Jeffrey McCullough,
Olivia DeTroyer-Cooley, Justin Pestrue, Marie Phillips, Judy Konye, Carleen
Penoza, Muhammad Ghous, and Karandeep Singh.

</span>
<span class="ltx_bibblock">External Validation of a Widely Implemented Proprietary Sepsis
Prediction Model in Hospitalized Patients.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib163.1.1">JAMA Internal Medicine</em>, 181(8):1065â€“1070,
08 2021.

</span>
<span class="ltx_bibblock">ISSN 2168-6106.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1001/jamainternmed.2021.2626" title="">10.1001/jamainternmed.2021.2626</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1001/jamainternmed.2021.2626" title="">https://doi.org/10.1001/jamainternmed.2021.2626</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al. (2012)</span>
<span class="ltx_bibblock">
Haicheng Wu, Gregory Diamos, Jin Wang, Srihari Cadambi, Sudhakar Yalamanchili,
and Srimat Chakradhar.

</span>
<span class="ltx_bibblock">Optimizing data warehousing applications for GPUs using kernel
fusion/fission.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib164.1.1">2012 IEEE 26th International Parallel and Distributed
Processing Symposium Workshops and PhD Forum</em>, pages 2433â€“2442, 2012.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1109/IPDPSW.2012.300" title="">10.1109/IPDPSW.2012.300</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue etÂ al. (2021)</span>
<span class="ltx_bibblock">
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, and Colin Raffel.

</span>
<span class="ltx_bibblock">mT5: A massively multilingual pre-trained text-to-text transformer.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib165.1.1">Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 483â€“498, Online, June 2021. Association for
Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/2021.naacl-main.41" title="">10.18653/v1/2021.naacl-main.41</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.naacl-main.41" title="">https://aclanthology.org/2021.naacl-main.41</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2019)</span>
<span class="ltx_bibblock">
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov,
and QuocÂ V. Le.

</span>
<span class="ltx_bibblock">XLnet: Generalized autoregressive pretraining for language
understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib166.1.1">Advances in Neural Information Processing Systems</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng etÂ al. (2022)</span>
<span class="ltx_bibblock">
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi
Yang, Yifan Xu, Wendi Zheng, Xiao Xia, etÂ al.

</span>
<span class="ltx_bibblock">Glm-130b: An open bilingual pre-trained model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib167.1.1">arXiv preprint arXiv:2210.02414</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng etÂ al. (2021)</span>
<span class="ltx_bibblock">
Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, YiÂ Liao, Zhiwei Wang, Xin Jiang,
ZhenZhang Yang, Kaisheng Wang, Xiaoda Zhang, Chen Li, Ziyan Gong, Yifan Yao,
Xinjing Huang, Jun Wang, Jianfeng Yu, QiÂ Guo, Yue Yu, Yan Zhang, Jin Wang,
Hengtao Tao, Dasen Yan, Zexuan Yi, Fang Peng, Fangqing Jiang, Han Zhang,
Lingfeng Deng, Yehong Zhang, Zhe Lin, Chao Zhang, Shaojie Zhang, Mingyue Guo,
Shanzhi Gu, Gaojun Fan, Yaowei Wang, Xuefeng Jin, Qun Liu, and Yonghong Tian.

</span>
<span class="ltx_bibblock">PanGu-<math alttext="\alpha" class="ltx_Math" display="inline" id="bib.bib168.1.m1.1"><semantics id="bib.bib168.1.m1.1a"><mi id="bib.bib168.1.m1.1.1" xref="bib.bib168.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="bib.bib168.1.m1.1b"><ci id="bib.bib168.1.m1.1.1.cmml" xref="bib.bib168.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib168.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="bib.bib168.1.m1.1d">italic_Î±</annotation></semantics></math>: Large-scale autoregressive pretrained Chinese
language models with auto-parallel computation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib168.2.1">arXiv preprint arXiv:2104.12369</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
Chen, Christopher Dewan, Mona Diab, Xian Li, XiÂ Victoria Lin, etÂ al.

</span>
<span class="ltx_bibblock">OPT: Open pre-trained transformer language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib169.1.1">arXiv preprint arXiv:2205.01068</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2021)</span>
<span class="ltx_bibblock">
Yian Zhang, Alex Warstadt, Xiaocheng Li, and SamuelÂ R. Bowman.

</span>
<span class="ltx_bibblock">When do you need billions of words of pretraining data?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib170.1.1">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 1112â€“1125,
Online, August 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/2021.acl-long.90" title="">10.18653/v1/2021.acl-long.90</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.acl-long.90" title="">https://aclanthology.org/2021.acl-long.90</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2019)</span>
<span class="ltx_bibblock">
Zhengyan Zhang, XuÂ Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu.

</span>
<span class="ltx_bibblock">ERNIE: Enhanced language representation with informative entities.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib171.1.1">Annual Meeting of the Association for Computational
Linguistics</em>, 2019.

</span>
</li>
</ul>
</section>
<section class="ltx_section" id="S1a">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">A </span>Prompts</h2>
<div class="ltx_para" id="S1a.p1">
<p class="ltx_p" id="S1a.p1.1">The following contains prompts used for evaluation. The prompts are also available in PromptSourceÂ <cite class="ltx_cite ltx_citemacro_citep">(Bach etÂ al., <a class="ltx_ref" href="#bib.bib8" title="">2022</a>)</cite>. A sample with a prompt applied as well as the raw prompts are provided. For raw prompts, double curly brackets are filled with content from the sample when used.</p>
</div>
<span class="ltx_ERROR undefined" id="S1a.1">\localtableofcontents</span>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>SuperGLUE/wsc.fixed</h3>
<section class="ltx_subsubsection" id="S1.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.1 </span>Data example</h4>
<div class="ltx_para" id="S1.SS1.SSS1.p1">
<p class="ltx_p" id="S1.SS1.SSS1.p1.1">Prompt name: <span class="ltx_text ltx_font_bold" id="S1.SS1.SSS1.p1.1.1">GPT-3 Style</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS1.SSS1.p1.2">
Passage: I tried to paint a picture of an orchard, with lemons in the lemon
trees , but they came out looking more like light bulbs.\n\nQuestion: In the
passage above, does the pronoun "they" refer to lemon trees?
</pre>
<p class="ltx_p" id="S1.SS1.SSS1.p1.3">Answer: <span class="ltx_text ltx_font_bold" id="S1.SS1.SSS1.p1.3.1">No</span></p>
</div>
</section>
<section class="ltx_subsubsection" id="S1.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.2 </span>Prompts</h4>
<div class="ltx_para" id="S1.SS1.SSS2.p1">
<p class="ltx_p" id="S1.SS1.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.SS1.SSS2.p1.1.1">GPT-3 Style</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS1.SSS2.p1.2">
Passage: {{ text }} \n\nQuestion: In the passage above, does the pronoun
"{{ span2_text }}" refer to {{ span1_text }}?\n\nAnswer:
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS1.SSS2.p2">
<p class="ltx_p" id="S1.SS1.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S1.SS1.SSS2.p2.1.1">replaced with</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS1.SSS2.p2.2">
{{ text }} In the previous sentence, can the pronoun "{{ span2_text }}"
be replaced with "{{ span1_text }}"? Yes or no?
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS1.SSS2.p3">
<p class="ltx_p" id="S1.SS1.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S1.SS1.SSS2.p3.1.1">the pronoun refers to</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS1.SSS2.p3.2">
{{ text }} \nIn the passage above, the pronoun "{{ span2_text }}" refers to
{{ span1_text }}. True or false?
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS1.SSS2.p4">
<p class="ltx_p" id="S1.SS1.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S1.SS1.SSS2.p4.1.1">does p stand for</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS1.SSS2.p4.2">
{{ text }} Here, does "{{ span2_text.lower() }}" stand for {{ span1_text }}?
Yes or no?
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS1.SSS2.p5">
<p class="ltx_p" id="S1.SS1.SSS2.p5.1"><span class="ltx_text ltx_font_bold" id="S1.SS1.SSS2.p5.1.1">the pronoun refers to</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS1.SSS2.p5.2">
{{ text }} \nIn the passage above, the pronoun "{{ span2_text }}" refers to
{{ span1_text }}. True or false?
</pre>
</div>
</section>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>SuperGLUE/wic</h3>
<section class="ltx_subsubsection" id="S1.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.1 </span>Data example</h4>
<div class="ltx_para" id="S1.SS2.SSS1.p1">
<p class="ltx_p" id="S1.SS2.SSS1.p1.1">Prompt name: <span class="ltx_text ltx_font_bold" id="S1.SS2.SSS1.p1.1.1">GPT-3 Style</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS2.SSS1.p1.2">
As he called the role he put a check mark by each studentâ€™s name.
\n\nA check on its dependability under stress.\n\nQuestion: Is the
word â€™checkâ€™ used in the same sense in the two sentences above?
</pre>
</div>
</section>
<section class="ltx_subsubsection" id="S1.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.2 </span>Prompts</h4>
<div class="ltx_para" id="S1.SS2.SSS2.p1">
<p class="ltx_p" id="S1.SS2.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.SS2.SSS2.p1.1.1">GPT-3 Style</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS2.SSS2.p1.2">
{{sentence1}}\n\n{{sentence2}}\n\nQuestion: Is the word â€™â€™{{word}}â€™â€™
used in the same sense in the two sentences above?
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS2.SSS2.p2">
<p class="ltx_p" id="S1.SS2.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S1.SS2.SSS2.p2.1.1">question-context-meaning-with-label</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS2.SSS2.p2.2">
Does the word "{{word}}" have the same meaning in these two sentences?
Yes, No?\n\n{{sentence1}}\n\n{{sentence2}}
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS2.SSS2.p3">
<p class="ltx_p" id="S1.SS2.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S1.SS2.SSS2.p3.1.1">GPT-3-prompt-with-label</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS2.SSS2.p3.2">
{sentence1}}\n\n{{sentence2}}\n\nQuestion: Is the word â€™â€™{{word}}â€™â€™ used
in the same sense in the two sentences above? Yes, No?
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS2.SSS2.p4">
<p class="ltx_p" id="S1.SS2.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S1.SS2.SSS2.p4.1.1">polysemous</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS2.SSS2.p4.2">
The word "{{word}}" has multiple meanings. Does it have the same meaning in
sentences 1 and 2? Yes or no? Sentence 1: {{sentence1}} Sentence 2:
{{sentence2}}
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS2.SSS2.p5">
<p class="ltx_p" id="S1.SS2.SSS2.p5.1"><span class="ltx_text ltx_font_bold" id="S1.SS2.SSS2.p5.1.1">similar-sense</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS2.SSS2.p5.2">
{{sentence1}}\n\n{{sentence2}}\n\nSimilar sense of {{word}}?
</pre>
</div>
</section>
</section>
<section class="ltx_subsection" id="S1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>SuperGLUE/boolq</h3>
<section class="ltx_subsubsection" id="S1.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.3.1 </span>Data example</h4>
<div class="ltx_para" id="S1.SS3.SSS1.p1">
<p class="ltx_p" id="S1.SS3.SSS1.p1.1">Prompt name: <span class="ltx_text ltx_font_bold" id="S1.SS3.SSS1.p1.1.1">GPT-3 Style</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS3.SSS1.p1.2">
Phantom pain -- Phantom pain sensations are described as perceptions that an
individual experiences relating to a limb or an organ that is not physically
part of the body. Limb loss is a result of either removal by amputation or
congenital limb deficiency. However, phantom limb sensations can also occur
following nerve avulsion or spinal cord injury.\nQuestion: is pain experienced
in a missing body part or paralyzed area\nAnswer:
</pre>
<p class="ltx_p" id="S1.SS3.SSS1.p1.3">Answer: <span class="ltx_text ltx_font_bold" id="S1.SS3.SSS1.p1.3.1">Yes</span></p>
</div>
</section>
<section class="ltx_subsubsection" id="S1.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.3.2 </span>Prompts</h4>
<div class="ltx_para" id="S1.SS3.SSS2.p1">
<p class="ltx_p" id="S1.SS3.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.SS3.SSS2.p1.1.1">GPT-3 Style</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS3.SSS2.p1.2">
{{ passage }} \nQuestion: {{ question }}\nAnswer:
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS3.SSS2.p2">
<p class="ltx_p" id="S1.SS3.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S1.SS3.SSS2.p2.1.1">yes_no_question</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS3.SSS2.p2.2">
Text: {{passage}}\n\nAnswer the following yes/no question:
{{question}}? Yes or no?
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS3.SSS2.p3">
<p class="ltx_p" id="S1.SS3.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S1.SS3.SSS2.p3.1.1">exam</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS3.SSS2.p3.2">
EXAM\n1. Answer by yes or no.\n\nDocument: {{passage}}\n
Question: {{question}}?
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS3.SSS2.p4">
<p class="ltx_p" id="S1.SS3.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S1.SS3.SSS2.p4.1.1">based on the following passage</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS3.SSS2.p4.2">
Based on the following passage, {{ question }}? {{ passage }}
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS3.SSS2.p5">
<p class="ltx_p" id="S1.SS3.SSS2.p5.1"><span class="ltx_text ltx_font_bold" id="S1.SS3.SSS2.p5.1.1">could you tell meâ€¦</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS3.SSS2.p5.2">
{ passage }} \n\nHaving read that, could you tell me {{ question }}?
</pre>
</div>
</section>
</section>
<section class="ltx_subsection" id="S1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>SuperGLUE/axb &amp; SuperGLUE/axg</h3>
<section class="ltx_subsubsection" id="S1.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.4.1 </span>Data example</h4>
<div class="ltx_para" id="S1.SS4.SSS1.p1">
<p class="ltx_p" id="S1.SS4.SSS1.p1.1">Prompt name: <span class="ltx_text ltx_font_bold" id="S1.SS4.SSS1.p1.1.1">GPT-3 style</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS4.SSS1.p1.2">
The taxpayer met with the accountant to get help filing his taxes.\n\n
Question: The accountant sought help filing taxes. True or False?
</pre>
<p class="ltx_p" id="S1.SS4.SSS1.p1.3">Answer: <span class="ltx_text ltx_font_bold" id="S1.SS4.SSS1.p1.3.1">False</span></p>
</div>
</section>
<section class="ltx_subsubsection" id="S1.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.4.2 </span>Prompts</h4>
<div class="ltx_para ltx_noindent" id="S1.SS4.SSS2.p1">
<p class="ltx_p" id="S1.SS4.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.SS4.SSS2.p1.1.1">GPT-3 style</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS4.SSS2.p1.2">
{{sentence1}}\n\nQuestion: {{sentence2}} True or False?
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS4.SSS2.p2">
<p class="ltx_p" id="S1.SS4.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S1.SS4.SSS2.p2.1.1">MNLI Crowdsource</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS4.SSS2.p2.2">
{{sentence1}} Using only the above description and what you know about
the world, is "{{sentence2}}" definitely correct? Yes or no?
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS4.SSS2.p3">
<p class="ltx_p" id="S1.SS4.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S1.SS4.SSS2.p3.1.1">can we infer</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS4.SSS2.p3.2">
Suppose {{sentence1}} Can we infer that "{{sentence2}}"? Yes or no?
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS4.SSS2.p4">
<p class="ltx_p" id="S1.SS4.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S1.SS4.SSS2.p4.1.1">guaranteed true</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS4.SSS2.p4.2">
Given {{sentence1}} Is it guaranteed true that "{{sentence2}}"? Yes or no?
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS4.SSS2.p5">
<p class="ltx_p" id="S1.SS4.SSS2.p5.1"><span class="ltx_text ltx_font_bold" id="S1.SS4.SSS2.p5.1.1">justified in saying</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS4.SSS2.p5.2">
{{sentence1}} Are we justified in saying that "{{sentence2}}"? Yes or no?
</pre>
</div>
</section>
</section>
<section class="ltx_subsection" id="S1.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5 </span>XNLI &amp; SuperGLUE/CB</h3>
<section class="ltx_subsubsection" id="S1.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.5.1 </span>Data example</h4>
<div class="ltx_para" id="S1.SS5.SSS1.p1">
<p class="ltx_p" id="S1.SS5.SSS1.p1.1">Prompt name: <span class="ltx_text ltx_font_bold" id="S1.SS5.SSS1.p1.1.1">GPT-3 style</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS5.SSS1.p1.2">
Well, I wasnâ€™t even thinking about that, but I was so frustrated, and, I ended up
talking to him again.\n\nQuestion: I havent spoken to him again. True, False, or
Neither?
</pre>
<p class="ltx_p" id="S1.SS5.SSS1.p1.3">Answer: <span class="ltx_text ltx_font_bold" id="S1.SS5.SSS1.p1.3.1">False</span></p>
</div>
</section>
<section class="ltx_subsubsection" id="S1.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.5.2 </span>Prompts</h4>
<div class="ltx_para ltx_noindent" id="S1.SS5.SSS2.p1">
<p class="ltx_p" id="S1.SS5.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.SS5.SSS2.p1.1.1">GPT-3 style</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS5.SSS2.p1.2">
{{premise}}\n\nQuestion: {{hypothesis}} True, False, or Neither?
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS5.SSS2.p2">
<p class="ltx_p" id="S1.SS5.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S1.SS5.SSS2.p2.1.1">MNLI crowdsource</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS5.SSS2.p2.2">
{{premise}} Using only the above description and what you know about the world,
"{{hypothesis}}" is definitely correct, incorrect, or inconclusive?
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS5.SSS2.p3">
<p class="ltx_p" id="S1.SS5.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S1.SS5.SSS2.p3.1.1">can we infer</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS5.SSS2.p3.2">
Suppose {{premise}} Can we infer that "{{hypothesis}}"? Yes, no, or maybe?
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS5.SSS2.p4">
<p class="ltx_p" id="S1.SS5.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S1.SS5.SSS2.p4.1.1">guaranteed/possible/impossible</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS5.SSS2.p4.2">
Assume it is true that {{premise}} \n\nTherefore, \"{{hypothesis}}\" is
{{\"guaranteed\"}}, {{\"possible\"}}, or {{\"impossible\"}}?
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS5.SSS2.p5">
<p class="ltx_p" id="S1.SS5.SSS2.p5.1"><span class="ltx_text ltx_font_bold" id="S1.SS5.SSS2.p5.1.1">justified in saying</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS5.SSS2.p5.2">
{{premise}} Are we justified in saying that "{{hypothesis}}"? Yes, no,
or maybe?
</pre>
</div>
</section>
</section>
<section class="ltx_subsection" id="S1.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.6 </span>XWinograd</h3>
<section class="ltx_subsubsection" id="S1.SS6.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.6.1 </span>Data example</h4>
<div class="ltx_para" id="S1.SS6.SSS1.p1">
<p class="ltx_p" id="S1.SS6.SSS1.p1.1">Prompt name: <span class="ltx_text ltx_font_bold" id="S1.SS6.SSS1.p1.1.1">Replace</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS6.SSS1.p1.2">
The city councilmen refused the demonstrators a permit because _ feared
violence.\nReplace the _ in the above sentence with the correct option:
\n- the demonstrators\n- The city councilmen
</pre>
<p class="ltx_p" id="S1.SS6.SSS1.p1.3">Answer: <span class="ltx_text ltx_font_bold" id="S1.SS6.SSS1.p1.3.1">The city councilmen</span></p>
</div>
</section>
<section class="ltx_subsubsection" id="S1.SS6.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.6.2 </span>Prompts</h4>
<div class="ltx_para" id="S1.SS6.SSS2.p1">
<p class="ltx_p" id="S1.SS6.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.SS6.SSS2.p1.1.1">Replace</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS6.SSS2.p1.2">
{{sentence}}\nReplace the _ in the above sentence with the correct option:
\n- {{option1}}\n- {{option2}}
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS6.SSS2.p2">
<p class="ltx_p" id="S1.SS6.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S1.SS6.SSS2.p2.1.1">True or False</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS6.SSS2.p2.2">
The _ in the sentence below refers to {{option1}}. True or False?
{{sentence}}
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS6.SSS2.p3">
<p class="ltx_p" id="S1.SS6.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S1.SS6.SSS2.p3.1.1">does underscore refer to</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS6.SSS2.p3.2">
{{sentence}} In the previous sentence, does _ refer to
{{ option1 }} or {{ option2 }}?
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS6.SSS2.p4">
<p class="ltx_p" id="S1.SS6.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S1.SS6.SSS2.p4.1.1">underscore refer to</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS6.SSS2.p4.2">
{{sentence}}\n What does the _ in the above sentence refer to?
{{ option1 }} or {{ option2 }}?
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS6.SSS2.p5">
<p class="ltx_p" id="S1.SS6.SSS2.p5.1"><span class="ltx_text ltx_font_bold" id="S1.SS6.SSS2.p5.1.1">stand for</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS6.SSS2.p5.2">
In the sentence below, does the _ stand for {{answer_choices[0]}} or
{{answer_choices[1]}}? {{sentence}}
</pre>
</div>
</section>
</section>
<section class="ltx_subsection" id="S1.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.7 </span>XCOPA &amp; SuperGLUE/COPA</h3>
<section class="ltx_subsubsection" id="S1.SS7.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.7.1 </span>Data example</h4>
<div class="ltx_para" id="S1.SS7.SSS1.p1">
<p class="ltx_p" id="S1.SS7.SSS1.p1.1">Prompt name: <span class="ltx_text ltx_font_bold" id="S1.SS7.SSS1.p1.1.1">C1 or C2? premise, so/becauseâ€¦</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS7.SSS1.p1.2">
"It was fragile." or "It was small."? The item was packaged in bubble wrap.
because
</pre>
<p class="ltx_p" id="S1.SS7.SSS1.p1.3">Answer: <span class="ltx_text ltx_font_bold" id="S1.SS7.SSS1.p1.3.1">It was fragile.</span></p>
</div>
</section>
<section class="ltx_subsubsection" id="S1.SS7.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.7.2 </span>Prompts</h4>
<div class="ltx_para ltx_noindent" id="S1.SS7.SSS2.p1">
<p class="ltx_p" id="S1.SS7.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.SS7.SSS2.p1.1.1">C1 or C2? premise, so/becauseâ€¦</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS7.SSS2.p1.2">
{{ answer_choices[0] }}" or "{{ answer_choices[1] }}"? {{ premise }}
{% if question == "cause" %} because {% else %} so {% endif %}
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS7.SSS2.p2">
<p class="ltx_p" id="S1.SS7.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S1.SS7.SSS2.p2.1.1">best_option</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS7.SSS2.p2.2">
{{ premise }} \n\nWhatâ€™s the best option?\n- {{choice1}}\n- {{choice2}}\n\
\nWe are looking for {% if question == \"cause\" %} a cause {% else %}
an effect {% endif %}
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS7.SSS2.p3">
<p class="ltx_p" id="S1.SS7.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S1.SS7.SSS2.p3.1.1">cause_effect</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS7.SSS2.p3.2">
{{ premise }}\nSelect the most plausible {% if question == "cause" %} cause:
{% else %} effect: {% endif %}\n- {{choice1}}\n- {{choice2}}
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS7.SSS2.p4">
<p class="ltx_p" id="S1.SS7.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S1.SS7.SSS2.p4.1.1">i_am_hesitating</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS7.SSS2.p4.2">
{{ premise }} \n\nI am hesitating between two options. Help me choose the
more  likely {% if question == \"cause\" %} cause: {% else %}
effect: {% endif %}\n- {{choice1}}\n- {{choice2}}
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS7.SSS2.p5">
<p class="ltx_p" id="S1.SS7.SSS2.p5.1"><span class="ltx_text ltx_font_bold" id="S1.SS7.SSS2.p5.1.1">plausible_alternatives</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS7.SSS2.p5.2">
{{ premise }} {% if question == "cause" %} This happened because...
{% else %} As a consequence... {% endif %} Help me pick the more
plausible option:\n- {{choice1}}\n- {{choice2}}
</pre>
</div>
</section>
</section>
<section class="ltx_subsection" id="S1.SS8">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.8 </span>XStoryCloze &amp; Story Cloze</h3>
<section class="ltx_subsubsection" id="S1.SS8.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.8.1 </span>Data example</h4>
<div class="ltx_para" id="S1.SS8.SSS1.p1">
<p class="ltx_p" id="S1.SS8.SSS1.p1.1">XStoryCloze and Story Cloze are not publicly available datasets. Please contact the authors of <cite class="ltx_cite ltx_citemacro_citet">Lin etÂ al. (<a class="ltx_ref" href="#bib.bib81" title="">2021</a>)</cite> for XStoryCloze and <cite class="ltx_cite ltx_citemacro_citet">Mostafazadeh etÂ al. (<a class="ltx_ref" href="#bib.bib97" title="">2017</a>)</cite> for Story Cloze samples.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S1.SS8.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.8.2 </span>Prompts</h4>
<div class="ltx_para ltx_noindent" id="S1.SS8.SSS2.p1">
<p class="ltx_p" id="S1.SS8.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.SS8.SSS2.p1.1.1">Answer Given options</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS8.SSS2.p1.2">
{{input_sentence_1}} {{input_sentence_2}} {{input_sentence_3}}
{{input_sentence_4}} What is a possible continuation for the story
given the following options ? - {{answer_choices | join("\n- ")}}
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS8.SSS2.p2">
<p class="ltx_p" id="S1.SS8.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S1.SS8.SSS2.p2.1.1">Choose Story Ending</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS8.SSS2.p2.2">
Read the following story :\n\n{{input_sentence_1}}\n{{input_sentence_2}}\n
{{input_sentence_3}}\n{{input_sentence_4}}\n\nChoose a possible ending for the
previous story from the following options: \n- {{answer_choices | join(\"\\\n- \")}}
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS8.SSS2.p3">
<p class="ltx_p" id="S1.SS8.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S1.SS8.SSS2.p3.1.1">Story Continuation and Options</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS8.SSS2.p3.2">
What is a possible continuation for the following story ? \n\n{{input_sentence_1}}
\n\{{input_sentence_2}}\n{{input_sentence_3}}\n{{input_sentence_4}}\n\nChoose from
the following options:\n- {{answer_choices | join(\"\\n- \")}}
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS8.SSS2.p4">
<p class="ltx_p" id="S1.SS8.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S1.SS8.SSS2.p4.1.1">Generate Ending</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS8.SSS2.p4.2">
Generate a possible ending for the following story: {{input_sentence_1}}
{{input_sentence_2}} {{input_sentence_3}} {{input_sentence_4}}
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS8.SSS2.p5">
<p class="ltx_p" id="S1.SS8.SSS2.p5.1"><span class="ltx_text ltx_font_bold" id="S1.SS8.SSS2.p5.1.1">Novel Correct Ending</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS8.SSS2.p5.2">
I read the following novel: {{input_sentence_1}} {{input_sentence_2}}
{{input_sentence_3}} {{input_sentence_4}} What do you think is the most probable
ending? You can choose from the following options: - {{answer_choices | join("\n-")}}
</pre>
</div>
</section>
</section>
<section class="ltx_subsection" id="S1.SS9">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.9 </span>WMT</h3>
<div class="ltx_para" id="S1.SS9.p1">
<p class="ltx_p" id="S1.SS9.p1.1">Prompts for SectionÂ <a class="ltx_ref" href="#S4.SS3.SSS1" title="4.3.1 WMT â€£ 4.3 Machine Translation â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">4.3.1</span></a>, where we compare prompts in both zero-shot and 1-shot settings for four language directions (en<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S1.SS9.p1.1.m1.1"><semantics id="S1.SS9.p1.1.m1.1a"><mo id="S1.SS9.p1.1.m1.1.1" stretchy="false" xref="S1.SS9.p1.1.m1.1.1.cmml">â†”</mo><annotation-xml encoding="MathML-Content" id="S1.SS9.p1.1.m1.1b"><ci id="S1.SS9.p1.1.m1.1.1.cmml" xref="S1.SS9.p1.1.m1.1.1">â†”</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS9.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S1.SS9.p1.1.m1.1d">â†”</annotation></semantics></math>{hi,fr}).</p>
</div>
<section class="ltx_subsubsection" id="S1.SS9.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.9.1 </span>Data example</h4>
<div class="ltx_para" id="S1.SS9.SSS1.p1">
<p class="ltx_p" id="S1.SS9.SSS1.p1.1">The prompt names and content are specific to the language direction. The prompts below each exist in four versions, where â€œl1â€ and â€œl2â€ are replaced by the language codes of the source and target languages respectively (en, fr or hi) and â€œL1â€ and â€œL2â€ are replaced by the language names of the source and target languages respectively (English, French or Hindi).</p>
</div>
<div class="ltx_para" id="S1.SS9.SSS1.p2">
<p class="ltx_p" id="S1.SS9.SSS1.p2.1">Prompt name: <span class="ltx_text ltx_font_bold" id="S1.SS9.SSS1.p2.1.1">a_good_translation-l1-l2-source+target</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS9.SSS1.p2.2">
Given the following source text in English: Spectacular Wingsuit Jump Over
Bogota , a good French translation is:
</pre>
<p class="ltx_p" id="S1.SS9.SSS1.p2.3">Answer: <span class="ltx_text ltx_font_bold" id="S1.SS9.SSS1.p2.3.1">Spectaculaire saut en â€wingsuitâ€ au-dessus de Bogota</span></p>
</div>
</section>
<section class="ltx_subsubsection" id="S1.SS9.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.9.2 </span>Prompts</h4>
<div class="ltx_para" id="S1.SS9.SSS2.p1">
<p class="ltx_p" id="S1.SS9.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.SS9.SSS2.p1.1.1">a_good_translation-l1-l2-source+target</span>
</p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS9.SSS2.p1.2">
Given the following source text in L1:  {{translation[l1]}} , a
good L2 translation is: ||| {{translation[l2]}}
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS9.SSS2.p2">
<p class="ltx_p" id="S1.SS9.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S1.SS9.SSS2.p2.1.1">gpt-3-l1-l2-target</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS9.SSS2.p2.2">
Q: What is the {{L2}} translation of {{translation[l2]}} A:
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS9.SSS2.p3">
<p class="ltx_p" id="S1.SS9.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S1.SS9.SSS2.p3.1.1">version-l1-l2-target</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS9.SSS2.p3.2">
If the original version says: {{translation[l1]}}; then the L2
version should say:
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS9.SSS2.p4">
<p class="ltx_p" id="S1.SS9.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S1.SS9.SSS2.p4.1.1">xglm-l1-l2-source+target</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS9.SSS2.p4.2">
{{L1}}: {{translation[l1]}} = {{L2}}:
</pre>
</div>
</section>
</section>
<section class="ltx_subsection" id="S1.SS10">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.10 </span>DiaBLa</h3>
<div class="ltx_para" id="S1.SS10.p1">
<p class="ltx_p" id="S1.SS10.p1.1">Prompts for contextual MT results shown in TableÂ <a class="ltx_ref" href="#S4.T7" title="Table 7 â€£ 4.3.2 DiaBLa â€£ 4.3 Machine Translation â€£ 4 Evaluation â€£ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<section class="ltx_subsubsection" id="S1.SS10.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.10.1 </span>Data example</h4>
<div class="ltx_para" id="S1.SS10.SSS1.p1">
<p class="ltx_p" id="S1.SS10.SSS1.p1.1">Prompt name: <span class="ltx_text ltx_font_bold" id="S1.SS10.SSS1.p1.1.1">xglm-source+target</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS10.SSS1.p1.2">
English: We appear to have stopped moving. = French:
</pre>
<p class="ltx_p" id="S1.SS10.SSS1.p1.3">Answer: <span class="ltx_text ltx_font_bold" id="S1.SS10.SSS1.p1.3.1">Jâ€™ai lâ€™impression quâ€™on sâ€™est arrÃªtÃ©s.</span></p>
</div>
</section>
<section class="ltx_subsubsection" id="S1.SS10.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.10.2 </span>Prompt</h4>
<div class="ltx_para" id="S1.SS10.SSS2.p1">
<p class="ltx_p" id="S1.SS10.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.SS10.SSS2.p1.1.1">xglm-source+target</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS10.SSS2.p1.2">
{% set trg_lang ="French" %}{% set src_lang ="English" %}
{% if utterance_meta.lang == "french" %}
  {% set trg_lang = "English" %}{% set src_lang = "French" %}
{% endif %}
{{ src_lang }}: {{ orig }} = {{ trg_lang }}: ||| {{ ref }}
</pre>
</div>
<div class="ltx_para" id="S1.SS10.SSS2.p2">
<p class="ltx_p" id="S1.SS10.SSS2.p2.2">The dialogue set is bilingual (between native English and native French speakers). In few-shot setups, few-shot examples are by default in either language direction (English<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S1.SS10.SSS2.p2.1.m1.1"><semantics id="S1.SS10.SSS2.p2.1.m1.1a"><mo id="S1.SS10.SSS2.p2.1.m1.1.1" stretchy="false" xref="S1.SS10.SSS2.p2.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S1.SS10.SSS2.p2.1.m1.1b"><ci id="S1.SS10.SSS2.p2.1.m1.1.1.cmml" xref="S1.SS10.SSS2.p2.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS10.SSS2.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S1.SS10.SSS2.p2.1.m1.1d">â†’</annotation></semantics></math>French or French<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S1.SS10.SSS2.p2.2.m2.1"><semantics id="S1.SS10.SSS2.p2.2.m2.1a"><mo id="S1.SS10.SSS2.p2.2.m2.1.1" stretchy="false" xref="S1.SS10.SSS2.p2.2.m2.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S1.SS10.SSS2.p2.2.m2.1b"><ci id="S1.SS10.SSS2.p2.2.m2.1.1.cmml" xref="S1.SS10.SSS2.p2.2.m2.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS10.SSS2.p2.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S1.SS10.SSS2.p2.2.m2.1d">â†’</annotation></semantics></math>English) regardless of the direction of the current example. We implemented some additional DiaBLa tasks that control the language direction of the few-shot example to be the same or the opposite direction to the current example (<span class="ltx_text ltx_font_typewriter" id="S1.SS10.SSS2.p2.2.1">diabla_1_shot_context_same</span> and <span class="ltx_text ltx_font_typewriter" id="S1.SS10.SSS2.p2.2.2">diabla_1_shot_context_opposite</span>).
</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S1.SS11">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.11 </span>Flores-101 (MT)</h3>
<div class="ltx_para" id="S1.SS11.p1">
<p class="ltx_p" id="S1.SS11.p1.1">The prompts are specific to each language pair as indicated by the source and target languages in the prompt name. Below is an example for French<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S1.SS11.p1.1.m1.1"><semantics id="S1.SS11.p1.1.m1.1a"><mo id="S1.SS11.p1.1.m1.1.1" stretchy="false" xref="S1.SS11.p1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S1.SS11.p1.1.m1.1b"><ci id="S1.SS11.p1.1.m1.1.1.cmml" xref="S1.SS11.p1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS11.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S1.SS11.p1.1.m1.1d">â†’</annotation></semantics></math>Catalan translation.</p>
</div>
<section class="ltx_subsubsection" id="S1.SS11.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.11.1 </span>Data example</h4>
<div class="ltx_para" id="S1.SS11.SSS1.p1">
<p class="ltx_p" id="S1.SS11.SSS1.p1.1">Prompt name: <span class="ltx_text ltx_font_bold" id="S1.SS11.SSS1.p1.1.1">xglm-French-Catalan-source+target</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS11.SSS1.p1.2">
French: Lâ€™amnagement vieillot, lâ€™absence dâ€™quipements rcents et un certain
charme  lâ€™ancienne font galement partie de leur caractre. = Catalan: |||
Els accessoris vintage, la manca de les comoditats modernes i un cert envelliment
elegant tamb formen part del seu carcter
</pre>
</div>
</section>
<section class="ltx_subsubsection" id="S1.SS11.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.11.2 </span>Prompt</h4>
<div class="ltx_para" id="S1.SS11.SSS2.p1">
<p class="ltx_p" id="S1.SS11.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.SS11.SSS2.p1.1.1">xglm-French-Catalan-source+target</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS11.SSS2.p1.2">
French: {{ sentence_fra }} = Catalan: ||| {{ sentence_cat }}
</pre>
</div>
</section>
</section>
<section class="ltx_subsection" id="S1.SS12">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.12 </span>CrowS-Pairs</h3>
<section class="ltx_subsubsection" id="S1.SS12.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.12.1 </span>Data example</h4>
<div class="ltx_para" id="S1.SS12.SSS1.p1">
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS12.SSS1.p1.1">
Sentence A: He couldnâ€™t figure out the issue with the rope, but he tried not to get
too down on himself, most black kids donâ€™t have the best education growing up.\n
Sentence B: He couldnâ€™t figure out the issue with the rope, but he tried not to get
too down on himself, most white kids donâ€™t have the best education growing up.\n
Select the option that is the most plausible:\n- Sentence A demonstrates
a stereotype\n- Sentence B violates a stereotype
</pre>
<p class="ltx_p" id="S1.SS12.SSS1.p1.2">Answer: <span class="ltx_text ltx_font_bold" id="S1.SS12.SSS1.p1.2.1">Sentence A demonstrates a stereotype</span></p>
</div>
</section>
<section class="ltx_subsubsection" id="S1.SS12.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.12.2 </span>Prompts</h4>
<div class="ltx_para" id="S1.SS12.SSS2.p1">
<p class="ltx_p" id="S1.SS12.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.SS12.SSS2.p1.1.1">demontraste_or_violate</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="S1.SS12.SSS2.p1.2">
Sentence A: {{sent_more}}\nSentence B: {{sent_less}}\nSelect the option
that is the most plausible:\n- {{answer_choices[0]}}\n- {{answer_choices[1]}}
</pre>
</div>
</section>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Jul 13 17:51:19 2023 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
