<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems</title>
<!--Generated on Tue Apr  8 13:16:37 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Conversational recommender systems,  preference elicitation,  question generation" lang="en" name="keywords"/>
<base href="/html/2111.13463v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S1" title="In Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S2" title="In Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S2.SS1" title="In 2. Related Work ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Conversational Recommender Systems</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S2.SS2" title="In 2. Related Work ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Preference Elicitation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S2.SS3" title="In 2. Related Work ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Question Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S2.SS4" title="In 2. Related Work ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Sequence-to-Sequence Modeling</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S3" title="In Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S3.SS1" title="In 3. Approach ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Baseline 1: Template-based Question Generation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S3.SS1.SSS1" title="In 3.1. Baseline 1: Template-based Question Generation ‣ 3. Approach ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Candidate Sentence Selection</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S3.SS1.SSS1.Px1" title="In 3.1.1. Candidate Sentence Selection ‣ 3.1. Baseline 1: Template-based Question Generation ‣ 3. Approach ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title">Aspect-Value Pair Extraction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S3.SS1.SSS1.Px2" title="In 3.1.1. Candidate Sentence Selection ‣ 3.1. Baseline 1: Template-based Question Generation ‣ 3. Approach ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title">Activity Identification</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S3.SS1.SSS2" title="In 3.1. Baseline 1: Template-based Question Generation ‣ 3. Approach ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Question Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S3.SS2" title="In 3. Approach ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Baseline 2: Template-based Question Generation with Classification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S3.SS3" title="In 3. Approach ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Neural Sentence-based Question Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S3.SS4" title="In 3. Approach ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Neural Review-based Question Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S4" title="In Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Data Collection</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S4.SS1" title="In 4. Data Collection ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Candidate Sentence Selection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S4.SS2" title="In 4. Data Collection ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Question Generation using Crowdsourcing</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S4.SS2.SSS1" title="In 4.2. Question Generation using Crowdsourcing ‣ 4. Data Collection ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Step 1: Question Collection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S4.SS2.SSS2" title="In 4.2. Question Generation using Crowdsourcing ‣ 4. Data Collection ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Step 2: Validation and Filtering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S4.SS2.SSS3" title="In 4.2. Question Generation using Crowdsourcing ‣ 4. Data Collection ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Step 3: Expanding Question Variety</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S4.SS3" title="In 4. Data Collection ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Final Dataset</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S5" title="In Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S5.SS1" title="In 5. Experimental Setup ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Question Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S5.SS2" title="In 5. Experimental Setup ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Automatic Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S5.SS3" title="In 5. Experimental Setup ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Human Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S6" title="In Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Results and Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S6.SS1" title="In 6. Results and Analysis ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Automatic Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S6.SS2" title="In 6. Results and Analysis ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Human evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S6.SS3" title="In 6. Results and Analysis ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Model Size</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S6.SS4" title="In 6. Results and Analysis ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Training Data Volume</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S6.SS5" title="In 6. Results and Analysis ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5 </span>Success/Failure Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S7" title="In Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion and Future Directions</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ivica Kostric
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">University of Stavanger</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Stavanger</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">Norway</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:ivica.kostric@uis.no">ivica.kostric@uis.no</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Krisztian Balog
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">University of Stavanger</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Stavanger</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">Norway</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:krisztian.balog@uis.no">krisztian.balog@uis.no</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Filip Radlinski
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Google</span><span class="ltx_text ltx_affiliation_country" id="id8.2.id2">UK</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:filiprad@google.com">filiprad@google.com</a>
</span></span></span>
</div>
<div class="ltx_dates">(2024; 2022)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id9.id1">A key distinguishing feature of conversational recommender systems over traditional recommender systems is their ability to elicit user preferences using natural language. Currently, the predominant approach to preference elicitation is to ask questions directly about items or item attributes.
Users searching for recommendations may not have deep knowledge of the available options in a given domain. As such, they might not be aware of key attributes or desirable values for them.
However, in many settings, talking about the <em class="ltx_emph ltx_font_italic" id="id9.id1.1">planned use</em> of items does not present any difficulties, even for those that are new to a domain. In this paper, we propose a novel approach to preference elicitation by asking implicit questions based on item usage. As one of the main contributions of this work, we develop a multi-stage data annotation protocol using crowdsourcing, to create a high-quality labeled training dataset.
Another main contribution is the development of four models for the question generation task: two template-based baseline models and two neural text-to-text models. The template-based models use heuristically extracted common patterns found in the training data, while the neural models use the training data to learn to generate questions automatically.
Using common metrics from machine translation for automatic evaluation, we show that our approaches are effective in generating elicitation questions, even with limited training data.
We further employ human evaluation for comparing the generated questions using both pointwise and pairwise evaluation designs. We find that the human evaluation results are consistent with the automatic ones, allowing us to draw conclusions about the quality of the generated questions with certainty. Finally, we provide a detailed analysis of cases where the models show their limitations.</p>
</div>
<div class="ltx_keywords">Conversational recommender systems, preference elicitation, question generation
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>rightsretained</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3629981</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journal" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journal: </span>TORS</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalvolume" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalvolume: </span>2</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalnumber" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalnumber: </span>2</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_article" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">article: </span>12</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_publicationmonth" id="id9"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">publicationmonth: </span>4</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id10"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Recommender systems</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id11"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Users and interactive retrieval</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Traditionally, recommender systems predict users’ preference towards an item by performing offline analysis of past interaction data (e.g., click history, past visits, item ratings) <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib16" title="">2021</a>)</cite>. These systems often do not take into account that users might have made mistakes
in the past (e.g., regarding purchases) <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib68" title="">2021b</a>)</cite> or that their preferences change over time <cite class="ltx_cite ltx_citemacro_citep">(Jagerman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib22" title="">2019</a>)</cite>. Additionally, for some users, there is little historical data which makes modeling their preferences difficult <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib28" title="">2019</a>)</cite>.
A <em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">conversational recommender system</em> (CRS), on the other hand, is a multi-turn, interactive recommender system that can elicit user preferences in real-time using natural language <cite class="ltx_cite ltx_citemacro_citep">(Jannach et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib23" title="">2022</a>)</cite>. Given its interactive nature, it is capable of modeling dynamic user preferences and taking actions based on users current needs <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib16" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">One of the main tasks of a conversational recommender system is to elicit preferences from users. This is traditionally done by asking questions either about items directly or item attributes <cite class="ltx_cite ltx_citemacro_citep">(Christakopoulou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib14" title="">2016</a>; Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib16" title="">2021</a>; Sepliarskaia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib60" title="">2018</a>; Christakopoulou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib13" title="">2018</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib76" title="">2018</a>; Chen and Pu, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib11" title="">2012</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib71" title="">2019</a>; Sun and Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib61" title="">2018</a>; Lei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib30" title="">2020b</a>)</cite>.
Asking people to review individual recommendations to establish the characteristics of a single item they need, especially in a domain that they are not expert in, is particularly time consuming; therefore,
the research is commonly focused on the estimation and utilization of users preferences towards attributes <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib16" title="">2021</a>)</cite>. Common to these approaches is that the user is explicitly asked about the desired values for a specific product attribute, much in the spirit of slot-filling dialogue systems <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib17" title="">2018</a>)</cite>. For example, in the context of looking for a bicycle recommendation, we might have wheel dimensions or the number of gears as attributes in our item collection. In this case, a system might want to ask a question like <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">“How thick should the tires be?”</em> or <em class="ltx_emph ltx_font_italic" id="S1.p2.1.2">“How many gears should the bike have?”</em> However, ordinary users often do not possess this kind of attribute understanding, which might require extensive domain-specific knowledge. Instead, they only know where or how they intend to use the item. For example, a user might only be interested in using this bike for commuting but does not know what attributes might be good for that purpose.
The novel research objective of this work is to generate <em class="ltx_emph ltx_font_italic" id="S1.p2.1.3">implicit</em> attribute questions for eliciting user preferences, related to the intended use of items. This stands in contrast to explicit questions that ask about specific item attributes.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Our approach hinges on the observation that usage-related experiences are often captured in item reviews. By identifying review sentences that discuss particular item features or aspects (e.g., <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">“fat tires”</em>) that matter in the context of various activities or usage scenarios (e.g., <em class="ltx_emph ltx_font_italic" id="S1.p3.1.2">“for conquering tough terrain”</em>), those sentences can then be turned into preference elicitation questions.
In our envisaged scenario, a large collection of implicit preference elicitation questions is generated offline, and then utilized later in real-time interactions by a CRS; see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">1</span></a> for an illustration.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="142" id="S1.F1.g1" src="x1.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Conceptual system overview. Our focus in this paper is on the implicit question generator component.</figcaption>
</figure>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, our focus is on the offline question generation part, whereas the actual item recommendation is left as a separate, downstream task to be addressed.
A main challenge associated with the question generation task is the collection of high-quality training data.
As our first contribution, we address the problem of creating a sentence-to-question dataset by developing a multi-stage data generation protocol.
It starts with <em class="ltx_emph ltx_font_italic" id="S1.p4.1.1">candidate sentence selection</em>, which can be automated effectively based on part-of-speech tagging and simple linguistic patterns.
Then, we employ a multi-step manual data annotation process via crowdsourcing, which involves (1) question generation (given an input sentence, turn it into a question, if possible), (2) question validation (filtering the responses collected in the previous step), and (3) expanding question variety (producing paraphrased versions of the input questions).
As our second contribution, we propose four <em class="ltx_emph ltx_font_italic" id="S1.p4.1.2">question generation</em> models that, given a review as input, produce an implicit question in an end-to-end fashion. The simplest, template-based model uses the most common n-grams found in the training data to construct questions. The second model extends this template-based baseline by adding a classifier to discard non-applicable sentences before generating a question. The last two are neural models we fine-tuned for this particular task, from a pre-trained checkpoint of a sequence-to-sequence model for text generation <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib53" title="">2020</a>)</cite>. The difference between the latter two lies in what is taken as the input—the first model uses heuristically extracted sentences, while the second one uses an entire review.
The evaluation of our proposed approach is done against held-back test data using standard metrics for <em class="ltx_emph ltx_font_italic" id="S1.p4.1.3">automatic evaluation</em> of text generation (BLEU, ROUGE, and METEOR). Additionally, we evaluate the task in terms of Accuracy, i.e., whether a question can be constructed based on the given input.
In <em class="ltx_emph ltx_font_italic" id="S1.p4.1.4">human evaluation</em> we measure the effectiveness and the capability of our model to generate questions that are suitable for preference elicitation, can be answered easily, and are grammatically correct. The evaluation is performed both in pointwise and pairwise fashion, using a 5-point Likert scale.
We find that all evaluations results (both automatic and two flavors of human evaluation) point to the same conclusions: that our proposed neural models outperform the strong template-based baseline. There are advantages to both neural models: the sentence-based model generates questions of slightly higher quality, while the review-based one has the advantage of being an end-to-end model with a simpler architecture.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In summary, our main contributions in this paper are as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">Introduce the novel task of eliciting preferences in CRSs via usage-related questions.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Develop a multi-stage data annotation protocol using crowdsourcing for collecting high-quality ground truth data.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Introduce two template-based and two neural approaches for generating usage-related questions based on a corpus of item reviews.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">Develop human evaluation protocols, conduct both automatic and manual evaluation of the proposed approaches, and perform an extensive analysis of results.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S1.p5.2">The resources developed in this paper (crowdsourced dataset and question generation model) are made publicly available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/iai-group/tors2023-crs-questions" title="">https://github.com/iai-group/tors2023-crs-questions</a>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The focus of this work is preference elicitation via natural language in the context of conversational recommender systems. In this section, we discuss related work on conversational recommender systems, preference elicitation, and question generation.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Conversational Recommender Systems</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Static recommendation models predict users’ preferences based on their previous interactions with the system. Some of the more common early approaches include collaborative filtering (CF) <cite class="ltx_cite ltx_citemacro_citep">(Sarwar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib58" title="">2001</a>)</cite>, logistic regression (LR) <cite class="ltx_cite ltx_citemacro_citep">(Nelder and Wedderburn, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib47" title="">1972</a>)</cite> and gradient boosting decision tree (GBDT) <cite class="ltx_cite ltx_citemacro_citep">(Blanco et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib7" title="">2013</a>)</cite>.
The availability of datasets on user behavior data (e.g., click history, visit logs, ratings on items<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://grouplens.org/datasets/movielens/</span></span></span><sup class="ltx_sup" id="S2.SS1.p1.1.1">,</sup><span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://www.baltrunas.info/context-aware</span></span></span>) has inspired, in recent years, the development of more sophisticated neural models such as neural factorization machines (NFM) <cite class="ltx_cite ltx_citemacro_citep">(He and Chua, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib19" title="">2017</a>)</cite> or graph convolutional networks (GCN) <cite class="ltx_cite ltx_citemacro_citep">(Ying et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib74" title="">2018</a>)</cite>.
A significant drawback of static recommenders is that they treat recommendation as a <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.2">one-shot</em> interaction process under the assumption that the user’s preferences lie in historical data. However, this does not hold in cases where there are no past observations <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib28" title="">2019</a>)</cite>. This is often the case in scenarios where the user has not interacted with the system (cold-start problem) or in the case with high-involvement products (i.e., products that customers do not buy frequently and tend to invest more time and effort when selecting them) <cite class="ltx_cite ltx_citemacro_citep">(Jannach et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib23" title="">2022</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Wang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib68" title="">2021b</a>)</cite> note that data on clicks and purchases could be misleading, because a large portion of clicks do not lead to purchases, and when they do, users might have regretted their choice. Furthermore, the user’s preferences might change over time <cite class="ltx_cite ltx_citemacro_citep">(Jagerman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib22" title="">2019</a>)</cite> and capturing their past interactions can lead to recommendations that are no longer relevant.
To deal with short-term but dynamic preferences, <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.3">session-based recommenders</em> have emerged and received considerable attention in recent years <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib67" title="">2021a</a>)</cite>. These algorithms provide recommendations solely based on the user’s interactions during a continuous period of time (i.e., a session).</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">A <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.1">conversational recommender system</em> (CRS)
helps users reach their recommendation-oriented goals via multi-turn conversation <cite class="ltx_cite ltx_citemacro_citep">(Jannach et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib23" title="">2022</a>)</cite>. While they share the goal of recommending items to users with traditional, static recommender systems, they do so by eliciting the detailed and current user preferences interactively in real-time.
In contrast to session-based recommenders, where user preferences are implicit and inferred from interactions, users explicitly express their preferences here using natural language.
Additionally, a CRS can provide explanations for the suggested items and process user feedback on the recommendation.
While there are many open issues around CRSs, <cite class="ltx_cite ltx_citemacro_citet">Gao et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib16" title="">2021</a>)</cite> identified the following five as primary challenges:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><em class="ltx_emph ltx_font_italic" id="S2.I1.i1.p1.1.1">Question-based User Preference Elicitation.</em> The challenge is to generate questions that elicit as much information as possible and to use the provided information to make better recommendations. Two main lines of research are item-based <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib79" title="">2013</a>; Christakopoulou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib14" title="">2016</a>; Sepliarskaia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib60" title="">2018</a>)</cite> and attribute-based preference elicitation <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib76" title="">2018</a>; Lei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib29" title="">2020a</a>)</cite>. Both approaches try to answer the questions of what to ask and how to adjust the recommendation based on user response.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><em class="ltx_emph ltx_font_italic" id="S2.I1.i2.p1.1.1">Multi-turn Conversational Recommendation Strategies.</em> The main challenge is to balance continued question asking to reduce preference uncertainty and provide recommendations using the least number of conversation turns.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1"><em class="ltx_emph ltx_font_italic" id="S2.I1.i3.p1.1.1">Natural Language Understanding and Generation.</em> One of the hardest challenges in CRSs is to communicate like a human <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib16" title="">2021</a>)</cite>. Commonly, this involves providing a recommendation list directly or incorporating recommended items in a rule-based natural language template <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib17" title="">2018</a>; Habib et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib18" title="">2020</a>; Zhang and Balog, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib75" title="">2020</a>)</cite>. Recently, end-to-end frameworks have been proposed to understand users’ intents and generate readable, fluent, and meaningful natural language responses <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib32" title="">2018</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i4.p1">
<p class="ltx_p" id="S2.I1.i4.p1.1"><em class="ltx_emph ltx_font_italic" id="S2.I1.i4.p1.1.1">Trade-offs between Exploration and Exploitation.</em> The dynamic nature of CRSs allows them to actively explore unseen items to capture user preferences. However, users generally have limited time and energy to interact with the system, therefore systems need to balance exploration with exploitation to make accurate recommendation.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i5.p1">
<p class="ltx_p" id="S2.I1.i5.p1.1"><em class="ltx_emph ltx_font_italic" id="S2.I1.i5.p1.1.1">Evaluation and User Simulation.</em> The complexity of evaluating CRSs comes from the emphasis on user experience during interactions. Systems need to be evaluated both on the turn and on the conversation level. While static recommenders can utilize large quantities of historical data to evaluate models, obtaining large number of user interactions to evaluate CRS is expensive <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib21" title="">2020</a>)</cite>. Therefore, user simulation-based evaluation has been identified as a promising direction <cite class="ltx_cite ltx_citemacro_citep">(Zhang and Balog, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib75" title="">2020</a>; Afzali et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib2" title="">2023</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">In this paper, we focus on question-based user preference elicitation and natural language generation. That is, we provide novel answers to questions <em class="ltx_emph ltx_font_italic" id="S2.SS1.p3.1.1">what to ask</em> and <em class="ltx_emph ltx_font_italic" id="S2.SS1.p3.1.2">how to ask</em>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Preference Elicitation</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Commonly, preference elicitation questions target either items or their attributes.
Typical of early studies on CRSs, <em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.1">item-based elicitation</em> approaches to ask for users’ opinions on an item itself, using a combination of methods from traditional recommender systems, such as collaborative filtering, with user interaction in real time <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib79" title="">2013</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib66" title="">2019</a>)</cite>. These systems continuously recommend items and refine the recommendations based on user feedback.
In case of <em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.2">choice-based methods</em>, users are presented with two or more items. In every turn, the recommendation is updated based on the selected choice. The selection of items may be approached as an optimization problem using a static preference questionnaire method <cite class="ltx_cite ltx_citemacro_citep">(Sepliarskaia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib60" title="">2018</a>)</cite>.
Another line of research is using probabilistic, multi-armed bandit algorithms that maximize the cumulative expected reward over some fixed number of rounds. There is an inherent exploration-exploitation trade-off in these systems where exploration refers to acquiring information about arms, while exploitation is optimizing for the immediate reward in the current round <cite class="ltx_cite ltx_citemacro_citep">(Christakopoulou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib14" title="">2016</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib66" title="">2019</a>)</cite>. This method has a natural setup in the CRS setting where items can be seen as arms and rounds as the conversation turns.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Asking about items directly can be inefficient, as large item sets would require several conversational turns and in turn increase the likelihood of users losing interest <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib16" title="">2021</a>)</cite>.
Alternatively, <em class="ltx_emph ltx_font_italic" id="S2.SS2.p2.1.1">attribute-based elicitation</em> aims to predict the next attribute to ask about. It is often cast as a sequence-to-sequence prediction problem, lending itself naturally to sequential neural networks <cite class="ltx_cite ltx_citemacro_citep">(Hochreiter and Schmidhuber, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib20" title="">1997</a>; Cho et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib12" title="">2014</a>)</cite>.
There has been an effort to create large datasets consisting of human conversations that can be used as training data. However, non-conversational data is often leveraged, especially when there is a lack of relevant information in the recorded dialogues <cite class="ltx_cite ltx_citemacro_citep">(Jannach et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib23" title="">2022</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_citet">Christakopoulou et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib13" title="">2018</a>)</cite> propose a question &amp; recommendation (Q&amp;R) method, utilize data from a non-conversational recommendation system, and develop surrogate tasks to answer questions: <em class="ltx_emph ltx_font_italic" id="S2.SS2.p2.1.2">What to ask?</em> and <em class="ltx_emph ltx_font_italic" id="S2.SS2.p2.1.3">How to respond?</em> To answer the first question, they develop a surrogate task where the goal is to predict the next likely topic a user would be interested in, based on recently watched videos.
The second question is answered by predicting what video the user would be most interested in, based on the most relevant predicted topic.
A similar approach of training a sequential neural network on non-conversational data is taken by <cite class="ltx_cite ltx_citemacro_citet">Zhang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib76" title="">2018</a>)</cite>, who convert Amazon reviews into artificial conversations. Sentences with aspect-value pairs are extracted from reviews and serve as utterances in one round of conversation. The extracted aspect-value pairs are modeled as user information needs.
The assumption is that the earlier aspect-value pairs appear in the review, the more important they are to the user, and thus should be prioritized as questions. Additionally, they develop a heuristic trigger to decide whether the model should ask about another attribute or recommend an item.
The drawback of these systems is they have no way of modeling the rejection of recommendations by the user, since the goal is to fit historical data as it happened. Furthermore, it is not possible to determine the reason behind the user interaction, i.e., why the user chose that particular item <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib16" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Another way to elicit preferences is in the form of <em class="ltx_emph ltx_font_italic" id="S2.SS2.p3.1.1">critiques</em>, i.e., feedback on attribute values of recommended items <cite class="ltx_cite ltx_citemacro_citep">(Chen and Pu, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib11" title="">2012</a>)</cite>.
For example, if the recommendation is for a <em class="ltx_emph ltx_font_italic" id="S2.SS2.p3.1.2">phone</em>, a critique might be <em class="ltx_emph ltx_font_italic" id="S2.SS2.p3.1.3">“not so big”</em> or <em class="ltx_emph ltx_font_italic" id="S2.SS2.p3.1.4">“something cheaper.”</em>
Such methods often employ heuristics as elicitation tactics <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib41" title="">2020b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib40" title="">a</a>)</cite>.
In recent work, <cite class="ltx_cite ltx_citemacro_citet">Balog et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib5" title="">2021</a>)</cite> study the problem of robustly interpreting unconstrained natural language feedback on attributes.
Our work differs from prior efforts in that we do not ask about specific attribute values directly, but instead ask indirect questions related to the planned use of an item.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">To help interactively search and navigate the space of item, <em class="ltx_emph ltx_font_italic" id="S2.SS2.p4.1.1">facet-based selection</em> is a commonly used interaction paradigm, especially in e-commerce <cite class="ltx_cite ltx_citemacro_citep">(Tunkelang, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib62" title="">2009</a>)</cite>.
Facets correspond to a particular way of grouping items, based on attribute-value combinations. For a given item category, facets may be identified by domain experts or sorted dynamically in order to allow for a quick
drill-down of the results <cite class="ltx_cite ltx_citemacro_citep">(Vandic et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib64" title="">2017</a>)</cite>.
Our work may be seen as a different way of clustering items, around item usage. However, different from facet selection, there is no linear constraint on a single facet—item usage maps to a subset of the attribute space, without the user necessarily knowing what the facets are. In practice, item selection often involves balancing a trade-off, e.g., a bike that is practical for daily usage and can be taken off-road occasionally. This type of selection can be done based on usage, but not with facets/attributes.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Question Generation</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">While there is research on end-to-end frameworks to enable CRSs to both understand user intentions as well as generate fluent and meaningful natural language responses <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib32" title="">2018</a>)</cite>, the predominant approach is still to use templates or construct the utterances using predefined language patterns <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib16" title="">2021</a>)</cite>.
In recent years, the broader field of dialogue systems has brought forth two additional strands of research applicable to CRSs as well: retrieval-based and generation-based methods <cite class="ltx_cite ltx_citemacro_citep">(Manzoor and Jannach, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib44" title="">2021</a>)</cite>.
Instead of relying on a handful of templates, <em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.1.1">retrieval-based methods</em> utilize a large collection of possible responses. The basic approach to retrieving the appropriate response is based on some notion of similarity between the user query and candidate responses, with the simplest being inner product <cite class="ltx_cite ltx_citemacro_citep">(Wu and Yan, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib72" title="">2019</a>)</cite>.
<em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.1.2">Generation-based methods</em> in dialogue systems are typically based on sequence-to-sequence modeling. These models are usually trained on a hand-labeled corpus of task-oriented dialogue <cite class="ltx_cite ltx_citemacro_citep">(Budzianowski et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib8" title="">2018</a>)</cite>.
Due to the limited amount of training data, <em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.1.3">delexicalization</em> is used to increase the generality of the systems. It is the process of disassociating specific words from the lexicon by replacing them in the training set with generic placeholders.
The sequence-to-sequence model is then trained to produce a delexicalized sentence (utterance skeleton) as output. To get the final sentence, the output utterance is <em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.1.4">relexicalized</em> based on user need <cite class="ltx_cite ltx_citemacro_citep">(Jurafsky and Martin, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib24" title="">2020</a>)</cite>.
Although retrieval-based approaches have been explored to a lesser extent than generation-based methods, their potential to leverage large, existing dialogue datasets to provide contextually relevant and high-quality responses has been demonstrated, resulting in an improved conversational user experience <cite class="ltx_cite ltx_citemacro_citep">(Manzoor and Jannach, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib44" title="">2021</a>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib45" title="">2022</a>)</cite>.
Our proposed approach shares elements of both of retrieval-based and generation-based methods: it generates questions using a sequence-to-sequence model and stores them in a collection that can be queried using retrieval-based methods. However, the task we focus on is fundamentally different. Namely, we are concerned with preference elicitation through the generation of implicit questions based on item usage, rather than simply responding to user queries or generating dialogue. This renders existing approaches inadequate for our task.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">The problem of preference elicitation is also related to that of clarification of information needs in information-seeking scenarios.
When searching for information, user queries are often ambiguous, faceted, or incomplete. To improve the user satisfaction, systems may decide not to provide an answer (e.g., based on their estimated confidence in the results) but instead proactively ask the user questions to clarify their needs <cite class="ltx_cite ltx_citemacro_citep">(Aliannejadi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib3" title="">2019</a>)</cite>. This is especially important in conversational information seeking scenarios, where the system can return only limited number of results due to the limited bandwith user interface. Similar to research in CRS, existing approaches to generating clarifying questions include retrieval-based methods <cite class="ltx_cite ltx_citemacro_citep">(Aliannejadi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib3" title="">2019</a>; Rao and Daumé III, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib54" title="">2018</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib73" title="">2020</a>)</cite> and generation-based methods <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib69" title="">2018</a>; Rao and Daumé III, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib55" title="">2019</a>)</cite>. Our work differs from this line of work in that instead of clarifying an already expressed need, we are trying to elicit a new user information need.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4. </span>Sequence-to-Sequence Modeling</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">The task of sequence-to-sequence models is to generate a sequence of output tokens conditioned on the input sequence. To generate high-quality output, transfer learning has proved to be a powerful technique. In transfer learning, a model is first pre-trained on a data-rich task, then fine-tuned on a downstream task. Early implementations used recurrent neural networks <cite class="ltx_cite ltx_citemacro_citep">(Peters et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib50" title="">2018</a>)</cite>, however, in recent years, the Transformer architecture is more commonly used <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib65" title="">2017</a>)</cite>.
Within the Transformer framework, three main variants emerged: encoder-only, decoder-only, and encoder-decoder models. Encoder-only models, like BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib15" title="">2019</a>)</cite>, are mainly used for classification.
On the other hand, for text generation tasks, decoder-only <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib52" title="">2019</a>)</cite> and encoder-decoder models <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib53" title="">2020</a>; Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib31" title="">2020</a>)</cite> are often employed.
One of the main differences of the two variants used for generation, apart from the architecture, is in the pre-training regime. Encoder-decoder models are generally pre-trained using causal masked token prediction, where a number of tokens in any position of the input sequence are masked and the model predicts the masked tokens based on the context. Decoder models, on the other hand, are pre-trained using a next token prediction strategy based on the input sequence plus the tokens predicted thus far.
Both training regimes are conducted in an unsupervised fashion, and the goal is to learn language syntax and semantics, and store that information in the model weights.
In this work, we apply sequence-to-sequence models to the question generation task with the goal of generating usage-related questions using different inputs (sentences or entire reviews) as context.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Approach</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Our objective is to understand users’ needs with minimal cognitive effort on their part. To overcome the shortcomings associated with item-based elicitation (large item space and slow narrowing of the recommendation candidates) and attribute-based elicitation (domain-specific knowledge required), we propose asking usage-related questions instead. These should be easier for users to answer and can thus lead to a better conversational user experience.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">As a first step toward that objective, in this work, we focus on the generation of implicit elicitation questions—implicit in the sense that we ask users about the intended use of items as opposed to soliciting the values of specific attributes. To generate usage-related questions, we leverage review corpora under the assumption that reviewers bring attention to item usage, where or how an item was used, and whether or not it was suitable for the intended purpose.
We want to identify item uses that occur sufficiently frequently and could be converted to a good question to present to a new user.
Item review datasets tend to be very large, with both the number of items and reviews in the thousands or even millions, making manual labeling the entire dataset extremely expensive <cite class="ltx_cite ltx_citemacro_citep">(Liao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib33" title="">2021</a>)</cite>.
To overcome this, we develop automated approaches that can take a review as input and generate one or multiple preference elicitation questions out of that, if it is possible.
We present multiple methods with an increasing degree of automation:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">We start with a <em class="ltx_emph ltx_font_italic" id="S3.I1.i1.p1.1.1">template-based</em> baseline approach that follows a pipeline of steps: first splitting reviews to sentences, then selecting sentences that mention some item-related activity or usage, and finally turning those sentences to questions using a pre-defined pattern (Section <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S3.SS1" title="3.1. Baseline 1: Template-based Question Generation ‣ 3. Approach ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">3.1</span></a>). This approach will always yield a question if the input sentence mentions an activity.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">Our second baseline extends the template-based approach by adding a classifier that is tasked with selecting only those sentences that could be converted to good questions. Otherwise, it still uses templates to construct questions from the selected sentences (Section <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S3.SS2" title="3.2. Baseline 2: Template-based Question Generation with Classification ‣ 3. Approach ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">3.2</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">Next, we introduce a <em class="ltx_emph ltx_font_italic" id="S3.I1.i3.p1.1.1">neural sentence-based</em> approach, which still operates on the sentence level, but handles activity detection and question generation in an end-to-end manner using a large pre-trained language model (Section <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S3.SS3" title="3.3. Neural Sentence-based Question Generation ‣ 3. Approach ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1">Finally, we present a <em class="ltx_emph ltx_font_italic" id="S3.I1.i4.p1.1.1">neural review-based</em> method, which takes an entire review as input and generates a review questions from that, if it is possible (Section <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S3.SS4" title="3.4. Neural Review-based Question Generation ‣ 3. Approach ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">3.4</span></a>).</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S3.p2.2">Figures <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S3.F2" title="Figure 2 ‣ 3.1. Baseline 1: Template-based Question Generation ‣ 3. Approach ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">2</span></a>–<a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S3.F4" title="Figure 4 ‣ 3.4. Neural Review-based Question Generation ‣ 3. Approach ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">4</span></a> present schematic overviews of the different methods.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Baseline 1: Template-based Question Generation</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S3.F2" title="Figure 2 ‣ 3.1. Baseline 1: Template-based Question Generation ‣ 3. Approach ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates the components of our <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.1">template-based question generation</em> (TQG) approach.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="236" id="S3.F2.g1" src="x2.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Components of our template-based question generation system.</figcaption>
</figure>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1. </span>Candidate Sentence Selection</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">We identify sentences that describe some item feature or aspect and mention some activity or usage. For example:</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.3">
<p class="ltx_p ltx_align_center" id="S3.SS1.SSS1.3.3"><span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS1.3.3.3" style="font-size:90%;">The <math alttext="\overbrace{\text{fat}}^{value}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.1.1.1.m1.1"><semantics id="S3.SS1.SSS1.1.1.1.m1.1a"><mover id="S3.SS1.SSS1.1.1.1.m1.1.1" xref="S3.SS1.SSS1.1.1.1.m1.1.1.cmml"><mover accent="true" id="S3.SS1.SSS1.1.1.1.m1.1.1.2" xref="S3.SS1.SSS1.1.1.1.m1.1.1.2.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS1.SSS1.1.1.1.m1.1.1.2.2" xref="S3.SS1.SSS1.1.1.1.m1.1.1.2.2a.cmml">fat</mtext><mo id="S3.SS1.SSS1.1.1.1.m1.1.1.2.1" xref="S3.SS1.SSS1.1.1.1.m1.1.1.2.1.cmml">⏞</mo></mover><mrow id="S3.SS1.SSS1.1.1.1.m1.1.1.3" xref="S3.SS1.SSS1.1.1.1.m1.1.1.3.cmml"><mi id="S3.SS1.SSS1.1.1.1.m1.1.1.3.2" xref="S3.SS1.SSS1.1.1.1.m1.1.1.3.2.cmml">v</mi><mo id="S3.SS1.SSS1.1.1.1.m1.1.1.3.1" xref="S3.SS1.SSS1.1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.1.1.1.m1.1.1.3.3" xref="S3.SS1.SSS1.1.1.1.m1.1.1.3.3.cmml">a</mi><mo id="S3.SS1.SSS1.1.1.1.m1.1.1.3.1a" xref="S3.SS1.SSS1.1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.1.1.1.m1.1.1.3.4" xref="S3.SS1.SSS1.1.1.1.m1.1.1.3.4.cmml">l</mi><mo id="S3.SS1.SSS1.1.1.1.m1.1.1.3.1b" xref="S3.SS1.SSS1.1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.1.1.1.m1.1.1.3.5" xref="S3.SS1.SSS1.1.1.1.m1.1.1.3.5.cmml">u</mi><mo id="S3.SS1.SSS1.1.1.1.m1.1.1.3.1c" xref="S3.SS1.SSS1.1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.1.1.1.m1.1.1.3.6" xref="S3.SS1.SSS1.1.1.1.m1.1.1.3.6.cmml">e</mi></mrow></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.1.1.1.m1.1b"><apply id="S3.SS1.SSS1.1.1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.1.1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.1.1.1.m1.1.1">superscript</csymbol><apply id="S3.SS1.SSS1.1.1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.1.1.1.m1.1.1.2"><ci id="S3.SS1.SSS1.1.1.1.m1.1.1.2.1.cmml" xref="S3.SS1.SSS1.1.1.1.m1.1.1.2.1">⏞</ci><ci id="S3.SS1.SSS1.1.1.1.m1.1.1.2.2a.cmml" xref="S3.SS1.SSS1.1.1.1.m1.1.1.2.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS1.SSS1.1.1.1.m1.1.1.2.2.cmml" xref="S3.SS1.SSS1.1.1.1.m1.1.1.2.2">fat</mtext></ci></apply><apply id="S3.SS1.SSS1.1.1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS1.1.1.1.m1.1.1.3"><times id="S3.SS1.SSS1.1.1.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS1.1.1.1.m1.1.1.3.1"></times><ci id="S3.SS1.SSS1.1.1.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS1.1.1.1.m1.1.1.3.2">𝑣</ci><ci id="S3.SS1.SSS1.1.1.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS1.1.1.1.m1.1.1.3.3">𝑎</ci><ci id="S3.SS1.SSS1.1.1.1.m1.1.1.3.4.cmml" xref="S3.SS1.SSS1.1.1.1.m1.1.1.3.4">𝑙</ci><ci id="S3.SS1.SSS1.1.1.1.m1.1.1.3.5.cmml" xref="S3.SS1.SSS1.1.1.1.m1.1.1.3.5">𝑢</ci><ci id="S3.SS1.SSS1.1.1.1.m1.1.1.3.6.cmml" xref="S3.SS1.SSS1.1.1.1.m1.1.1.3.6">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.1.1.1.m1.1c">\overbrace{\text{fat}}^{value}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.1.1.1.m1.1d">over⏞ start_ARG fat end_ARG start_POSTSUPERSCRIPT italic_v italic_a italic_l italic_u italic_e end_POSTSUPERSCRIPT</annotation></semantics></math> <math alttext="\overbrace{\text{tires}}^{aspect}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.2.2.2.m2.1"><semantics id="S3.SS1.SSS1.2.2.2.m2.1a"><mover id="S3.SS1.SSS1.2.2.2.m2.1.1" xref="S3.SS1.SSS1.2.2.2.m2.1.1.cmml"><mover accent="true" id="S3.SS1.SSS1.2.2.2.m2.1.1.2" xref="S3.SS1.SSS1.2.2.2.m2.1.1.2.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS1.SSS1.2.2.2.m2.1.1.2.2" xref="S3.SS1.SSS1.2.2.2.m2.1.1.2.2a.cmml">tires</mtext><mo id="S3.SS1.SSS1.2.2.2.m2.1.1.2.1" xref="S3.SS1.SSS1.2.2.2.m2.1.1.2.1.cmml">⏞</mo></mover><mrow id="S3.SS1.SSS1.2.2.2.m2.1.1.3" xref="S3.SS1.SSS1.2.2.2.m2.1.1.3.cmml"><mi id="S3.SS1.SSS1.2.2.2.m2.1.1.3.2" xref="S3.SS1.SSS1.2.2.2.m2.1.1.3.2.cmml">a</mi><mo id="S3.SS1.SSS1.2.2.2.m2.1.1.3.1" xref="S3.SS1.SSS1.2.2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.2.2.2.m2.1.1.3.3" xref="S3.SS1.SSS1.2.2.2.m2.1.1.3.3.cmml">s</mi><mo id="S3.SS1.SSS1.2.2.2.m2.1.1.3.1a" xref="S3.SS1.SSS1.2.2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.2.2.2.m2.1.1.3.4" xref="S3.SS1.SSS1.2.2.2.m2.1.1.3.4.cmml">p</mi><mo id="S3.SS1.SSS1.2.2.2.m2.1.1.3.1b" xref="S3.SS1.SSS1.2.2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.2.2.2.m2.1.1.3.5" xref="S3.SS1.SSS1.2.2.2.m2.1.1.3.5.cmml">e</mi><mo id="S3.SS1.SSS1.2.2.2.m2.1.1.3.1c" xref="S3.SS1.SSS1.2.2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.2.2.2.m2.1.1.3.6" xref="S3.SS1.SSS1.2.2.2.m2.1.1.3.6.cmml">c</mi><mo id="S3.SS1.SSS1.2.2.2.m2.1.1.3.1d" xref="S3.SS1.SSS1.2.2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.2.2.2.m2.1.1.3.7" xref="S3.SS1.SSS1.2.2.2.m2.1.1.3.7.cmml">t</mi></mrow></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.2.2.2.m2.1b"><apply id="S3.SS1.SSS1.2.2.2.m2.1.1.cmml" xref="S3.SS1.SSS1.2.2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.2.2.2.m2.1.1.1.cmml" xref="S3.SS1.SSS1.2.2.2.m2.1.1">superscript</csymbol><apply id="S3.SS1.SSS1.2.2.2.m2.1.1.2.cmml" xref="S3.SS1.SSS1.2.2.2.m2.1.1.2"><ci id="S3.SS1.SSS1.2.2.2.m2.1.1.2.1.cmml" xref="S3.SS1.SSS1.2.2.2.m2.1.1.2.1">⏞</ci><ci id="S3.SS1.SSS1.2.2.2.m2.1.1.2.2a.cmml" xref="S3.SS1.SSS1.2.2.2.m2.1.1.2.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS1.SSS1.2.2.2.m2.1.1.2.2.cmml" xref="S3.SS1.SSS1.2.2.2.m2.1.1.2.2">tires</mtext></ci></apply><apply id="S3.SS1.SSS1.2.2.2.m2.1.1.3.cmml" xref="S3.SS1.SSS1.2.2.2.m2.1.1.3"><times id="S3.SS1.SSS1.2.2.2.m2.1.1.3.1.cmml" xref="S3.SS1.SSS1.2.2.2.m2.1.1.3.1"></times><ci id="S3.SS1.SSS1.2.2.2.m2.1.1.3.2.cmml" xref="S3.SS1.SSS1.2.2.2.m2.1.1.3.2">𝑎</ci><ci id="S3.SS1.SSS1.2.2.2.m2.1.1.3.3.cmml" xref="S3.SS1.SSS1.2.2.2.m2.1.1.3.3">𝑠</ci><ci id="S3.SS1.SSS1.2.2.2.m2.1.1.3.4.cmml" xref="S3.SS1.SSS1.2.2.2.m2.1.1.3.4">𝑝</ci><ci id="S3.SS1.SSS1.2.2.2.m2.1.1.3.5.cmml" xref="S3.SS1.SSS1.2.2.2.m2.1.1.3.5">𝑒</ci><ci id="S3.SS1.SSS1.2.2.2.m2.1.1.3.6.cmml" xref="S3.SS1.SSS1.2.2.2.m2.1.1.3.6">𝑐</ci><ci id="S3.SS1.SSS1.2.2.2.m2.1.1.3.7.cmml" xref="S3.SS1.SSS1.2.2.2.m2.1.1.3.7">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.2.2.2.m2.1c">\overbrace{\text{tires}}^{aspect}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.2.2.2.m2.1d">over⏞ start_ARG tires end_ARG start_POSTSUPERSCRIPT italic_a italic_s italic_p italic_e italic_c italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> are perfect for <math alttext="\overbrace{\text{conquering tough terrain}}^{usage/activity}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.3.3.3.m3.1"><semantics id="S3.SS1.SSS1.3.3.3.m3.1a"><mover id="S3.SS1.SSS1.3.3.3.m3.1.1" xref="S3.SS1.SSS1.3.3.3.m3.1.1.cmml"><mover accent="true" id="S3.SS1.SSS1.3.3.3.m3.1.1.2" xref="S3.SS1.SSS1.3.3.3.m3.1.1.2.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS1.SSS1.3.3.3.m3.1.1.2.2" xref="S3.SS1.SSS1.3.3.3.m3.1.1.2.2a.cmml">conquering tough terrain</mtext><mo id="S3.SS1.SSS1.3.3.3.m3.1.1.2.1" xref="S3.SS1.SSS1.3.3.3.m3.1.1.2.1.cmml">⏞</mo></mover><mrow id="S3.SS1.SSS1.3.3.3.m3.1.1.3" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.cmml"><mrow id="S3.SS1.SSS1.3.3.3.m3.1.1.3.2" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.cmml"><mrow id="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.cmml"><mi id="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.2" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.2.cmml">u</mi><mo id="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.1" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.3" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.3.cmml">s</mi><mo id="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.1a" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.4" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.4.cmml">a</mi><mo id="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.1b" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.5" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.5.cmml">g</mi><mo id="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.1c" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.6" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.6.cmml">e</mi></mrow><mo id="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.1" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.1.cmml">/</mo><mi id="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.3" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.3.cmml">a</mi></mrow><mo id="S3.SS1.SSS1.3.3.3.m3.1.1.3.1" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.3.3.3.m3.1.1.3.3" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.3.cmml">c</mi><mo id="S3.SS1.SSS1.3.3.3.m3.1.1.3.1a" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.3.3.3.m3.1.1.3.4" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.4.cmml">t</mi><mo id="S3.SS1.SSS1.3.3.3.m3.1.1.3.1b" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.3.3.3.m3.1.1.3.5" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.5.cmml">i</mi><mo id="S3.SS1.SSS1.3.3.3.m3.1.1.3.1c" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.3.3.3.m3.1.1.3.6" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.6.cmml">v</mi><mo id="S3.SS1.SSS1.3.3.3.m3.1.1.3.1d" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.3.3.3.m3.1.1.3.7" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.7.cmml">i</mi><mo id="S3.SS1.SSS1.3.3.3.m3.1.1.3.1e" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.3.3.3.m3.1.1.3.8" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.8.cmml">t</mi><mo id="S3.SS1.SSS1.3.3.3.m3.1.1.3.1f" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.3.3.3.m3.1.1.3.9" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.9.cmml">y</mi></mrow></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.3.3.3.m3.1b"><apply id="S3.SS1.SSS1.3.3.3.m3.1.1.cmml" xref="S3.SS1.SSS1.3.3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.3.3.3.m3.1.1.1.cmml" xref="S3.SS1.SSS1.3.3.3.m3.1.1">superscript</csymbol><apply id="S3.SS1.SSS1.3.3.3.m3.1.1.2.cmml" xref="S3.SS1.SSS1.3.3.3.m3.1.1.2"><ci id="S3.SS1.SSS1.3.3.3.m3.1.1.2.1.cmml" xref="S3.SS1.SSS1.3.3.3.m3.1.1.2.1">⏞</ci><ci id="S3.SS1.SSS1.3.3.3.m3.1.1.2.2a.cmml" xref="S3.SS1.SSS1.3.3.3.m3.1.1.2.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS1.SSS1.3.3.3.m3.1.1.2.2.cmml" xref="S3.SS1.SSS1.3.3.3.m3.1.1.2.2">conquering tough terrain</mtext></ci></apply><apply id="S3.SS1.SSS1.3.3.3.m3.1.1.3.cmml" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3"><times id="S3.SS1.SSS1.3.3.3.m3.1.1.3.1.cmml" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.1"></times><apply id="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.cmml" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.2"><divide id="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.1.cmml" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.1"></divide><apply id="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.cmml" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2"><times id="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.1.cmml" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.1"></times><ci id="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.2.cmml" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.2">𝑢</ci><ci id="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.3.cmml" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.3">𝑠</ci><ci id="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.4.cmml" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.4">𝑎</ci><ci id="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.5.cmml" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.5">𝑔</ci><ci id="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.6.cmml" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.2.6">𝑒</ci></apply><ci id="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.3.cmml" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.2.3">𝑎</ci></apply><ci id="S3.SS1.SSS1.3.3.3.m3.1.1.3.3.cmml" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.3">𝑐</ci><ci id="S3.SS1.SSS1.3.3.3.m3.1.1.3.4.cmml" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.4">𝑡</ci><ci id="S3.SS1.SSS1.3.3.3.m3.1.1.3.5.cmml" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.5">𝑖</ci><ci id="S3.SS1.SSS1.3.3.3.m3.1.1.3.6.cmml" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.6">𝑣</ci><ci id="S3.SS1.SSS1.3.3.3.m3.1.1.3.7.cmml" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.7">𝑖</ci><ci id="S3.SS1.SSS1.3.3.3.m3.1.1.3.8.cmml" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.8">𝑡</ci><ci id="S3.SS1.SSS1.3.3.3.m3.1.1.3.9.cmml" xref="S3.SS1.SSS1.3.3.3.m3.1.1.3.9">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.3.3.3.m3.1c">\overbrace{\text{conquering tough terrain}}^{usage/activity}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.3.3.3.m3.1d">over⏞ start_ARG conquering tough terrain end_ARG start_POSTSUPERSCRIPT italic_u italic_s italic_a italic_g italic_e / italic_a italic_c italic_t italic_i italic_v italic_i italic_t italic_y end_POSTSUPERSCRIPT</annotation></semantics></math>.<span class="ltx_text ltx_font_serif" id="S3.SS1.SSS1.3.3.3.1"></span></span></p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Aspect-Value Pair Extraction</h5>
<div class="ltx_para" id="S3.SS1.SSS1.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px1.p1.1">An aspect in this context is a term that characterizes a particular feature of an item <cite class="ltx_cite ltx_citemacro_citep">(Lu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib39" title="">2011</a>)</cite> (e.g., <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.Px1.p1.1.1">wheel</em>, <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.Px1.p1.1.2">seat</em> or <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.Px1.p1.1.3">gear</em> are aspects of a bicycle). Value words are terms that describe an aspect (e.g., a <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.Px1.p1.1.4">wheel</em> might be <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.Px1.p1.1.5">large</em> or <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.Px1.p1.1.6">small</em>, a <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.Px1.p1.1.7">seat</em> can be <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.Px1.p1.1.8">hard</em> or <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.Px1.p1.1.9">comfortable</em>). Here, we extract all sentences that mention some aspect-value pair for a given category of items, using phrase-level sentiment analysis proposed by <cite class="ltx_cite ltx_citemacro_citet">Zhang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib77" title="">2014a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib78" title="">b</a>)</cite>.
The motivation for this step stems from the assumption that an activity or usage can be mapped to a particular aspect of an item.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>This concerns future utilization of responses given to these elicitation questions, where the CRS might want to map activities to specific attribute values.</span></span></span></p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Activity Identification</h5>
<div class="ltx_para" id="S3.SS1.SSS1.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px2.p1.1">In this step, the goal is to classify sentences that mention some item-related activity or usage. Inspired by <cite class="ltx_cite ltx_citemacro_citet">Benetka et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib6" title="">2019</a>)</cite>, our approach revolves around using <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.Px2.p1.1.1">part-of-speech</em> (POS) analysis and rules of the English language. We filter for the preposition <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.Px2.p1.1.2">for</em> followed by a verb in progressive tense heuristically, by looking for <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.Px2.p1.1.3">-ing</em> endings (e.g., <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.Px2.p1.1.4">for commuting</em>, <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.Px2.p1.1.5">for hiking</em>).
This choice is driven by our intuition and was verified by manually inspecting a sample of the data.
Note that there might be other formulations that describe activity or usage. Our goal is not to extract all possible sentences containing mentions of activity or usage; a high recall approach would likely come at the cost of a larger fraction of false positives. Instead, we focus on achieving high precision.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2. </span>Question Generation</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">The main motivation for this step is generating natural-sounding questions that are easy for users to understand and answer, without needing any additional context.
Consider the sentence <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p1.1.1">“The fat tires are perfect for conquering tough terrain.”</em> An example of converting it to a yes or no usage-related question might be <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p1.1.2">“Would you like a bike that is perfect for conquering tough terrain?”</em>
We approach this task using a template-based method, which is a common approach in CRS question generation <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib17" title="">2018</a>; Habib et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib18" title="">2020</a>; Zhang and Balog, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib75" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.1">There are many possible ways of articulating questions. To ensure that they are as natural-sounding as possible, we develop our template based on actual questions that humans formulated from review sentences. That is, we assume the presence of a training dataset consisting of sentence-question pairs, and inspect the most commonly appearing n-grams from the questions in that dataset. Specifically, in our training dataset (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S4" title="4. Data Collection ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">4</span></a>), we observe the following as the most frequent question pattern:</p>
<p class="ltx_p ltx_align_center" id="S3.SS1.SSS2.p2.2"><span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS2.p2.2.1" style="font-size:90%;">Are you looking for a [category] that is great for [usage]?<span class="ltx_text ltx_font_serif" id="S3.SS1.SSS2.p2.2.1.1"></span></span></p>
<p class="ltx_p" id="S3.SS1.SSS2.p2.3">An example question, based on this template, would be: <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p2.3.1">“Are you looking for a bike that is great for commuting?”</em></p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p3">
<p class="ltx_p" id="S3.SS1.SSS2.p3.1">Note that not all candidate sentences that pass our selection heuristic are viable for conversion to a question, e.g., <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS2.p3.1.1">“Thank you so much for coming up with such a great product.”</em> This sentence is too vague and does not mention any action or usage for the item, and thus should be labeled as not applicable (N/A). However, the simple template-based approach is not capable of making such distinction and would generate a question regardless.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Baseline 2: Template-based Question Generation with Classification</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">With our second model (TQG+CLS), we address some of the limitations of the first baseline model. Specifically, we aim to avoid generating questions that would not help with recommendations, because they would either be trivially answered affirmatively or would not make it easier to make a recommendation. Before generating a question, we classify the sentence as applicable or not applicable. If the sentence is not applicable, we do not generate a question.
To achieve this, we train a transformer-based classifier to predict whether a sentence is applicable or not. We choose RoBERTa <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib37" title="">2019</a>)</cite>, a high-performant BERT-based transformer model.
The input to the model is:</p>
<p class="ltx_p ltx_align_center" id="S3.SS2.p1.2"><span class="ltx_text ltx_font_typewriter" id="S3.SS2.p1.2.1" style="font-size:90%;">input_seq = &lt;cls&gt; [sentence] &lt;eos&gt;<span class="ltx_text ltx_font_serif" id="S3.SS2.p1.2.1.1"></span></span></p>
<p class="ltx_p" id="S3.SS2.p1.3">where <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p1.3.1">&lt;cls&gt;</span> and <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p1.3.2">&lt;eos&gt;</span> are special tokens that mark the beginning and end of the sequence, respectively. <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p1.3.3">cls</span> is used as an input to a simple linear and a softmax layer, whose output gives us probability distribution over the two classes: applicable and not applicable.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Neural Sentence-based Question Generation</h3>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="236" id="S3.F3.g1" src="x3.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Components of our neural sentence-based question generation system. The approach is similar to that of the template-based question generation, but instead of creating rigid templates, the model learns question patterns from the entire dataset automatically using a neural model.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">In our third model, <em class="ltx_emph ltx_font_italic" id="S3.SS3.p1.1.1">neural sentence-based question generation</em> (NSQG), depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S3.F3" title="Figure 3 ‣ 3.3. Neural Sentence-based Question Generation ‣ 3. Approach ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">3</span></a>, we further address some of the limitations of the template-based approach.
First, similarly to Baseline 2, we want to produce relevant questions for recommendations by avoiding those that are either easily answered affirmatively or do not contribute to facilitating a recommendation. For example, instead of generating the question <em class="ltx_emph ltx_font_italic" id="S3.SS3.p1.1.2">“Do you want a grill that is good for grilling certain things?,”</em> we want the model to output a special <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p1.1.3">[N/A]</span> (not applicable) token.
Second, we would want to generate a richer variety of natural-sounding questions.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">Learning to generate questions is done by fine-tuning a large, pre-trained, sequence-to-sequence language model.
There are two main benefits of using transfer learning from a pre-trained model. First, it increases the learning speed; as both syntax and semantics of the English language are already learned, there are fewer things the model needs to learn. Second, it reduces the amount of labeled data needed to train models to high performance. Specifically, we use T5 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib53" title="">2020</a>)</cite> as it has shown competitive performance across a variety of language generation tasks (e.g., conversational query rewriting <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib35" title="">2020</a>)</cite> and document re-ranking <cite class="ltx_cite ltx_citemacro_citep">(Pradeep et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib51" title="">2021</a>)</cite>), and can be used for both N/A-classification and generation, where N/A-classification is posed as a text-to-text problem.
We form the input to the T5 model as follows:</p>
<p class="ltx_p ltx_align_center" id="S3.SS3.p2.2"><span class="ltx_text ltx_font_typewriter" id="S3.SS3.p2.2.1" style="font-size:90%;">input_seq = Ask question: [category] &lt;sep&gt; [sentence] &lt;eos&gt;<span class="ltx_text ltx_font_serif" id="S3.SS3.p2.2.1.1"></span></span></p>
<p class="ltx_p" id="S3.SS3.p2.3">where “<span class="ltx_text ltx_font_typewriter" id="S3.SS3.p2.3.1">Ask question:</span>” is a task-specific prefix, and <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p2.3.2">&lt;sep&gt;</span> and <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p2.3.3">&lt;eos&gt;</span> are the separation and end-of-sequence tokens, respectively. Considering that T5 was pre-trained on various tasks, a task-specific prefix is used to specify which task the model should perform. The output of the model is either a question or the <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p2.3.4">[N/A]</span> token.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.4">We employ state-of-the-art techniques when performing model inference. Specifically, we use temperature-controlled stochastic sampling with top-<math alttext="k=25" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.1"><semantics id="S3.SS3.p3.1.m1.1a"><mrow id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mi id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml">k</mi><mo id="S3.SS3.p3.1.m1.1.1.1" xref="S3.SS3.p3.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml">25</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><eq id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1.1"></eq><ci id="S3.SS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2">𝑘</ci><cn id="S3.SS3.p3.1.m1.1.1.3.cmml" type="integer" xref="S3.SS3.p3.1.m1.1.1.3">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">k=25</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.1d">italic_k = 25</annotation></semantics></math> and top-<math alttext="p=0.90" class="ltx_Math" display="inline" id="S3.SS3.p3.2.m2.1"><semantics id="S3.SS3.p3.2.m2.1a"><mrow id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml"><mi id="S3.SS3.p3.2.m2.1.1.2" xref="S3.SS3.p3.2.m2.1.1.2.cmml">p</mi><mo id="S3.SS3.p3.2.m2.1.1.1" xref="S3.SS3.p3.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS3.p3.2.m2.1.1.3" xref="S3.SS3.p3.2.m2.1.1.3.cmml">0.90</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><apply id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1"><eq id="S3.SS3.p3.2.m2.1.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1.1"></eq><ci id="S3.SS3.p3.2.m2.1.1.2.cmml" xref="S3.SS3.p3.2.m2.1.1.2">𝑝</ci><cn id="S3.SS3.p3.2.m2.1.1.3.cmml" type="float" xref="S3.SS3.p3.2.m2.1.1.3">0.90</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">p=0.90</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.2.m2.1d">italic_p = 0.90</annotation></semantics></math> (nucleus) filtering. Top-<em class="ltx_emph ltx_font_italic" id="S3.SS3.p3.4.1">k</em> sampling restricts the sampling to consider only the <math alttext="25" class="ltx_Math" display="inline" id="S3.SS3.p3.3.m3.1"><semantics id="S3.SS3.p3.3.m3.1a"><mn id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><cn id="S3.SS3.p3.3.m3.1.1.cmml" type="integer" xref="S3.SS3.p3.3.m3.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">25</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.3.m3.1d">25</annotation></semantics></math> most likely next tokens. However, since some distributions from which tokens are sampled are flat while others are sharp, fixed <em class="ltx_emph ltx_font_italic" id="S3.SS3.p3.4.2">k</em> sampling is not optimal. To mitigate this shortcoming, nucleus filtering restricts the number of considered tokens to the minimum number of tokens whose total probability sums to <math alttext="p" class="ltx_Math" display="inline" id="S3.SS3.p3.4.m4.1"><semantics id="S3.SS3.p3.4.m4.1a"><mi id="S3.SS3.p3.4.m4.1.1" xref="S3.SS3.p3.4.m4.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m4.1b"><ci id="S3.SS3.p3.4.m4.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m4.1c">p</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.4.m4.1d">italic_p</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Neural Review-based Question Generation</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">The main motivation behind our last model, <em class="ltx_emph ltx_font_italic" id="S3.SS4.p1.1.1">neural review-based question generation</em> (NRQG), is to simplify the process of question generation.
The model is an extension of the previous sentence-based question generation (NSQG) model, except the input being an entire review instead of a single sentence.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="229" id="S3.F4.g1" src="x4.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Components of our neural review-based question generation system. The model drastically simplifies inference as we do not rely on heuristics to extract candidate sentences, but take entire reviews as input to generate questions.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">On a high level, review-based question generation is a two-step process: a text extraction step, to identify a review sentence, followed by a text generation step where the sentence is “translated” into a question.
That is, meaningful usage-related information first needs to be found in a longer text and then be converted into a question.
While sentence-based approaches use a heuristic for the first step and either a template (TQG) or a trained neural model (NSQG) for the second, with NRQG we simplify the pipeline considerably.
Using a neural architecture allows us to perform both steps jointly by fine-tuning a large pre-trained language model in an end-to-end fashion, as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S3.F4" title="Figure 4 ‣ 3.4. Neural Review-based Question Generation ‣ 3. Approach ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">The input to the NRQG model follows a similar structure to NSQG, except we replace the sentence with a review.</p>
<p class="ltx_p ltx_align_center" id="S3.SS4.p3.2"><span class="ltx_text ltx_font_typewriter" id="S3.SS4.p3.2.1" style="font-size:90%;">input_seq = Ask question: [category] &lt;sep&gt; [review] &lt;eos&gt;<span class="ltx_text ltx_font_serif" id="S3.SS4.p3.2.1.1"></span></span></p>
<p class="ltx_p" id="S3.SS4.p3.3">While many neural models have an input limitation, usually of 512 tokens, T5 has no such limitation. However, long input sequences drastically slow down generation, and the common practice is to avoid them. In our experiments, only a handful of reviews were longer than 512 tokens with the longest having 2000 tokens. If long reviews were common, a solution to processing longer reviews would be to use the same approach by splitting the reviews into manageable-sized chunks.
Another consideration is dealing with reviews that mention multiple possible uses for an item. This is not a common scenario, and there are indeed no examples of such cases in our dataset. Therefore, we make the simplifying assumption that at most a single question may be generated from one review.</p>
</div>
<div class="ltx_para" id="S3.SS4.p4">
<p class="ltx_p" id="S3.SS4.p4.1">It is important to note that while we use an existing model for text generation in both sentence-based and review-based models, obtaining high-quality labeled data for fine-tuning the model is a challenge on its own. As one of the contributions of this paper, we develop a multi-step data collection protocol using crowdsourcing, which we discuss in Section <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S4.SS2" title="4.2. Question Generation using Crowdsourcing ‣ 4. Data Collection ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">4.2</span></a>. Furthermore, while NRQG simplifies the modeling part considerably, it still relies on high-quality training data. The candidate sentence selections described in Section <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S4.SS1" title="4.1. Candidate Sentence Selection ‣ 4. Data Collection ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">4.1</span></a> is thus instrumental to facilitating data collection. In our experiments (in Section <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S6" title="6. Results and Analysis ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">6</span></a>), we will analyze the impact of the pre-trained language model (i.e., number of parameters) as well as the amount of training data available for fine-tuning.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Data Collection</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">This section describes the process of creating our dataset, which consists of a set of review sentences and either (i) a corresponding set of five preference elicitation questions or (ii) the label not applicable (N/A) for each. The data collection pipeline is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S4.F5" title="Figure 5 ‣ 4.1. Candidate Sentence Selection ‣ 4. Data Collection ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Candidate Sentence Selection</h3>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="189" id="S4.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Data collection pipeline, consisting of automatic candidate sentence extraction based on linguistic patterns and multi-step manual data annotation via crowdsourcing.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.2">The starting point for getting the candidate sentences are the Amazon review and metadata datasets <cite class="ltx_cite ltx_citemacro_citep">(Ni et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib48" title="">2019</a>)</cite>,<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://nijianmo.github.io/amazon/index.html</span></span></span> where item reviews from Amazon are extracted along with product metadata information such as <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.2.1">title</em>, <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.2.2">description</em>, <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.2.3">price</em>, and <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.2.4">categories</em>. There are, in total, <math alttext="233.1" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mn id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">233.1</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><cn id="S4.SS1.p1.1.m1.1.1.cmml" type="float" xref="S4.SS1.p1.1.m1.1.1">233.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">233.1</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">233.1</annotation></semantics></math>M reviews about <math alttext="15.5" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.1"><semantics id="S4.SS1.p1.2.m2.1a"><mn id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">15.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><cn id="S4.SS1.p1.2.m2.1.1.cmml" type="float" xref="S4.SS1.p1.2.m2.1.1">15.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">15.5</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.1d">15.5</annotation></semantics></math>M products. Due to the sheer dataset size, we focus our research on three main categories: <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.2.5">Home and Kitchen</em>; <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.2.6">Patio, Lawn and Garden</em>; and <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.2.7">Sports and Outdoors</em>. From these (40M reviews), we further sub-select 12 diverse subcategories (referred to as <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.2.8">categories</em> henceforth): <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.2.9">Backpacking Packs</em>, <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.2.10">Tents</em>, <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.2.11">Bikes</em>, <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.2.12">Jackets</em>, <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.2.13">Vacuums</em>, <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.2.14">Blenders</em>, <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.2.15">Espresso Machines</em>, <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.2.16">Grills</em>, <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.2.17">Walk-Behind Lawn Mowers</em>, <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.2.18">Birdhouses</em>, <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.2.19">Feeders</em>, and <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.2.20">Snow Shovels</em>. This narrowed down the number of reviews to 989k.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Sentence splitting and <em class="ltx_emph ltx_font_italic" id="S4.SS1.p2.1.1">aspect-value</em> pair extraction is performed using the Sentires toolkit <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib77" title="">2014a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib78" title="">b</a>)</cite>.<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://github.com/evison/Sentires</span></span></span> This step discards many non-viable sentences.
The remaining ones are POS-tagged using the Stanford NLP toolkit <cite class="ltx_cite ltx_citemacro_citep">(Manning et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib43" title="">2014</a>)</cite>.
Finally, we filter for sentences that match our activity detection heuristic (<em class="ltx_emph ltx_font_italic" id="S4.SS1.p2.1.2">“for + [verb in progressive tense]”</em>). After this step, we were left with 14,140 reviews.
Our sentence selection process is designed to favor precision over recall, and was validated by manual inspection of the results.
Upon completion of the crowdsourcing tasks (described in Section <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S4.SS2" title="4.2. Question Generation using Crowdsourcing ‣ 4. Data Collection ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">4.2</span></a>), we find that over 75% of the selected sentences can be turned into questions.
This shows that our simple method can indeed identify candidate sentences with relatively high precision.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Our final <em class="ltx_emph ltx_font_italic" id="S4.SS1.p3.1.1">candidate sentence set</em> contains approximately 100 sentences per category. An exception is the <em class="ltx_emph ltx_font_italic" id="S4.SS1.p3.1.2">Birdhouses</em> category, where only 15 candidate sentences are found due to the size of that category. In total, the candidate set consists of 1,098 sentences over 12 categories.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Question Generation using Crowdsourcing</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Crowdsourcing was done on the Amazon Mechanical Turk (AMT) platform in three steps. The task was available to workers with <math alttext="95\%" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">95</mn><mo id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1">percent</csymbol><cn id="S4.SS2.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS2.p1.1.m1.1.1.2">95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">95\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">95 %</annotation></semantics></math> approval rate and with at least 1,000 approved human intelligence tasks (HITs).</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span>Step 1: Question Collection</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">Crowd workers are given a review sentence (describing some aspect or use for a product) and a product category as input, and tasked with rewriting it into a question or marking it as not applicable.
They are specifically instructed to formulate a question that a salesperson or a recommender agent might ask a customer, such that it is a standalone question that can simply be answered with yes/no.
For every input sentence, we collected responses from three different workers. Sentences found non-applicable by at least two workers are set as N/A. The task was re-run if a single worker responded with N/A. This process resulted in approximately 2,600 sentence-question pairs.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span>Step 2: Validation and Filtering</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">Next, we validate all responses (i.e., generated questions) for applicable sentences collected in Step 1 using crowdsourcing.
We employ three different workers in Step 2, who are requested to answer four multiple-choice questions:
(1) <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS2.p1.1.1">Is the question grammatically correct?</em> [Yes/No]
(2) <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS2.p1.1.2">Can the question be answered by yes or no?</em> [Yes/No]
(3) <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS2.p1.1.3">Does the question mention any trait or use for a product?</em> [Yes/No]
(4) <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS2.p1.1.4">Who is most likely to ask this question in a sales setting?</em> [Buyer/Salesperson/Neither].
Generated questions that are found invalid by all three workers on a single aspect or at least two workers on at least two aspects are automatically rejected. Those that are marked invalid on multiple aspects but do not fall into the former category are manually checked by an expert annotator (one of the authors). All other questions are approved. Steps 1 and 2 were run multiple times until all questions were resolved.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3. </span>Step 3: Expanding Question Variety</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">Our main motivation for expanding the question variety is to add new ways of asking implicit questions. To this end, we task a new set of workers to paraphrase the questions we obtained and validated in Steps 1 and 2. Each worker receives all three versions of the questions from Step  1 as input and is asked to produce a new (paraphrased) question that expresses the same meaning. Note that this set of workers do not get to see the original sentences, only the questions generated from them by other workers. For each set of three questions, two additional paraphrases were collected. Considering that generating paraphrases proved to be a much simpler task than generating questions from review sentences, no additional quality assurance steps were necessary.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Final Dataset</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.3">Out of the 1,115 candidate sentences, 277 were labeled as non-applicable (not containing relevant usage-related information), which is below <math alttext="25\%" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mn id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">25</mn><mo id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1">percent</csymbol><cn id="S4.SS3.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS3.p1.1.m1.1.1.2">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">25\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">25 %</annotation></semantics></math>. This shows that our high-precision approach to selecting candidate sentences is effective. We note that our sentence selection method works better for some categories than for others. The fraction of viable sentences ranges between <math alttext="52\%" class="ltx_Math" display="inline" id="S4.SS3.p1.2.m2.1"><semantics id="S4.SS3.p1.2.m2.1a"><mrow id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mn id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml">52</mn><mo id="S4.SS3.p1.2.m2.1.1.1" xref="S4.SS3.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1.1">percent</csymbol><cn id="S4.SS3.p1.2.m2.1.1.2.cmml" type="integer" xref="S4.SS3.p1.2.m2.1.1.2">52</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">52\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.2.m2.1d">52 %</annotation></semantics></math> (<em class="ltx_emph ltx_font_italic" id="S4.SS3.p1.3.1">Espresso machine</em> category) to <math alttext="84\%" class="ltx_Math" display="inline" id="S4.SS3.p1.3.m3.1"><semantics id="S4.SS3.p1.3.m3.1a"><mrow id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml"><mn id="S4.SS3.p1.3.m3.1.1.2" xref="S4.SS3.p1.3.m3.1.1.2.cmml">84</mn><mo id="S4.SS3.p1.3.m3.1.1.1" xref="S4.SS3.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><apply id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1"><csymbol cd="latexml" id="S4.SS3.p1.3.m3.1.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1.1">percent</csymbol><cn id="S4.SS3.p1.3.m3.1.1.2.cmml" type="integer" xref="S4.SS3.p1.3.m3.1.1.2">84</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">84\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.3.m3.1d">84 %</annotation></semantics></math> (<em class="ltx_emph ltx_font_italic" id="S4.SS3.p1.3.2">Backpacking pack</em> category).
For the remaining 838 sentences, a total of five questions are generated, three based on the candidate sentence and two via paraphrasing. Table <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S4.T1" title="Table 1 ‣ 4.3. Final Dataset ‣ 4. Data Collection ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">1</span></a> shows two example sentences from our dataset.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Example sentence-question pairs from our dataset.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1" style="font-size:80%;">Category</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.1.2.1">
<span class="ltx_p" id="S4.T1.1.1.1.2.1.1"><span class="ltx_text" id="S4.T1.1.1.1.2.1.1.1" style="font-size:80%;">Blender</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.1.1.1" style="font-size:80%;">Sentence</span></th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T1.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.2.1.2.1">
<span class="ltx_p" id="S4.T1.1.2.1.2.1.1"><span class="ltx_text" id="S4.T1.1.2.1.2.1.1.1" style="font-size:80%;">Great for making smoothies with frozen fruit.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.3.2.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.1.1" style="font-size:80%;">Generated questions</span></th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T1.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.3.2.2.1">
<span class="ltx_p" id="S4.T1.1.3.2.2.1.1"><span class="ltx_text" id="S4.T1.1.3.2.2.1.1.1" style="font-size:80%;">-   Are you looking for a blender that’s great for making smoothies with frozen fruit?</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<th class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T1.1.4.3.1"></th>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S4.T1.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.4.3.2.1">
<span class="ltx_p" id="S4.T1.1.4.3.2.1.1"><span class="ltx_text" id="S4.T1.1.4.3.2.1.1.1" style="font-size:80%;">-   Would you be interested in a blender that is great for making smoothies with frozen fruit?</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.4">
<th class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T1.1.5.4.1"></th>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S4.T1.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.5.4.2.1">
<span class="ltx_p" id="S4.T1.1.5.4.2.1.1"><span class="ltx_text" id="S4.T1.1.5.4.2.1.1.1" style="font-size:80%;">-   Are you interested in a blender for making smoothies with frozen fruit?</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.6.5.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.6.5.1.1" style="font-size:80%;">Paraphrases</span></th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T1.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.6.5.2.1">
<span class="ltx_p" id="S4.T1.1.6.5.2.1.1"><span class="ltx_text" id="S4.T1.1.6.5.2.1.1.1" style="font-size:80%;">-   Do you want a blender that’s great for making smoothies with frozen fruit?</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.6">
<th class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T1.1.7.6.1"></th>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S4.T1.1.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.7.6.2.1">
<span class="ltx_p" id="S4.T1.1.7.6.2.1.1"><span class="ltx_text" id="S4.T1.1.7.6.2.1.1.1" style="font-size:80%;">-   Would you like a blender that is great for making smoothies with frozen fruit?</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt" id="S4.T1.1.8.7.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.8.7.1.1" style="font-size:80%;">Category</span></th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S4.T1.1.8.7.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.8.7.2.1">
<span class="ltx_p" id="S4.T1.1.8.7.2.1.1"><span class="ltx_text" id="S4.T1.1.8.7.2.1.1.1" style="font-size:80%;">Snow shovel</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.9.8.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.9.8.1.1" style="font-size:80%;">Sentence</span></th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T1.1.9.8.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.9.8.2.1">
<span class="ltx_p" id="S4.T1.1.9.8.2.1.1"><span class="ltx_text" id="S4.T1.1.9.8.2.1.1.1" style="font-size:80%;">This product is excellent for doing the job</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.10.9.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.10.9.1.1" style="font-size:80%;">Generated questions</span></th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T1.1.10.9.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.10.9.2.1">
<span class="ltx_p" id="S4.T1.1.10.9.2.1.1"><span class="ltx_text" id="S4.T1.1.10.9.2.1.1.1" style="font-size:80%;">n/a</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.11.10">
<th class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T1.1.11.10.1"></th>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S4.T1.1.11.10.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.11.10.2.1">
<span class="ltx_p" id="S4.T1.1.11.10.2.1.1"><span class="ltx_text" id="S4.T1.1.11.10.2.1.1.1" style="font-size:80%;">n/a</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.12.11">
<th class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T1.1.12.11.1"></th>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S4.T1.1.12.11.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.12.11.2.1">
<span class="ltx_p" id="S4.T1.1.12.11.2.1.1"><span class="ltx_text" id="S4.T1.1.12.11.2.1.1.1" style="font-size:80%;">n/a</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.13.12.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.13.12.1.1" style="font-size:80%;">Paraphrases</span></th>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.13.12.2"></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Experimental Setup</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This section presents the experimental setup for the three methods explored in this paper. We evaluate our models using standard automatic metrics for evaluating text generation. We also perform human evaluation via a set of crowdsourcing studies to assess question quality across multiple dimensions.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Question Generation</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">For our neural approaches (NSQG and NRQG), we train <em class="ltx_emph ltx_font_italic" id="S5.SS1.p1.1.1">small</em>, <em class="ltx_emph ltx_font_italic" id="S5.SS1.p1.1.2">base</em>, and <em class="ltx_emph ltx_font_italic" id="S5.SS1.p1.1.3">large</em> T5 models, which vary in the number of layers, self-attention heads, and the dimension of the final feedforward layer. The difference is shown in the number of parameters in Table <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S6.T5" title="Table 5 ‣ 6.3. Model Size ‣ 6. Results and Analysis ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">5</span></a>. We use <math alttext="80\%" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1"><semantics id="S5.SS1.p1.1.m1.1a"><mrow id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><mn id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml">80</mn><mo id="S5.SS1.p1.1.m1.1.1.1" xref="S5.SS1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><csymbol cd="latexml" id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1">percent</csymbol><cn id="S5.SS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S5.SS1.p1.1.m1.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">80\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">80 %</annotation></semantics></math> of the data for training, while the rest is test data. In our training, we employ teacher forcing <cite class="ltx_cite ltx_citemacro_citep">(Williams and Zipser, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib70" title="">1989</a>)</cite>, regularization by early stopping <cite class="ltx_cite ltx_citemacro_citep">(Morgan and Bourlard, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib46" title="">1989</a>)</cite>, and adaptive gradient method AdamW <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov and Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib38" title="">2022</a>)</cite> with linear learning rate decay.
For each sentence, we have either N/A or a set of reference questions as ground truth.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Automatic Evaluation</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We evaluate question generation as a classification task in terms of Accuracy (detecting N/A), and as a machine translation task, where the set of human-generated questions serve as reference translations.
Specifically, we report on BLEU-4, which uses modified n-gram precision up to 4-grams <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib49" title="">2002</a>)</cite>, and ROUGE-L, a recall-based metric based on the longest common subsequence <cite class="ltx_cite ltx_citemacro_citep">(Lin, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib34" title="">2004</a>)</cite>. Additionally, we report METEOR, which has been found to have a better correlation with human judgments compared to BLEU and ROUGE <cite class="ltx_cite ltx_citemacro_citep">(Lavie and Agarwal, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib27" title="">2007</a>)</cite>. It does this by considering word stems, WordNet synonyms, and paraphrases in addition to n-gram overlap.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">While evaluating sentence-based models (TQG and NSQG) is straightforward, there is a detail we have to consider when generating and evaluating questions using the review-based (NRQG) method. Each review may contain multiple sentences mentioning usage that could potentially be used to generate questions. However, in our dataset we do not have any such instances (i.e., no two sentences happen to come from the same review). While this is not by design, intuitively it makes sense that people do not discuss multiple usages of an item within a single review.
Therefore, we can evaluate the NRQG model exactly the same way we evaluate the TQG and NSQG models—that is, for each input review we expect a single generated question. The generated question is then compared to the ground truth questions in the dataset.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Human Evaluation</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Recent studies have shown that automatic measures often have a low correlation with human judgement <cite class="ltx_cite ltx_citemacro_citep">(Sai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib57" title="">2022</a>; Mairesse et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib42" title="">2010</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib36" title="">2016</a>; Callison-Burch et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib9" title="">2006</a>; Sekulić et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib59" title="">2021</a>)</cite>. To thoroughly investigate the differences between our question generation models, we additionally evaluate them by human assessors. Human evaluation of natural language generation is most commonly done with respect to a single dimension, however, it has been observed that there are many aspects of language generation that cannot be captured in a single metric <cite class="ltx_cite ltx_citemacro_citep">(van der Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib63" title="">2021</a>)</cite>. In our work, we consider three quality dimensions: grammar and fluency, usability, and answerability.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">We compare the generated questions both on their own (<em class="ltx_emph ltx_font_italic" id="S5.SS3.p2.1.1">pointwise</em> evaluation) and relative to each other (<em class="ltx_emph ltx_font_italic" id="S5.SS3.p2.1.2">pairwise</em> evaluation) with the help of crowd workers.
Research suggests that pairwise comparison might be more reliable <cite class="ltx_cite ltx_citemacro_citep">(Carterette et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib10" title="">2008</a>)</cite>, however, the cost of evaluation increases with the number of models.
Another reason for performing pointwise evaluation is that it yields an <em class="ltx_emph ltx_font_italic" id="S5.SS3.p2.1.3">absolute</em> measure by averaging over a set of questions. Pairwise evaluation, on the other hand, can only establish a <em class="ltx_emph ltx_font_italic" id="S5.SS3.p2.1.4">relative</em> ordering between two approaches.
Nevertheless, both absolute and relative measurements can be insightful and we are particularly interested in seeing if the observations we can draw from them are in alignment. In both cases, we focus on three different aspects of the questions, which in combination describe their <em class="ltx_emph ltx_font_italic" id="S5.SS3.p2.1.5">naturalness</em>.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">An important aspect of conversational systems is to generate fluent, coherent, and grammatically correct utterances <cite class="ltx_cite ltx_citemacro_citep">(Anand et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib4" title="">2020</a>)</cite>. Therefore, the first evaluation dimension focuses on <em class="ltx_emph ltx_font_italic" id="S5.SS3.p3.1.1">grammar and fluency.</em> In pointwise evaluation, we ask <em class="ltx_emph ltx_font_italic" id="S5.SS3.p3.1.2">“Is the question fluent and grammatically correct?,”</em> while in the pairwise case we ask <em class="ltx_emph ltx_font_italic" id="S5.SS3.p3.1.3">“Which question is more fluent and grammatically correct?”</em> Another important aspect when generating questions that are supposed to elicit user preferences is <em class="ltx_emph ltx_font_italic" id="S5.SS3.p3.1.4">usefulness</em>. <cite class="ltx_cite ltx_citemacro_citet">Rosset et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib56" title="">2020</a>)</cite> introduce the concept of usefulness in conversational search and describe it as a measurement for how well a suggestion leads the user to useful information. In our case, we are trying to evaluate how useful the question is in making a good recommendation. In other words, does answering the question help with giving a better recommendation?
We ask (pointwise) <em class="ltx_emph ltx_font_italic" id="S5.SS3.p3.1.5">“If you were making a recommendation for a friend, would knowing the answer be useful for you to make a better recommendation?”</em> or (pairwise) <em class="ltx_emph ltx_font_italic" id="S5.SS3.p3.1.6">“If you were making a recommendation for a friend, the answer to which question would be more useful for you to make a better recommendation?”</em> The final aspect we explore is that of <em class="ltx_emph ltx_font_italic" id="S5.SS3.p3.1.7">answerability</em>, i.e., how easy or difficult it is for the user to answer the question. Is the question ambiguous or straightforward? For example, a question <em class="ltx_emph ltx_font_italic" id="S5.SS3.p3.1.8">“Are you looking for a snow shovel that is extremely good snow shovel for Wyoming?”</em> might be easy to answer for most people living in Wyoming or if we know what the typical winter is like there. However, this kind of question is very specific and difficult to answer for most people outside Wyoming. In pointwise evaluation, we ask <em class="ltx_emph ltx_font_italic" id="S5.SS3.p3.1.9">“Would you expect someone looking for a recommendation to be able to answer this question easily?,”</em> while in pairwise evaluation, we ask <em class="ltx_emph ltx_font_italic" id="S5.SS3.p3.1.10">“Which question is easier to answer when looking for a recommendation?”</em></p>
</div>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="168" id="S5.F6.g1" src="extracted/6345374/figures/pointwise_Q3.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>Example question for pointwise evaluation. The specific task addresses the answerability of the question in the context of providing better recommendations.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1">In pointwise evaluation we solicit answers on a 5-point Likert scale. An example can be seen in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S5.F6" title="Figure 6 ‣ 5.3. Human Evaluation ‣ 5. Experimental Setup ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">6</span></a> where the responses range from “definitely not” to “definitely.”
In pairwise evaluation we also employ a 5-point scale, where the two ends of the spectrum correspond to strong preferences for each question, with gradually weaker preferences in between and “no preference” in the middle. An example pairwise evaluation task is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S5.F7" title="Figure 7 ‣ 5.3. Human Evaluation ‣ 5. Experimental Setup ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS3.p5">
<p class="ltx_p" id="S5.SS3.p5.1">We use crowdsourcing on Amazon Mechanical Turk to evaluate the generated questions in our study. Each question is annotated by three different workers, all based in the United States or Great Britain, with a minimum approval rate of 95%, and a minimum number of accepted HITs 1000. We take the mean of the three annotations as the final score for each question.</p>
</div>
<figure class="ltx_figure" id="S5.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="158" id="S5.F7.g1" src="extracted/6345374/figures/pairwise_Q2.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>Example question for pairwise evaluation. The specific task addresses the usefulness of the presented question.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Results and Analysis</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">The main research question we wish to answer with our experiments is the following: <em class="ltx_emph ltx_font_italic" id="S6.p1.1.1">Given a review from a corpus, can item-usage questions for preference elicitation be automatically generated?</em> To address this question, we break the problem down into more specific research questions:</p>
<ul class="ltx_itemize" id="S6.I1">
<li class="ltx_item" id="S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i1.p1">
<p class="ltx_p" id="S6.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i1.p1.1.1">RQ1</span> Can neural models generate more natural questions when compared with template-based baselines?</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i2.p1">
<p class="ltx_p" id="S6.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i2.p1.1.1">RQ2</span> How does (a) the size of the pre-trained language model and (b) the volume of available training data affect the performance of the neural models?</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S6.p1.2">Specifically, given a review as input, our approaches should either generate a question or label it as not applicable (N/A) if a usage-related question cannot be generated or where the generated question would not be useful.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Automatic Evaluation</h3>
<figure class="ltx_table" id="S6.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>Performance comparison of different question generation models: template-based (TQG), neural sentence-based (NSQG), and neural review-based (NRQG). All models utilize all available training data (i.e., five questions or N/A per sentence). The best scores for each measure are in boldface. The symbols <sup class="ltx_sup" id="S6.T2.18.1">∗</sup> and <sup class="ltx_sup" id="S6.T2.19.2">+</sup> denote statistically significant improvements over the two baselines, respectively (p-value <math alttext="&lt;0.05" class="ltx_Math" display="inline" id="S6.T2.6.m3.1"><semantics id="S6.T2.6.m3.1b"><mrow id="S6.T2.6.m3.1.1" xref="S6.T2.6.m3.1.1.cmml"><mi id="S6.T2.6.m3.1.1.2" xref="S6.T2.6.m3.1.1.2.cmml"></mi><mo id="S6.T2.6.m3.1.1.1" xref="S6.T2.6.m3.1.1.1.cmml">&lt;</mo><mn id="S6.T2.6.m3.1.1.3" xref="S6.T2.6.m3.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.6.m3.1c"><apply id="S6.T2.6.m3.1.1.cmml" xref="S6.T2.6.m3.1.1"><lt id="S6.T2.6.m3.1.1.1.cmml" xref="S6.T2.6.m3.1.1.1"></lt><csymbol cd="latexml" id="S6.T2.6.m3.1.1.2.cmml" xref="S6.T2.6.m3.1.1.2">absent</csymbol><cn id="S6.T2.6.m3.1.1.3.cmml" type="float" xref="S6.T2.6.m3.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.6.m3.1d">&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S6.T2.6.m3.1e">&lt; 0.05</annotation></semantics></math>). Statistical significance for accuracy is measured using McNemar’s test, while for BLEU, ROUGE, and METEOR we use paired bootstrap resampling <cite class="ltx_cite ltx_citemacro_citep">(Koehn, <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#bib.bib25" title="">2004</a>)</cite>.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T2.15">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T2.15.10.1">
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.T2.15.10.1.1"><span class="ltx_text ltx_font_bold" id="S6.T2.15.10.1.1.1">Model</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.T2.15.10.1.2"><span class="ltx_text ltx_font_bold" id="S6.T2.15.10.1.2.1">N/A Accuracy</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S6.T2.15.10.1.3"><span class="ltx_text ltx_font_bold" id="S6.T2.15.10.1.3.1">BLEU-4</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S6.T2.15.10.1.4"><span class="ltx_text ltx_font_bold" id="S6.T2.15.10.1.4.1">ROUGE-L</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S6.T2.15.10.1.5"><span class="ltx_text ltx_font_bold" id="S6.T2.15.10.1.5.1">METEOR</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T2.15.11.1">
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T2.15.11.1.1">Baseline 1 (TQG)</th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T2.15.11.1.2">0.728</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.15.11.1.3">0.604</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.15.11.1.4">0.723</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.15.11.1.5">0.418</td>
</tr>
<tr class="ltx_tr" id="S6.T2.7.1">
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.1.2">Baseline 2 (TQG+CLS)</th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.1.1"><span class="ltx_text ltx_font_bold" id="S6.T2.7.1.1.1">0.870<sup class="ltx_sup" id="S6.T2.7.1.1.1.1"><span class="ltx_text ltx_font_medium" id="S6.T2.7.1.1.1.1.1">∗</span></sup></span></th>
<td class="ltx_td ltx_align_left" id="S6.T2.7.1.3">0.607</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.1.4">0.727</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.1.5">0.420</td>
</tr>
<tr class="ltx_tr" id="S6.T2.11.5">
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S6.T2.11.5.5">NSQG</th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" id="S6.T2.8.2.1">0.858<sup class="ltx_sup" id="S6.T2.8.2.1.1">∗</sup>
</th>
<td class="ltx_td ltx_align_left" id="S6.T2.9.3.2"><span class="ltx_text ltx_font_bold" id="S6.T2.9.3.2.1">0.730<sup class="ltx_sup" id="S6.T2.9.3.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S6.T2.9.3.2.1.1.1">∗+</span></sup></span></td>
<td class="ltx_td ltx_align_left" id="S6.T2.10.4.3"><span class="ltx_text ltx_font_bold" id="S6.T2.10.4.3.1">0.806<sup class="ltx_sup" id="S6.T2.10.4.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S6.T2.10.4.3.1.1.1">∗+</span></sup></span></td>
<td class="ltx_td ltx_align_left" id="S6.T2.11.5.4"><span class="ltx_text ltx_font_bold" id="S6.T2.11.5.4.1">0.494<sup class="ltx_sup" id="S6.T2.11.5.4.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S6.T2.11.5.4.1.1.1">∗+</span></sup></span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.15.9">
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T2.15.9.5">NRQG</th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T2.12.6.1">0.832<sup class="ltx_sup" id="S6.T2.12.6.1.1">∗</sup>
</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T2.13.7.2">0.684<sup class="ltx_sup" id="S6.T2.13.7.2.1"><span class="ltx_text ltx_font_italic" id="S6.T2.13.7.2.1.1">∗+</span></sup>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T2.14.8.3">0.769<sup class="ltx_sup" id="S6.T2.14.8.3.1"><span class="ltx_text ltx_font_italic" id="S6.T2.14.8.3.1.1">∗+</span></sup>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T2.15.9.4">0.466<sup class="ltx_sup" id="S6.T2.15.9.4.1"><span class="ltx_text ltx_font_italic" id="S6.T2.15.9.4.1.1">∗+</span></sup>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">First, we compare the neural models with the two baseline models using automatic evaluation (RQ1). We train <em class="ltx_emph ltx_font_italic" id="S6.SS1.p1.1.1">T5-large</em> for both NSQG and NRQG as it was found to be the most effective model for the task. The results are reported in Table <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S6.T2" title="Table 2 ‣ 6.1. Automatic Evaluation ‣ 6. Results and Analysis ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">2</span></a>. We find that both neural models significantly outperform the template-based models on all generation evaluation metrics. This is expected, as neural models are capable of using both syntax and semantics present in the original sentence when generating questions. Unsurprisingly, they significantly outperform Baseline 1 on the classification task as well. Baseline 2 achieves the highest accuracy, suggesting that a dedicated classifier performs better than a general-purpose model on the classification task.
We also observe that the sentence-based (NSQG) model outperforms the review-based one (NRQG) on all metrics. This is unsurprising as the NSQG model has a simpler task to perform, as it receives the already extracted candidate sentences as input. Note that the accuracy of the review-based model is much higher than that of the template-based model, and only slightly worse than that of the sentence-based model, which suggests that despite the larger (and arguably noisier) input, the model can predict with high accuracy if useful questions can be generated.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Human evaluation</h3>
<figure class="ltx_table" id="S6.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3. </span>Pointwise evaluation of our three models for each quality dimension, on a scale of 1 to 5. The best scores for each measure are in boldface. The symbols <sup class="ltx_sup" id="S6.T3.16.1">∗</sup> and <sup class="ltx_sup" id="S6.T3.17.2">+</sup> denote statistically significance improvements over the two baselines, respectively (p-value <math alttext="&lt;0.05" class="ltx_Math" display="inline" id="S6.T3.6.m3.1"><semantics id="S6.T3.6.m3.1b"><mrow id="S6.T3.6.m3.1.1" xref="S6.T3.6.m3.1.1.cmml"><mi id="S6.T3.6.m3.1.1.2" xref="S6.T3.6.m3.1.1.2.cmml"></mi><mo id="S6.T3.6.m3.1.1.1" xref="S6.T3.6.m3.1.1.1.cmml">&lt;</mo><mn id="S6.T3.6.m3.1.1.3" xref="S6.T3.6.m3.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T3.6.m3.1c"><apply id="S6.T3.6.m3.1.1.cmml" xref="S6.T3.6.m3.1.1"><lt id="S6.T3.6.m3.1.1.1.cmml" xref="S6.T3.6.m3.1.1.1"></lt><csymbol cd="latexml" id="S6.T3.6.m3.1.1.2.cmml" xref="S6.T3.6.m3.1.1.2">absent</csymbol><cn id="S6.T3.6.m3.1.1.3.cmml" type="float" xref="S6.T3.6.m3.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.6.m3.1d">&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S6.T3.6.m3.1e">&lt; 0.05</annotation></semantics></math>), measured using a non-parametric Mann-Whitney U test.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T3.13">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T3.13.8.1">
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S6.T3.13.8.1.1"><span class="ltx_text ltx_font_bold" id="S6.T3.13.8.1.1.1">Model</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S6.T3.13.8.1.2"><span class="ltx_text ltx_font_bold" id="S6.T3.13.8.1.2.1">Grammar and Fluency</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S6.T3.13.8.1.3"><span class="ltx_text ltx_font_bold" id="S6.T3.13.8.1.3.1">Usefulness</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S6.T3.13.8.1.4"><span class="ltx_text ltx_font_bold" id="S6.T3.13.8.1.4.1">Answerability</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T3.13.9.1">
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S6.T3.13.9.1.1">Baseline 1 (TQG)</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S6.T3.13.9.1.2">3.69</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T3.13.9.1.3">3.48</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T3.13.9.1.4">3.71</td>
</tr>
<tr class="ltx_tr" id="S6.T3.8.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T3.8.2.3">Baseline 2 (TQG+CLS)</td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S6.T3.7.1.1">3.85<sup class="ltx_sup" id="S6.T3.7.1.1.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S6.T3.8.2.2">3.67<sup class="ltx_sup" id="S6.T3.8.2.2.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left" id="S6.T3.8.2.4">3.86</td>
</tr>
<tr class="ltx_tr" id="S6.T3.11.5">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T3.11.5.4">NSQG</td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S6.T3.9.3.1"><span class="ltx_text ltx_font_bold" id="S6.T3.9.3.1.1">4.02<sup class="ltx_sup" id="S6.T3.9.3.1.1.1"><span class="ltx_text ltx_font_medium" id="S6.T3.9.3.1.1.1.1">∗</span></sup></span></td>
<td class="ltx_td ltx_align_left" id="S6.T3.10.4.2"><span class="ltx_text ltx_font_bold" id="S6.T3.10.4.2.1">3.81<sup class="ltx_sup" id="S6.T3.10.4.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S6.T3.10.4.2.1.1.1">∗+</span></sup></span></td>
<td class="ltx_td ltx_align_left" id="S6.T3.11.5.3"><span class="ltx_text ltx_font_bold" id="S6.T3.11.5.3.1">3.98<sup class="ltx_sup" id="S6.T3.11.5.3.1.1"><span class="ltx_text ltx_font_medium" id="S6.T3.11.5.3.1.1.1">∗</span></sup></span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.13.7">
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="S6.T3.13.7.3">NRQG</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_bb" id="S6.T3.13.7.4">3.80</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T3.12.6.1">3.73<sup class="ltx_sup" id="S6.T3.12.6.1.1">∗</sup>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T3.13.7.2">3.93<sup class="ltx_sup" id="S6.T3.13.7.2.1">∗</sup>
</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S6.F8"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="277" id="S6.F8.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>Pointwise evaluation of the NSQG model per category. Catrgories are sorted by the usefulness score.</figcaption>
</figure>
<figure class="ltx_table" id="S6.T4">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4. </span>Pairwise evaluation according to different quality dimensions. The main values are percentages of how often the model in the row wins over the model in the column. The value in brackets are percentages of how often the model is strongly preferred.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<table class="ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="S6.T4.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T4.3.1.1">
<th class="ltx_td ltx_align_center ltx_align_middle ltx_th ltx_th_column ltx_th_row" colspan="4" id="S6.T4.3.1.1.1"><em class="ltx_emph ltx_font_italic" id="S6.T4.3.1.1.1.1" style="font-size:90%;">Grammar and fluency</em></th>
</tr>
<tr class="ltx_tr" id="S6.T4.3.2.2">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S6.T4.3.2.2.1" style="width:19.1pt;"></th>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_th ltx_th_column ltx_border_t" colspan="3" id="S6.T4.3.2.2.2"><span class="ltx_text ltx_font_bold" id="S6.T4.3.2.2.2.1" style="font-size:90%;">Wins over</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T4.3.3.1">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r" id="S6.T4.3.3.1.1" style="width:19.1pt;"></th>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.3.3.1.2" style="width:21.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.3.3.1.2.1">
<span class="ltx_p" id="S6.T4.3.3.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.3.3.1.2.1.1.1" style="font-size:90%;">TQG</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.3.3.1.3" style="width:21.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.3.3.1.3.1">
<span class="ltx_p" id="S6.T4.3.3.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.3.3.1.3.1.1.1" style="font-size:90%;">NSQG</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.3.3.1.4" style="width:21.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.3.3.1.4.1">
<span class="ltx_p" id="S6.T4.3.3.1.4.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.3.3.1.4.1.1.1" style="font-size:90%;">NRQG</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T4.3.4.2">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T4.3.4.2.1" style="width:19.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.3.4.2.1.1">
<span class="ltx_p" id="S6.T4.3.4.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.3.4.2.1.1.1.1" style="font-size:90%;">TQG</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S6.T4.3.4.2.2" style="width:21.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.3.4.2.2.1">
<span class="ltx_p" id="S6.T4.3.4.2.2.1.1"><span class="ltx_text" id="S6.T4.3.4.2.2.1.1.1" style="font-size:90%;">–</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S6.T4.3.4.2.3" style="width:21.1pt;background-color:#9BD8B9;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.3.4.2.3.1">
<span class="ltx_p" id="S6.T4.3.4.2.3.1.1"><span class="ltx_text" id="S6.T4.3.4.2.3.1.1.1" style="font-size:90%;">31% </span><span class="ltx_text" id="S6.T4.3.4.2.3.1.1.2" style="font-size:70%;">(10%)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S6.T4.3.4.2.4" style="width:21.1pt;background-color:#79CBC5;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.3.4.2.4.1">
<span class="ltx_p" id="S6.T4.3.4.2.4.1.1"><span class="ltx_text" id="S6.T4.3.4.2.4.1.1.1" style="font-size:90%;">38% </span><span class="ltx_text" id="S6.T4.3.4.2.4.1.1.2" style="font-size:70%;">(16%)</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T4.3.5.3">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r" id="S6.T4.3.5.3.1" style="width:19.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.3.5.3.1.1">
<span class="ltx_p" id="S6.T4.3.5.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.3.5.3.1.1.1.1" style="font-size:90%;">NSQG</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.3.5.3.2" style="width:21.1pt;background-color:#5BBACF;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.3.5.3.2.1">
<span class="ltx_p" id="S6.T4.3.5.3.2.1.1"><span class="ltx_text" id="S6.T4.3.5.3.2.1.1.1" style="font-size:90%;">44% </span><span class="ltx_text" id="S6.T4.3.5.3.2.1.1.2" style="font-size:70%;">(20%)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.3.5.3.3" style="width:21.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.3.5.3.3.1">
<span class="ltx_p" id="S6.T4.3.5.3.3.1.1"><span class="ltx_text" id="S6.T4.3.5.3.3.1.1.1" style="font-size:90%;">–</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.3.5.3.4" style="width:21.1pt;background-color:#83CFC1;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.3.5.3.4.1">
<span class="ltx_p" id="S6.T4.3.5.3.4.1.1"><span class="ltx_text" id="S6.T4.3.5.3.4.1.1.1" style="font-size:90%;">36% </span><span class="ltx_text" id="S6.T4.3.5.3.4.1.1.2" style="font-size:70%;">(15%)</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T4.3.6.4">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r" id="S6.T4.3.6.4.1" style="width:19.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.3.6.4.1.1">
<span class="ltx_p" id="S6.T4.3.6.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.3.6.4.1.1.1.1" style="font-size:90%;">NRQG</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.3.6.4.2" style="width:21.1pt;background-color:#83CFC1;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.3.6.4.2.1">
<span class="ltx_p" id="S6.T4.3.6.4.2.1.1"><span class="ltx_text" id="S6.T4.3.6.4.2.1.1.1" style="font-size:90%;">36% </span><span class="ltx_text" id="S6.T4.3.6.4.2.1.1.2" style="font-size:70%;">(17%)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.3.6.4.3" style="width:21.1pt;background-color:#A9DDB5;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.3.6.4.3.1">
<span class="ltx_p" id="S6.T4.3.6.4.3.1.1"><span class="ltx_text" id="S6.T4.3.6.4.3.1.1.1" style="font-size:90%;">28% </span><span class="ltx_text" id="S6.T4.3.6.4.3.1.1.2" style="font-size:70%;">(10%)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.3.6.4.4" style="width:21.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.3.6.4.4.1">
<span class="ltx_p" id="S6.T4.3.6.4.4.1.1"><span class="ltx_text" id="S6.T4.3.6.4.4.1.1.1" style="font-size:90%;">–</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<table class="ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="S6.T4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T4.4.1.1">
<th class="ltx_td ltx_align_center ltx_align_middle ltx_th ltx_th_column ltx_th_row" colspan="4" id="S6.T4.4.1.1.1"><em class="ltx_emph ltx_font_italic" id="S6.T4.4.1.1.1.1" style="font-size:90%;">Usefulness</em></th>
</tr>
<tr class="ltx_tr" id="S6.T4.4.2.2">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S6.T4.4.2.2.1" style="width:19.1pt;"></th>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_th ltx_th_column ltx_border_t" colspan="3" id="S6.T4.4.2.2.2"><span class="ltx_text ltx_font_bold" id="S6.T4.4.2.2.2.1" style="font-size:90%;">Wins over</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T4.4.3.1">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r" id="S6.T4.4.3.1.1" style="width:19.1pt;"></th>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.4.3.1.2" style="width:21.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.4.3.1.2.1">
<span class="ltx_p" id="S6.T4.4.3.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.4.3.1.2.1.1.1" style="font-size:90%;">TQG</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.4.3.1.3" style="width:21.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.4.3.1.3.1">
<span class="ltx_p" id="S6.T4.4.3.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.4.3.1.3.1.1.1" style="font-size:90%;">NSQG</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.4.3.1.4" style="width:21.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.4.3.1.4.1">
<span class="ltx_p" id="S6.T4.4.3.1.4.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.4.3.1.4.1.1.1" style="font-size:90%;">NRQG</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T4.4.4.2">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T4.4.4.2.1" style="width:19.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.4.4.2.1.1">
<span class="ltx_p" id="S6.T4.4.4.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.4.4.2.1.1.1.1" style="font-size:90%;">TQG</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S6.T4.4.4.2.2" style="width:21.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.4.4.2.2.1">
<span class="ltx_p" id="S6.T4.4.4.2.2.1.1"><span class="ltx_text" id="S6.T4.4.4.2.2.1.1.1" style="font-size:90%;">–</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S6.T4.4.4.2.3" style="width:21.1pt;background-color:#BFE6BF;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.4.4.2.3.1">
<span class="ltx_p" id="S6.T4.4.4.2.3.1.1"><span class="ltx_text" id="S6.T4.4.4.2.3.1.1.1" style="font-size:90%;">22% </span><span class="ltx_text" id="S6.T4.4.4.2.3.1.1.2" style="font-size:70%;">(6%)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S6.T4.4.4.2.4" style="width:21.1pt;background-color:#B4E2BA;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.4.4.2.4.1">
<span class="ltx_p" id="S6.T4.4.4.2.4.1.1"><span class="ltx_text" id="S6.T4.4.4.2.4.1.1.1" style="font-size:90%;">25% </span><span class="ltx_text" id="S6.T4.4.4.2.4.1.1.2" style="font-size:70%;">(7%)</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T4.4.5.3">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r" id="S6.T4.4.5.3.1" style="width:19.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.4.5.3.1.1">
<span class="ltx_p" id="S6.T4.4.5.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.4.5.3.1.1.1.1" style="font-size:90%;">NSQG</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.4.5.3.2" style="width:21.1pt;background-color:#83CFC1;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.4.5.3.2.1">
<span class="ltx_p" id="S6.T4.4.5.3.2.1.1"><span class="ltx_text" id="S6.T4.4.5.3.2.1.1.1" style="font-size:90%;">36% </span><span class="ltx_text" id="S6.T4.4.5.3.2.1.1.2" style="font-size:70%;">(17%)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.4.5.3.3" style="width:21.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.4.5.3.3.1">
<span class="ltx_p" id="S6.T4.4.5.3.3.1.1"><span class="ltx_text" id="S6.T4.4.5.3.3.1.1.1" style="font-size:90%;">–</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.4.5.3.4" style="width:21.1pt;background-color:#B4E2BA;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.4.5.3.4.1">
<span class="ltx_p" id="S6.T4.4.5.3.4.1.1"><span class="ltx_text" id="S6.T4.4.5.3.4.1.1.1" style="font-size:90%;">25% </span><span class="ltx_text" id="S6.T4.4.5.3.4.1.1.2" style="font-size:70%;">(7%)</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T4.4.6.4">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r" id="S6.T4.4.6.4.1" style="width:19.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.4.6.4.1.1">
<span class="ltx_p" id="S6.T4.4.6.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.4.6.4.1.1.1.1" style="font-size:90%;">NRQG</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.4.6.4.2" style="width:21.1pt;background-color:#83CFC1;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.4.6.4.2.1">
<span class="ltx_p" id="S6.T4.4.6.4.2.1.1"><span class="ltx_text" id="S6.T4.4.6.4.2.1.1.1" style="font-size:90%;">36% </span><span class="ltx_text" id="S6.T4.4.6.4.2.1.1.2" style="font-size:70%;">(16%)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.4.6.4.3" style="width:21.1pt;background-color:#BCE5BE;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.4.6.4.3.1">
<span class="ltx_p" id="S6.T4.4.6.4.3.1.1"><span class="ltx_text" id="S6.T4.4.6.4.3.1.1.1" style="font-size:90%;">23% </span><span class="ltx_text" id="S6.T4.4.6.4.3.1.1.2" style="font-size:70%;">(7%)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.4.6.4.4" style="width:21.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.4.6.4.4.1">
<span class="ltx_p" id="S6.T4.4.6.4.4.1.1"><span class="ltx_text" id="S6.T4.4.6.4.4.1.1.1" style="font-size:90%;">–</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<table class="ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="S6.T4.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T4.5.1.1">
<th class="ltx_td ltx_align_center ltx_align_middle ltx_th ltx_th_column ltx_th_row" colspan="4" id="S6.T4.5.1.1.1"><em class="ltx_emph ltx_font_italic" id="S6.T4.5.1.1.1.1" style="font-size:90%;">Answerability</em></th>
</tr>
<tr class="ltx_tr" id="S6.T4.5.2.2">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S6.T4.5.2.2.1" style="width:19.1pt;"></th>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_th ltx_th_column ltx_border_t" colspan="3" id="S6.T4.5.2.2.2"><span class="ltx_text ltx_font_bold" id="S6.T4.5.2.2.2.1" style="font-size:90%;">Wins over</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T4.5.3.1">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r" id="S6.T4.5.3.1.1" style="width:19.1pt;"></th>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.5.3.1.2" style="width:21.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.5.3.1.2.1">
<span class="ltx_p" id="S6.T4.5.3.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.5.3.1.2.1.1.1" style="font-size:90%;">TQG</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.5.3.1.3" style="width:21.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.5.3.1.3.1">
<span class="ltx_p" id="S6.T4.5.3.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.5.3.1.3.1.1.1" style="font-size:90%;">NSQG</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.5.3.1.4" style="width:21.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.5.3.1.4.1">
<span class="ltx_p" id="S6.T4.5.3.1.4.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.5.3.1.4.1.1.1" style="font-size:90%;">NRQG</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T4.5.4.2">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T4.5.4.2.1" style="width:19.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.5.4.2.1.1">
<span class="ltx_p" id="S6.T4.5.4.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.5.4.2.1.1.1.1" style="font-size:90%;">TQG</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S6.T4.5.4.2.2" style="width:21.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.5.4.2.2.1">
<span class="ltx_p" id="S6.T4.5.4.2.2.1.1"><span class="ltx_text" id="S6.T4.5.4.2.2.1.1.1" style="font-size:90%;">–</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S6.T4.5.4.2.3" style="width:21.1pt;background-color:#95D6BB;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.5.4.2.3.1">
<span class="ltx_p" id="S6.T4.5.4.2.3.1.1"><span class="ltx_text" id="S6.T4.5.4.2.3.1.1.1" style="font-size:90%;">32% </span><span class="ltx_text" id="S6.T4.5.4.2.3.1.1.2" style="font-size:70%;">(11%)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S6.T4.5.4.2.4" style="width:21.1pt;background-color:#83CFC1;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.5.4.2.4.1">
<span class="ltx_p" id="S6.T4.5.4.2.4.1.1"><span class="ltx_text" id="S6.T4.5.4.2.4.1.1.1" style="font-size:90%;">36% </span><span class="ltx_text" id="S6.T4.5.4.2.4.1.1.2" style="font-size:70%;">(16%)</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T4.5.5.3">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r" id="S6.T4.5.5.3.1" style="width:19.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.5.5.3.1.1">
<span class="ltx_p" id="S6.T4.5.5.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.5.5.3.1.1.1.1" style="font-size:90%;">NSQG</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.5.5.3.2" style="width:21.1pt;background-color:#6BC3C9;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.5.5.3.2.1">
<span class="ltx_p" id="S6.T4.5.5.3.2.1.1"><span class="ltx_text" id="S6.T4.5.5.3.2.1.1.1" style="font-size:90%;">41% </span><span class="ltx_text" id="S6.T4.5.5.3.2.1.1.2" style="font-size:70%;">(18%)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.5.5.3.3" style="width:21.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.5.5.3.3.1">
<span class="ltx_p" id="S6.T4.5.5.3.3.1.1"><span class="ltx_text" id="S6.T4.5.5.3.3.1.1.1" style="font-size:90%;">–</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.5.5.3.4" style="width:21.1pt;background-color:#8BD2BE;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.5.5.3.4.1">
<span class="ltx_p" id="S6.T4.5.5.3.4.1.1"><span class="ltx_text" id="S6.T4.5.5.3.4.1.1.1" style="font-size:90%;">34% </span><span class="ltx_text" id="S6.T4.5.5.3.4.1.1.2" style="font-size:70%;">(16%)</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T4.5.6.4">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r" id="S6.T4.5.6.4.1" style="width:19.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.5.6.4.1.1">
<span class="ltx_p" id="S6.T4.5.6.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.5.6.4.1.1.1.1" style="font-size:90%;">NRQG</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.5.6.4.2" style="width:21.1pt;background-color:#73C8C7;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.5.6.4.2.1">
<span class="ltx_p" id="S6.T4.5.6.4.2.1.1"><span class="ltx_text" id="S6.T4.5.6.4.2.1.1.1" style="font-size:90%;">39% </span><span class="ltx_text" id="S6.T4.5.6.4.2.1.1.2" style="font-size:70%;">(19%)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.5.6.4.3" style="width:21.1pt;background-color:#91D4BD;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.5.6.4.3.1">
<span class="ltx_p" id="S6.T4.5.6.4.3.1.1"><span class="ltx_text" id="S6.T4.5.6.4.3.1.1.1" style="font-size:90%;">33% </span><span class="ltx_text" id="S6.T4.5.6.4.3.1.1.2" style="font-size:70%;">(14%)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S6.T4.5.6.4.4" style="width:21.1pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T4.5.6.4.4.1">
<span class="ltx_p" id="S6.T4.5.6.4.4.1.1"><span class="ltx_text" id="S6.T4.5.6.4.4.1.1.1" style="font-size:90%;">–</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">The questions generated by the four models are also evaluated using human assessors along three dimensions: grammar and fluency, usefulness, and answerability.
The pointwise evaluation results are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S6.T3" title="Table 3 ‣ 6.2. Human evaluation ‣ 6. Results and Analysis ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">3</span></a>. Overall, all models score above average (<math alttext="&gt;" class="ltx_Math" display="inline" id="S6.SS2.p1.1.m1.1"><semantics id="S6.SS2.p1.1.m1.1a"><mo id="S6.SS2.p1.1.m1.1.1" xref="S6.SS2.p1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.1.m1.1b"><gt id="S6.SS2.p1.1.m1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p1.1.m1.1d">&gt;</annotation></semantics></math> 3) along all evaluation dimensions. The neural models outperform Baseline 1 when comparing <em class="ltx_emph ltx_font_italic" id="S6.SS2.p1.1.1">grammar and fluency</em>; the differences are significant for NSQG. This is expected as the characteristic property of using large pre-trained language models is their capability to use grammar correctly. When constructing templates using the most frequent n-grams, we have no guarantees of fluency or adherence to grammatical rules. However, it is interesting to note that grammar is adequate (i.e., scoring 3 or greater) in over 80% of the test cases and that Baseline 2 significantly improves it. In both <em class="ltx_emph ltx_font_italic" id="S6.SS2.p1.1.2">usefulness</em> and <em class="ltx_emph ltx_font_italic" id="S6.SS2.p1.1.3">answerability</em>, the neural models perform similarly, with the review-based (NRQG) being only slightly worse than the sentence-based (NSQG) model. They both significantly outperform Baseline 1, which likely follows from the fact that these models can accurately determine when not to generate a question and predict N/A instead.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S6.F8" title="Figure 8 ‣ 6.2. Human evaluation ‣ 6. Results and Analysis ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">8</span></a> shows the breakdown of the pointwise evaluation across all 12 product categories for the sentence-based neural model (NSQG). We see the scores are above average (i.e., above 3) for all categories on all three dimensions.
Of the three dimensions, the scores for <em class="ltx_emph ltx_font_italic" id="S6.SS2.p2.1.1">grammar and fluency</em> are the highest overall, as well as for most categories. Interestingly, there is still a large variance between different categories, with the categories <em class="ltx_emph ltx_font_italic" id="S6.SS2.p2.1.2">Bird feeder</em> and <em class="ltx_emph ltx_font_italic" id="S6.SS2.p2.1.3">Espresso machine</em> having the lowest scores, and <em class="ltx_emph ltx_font_italic" id="S6.SS2.p2.1.4">Tent</em> and <em class="ltx_emph ltx_font_italic" id="S6.SS2.p2.1.5">Backpacking pack</em> highest scores.
The categories <em class="ltx_emph ltx_font_italic" id="S6.SS2.p2.1.6">Bike</em>, <em class="ltx_emph ltx_font_italic" id="S6.SS2.p2.1.7">Espresso machine</em>, and <em class="ltx_emph ltx_font_italic" id="S6.SS2.p2.1.8">Snow shovel</em> have the lowest scores in terms of <em class="ltx_emph ltx_font_italic" id="S6.SS2.p2.1.9">usefulness</em>. It suggests that the model should label sentences as N/A more often for those categories.</p>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1">The pairwise evaluation follows the same patterns as the pointwise evaluation. In all cases, annotators prefer the outputs of the NSQG model, followed by the NRQG model. The biggest distinction between the template-based and neural models is seen in <em class="ltx_emph ltx_font_italic" id="S6.SS2.p3.1.1">usefulness</em>, where the annotators prefer the neural models, often strongly so, in the vast majority of cases. There is almost no distinction between the neural models. However, NSQG is a clear favorite in the other two dimensions (i.e., <em class="ltx_emph ltx_font_italic" id="S6.SS2.p3.1.2">grammar and fluency</em> and <em class="ltx_emph ltx_font_italic" id="S6.SS2.p3.1.3">answerability</em>).</p>
</div>
<div class="ltx_para" id="S6.SS2.p4">
<p class="ltx_p" id="S6.SS2.p4.1">To answer our main research question, we conclude that overall, we can generate high-quality questions according to both automatic and human evaluation. Furthermore, based on human evaluation experiments, the neural models generate more natural questions compared to the template-based baselines (RQ1).</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Model Size</h3>
<figure class="ltx_table" id="S6.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5. </span>Performance of the sentence-based question generation (NSQG) model using different pre-trained language models that are fine-tuned on all available training data (i.e., five questions or N/A per sentence). The best scores for each measure are in boldface.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T5.1.1.1">
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S6.T5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.2.1">#Parameters</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S6.T5.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.3.1">N/A Accuracy</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S6.T5.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.4.1">BLEU-4</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S6.T5.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.5.1">ROUGE-L</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S6.T5.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.6.1">METEOR</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T5.1.2.1">
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T5.1.2.1.1">T5-small</th>
<td class="ltx_td ltx_nopad_l ltx_align_right ltx_border_t" id="S6.T5.1.2.1.2">60.5 M</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T5.1.2.1.3">0.724</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T5.1.2.1.4">0.716</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T5.1.2.1.5"><span class="ltx_text ltx_font_bold" id="S6.T5.1.2.1.5.1">0.810</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T5.1.2.1.6"><span class="ltx_text ltx_font_bold" id="S6.T5.1.2.1.6.1">0.497</span></td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.3.2">
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S6.T5.1.3.2.1">T5-base</th>
<td class="ltx_td ltx_nopad_l ltx_align_right" id="S6.T5.1.3.2.2">222 M</td>
<td class="ltx_td ltx_align_right" id="S6.T5.1.3.2.3">0.819</td>
<td class="ltx_td ltx_align_right" id="S6.T5.1.3.2.4">0.693</td>
<td class="ltx_td ltx_align_right" id="S6.T5.1.3.2.5">0.794</td>
<td class="ltx_td ltx_align_right" id="S6.T5.1.3.2.6">0.493</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.4.3">
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T5.1.4.3.1">T5-large</th>
<td class="ltx_td ltx_nopad_l ltx_align_right ltx_border_bb" id="S6.T5.1.4.3.2">737 M</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T5.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.4.3.3.1">0.858</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T5.1.4.3.4"><span class="ltx_text ltx_font_bold" id="S6.T5.1.4.3.4.1">0.730</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T5.1.4.3.5">0.806</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T5.1.4.3.6">0.494</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">Next, we explore what effect the size of the pre-trained language model has on the performance of neural question generation (RQ2a). Specifically, we fine-tune three T5 models of different sizes when employing neural sentence-based question generation (NSQG). Table <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S6.T5" title="Table 5 ‣ 6.3. Model Size ‣ 6. Results and Analysis ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">5</span></a> shows the results in terms of non-applicability classification (Accuracy) and question generation (BLEU, ROUGE, and METEOR).
The model size does not have a large impact on the question generation task. The difference, however, is more pronounced for non-applicability (N/A) detection than for question generation. Detecting N/A is one of the most important parts of the pipeline since question generation quality heavily depends on only converting useful item-usage sentences to questions. Furthermore, while there is a trade-off between model size and accuracy, note that the planned usage is to generate questions offline and store them as a question collection. Thus, efficiency is not the main concern in this scenario. For this reason, we conclude that larger pre-trained models yield more effective questions.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4. </span>Training Data Volume</h3>
<figure class="ltx_figure" id="S6.F9">
<table class="ltx_tabular ltx_align_middle" id="S6.F9.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.F9.2.2">
<td class="ltx_td ltx_align_left" id="S6.F9.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="232" id="S6.F9.1.1.1.g1" src="x7.png" width="398"/></td>
<td class="ltx_td ltx_align_left" id="S6.F9.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="232" id="S6.F9.2.2.2.g1" src="x8.png" width="398"/></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9. </span>Model performance (T5-large) with sentence-based (Left) or question-based (Right) training data reduction for the T5-large version of the NSQG model.
</figcaption>
</figure>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">We further investigate how the amount of training data affects model performance (RQ2b) by considering different ways and degrees of data reduction. As before, we use the best-performing NSQG model for this experiment, i.e., T5 large.
In <em class="ltx_emph ltx_font_italic" id="S6.SS4.p1.1.1">sentence-based</em> data reduction, shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S6.F9" title="Figure 9 ‣ 6.4. Training Data Volume ‣ 6. Results and Analysis ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">9</span></a> (Left), only a subset of the available sentences is used for training (using all available questions corresponding to those sentences).
We observe a drop in accuracy when we reduce the amount of training data to 25% or lower (i.e., less than 1000 training samples), while question generation performance is less severely affected.
In <em class="ltx_emph ltx_font_italic" id="S6.SS4.p1.1.2">question-based</em> data reduction, shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S6.F9" title="Figure 9 ‣ 6.4. Training Data Volume ‣ 6. Results and Analysis ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">9</span></a> (Right), we split the dataset based on the number of questions available for each sentence. We consider using a single question (1), the three initially generated questions (3), and the three initial questions plus the two paraphrases (5). We find that reducing the number of questions has surprisingly little effect.
This suggests that it is more beneficial to collect a small number of questions for a larger set of sentences than vice versa.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5. </span>Success/Failure Analysis</h3>
<figure class="ltx_table" id="S6.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6. </span>Examples of question generation outputs for all four models</figcaption>
<p class="ltx_p ltx_align_center" id="S6.T6.1">.


<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T6.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S6.T6.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T6.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T6.1.1.1.1.1.1" style="font-size:80%;">Pattern</span></span>
<span class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T6.1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.1.1.2.1">
<span class="ltx_p" id="S6.T6.1.1.1.1.2.1.1"><em class="ltx_emph ltx_font_italic" id="S6.T6.1.1.1.1.2.1.1.1" style="font-size:80%;">Generic questions</em></span>
</span></span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S6.T6.1.1.2.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_3" id="S6.T6.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S6.T6.1.1.2.1.1.1" style="font-size:80%;">Ground truth</span></span>
<span class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T6.1.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.2.1.2.1">
<span class="ltx_p" id="S6.T6.1.1.2.1.2.1.1"><span class="ltx_text" id="S6.T6.1.1.2.1.2.1.1.1" style="font-size:80%;">-   n/a</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.3.2">
<span class="ltx_td ltx_align_justify ltx_border_r" id="S6.T6.1.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.3.2.1.1">
<span class="ltx_p" id="S6.T6.1.1.3.2.1.1.1"><span class="ltx_text" id="S6.T6.1.1.3.2.1.1.1.1" style="font-size:80%;">-   n/a</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.4.3">
<span class="ltx_td ltx_align_justify ltx_border_r" id="S6.T6.1.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.4.3.1.1">
<span class="ltx_p" id="S6.T6.1.1.4.3.1.1.1"><span class="ltx_text" id="S6.T6.1.1.4.3.1.1.1.1" style="font-size:80%;">-   n/a</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.5.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_3" id="S6.T6.1.1.5.4.1"><span class="ltx_text" id="S6.T6.1.1.5.4.1.1" style="font-size:80%;">
<span class="ltx_inline-block ltx_align_left" id="S6.T6.1.1.5.4.1.1.1">
<span class="ltx_p" id="S6.T6.1.1.5.4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T6.1.1.5.4.1.1.1.1.1">TQG</span></span>
<span class="ltx_p" id="S6.T6.1.1.5.4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T6.1.1.5.4.1.1.1.2.1">TQG+CLS</span></span>
</span></span></span>
<span class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T6.1.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.5.4.2.1">
<span class="ltx_p" id="S6.T6.1.1.5.4.2.1.1"><span class="ltx_text" id="S6.T6.1.1.5.4.2.1.1.1" style="font-size:80%;">-   Are you looking for a grill that is great for grilling certain things–not good for everything but what is?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.6.5">
<span class="ltx_td ltx_align_justify ltx_border_r" id="S6.T6.1.1.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.6.5.1.1">
<span class="ltx_p" id="S6.T6.1.1.6.5.1.1.1"><span class="ltx_text" id="S6.T6.1.1.6.5.1.1.1.1" style="font-size:80%;">-   Are you looking for a vacuum that is great for something to keep my floors clean – not ’eat off of’ clean?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.7.6">
<span class="ltx_td ltx_align_justify ltx_border_r" id="S6.T6.1.1.7.6.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.7.6.1.1">
<span class="ltx_p" id="S6.T6.1.1.7.6.1.1.1"><span class="ltx_text" id="S6.T6.1.1.7.6.1.1.1.1" style="font-size:80%;">-   Are you looking for a espresso machine that is great for making espresso drinks?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.8.7">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_3" id="S6.T6.1.1.8.7.1"><span class="ltx_text ltx_font_bold" id="S6.T6.1.1.8.7.1.1" style="font-size:80%;">NSQG</span></span>
<span class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T6.1.1.8.7.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.8.7.2.1">
<span class="ltx_p" id="S6.T6.1.1.8.7.2.1.1"><span class="ltx_text" id="S6.T6.1.1.8.7.2.1.1.1" style="font-size:80%;">-   Do you need a grill that is good for grilling certain things?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.9.8">
<span class="ltx_td ltx_align_justify ltx_border_r" id="S6.T6.1.1.9.8.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.9.8.1.1">
<span class="ltx_p" id="S6.T6.1.1.9.8.1.1.1"><span class="ltx_text" id="S6.T6.1.1.9.8.1.1.1.1" style="font-size:80%;">-   Are you looking for a vacuum to clean your floors?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.10.9">
<span class="ltx_td ltx_align_justify ltx_border_r" id="S6.T6.1.1.10.9.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.10.9.1.1">
<span class="ltx_p" id="S6.T6.1.1.10.9.1.1.1"><span class="ltx_text" id="S6.T6.1.1.10.9.1.1.1.1" style="font-size:80%;">-   Do you want an espresso machine that is good for making espresso drinks?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.11.10">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_3" id="S6.T6.1.1.11.10.1"><span class="ltx_text ltx_font_bold" id="S6.T6.1.1.11.10.1.1" style="font-size:80%;">NRQG</span></span>
<span class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T6.1.1.11.10.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.11.10.2.1">
<span class="ltx_p" id="S6.T6.1.1.11.10.2.1.1"><span class="ltx_text" id="S6.T6.1.1.11.10.2.1.1.1" style="font-size:80%;">-   Are you looking for a grill that is perfect for satay and quick grilling using smaller amounts of charcoal?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.12.11">
<span class="ltx_td ltx_align_justify ltx_border_r" id="S6.T6.1.1.12.11.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.12.11.1.1">
<span class="ltx_p" id="S6.T6.1.1.12.11.1.1.1"><span class="ltx_text" id="S6.T6.1.1.12.11.1.1.1.1" style="font-size:80%;">-   Are you looking for a vacuum cleaner that can keep your floors clean?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.13.12">
<span class="ltx_td ltx_align_justify ltx_border_r" id="S6.T6.1.1.13.12.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.13.12.1.1">
<span class="ltx_p" id="S6.T6.1.1.13.12.1.1.1"><span class="ltx_text" id="S6.T6.1.1.13.12.1.1.1.1" style="font-size:80%;">-   Are you looking for an espresso machine that is good for making espresso drinks?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.14.13">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt" id="S6.T6.1.1.14.13.1"><span class="ltx_text ltx_font_bold" id="S6.T6.1.1.14.13.1.1" style="font-size:80%;">Pattern</span></span>
<span class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S6.T6.1.1.14.13.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.14.13.2.1">
<span class="ltx_p" id="S6.T6.1.1.14.13.2.1.1"><em class="ltx_emph ltx_font_italic" id="S6.T6.1.1.14.13.2.1.1.1" style="font-size:80%;">Complex questions</em></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.15.14">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S6.T6.1.1.15.14.1"><span class="ltx_text ltx_font_bold" id="S6.T6.1.1.15.14.1.1" style="font-size:80%;">Ground truth</span></span>
<span class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T6.1.1.15.14.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.15.14.2.1">
<span class="ltx_p" id="S6.T6.1.1.15.14.2.1.1"><span class="ltx_text" id="S6.T6.1.1.15.14.2.1.1.1" style="font-size:80%;">-   n/a</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.16.15">
<span class="ltx_td ltx_align_justify ltx_border_r" id="S6.T6.1.1.16.15.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.16.15.1.1">
<span class="ltx_p" id="S6.T6.1.1.16.15.1.1.1"><span class="ltx_text" id="S6.T6.1.1.16.15.1.1.1.1" style="font-size:80%;">-   Do you need a backpack with a good size for traveling on an airplane or going on a camping trip for a few days or packing for a few days trip?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.17.16">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S6.T6.1.1.17.16.1"><span class="ltx_text" id="S6.T6.1.1.17.16.1.1" style="font-size:80%;">
<span class="ltx_inline-block ltx_align_left" id="S6.T6.1.1.17.16.1.1.1">
<span class="ltx_p" id="S6.T6.1.1.17.16.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T6.1.1.17.16.1.1.1.1.1">TQG</span></span>
<span class="ltx_p" id="S6.T6.1.1.17.16.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T6.1.1.17.16.1.1.1.2.1">TQG+CLS</span></span>
</span></span></span>
<span class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T6.1.1.17.16.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.17.16.2.1">
<span class="ltx_p" id="S6.T6.1.1.17.16.2.1.1"><span class="ltx_text" id="S6.T6.1.1.17.16.2.1.1.1" style="font-size:80%;">-   Are you looking for a espresso machine that is great for something higher quality that’ll last longer and has more features or if you’d like an introductory machine that makes delicious espresso that’s much more affordable?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.18.17">
<span class="ltx_td ltx_align_justify ltx_border_r" id="S6.T6.1.1.18.17.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.18.17.1.1">
<span class="ltx_p" id="S6.T6.1.1.18.17.1.1.1"><span class="ltx_text" id="S6.T6.1.1.18.17.1.1.1.1" style="font-size:80%;">-   Are you looking for a backpacking pack that is great for traveling on an airplane (come up with ways to shrink it by removing the frame and using the straps) or going on a camping trip for a few days or packing for a few days trip?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.19.18">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S6.T6.1.1.19.18.1"><span class="ltx_text ltx_font_bold" id="S6.T6.1.1.19.18.1.1" style="font-size:80%;">NSQG</span></span>
<span class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T6.1.1.19.18.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.19.18.2.1">
<span class="ltx_p" id="S6.T6.1.1.19.18.2.1.1"><span class="ltx_text" id="S6.T6.1.1.19.18.2.1.1.1" style="font-size:80%;">-   Are you looking for an espresso machine that has a lot of features or if you’d rather spend a lot of money up front for something higher quality that’ll last longer and has more features?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.20.19">
<span class="ltx_td ltx_align_justify ltx_border_r" id="S6.T6.1.1.20.19.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.20.19.1.1">
<span class="ltx_p" id="S6.T6.1.1.20.19.1.1.1"><span class="ltx_text" id="S6.T6.1.1.20.19.1.1.1.1" style="font-size:80%;">-   Are you looking for a good size backpack that can be easily shrunk by removing frame and be used with straps?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.21.20">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S6.T6.1.1.21.20.1"><span class="ltx_text ltx_font_bold" id="S6.T6.1.1.21.20.1.1" style="font-size:80%;">NRQG</span></span>
<span class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T6.1.1.21.20.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.21.20.2.1">
<span class="ltx_p" id="S6.T6.1.1.21.20.2.1.1"><span class="ltx_text" id="S6.T6.1.1.21.20.2.1.1.1" style="font-size:80%;">-   Are you looking for an espresso machine that lasts longer and has more features?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.22.21">
<span class="ltx_td ltx_align_justify ltx_border_r" id="S6.T6.1.1.22.21.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.22.21.1.1">
<span class="ltx_p" id="S6.T6.1.1.22.21.1.1.1"><span class="ltx_text" id="S6.T6.1.1.22.21.1.1.1.1" style="font-size:80%;">-   Are you looking for a backpack that is a good size for traveling on an airplane or going on a camping trip for a few days or packing for a few days trip?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.23.22">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt" id="S6.T6.1.1.23.22.1"><span class="ltx_text ltx_font_bold" id="S6.T6.1.1.23.22.1.1" style="font-size:80%;">Pattern</span></span>
<span class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S6.T6.1.1.23.22.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.23.22.2.1">
<span class="ltx_p" id="S6.T6.1.1.23.22.2.1.1"><em class="ltx_emph ltx_font_italic" id="S6.T6.1.1.23.22.2.1.1.1" style="font-size:80%;">Success</em></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.24.23">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_3" id="S6.T6.1.1.24.23.1"><span class="ltx_text ltx_font_bold" id="S6.T6.1.1.24.23.1.1" style="font-size:80%;">Ground truth</span></span>
<span class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T6.1.1.24.23.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.24.23.2.1">
<span class="ltx_p" id="S6.T6.1.1.24.23.2.1.1"><span class="ltx_text" id="S6.T6.1.1.24.23.2.1.1.1" style="font-size:80%;">-   Would you like a spacious backpacking pack?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.25.24">
<span class="ltx_td ltx_align_justify ltx_border_r" id="S6.T6.1.1.25.24.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.25.24.1.1">
<span class="ltx_p" id="S6.T6.1.1.25.24.1.1.1"><span class="ltx_text" id="S6.T6.1.1.25.24.1.1.1.1" style="font-size:80%;">-   Are you looking for a grill that you can take on camping trips,even the long ones?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.26.25">
<span class="ltx_td ltx_align_justify ltx_border_r" id="S6.T6.1.1.26.25.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.26.25.1.1">
<span class="ltx_p" id="S6.T6.1.1.26.25.1.1.1"><span class="ltx_text" id="S6.T6.1.1.26.25.1.1.1.1" style="font-size:80%;">-   Do you want a perfect tent for backpacking?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.27.26">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_3" id="S6.T6.1.1.27.26.1"><span class="ltx_text" id="S6.T6.1.1.27.26.1.1" style="font-size:80%;">
<span class="ltx_inline-block ltx_align_left" id="S6.T6.1.1.27.26.1.1.1">
<span class="ltx_p" id="S6.T6.1.1.27.26.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T6.1.1.27.26.1.1.1.1.1">TQG</span></span>
<span class="ltx_p" id="S6.T6.1.1.27.26.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T6.1.1.27.26.1.1.1.2.1">TQG+CLS</span></span>
</span></span></span>
<span class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T6.1.1.27.26.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.27.26.2.1">
<span class="ltx_p" id="S6.T6.1.1.27.26.2.1.1"><span class="ltx_text" id="S6.T6.1.1.27.26.2.1.1.1" style="font-size:80%;">-   Are you looking for a backpacking pack that is great for everything i would need for a three day isolation and more?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.28.27">
<span class="ltx_td ltx_align_justify ltx_border_r" id="S6.T6.1.1.28.27.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.28.27.1.1">
<span class="ltx_p" id="S6.T6.1.1.28.27.1.1.1"><span class="ltx_text" id="S6.T6.1.1.28.27.1.1.1.1" style="font-size:80%;">-   Are you looking for a grill that is great for taking on camping trips (even long ones)?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.29.28">
<span class="ltx_td ltx_align_justify ltx_border_r" id="S6.T6.1.1.29.28.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.29.28.1.1">
<span class="ltx_p" id="S6.T6.1.1.29.28.1.1.1"><span class="ltx_text" id="S6.T6.1.1.29.28.1.1.1.1" style="font-size:80%;">-   Are you looking for a tent that is great for backpacking?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.30.29">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_3" id="S6.T6.1.1.30.29.1"><span class="ltx_text ltx_font_bold" id="S6.T6.1.1.30.29.1.1" style="font-size:80%;">NSQG</span></span>
<span class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T6.1.1.30.29.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.30.29.2.1">
<span class="ltx_p" id="S6.T6.1.1.30.29.2.1.1"><span class="ltx_text" id="S6.T6.1.1.30.29.2.1.1.1" style="font-size:80%;">-   Are you in need of a backpack that has more than enough room for everything?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.31.30">
<span class="ltx_td ltx_align_justify ltx_border_r" id="S6.T6.1.1.31.30.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.31.30.1.1">
<span class="ltx_p" id="S6.T6.1.1.31.30.1.1.1"><span class="ltx_text" id="S6.T6.1.1.31.30.1.1.1.1" style="font-size:80%;">-   Are you looking for a grill that is perfect for long distance camping trips?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.32.31">
<span class="ltx_td ltx_align_justify ltx_border_r" id="S6.T6.1.1.32.31.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.32.31.1.1">
<span class="ltx_p" id="S6.T6.1.1.32.31.1.1.1"><span class="ltx_text" id="S6.T6.1.1.32.31.1.1.1.1" style="font-size:80%;">-   Are you looking for a tent that is perfect for backpacking?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.33.32">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_3" id="S6.T6.1.1.33.32.1"><span class="ltx_text ltx_font_bold" id="S6.T6.1.1.33.32.1.1" style="font-size:80%;">NRQG</span></span>
<span class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T6.1.1.33.32.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.33.32.2.1">
<span class="ltx_p" id="S6.T6.1.1.33.32.2.1.1"><span class="ltx_text" id="S6.T6.1.1.33.32.2.1.1.1" style="font-size:80%;">-   Are you in need of a backpack that is in great shape and has more than enough room for everything?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.34.33">
<span class="ltx_td ltx_align_justify ltx_border_r" id="S6.T6.1.1.34.33.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.34.33.1.1">
<span class="ltx_p" id="S6.T6.1.1.34.33.1.1.1"><span class="ltx_text" id="S6.T6.1.1.34.33.1.1.1.1" style="font-size:80%;">-   Are you looking for a grill that is perfect for camping trips?</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.35.34">
<span class="ltx_td ltx_align_justify ltx_border_b ltx_border_r" id="S6.T6.1.1.35.34.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T6.1.1.35.34.1.1">
<span class="ltx_p" id="S6.T6.1.1.35.34.1.1.1"><span class="ltx_text" id="S6.T6.1.1.35.34.1.1.1.1" style="font-size:80%;">-   Are you looking for a tent that is perfect for backpacking?</span></span>
</span></span></span>
</span>
</span><span class="ltx_text" id="S6.T6.1.2" style="font-size:80%;"></span></p>
</figure>
<div class="ltx_para" id="S6.SS5.p1">
<p class="ltx_p" id="S6.SS5.p1.1">A closer look at specific sentence-question pairs reveals two patterns that leave room for future improvement; Table <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S6.T6" title="Table 6 ‣ 6.5. Success/Failure Analysis ‣ 6. Results and Analysis ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">6</span></a> shows examples of these using the template-based baseline models (TQG and TQG+CLS) and the best-performing variants of the neural models (NSQG and NRQG).
We find that some of the generated questions are <em class="ltx_emph ltx_font_italic" id="S6.SS5.p1.1.1">too generic</em> (Table <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S6.T6" title="Table 6 ‣ 6.5. Success/Failure Analysis ‣ 6. Results and Analysis ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">6</span></a>, top block). These are correct in terms of grammar and structure, but unsuitable for eliciting meaningful user preferences, e.g., <em class="ltx_emph ltx_font_italic" id="S6.SS5.p1.1.2">“Do you need a grill that is good for grilling certain things?”</em>
Instead of returning N/A (which is indeed the corresponding response in our dataset), the model generated a question that is so vague and generic that it is hard to think of a scenario where it would not be answered affirmatively. Interestingly, the review-based model in this scenario utilized another part of the input instead of the heuristically extracted sentence, which sentence-based models operate on, to generate a more useful question <em class="ltx_emph ltx_font_italic" id="S6.SS5.p1.1.3">“Are you looking for a grill that is perfect for satay and quick grilling using smaller amounts of charcoal?”</em>
The second pattern concerns <em class="ltx_emph ltx_font_italic" id="S6.SS5.p1.1.4">complex questions</em> (Table <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S6.T6" title="Table 6 ‣ 6.5. Success/Failure Analysis ‣ 6. Results and Analysis ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">6</span></a>, middle block) that ask about more than one usage or activity, e.g., <em class="ltx_emph ltx_font_italic" id="S6.SS5.p1.1.5">“Are you looking for a backpacking pack that is a good size for traveling on an airplane or going on a camping trip for a few days or packing for a few days trip?”</em> This question is too complex and unlikely to elicit any meaningful information without the user having to elaborate which options they agree with and which they do not. Such questions should instead be split into several simpler ones where it is both easier to interpret the question and to answer it. Note that crowd workers were not instructed to simplify complex questions, therefore it is not surprising that is what the model has learned.
We also include examples of successes (Table <a class="ltx_ref" href="https://arxiv.org/html/2111.13463v2#S6.T6" title="Table 6 ‣ 6.5. Success/Failure Analysis ‣ 6. Results and Analysis ‣ Generating Usage-related Questions for Preference Elicitation in Conversational Recommender Systems"><span class="ltx_text ltx_ref_tag">6</span></a>, bottom block) where all three models generate valid questions. We notice that for shorter inputs, all three models generate useful and grammatically correct questions that are easy to answer. Since the template-based model is directly dependent on the structure of the input sentence, in some cases it does not produce a fluent question, e.g., <em class="ltx_emph ltx_font_italic" id="S6.SS5.p1.1.6">“Are you looking for a backpacking pack that is great for everything i would need for a three day isolation and more?”</em> However, the meaning is still understandable even if the usefulness is limited of such an over-specified question.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusion and Future Directions</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper, we have studied the question of how a conversational recommender system can solicit user’s needs through natural language by using indirect questions about how the wanted product will be used. This contrasts with most prior work that considers how to directly ask about desired product attributes.
We have developed, evaluated, and compared four models used on the task: two template-based and two neural models. In each case, the start is a corpus of reviews, and the goal is to generate preference elicitation questions, if possible. We show that all four models effectively extract relevant information from reviews (with high precision), and transform it into useful questions. For the sentence-based models, sentences containing usage-related statements are identified heuristically, while the review-based model works end-to-end. The generated questions from all models are of high quality, with the neural models achieving higher scores in the automated evaluation and also being preferred by human annotators.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S7.p2.1.1">Utilization.</em>
We emphasize that this work focuses on this first stage of recommendation in a conversational setting, eliciting the user’s needs in a natural and engaging way. The most important future direction is determining how answers to these questions should best be leveraged for the task of generating recommendations, once the user’s need is understood. Here, we anticipate that sentence embedding techniques would likely to be effective. Second, as this work builds on top of large language models, language safety is a key consideration warranting further study before our approach could be used in practice. Nevertheless, during experimentation, we did not observe concerning language nor hallucinations. We also note that the offline question generation process lends itself well to even manual control over the language model output.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S7.p3.1.1">Limitations.</em>
We focus on generating high-quality questions (precision) as opposed the having an extensive coverage of the possible item uses (recall).
We do not address the aspect of question diversity explicitly. Instead, it is assumed that human-created reviews naturally cover the different ways a given item is used.
Determining whether the coverage of usage-related questions is sufficient for a given category would be an interesting direction for further investigation.</p>
</div>
<div class="ltx_para" id="S7.p4">
<p class="ltx_p" id="S7.p4.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S7.p4.1.1">Generalizability.</em>
Our approach may be employed in other domains where items are associated with a certain activity. For instance, in a movie recommendation scenario, a statement like “This movie was perfect for watching with my kids,” could provide valuable usage-related insights, as could similar statements in the domain of travel, food, or restaurants.
Our approach may be less applicable in contexts where the items do not lend themselves to specific activities or usage scenarios, e.g., news recommendation.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Afzali et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jafar Afzali, Aleksander Mark Drzewiecki, Krisztian Balog, and Shuo Zhang. 2023.

</span>
<span class="ltx_bibblock">UserSimCRS: A User Simulation Toolkit for Evaluating Conversational Recommender Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining</em> <em class="ltx_emph ltx_font_italic" id="bib.bib2.4.2">(WSDM ’23)</em>. 1160–1163.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aliannejadi et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Mohammad Aliannejadi, Hamed Zamani, Fabio Crestani, and W. Bruce Croft. 2019.

</span>
<span class="ltx_bibblock">Asking clarifying questions in Open–Domain Information–Seeking conversations. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval</em> <em class="ltx_emph ltx_font_italic" id="bib.bib3.4.2">(SIGIR ’19)</em>. 475–484.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anand et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Avishek Anand, Lawrence Cavedon, Hideo Joho, Mark Sanderson, and Benno Stein. 2020.

</span>
<span class="ltx_bibblock">Conversational search (dagstuhl seminar 19461).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Dagstuhl Reports</em> 9, 11 (2020), 34–83.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Balog et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Krisztian Balog, Filip Radlinski, and Alexandros Karatzoglou. 2021.

</span>
<span class="ltx_bibblock">On interpretation and measurement of soft attributes for recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval</em> <em class="ltx_emph ltx_font_italic" id="bib.bib5.4.2">(SIGIR ’21)</em>. 890–899.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Benetka et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Jan R. Benetka, John Krumm, and Paul N. Bennett. 2019.

</span>
<span class="ltx_bibblock">Understanding context for tasks and activities. In <em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Proceedings of the 2019 conference on human information interaction and retrieval</em> <em class="ltx_emph ltx_font_italic" id="bib.bib6.4.2">(CHIIR ’19)</em>. 133–142.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blanco et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Roi Blanco, Berkant Barla Cambazoglu, Peter Mika, and Nicolas Torzec. 2013.

</span>
<span class="ltx_bibblock">Entity Recommendations in Web Search. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">The Semantic Web – ISWC 2013</em> <em class="ltx_emph ltx_font_italic" id="bib.bib7.4.2">(ISWC ’13)</em>. 33–48.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Budzianowski et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Pawe\l Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Iñigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Gašić. 2018.

</span>
<span class="ltx_bibblock">MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em> <em class="ltx_emph ltx_font_italic" id="bib.bib8.4.2">(EMNLP ’18)</em>. 5016–5026.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Callison-Burch et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2006)</span>
<span class="ltx_bibblock">
Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006.

</span>
<span class="ltx_bibblock">Re-evaluating the Role of Bleu in Machine Translation Research. In <em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">11th Conference of the European Chapter of the Association for Computational Linguistics</em> <em class="ltx_emph ltx_font_italic" id="bib.bib9.4.2">(EACL ’06)</em>. 249–256.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carterette et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2008)</span>
<span class="ltx_bibblock">
Ben Carterette, Paul N. Bennett, David Maxwell Chickering, and Susan T. Dumais. 2008.

</span>
<span class="ltx_bibblock">Here or there: preference judgments for relevance. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">Proceedings of the IR research, 30th European conference on Advances in information retrieval</em> <em class="ltx_emph ltx_font_italic" id="bib.bib10.4.2">(ECIR ’08)</em>. 16–27.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Pu (2012)</span>
<span class="ltx_bibblock">
Li Chen and Pearl Pu. 2012.

</span>
<span class="ltx_bibblock">Critiquing-based recommenders: survey and emerging trends.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">User Modeling and User-Adapted Interaction</em> 22, 1-2 (2012), 125–150.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Kyunghyun Cho, Bart van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014.

</span>
<span class="ltx_bibblock">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em> <em class="ltx_emph ltx_font_italic" id="bib.bib12.4.2">(EMNLP ’14)</em>. 1724–1734.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christakopoulou et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Konstantina Christakopoulou, Alex Beutel, Rui Li, Sagar Jain, and Ed H. Chi. 2018.

</span>
<span class="ltx_bibblock">Q&amp;R: A Two–Stage approach toward interactive recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em> <em class="ltx_emph ltx_font_italic" id="bib.bib13.4.2">(KDD ’18)</em>. 139–148.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christakopoulou et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Konstantina Christakopoulou, Filip Radlinski, and Katja Hofmann. 2016.

</span>
<span class="ltx_bibblock">Towards conversational recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</em> <em class="ltx_emph ltx_font_italic" id="bib.bib14.4.2">(KDD ’16)</em>. 815–824.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em> <em class="ltx_emph ltx_font_italic" id="bib.bib15.4.2">(NAACL ’19)</em>. 4171–4186.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Chongming Gao, Wenqiang Lei, Xiangnan He, Maarten de Rijke, and Tat-Seng Chua. 2021.

</span>
<span class="ltx_bibblock">Advances and challenges in conversational recommender systems: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">AI Open</em> 2 (2021), 100–126.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Jianfeng Gao, Michel Galley, and Lihong Li. 2018.

</span>
<span class="ltx_bibblock">Neural approaches to conversational AI. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">The 41st international ACM SIGIR conference on research &amp; development in information retrieval</em> <em class="ltx_emph ltx_font_italic" id="bib.bib17.4.2">(SIGIR ’18)</em>. 1371–1374.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Habib et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Javeria Habib, Shuo Zhang, and Krisztian Balog. 2020.

</span>
<span class="ltx_bibblock">IAI MovieBot: A Conversational Movie Recommender System. In <em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</em> <em class="ltx_emph ltx_font_italic" id="bib.bib18.4.2">(CIKM ’20)</em>. 3405–3408.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He and Chua (2017)</span>
<span class="ltx_bibblock">
Xiangnan He and Tat-Seng Chua. 2017.

</span>
<span class="ltx_bibblock">Neural Factorization Machines for Sparse Predictive Analytics. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</em> <em class="ltx_emph ltx_font_italic" id="bib.bib19.2.2">(SIGIR ’17)</em>. 355–364.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hochreiter and Schmidhuber (1997)</span>
<span class="ltx_bibblock">
S. Hochreiter and J. Schmidhuber. 1997.

</span>
<span class="ltx_bibblock">Long short-term memory.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Neural Computation</em> 9, 8 (1997), 1735–1780.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jin Huang, Harrie Oosterhuis, Maarten de Rijke, and Herke van Hoof. 2020.

</span>
<span class="ltx_bibblock">Keeping Dataset Biases out of the Simulation: A Debiased Simulator for Reinforcement Learning based Recommender Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">Proceedings of the 14th ACM Conference on Recommender Systems</em> <em class="ltx_emph ltx_font_italic" id="bib.bib21.4.2">(RecSys ’20)</em>. 190–199.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jagerman et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Rolf Jagerman, Ilya Markov, and Maarten de Rijke. 2019.

</span>
<span class="ltx_bibblock">When people change their mind: Off–Policy evaluation in Non–Stationary recommendation environments. In <em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">Proceedings of the twelfth ACM international conference on web search and data mining</em> <em class="ltx_emph ltx_font_italic" id="bib.bib22.4.2">(WSDM ’19)</em>. 447–455.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jannach et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Dietmar Jannach, Ahtsham Manzoor, Wanling Cai, and Li Chen. 2022.

</span>
<span class="ltx_bibblock">A Survey on Conversational Recommender Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">Comput. Surveys</em> 54, 5 (2022), 1–36.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jurafsky and Martin (2020)</span>
<span class="ltx_bibblock">
Dan Jurafsky and James H. Martin. 2020.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Speech and language processing</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn (2004)</span>
<span class="ltx_bibblock">
Philipp Koehn. 2004.

</span>
<span class="ltx_bibblock">Statistical Significance Tests for Machine Translation Evaluation. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</em> <em class="ltx_emph ltx_font_italic" id="bib.bib25.2.2">(EMNLP ’04)</em>. 388–395.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kostric et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Ivica Kostric, Krisztian Balog, and Filip Radlinski. 2021.

</span>
<span class="ltx_bibblock">Soliciting User Preferences in Conversational Recommender Systems via Usage-related Questions. In <em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Proceedings of the 15th ACM Conference on Recommender Systems</em> <em class="ltx_emph ltx_font_italic" id="bib.bib26.4.2">(RecSys ’21)</em>. 724–729.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lavie and Agarwal (2007)</span>
<span class="ltx_bibblock">
Alon Lavie and Abhaya Agarwal. 2007.

</span>
<span class="ltx_bibblock">Meteor: An automatic metric for MT evaluation with high levels of correlation with human judgments. In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the second workshop on statistical machine translation</em> <em class="ltx_emph ltx_font_italic" id="bib.bib27.2.2">(StatMT ’07)</em>. 228–231.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Hoyeop Lee, Jinbae Im, Seongwon Jang, Hyunsouk Cho, and Sehee Chung. 2019.

</span>
<span class="ltx_bibblock">MeLU: Meta–Learned user preference estimator for Cold–Start recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em> <em class="ltx_emph ltx_font_italic" id="bib.bib28.4.2">(KDD ’19)</em>. 1073–1082.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2020a)</span>
<span class="ltx_bibblock">
Wenqiang Lei, Xiangnan He, Yisong Miao, Qingyun Wu, Richang Hong, Min-Yen Kan, and Tat-Seng Chua. 2020a.

</span>
<span class="ltx_bibblock">Estimation-Action-Reflection: Towards Deep Interaction Between Conversational and Recommender Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">Proceedings of the 13th International Conference on Web Search and Data Mining</em> <em class="ltx_emph ltx_font_italic" id="bib.bib29.4.2">(WSDM ’20)</em>. 304–312.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2020b)</span>
<span class="ltx_bibblock">
Wenqiang Lei, Gangyi Zhang, Xiangnan He, Yisong Miao, Xiang Wang, Liang Chen, and Tat–Seng Chua. 2020b.

</span>
<span class="ltx_bibblock">Interactive path reasoning on graph for conversational recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em> <em class="ltx_emph ltx_font_italic" id="bib.bib30.4.2">(KDD ’20)</em>. 2073–2083.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020.

</span>
<span class="ltx_bibblock">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In <em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em> <em class="ltx_emph ltx_font_italic" id="bib.bib31.4.2">(ACL ’20)</em>. 7871–7880.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Raymond Li, Samira Ebrahimi Kahou, Hannes Schulz, Vincent Michalski, Laurent Charlin, and Chris Pal. 2018.

</span>
<span class="ltx_bibblock">Towards Deep Conversational Recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">Advances in Neural Information Processing Systems</em> <em class="ltx_emph ltx_font_italic" id="bib.bib32.4.2">(NIPS ’18, Vol. 31)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yuan-Hong Liao, Amlan Kar, and Sanja Fidler. 2021.

</span>
<span class="ltx_bibblock">Towards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2104.12690 [cs]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)</span>
<span class="ltx_bibblock">
Chin-Yew Lin. 2004.

</span>
<span class="ltx_bibblock">ROUGE: A Package for Automatic Evaluation of Summaries. In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Text Summarization Branches Out</em> <em class="ltx_emph ltx_font_italic" id="bib.bib34.2.2">(ACL ’04)</em>. 74–81.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira, Ming-Feng Tsai, Chuan-Ju Wang, and Jimmy Lin. 2020.

</span>
<span class="ltx_bibblock">Conversational Question Reformulation via Sequence-to-Sequence Architectures and Pretrained Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2004.01909 [cs]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau. 2016.

</span>
<span class="ltx_bibblock">How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em> <em class="ltx_emph ltx_font_italic" id="bib.bib36.4.2">(EMNLP ’16)</em>. 2122–2132.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock">RoBERTa: A Robustly Optimized BERT Pretraining Approach.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1907.11692 [cs]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2022)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter. 2022.

</span>
<span class="ltx_bibblock">Decoupled Weight Decay Regularization. In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">International Conference on Learning Representations</em> <em class="ltx_emph ltx_font_italic" id="bib.bib38.2.2">(ICLR ’19)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
Yue Lu, Malu Castellanos, Umeshwar Dayal, and ChengXiang Zhai. 2011.

</span>
<span class="ltx_bibblock">Automatic construction of a Context–Aware sentiment lexicon: An optimization approach. In <em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">Proceedings of the 20th international conference on world wide web</em> <em class="ltx_emph ltx_font_italic" id="bib.bib39.4.2">(WWW ’11)</em>. 347–356.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2020a)</span>
<span class="ltx_bibblock">
Kai Luo, Scott Sanner, Ga Wu, Hanze Li, and Hojin Yang. 2020a.

</span>
<span class="ltx_bibblock">Latent linear critiquing for conversational recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">Proceedings of the web conference 2020</em> <em class="ltx_emph ltx_font_italic" id="bib.bib40.4.2">(WWW ’20)</em>. 2535–2541.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2020b)</span>
<span class="ltx_bibblock">
Kai Luo, Hojin Yang, Ga Wu, and Scott Sanner. 2020b.

</span>
<span class="ltx_bibblock">Deep critiquing for VAE–Based recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval</em> <em class="ltx_emph ltx_font_italic" id="bib.bib41.4.2">(SIGIR ’20)</em>. 1269–1278.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mairesse et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2010)</span>
<span class="ltx_bibblock">
François Mairesse, Milica Gašić, Filip Jurčíček, Simon Keizer, Blaise Thomson, Kai Yu, and Steve Young. 2010.

</span>
<span class="ltx_bibblock">Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</em> <em class="ltx_emph ltx_font_italic" id="bib.bib42.4.2">(ACL ’10)</em>. 1552–1561.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manning et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, and David McClosky. 2014.

</span>
<span class="ltx_bibblock">The Stanford CoreNLP Natural Language Processing Toolkit. In <em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</em> <em class="ltx_emph ltx_font_italic" id="bib.bib43.4.2">(ACL ’14)</em>. 55–60.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manzoor and Jannach (2021)</span>
<span class="ltx_bibblock">
Ahtsham Manzoor and Dietmar Jannach. 2021.

</span>
<span class="ltx_bibblock">Generation-based vs. Retrieval-based Conversational Recommendation: A User-Centric Comparison. In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Proceedings of the 15th ACM Conference on Recommender Systems</em> <em class="ltx_emph ltx_font_italic" id="bib.bib44.2.2">(RecSys ’21)</em>. 515–520.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manzoor and Jannach (2022)</span>
<span class="ltx_bibblock">
Ahtsham Manzoor and Dietmar Jannach. 2022.

</span>
<span class="ltx_bibblock">Towards retrieval-based conversational recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Information Systems</em> 109, C (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Morgan and Bourlard (1989)</span>
<span class="ltx_bibblock">
N. Morgan and H. Bourlard. 1989.

</span>
<span class="ltx_bibblock">Generalization and parameter estimation in feedforward nets: Some experiments. In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the 2nd international conference on neural information processing systems</em> <em class="ltx_emph ltx_font_italic" id="bib.bib46.2.2">(NIPS ’89)</em>. 630–637.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nelder and Wedderburn (1972)</span>
<span class="ltx_bibblock">
J. A. Nelder and R. W. M. Wedderburn. 1972.

</span>
<span class="ltx_bibblock">Generalized Linear Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Journal of the Royal Statistical Society: Series A (General)</em> 135, 3 (1972), 370–384.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ni et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019.

</span>
<span class="ltx_bibblock">Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects. In <em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em> <em class="ltx_emph ltx_font_italic" id="bib.bib48.4.2">(EMNLP ’19)</em>. 188–197.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock">BLEU: a method for automatic evaluation of machine translation. In <em class="ltx_emph ltx_font_italic" id="bib.bib49.3.1">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</em> <em class="ltx_emph ltx_font_italic" id="bib.bib49.4.2">(ACL ’02)</em>. 311–318.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peters et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018.

</span>
<span class="ltx_bibblock">Deep Contextualized Word Representations. In <em class="ltx_emph ltx_font_italic" id="bib.bib50.3.1">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</em> <em class="ltx_emph ltx_font_italic" id="bib.bib50.4.2">(NAACL ’18)</em>. 2227–2237.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pradeep et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Ronak Pradeep, Rodrigo Nogueira, and Jimmy Lin. 2021.

</span>
<span class="ltx_bibblock">The Expando-Mono-Duo Design Pattern for Text Ranking with Pretrained Sequence-to-Sequence Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2101.05667 [cs.IR]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.

</span>
<span class="ltx_bibblock">Language Models are Unsupervised Multitask Learners.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al<span class="ltx_text" id="bib.bib53.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.

</span>
<span class="ltx_bibblock">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.3.1">Journal of Machine Learning Research</em> 21, 140 (2020), 1–67.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rao and Daumé III (2018)</span>
<span class="ltx_bibblock">
Sudha Rao and Hal Daumé III. 2018.

</span>
<span class="ltx_bibblock">Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information. In <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em> <em class="ltx_emph ltx_font_italic" id="bib.bib54.2.2">(ACL ’18)</em>. 2737–2746.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rao and Daumé III (2019)</span>
<span class="ltx_bibblock">
Sudha Rao and Hal Daumé III. 2019.

</span>
<span class="ltx_bibblock">Answer-based Adversarial Training for Generating Clarification Questions. In <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em> <em class="ltx_emph ltx_font_italic" id="bib.bib55.2.2">(NAACL ’19)</em>. 143–155.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rosset et al<span class="ltx_text" id="bib.bib56.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Corbin Rosset, Chenyan Xiong, Xia Song, Daniel Campos, Nick Craswell, Saurabh Tiwary, and Paul Bennett. 2020.

</span>
<span class="ltx_bibblock">Leading Conversational Search by Suggesting Useful Questions. In <em class="ltx_emph ltx_font_italic" id="bib.bib56.3.1">Proceedings of The Web Conference 2020</em> <em class="ltx_emph ltx_font_italic" id="bib.bib56.4.2">(WWW ’20)</em>. 1160–1170.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sai et al<span class="ltx_text" id="bib.bib57.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Ananya B. Sai, Akash Kumar Mohankumar, and Mitesh M. Khapra. 2022.

</span>
<span class="ltx_bibblock">A Survey of Evaluation Metrics Used for NLG Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.3.1">Comput. Surveys</em> 55, 2 (2022), 26:1–26:39.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarwar et al<span class="ltx_text" id="bib.bib58.2.2.1">.</span> (2001)</span>
<span class="ltx_bibblock">
Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001.

</span>
<span class="ltx_bibblock">Item-based collaborative filtering recommendation algorithms. In <em class="ltx_emph ltx_font_italic" id="bib.bib58.3.1">Proceedings of the 10th international conference on World Wide Web</em> <em class="ltx_emph ltx_font_italic" id="bib.bib58.4.2">(WWW ’01)</em>. 285–295.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sekulić et al<span class="ltx_text" id="bib.bib59.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Ivan Sekulić, Mohammad Aliannejadi, and Fabio Crestani. 2021.

</span>
<span class="ltx_bibblock">Towards Facet-Driven Generation of Clarifying Questions for Conversational Search. In <em class="ltx_emph ltx_font_italic" id="bib.bib59.3.1">Proceedings of the 2021 ACM SIGIR International Conference on Theory of Information Retrieval</em> <em class="ltx_emph ltx_font_italic" id="bib.bib59.4.2">(ICTIR ’21)</em>. 167–175.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sepliarskaia et al<span class="ltx_text" id="bib.bib60.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Anna Sepliarskaia, Julia Kiseleva, Filip Radlinski, and Maarten de Rijke. 2018.

</span>
<span class="ltx_bibblock">Preference elicitation as an optimization problem. In <em class="ltx_emph ltx_font_italic" id="bib.bib60.3.1">Proceedings of the 12th ACM Conference on Recommender Systems</em> <em class="ltx_emph ltx_font_italic" id="bib.bib60.4.2">(RecSys ’18)</em>. 172–180.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun and Zhang (2018)</span>
<span class="ltx_bibblock">
Yueming Sun and Yi Zhang. 2018.

</span>
<span class="ltx_bibblock">Conversational recommender system. In <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">The 41st international ACM SIGIR conference on research &amp; development in information retrieval</em> <em class="ltx_emph ltx_font_italic" id="bib.bib61.2.2">(SIGIR ’18)</em>. 235–244.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tunkelang (2009)</span>
<span class="ltx_bibblock">
Daniel Tunkelang. 2009.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Faceted search</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van der Lee et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Chris van der Lee, Albert Gatt, Emiel van Miltenburg, and Emiel Krahmer. 2021.

</span>
<span class="ltx_bibblock">Human evaluation of automatically generated text: Current trends and best practice guidelines.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.3.1">Computer Speech &amp; Language</em> 67 (2021), 101151.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vandic et al<span class="ltx_text" id="bib.bib64.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Damir Vandic, Steven Aanen, Flavius Frasincar, and Uzay Kaymak. 2017.

</span>
<span class="ltx_bibblock">Dynamic facet ordering for faceted product search engines.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.3.1">IEEE Transactions on Knowledge and Data Engineering</em> 29, 5 (2017), 1004–1016.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al<span class="ltx_text" id="bib.bib65.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need. In <em class="ltx_emph ltx_font_italic" id="bib.bib65.3.1">Proceedings of the 31st international conference on neural information processing systems</em> <em class="ltx_emph ltx_font_italic" id="bib.bib65.4.2">(NIPS ’17)</em>. 6000–6010.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib66.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Qing Wang, Chunqiu Zeng, Wubai Zhou, Tao Li, S. S. Iyengar, Larisa Shwartz, and Genady Ya. Grabarnik. 2019.

</span>
<span class="ltx_bibblock">Online Interactive Collaborative Filtering Using Multi-Armed Bandit with Dependent Arms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.3.1">IEEE Transactions on Knowledge and Data Engineering</em> 31, 8 (2019), 1569–1580.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib67.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Shoujin Wang, Longbing Cao, Yan Wang, Quan Z. Sheng, Mehmet A. Orgun, and Defu Lian. 2021a.

</span>
<span class="ltx_bibblock">A Survey on Session-based Recommender Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.3.1">Comput. Surveys</em> 54, 7 (2021), 154:1–154:38.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib68.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Wenjie Wang, Fuli Feng, Xiangnan He, Liqiang Nie, and Tat-Seng Chua. 2021b.

</span>
<span class="ltx_bibblock">Denoising Implicit Feedback for Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib68.3.1">Proceedings of the 14th ACM International Conference on Web Search and Data Mining</em> <em class="ltx_emph ltx_font_italic" id="bib.bib68.4.2">(WSDM ’21)</em>. 373–381.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib69.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Yansen Wang, Chenyi Liu, Minlie Huang, and Liqiang Nie. 2018.

</span>
<span class="ltx_bibblock">Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders. In <em class="ltx_emph ltx_font_italic" id="bib.bib69.3.1">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em> <em class="ltx_emph ltx_font_italic" id="bib.bib69.4.2">(ACL ’18)</em>. 2193–2203.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Williams and Zipser (1989)</span>
<span class="ltx_bibblock">
Ronald J. Williams and David Zipser. 1989.

</span>
<span class="ltx_bibblock">A learning algorithm for continually running fully recurrent neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">Neural Computation</em> 1, 2 (1989), 270–280.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib71.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Ga Wu, Kai Luo, Scott Sanner, and Harold Soh. 2019.

</span>
<span class="ltx_bibblock">Deep Language–Based critiquing for recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib71.3.1">Proceedings of the 13th ACM conference on recommender systems</em> <em class="ltx_emph ltx_font_italic" id="bib.bib71.4.2">(RecSys ’19)</em>. 137–145.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and Yan (2019)</span>
<span class="ltx_bibblock">
Wei Wu and Rui Yan. 2019.

</span>
<span class="ltx_bibblock">Deep Chit-Chat: Deep Learning for Chatbots. In <em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</em> <em class="ltx_emph ltx_font_italic" id="bib.bib72.2.2">(SIGIR ’19)</em>. 1413–1414.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib73.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Liu Yang, Minghui Qiu, Chen Qu, Cen Chen, Jiafeng Guo, Yongfeng Zhang, W. Bruce Croft, and Haiqing Chen. 2020.

</span>
<span class="ltx_bibblock">IART: Intent-aware Response Ranking with Transformers in Information-seeking Conversation Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib73.3.1">Proceedings of The Web Conference 2020</em> <em class="ltx_emph ltx_font_italic" id="bib.bib73.4.2">(WWW ’20)</em>. 2592–2598.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ying et al<span class="ltx_text" id="bib.bib74.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. 2018.

</span>
<span class="ltx_bibblock">Graph convolutional neural networks for web-scale recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib74.3.1">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em> <em class="ltx_emph ltx_font_italic" id="bib.bib74.4.2">(KDD ’18)</em>. 974–983.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Balog (2020)</span>
<span class="ltx_bibblock">
Shuo Zhang and Krisztian Balog. 2020.

</span>
<span class="ltx_bibblock">Evaluating Conversational Recommender Systems via User Simulation. In <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em> <em class="ltx_emph ltx_font_italic" id="bib.bib75.2.2">(KDD ’20)</em>. 1512–1520.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib76.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Yongfeng Zhang, Xu Chen, Qingyao Ai, Liu Yang, and W. Bruce Croft. 2018.

</span>
<span class="ltx_bibblock">Towards Conversational Search and Recommendation: System Ask, User Respond. In <em class="ltx_emph ltx_font_italic" id="bib.bib76.3.1">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</em> <em class="ltx_emph ltx_font_italic" id="bib.bib76.4.2">(CIKM ’18)</em>. 177–186.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib77.2.2.1">.</span> (2014a)</span>
<span class="ltx_bibblock">
Yongfeng Zhang, Guokun Lai, Min Zhang, Yi Zhang, Yiqun Liu, and Shaoping Ma. 2014a.

</span>
<span class="ltx_bibblock">Explicit factor models for explainable recommendation based on phrase-level sentiment analysis. In <em class="ltx_emph ltx_font_italic" id="bib.bib77.3.1">Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</em> <em class="ltx_emph ltx_font_italic" id="bib.bib77.4.2">(SIGIR ’14)</em>. 83–92.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib78.2.2.1">.</span> (2014b)</span>
<span class="ltx_bibblock">
Yongfeng Zhang, Haochen Zhang, Min Zhang, Yiqun Liu, and Shaoping Ma. 2014b.

</span>
<span class="ltx_bibblock">Do users rate or review? Boost Phrase–Level sentiment labeling with Review–Level sentiment classification. In <em class="ltx_emph ltx_font_italic" id="bib.bib78.3.1">Proceedings of the 37th international ACM SIGIR conference on research &amp; development in information retrieval</em> <em class="ltx_emph ltx_font_italic" id="bib.bib78.4.2">(SIGIR ’14)</em>. 1027–1030.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib79.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Xiaoxue Zhao, Weinan Zhang, and Jun Wang. 2013.

</span>
<span class="ltx_bibblock">Interactive collaborative filtering. In <em class="ltx_emph ltx_font_italic" id="bib.bib79.3.1">Proceedings of the 22nd ACM international conference on information &amp; knowledge management</em> <em class="ltx_emph ltx_font_italic" id="bib.bib79.4.2">(CIKM ’13)</em>. 1411–1420.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Apr  8 13:16:37 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
