<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VideoStreaming Paper Macro-Level Structure Visualization</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            background-color: #f5f5f5;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
        }
        h1 {
            text-align: center;
            color: #2c3e50;
            margin-bottom: 30px;
        }
        .structure-diagram {
            display: flex;
            flex-direction: column;
            gap: 20px;
        }
        .section {
            border: 2px solid #3498db;
            border-radius: 8px;
            padding: 15px;
            background: #ecf0f1;
            position: relative;
        }
        .section-header {
            background: #3498db;
            color: white;
            padding: 10px;
            margin: -15px -15px 15px -15px;
            border-radius: 6px 6px 0 0;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        .section-title {
            font-weight: bold;
            font-size: 1.2em;
        }
        .word-count {
            font-size: 0.9em;
            opacity: 0.9;
        }
        .moves {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 10px;
            margin-top: 10px;
        }
        .move {
            background: white;
            padding: 10px;
            border-radius: 5px;
            border-left: 4px solid #e74c3c;
        }
        .move-title {
            font-weight: bold;
            color: #e74c3c;
            margin-bottom: 5px;
        }
        .move-content {
            font-size: 0.9em;
            color: #555;
        }
        .key-excerpt {
            background: #fff3cd;
            border: 1px solid #ffeaa7;
            border-radius: 5px;
            padding: 10px;
            margin: 10px 0;
            font-style: italic;
        }
        .analysis {
            background: #d4edda;
            border: 1px solid #c3e6cb;
            border-radius: 5px;
            padding: 10px;
            margin: 10px 0;
            font-size: 0.9em;
        }
        .flow-arrow {
            text-align: center;
            font-size: 2em;
            color: #7f8c8d;
            margin: 10px 0;
        }
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: white;
        }
        .comparison-table th, .comparison-table td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        .comparison-table th {
            background-color: #f2f2f2;
            font-weight: bold;
        }
        .highlight {
            background-color: #fff3cd;
            font-weight: bold;
        }
        .legend {
            background: #ecf0f1;
            padding: 15px;
            border-radius: 5px;
            margin: 20px 0;
        }
        .legend-item {
            display: inline-block;
            margin: 5px 15px 5px 0;
            padding: 5px 10px;
            border-radius: 3px;
            font-size: 0.9em;
        }
        .cs-convention { background: #3498db; color: white; }
        .traditional { background: #e74c3c; color: white; }
        .universal { background: #27ae60; color: white; }
    </style>
</head>
<body>
    <div class="container">
        <h1>VideoStreaming Paper: Macro-Level Structure Analysis</h1>

        <div class="legend">
            <h3>Analysis Framework</h3>
            <div class="legend-item cs-convention">Computer Science Convention</div>
            <div class="legend-item traditional">Traditional Academic</div>
            <div class="legend-item universal">Universal Best Practice</div>
        </div>

        <div class="structure-diagram">

            <!-- Introduction Section -->
            <div class="section">
                <div class="section-header">
                    <span class="section-title">Section 1: Introduction</span>
                    <span class="word-count">~600 words</span>
                </div>

                <div class="moves">
                    <div class="move">
                        <div class="move-title">Move 1: Establishing Territory</div>
                        <div class="move-content">
                            <strong>Centrality Claims:</strong> LLMs have advanced AI, extended to multi-modal domains, but long video sequences pose challenges
                        </div>
                        <div class="key-excerpt">
                            "The evolution of Large Language Models (LLMs) has significantly advanced artificial intelligence, encompassing text generation and reasoning in complex language environments. Later, the community extends LLMs to multi-modal domains, demonstrating promising results in captioning and question-answering tasks that integrate diverse visual signals. Yet, within the domain of video understanding, long video sequences pose a formidable challenge."
                        </div>
                        <div class="analysis">
                            Establishes broad significance by tracing LLM evolution and identifying the specific challenge domain. Uses present perfect tense to show ongoing relevance.
                        </div>
                    </div>

                    <div class="move">
                        <div class="move-title">Move 2: Establishing Niche</div>
                        <div class="move-content">
                            <strong>Gaps Identified:</strong> Sparse sampling loses information; frame compression overlooks temporal dynamics; memory banks require explicit timestamps
                        </div>
                        <div class="key-excerpt">
                            "Among the recent works on general video understanding with LLMs, a prevalent strategy is using sparse temporal sampling or spatio-temporal pooling to reduce tokens. Unfortunately, this paradigm explicitly loses substantial information in the long time span. To address this limitation, [46,45,95] develop frame-wise compression, with LLaMA-VID [46] as a typical example. It compresses each frame into only two tokens but overlooks the inter-frame temporal dynamics which are vital in compressing temporal redundancy within videos."
                        </div>
                        <div class="analysis">
                            Systematic critique of existing approaches with specific technical limitations identified. Uses "unfortunately" and "overlooks" to emphasize gaps.
                        </div>
                    </div>

                    <div class="move">
                        <div class="move-title">Move 3: Occupying Niche</div>
                        <div class="move-content">
                            <strong>Solution:</strong> VideoStreaming with Memory-Propagated Streaming Encoding and Adaptive Memory Selection
                        </div>
                        <div class="key-excerpt">
                            "In this work, we propose VideoStreaming, a novel Memory-Propagated Streaming Encoding architecture with Adaptive Memory Selection to sequentially encode a long video into condensed memories and generate responses referring to relevant timestamps."
                        </div>
                        <div class="analysis">
                            Clear positioning with technical terminology. Introduces the method name and core components explicitly.
                        </div>
                    </div>
                </div>
            </div>

            <div class="flow-arrow">↓</div>

            <!-- Related Work Section -->
            <div class="section">
                <div class="section-header">
                    <span class="section-title">Section 2: Related Work</span>
                    <span class="word-count">~500 words</span>
                </div>

                <div class="moves">
                    <div class="move">
                        <div class="move-title">Move 1: Thematic Overview</div>
                        <div class="move-content">
                            <strong>Scope:</strong> LLMs evolution, Vision Language Models, Long Video Understanding
                        </div>
                        <div class="key-excerpt">
                            "Large Language Models (LLMs) have revolutionized natural language processing. Early works establish encoder-decoder models with masked language modeling, while later decoder-only models like GPT showcase remarkable performance and scalability."
                        </div>
                        <div class="analysis">
                            Chronological progression from early to recent works, establishing the foundation for video understanding applications.
                        </div>
                    </div>

                    <div class="move">
                        <div class="move-title">Move 2: Critical Analysis</div>
                        <div class="move-content">
                            <strong>Themes:</strong> Vision Language Models, video processing strategies, memory banks with systematic critique
                        </div>
                        <div class="key-excerpt">
                            "[53,51,47,29,93] use sparse sampling or simple temporal pooling to obtain compact video tokens for LLMs. [45,95] employ Q-Former to project frame-wise features into the textual space. To handle longer videos, [36,82] utilize token merging to reduce redundancy and alleviate computational burden. LLaMA-VID [46] proposes an instruction-aware compression strategy to represent each frame with only two tokens, but it overlooks the temporal relations in the compression step."
                        </div>
                        <div class="analysis">
                            Detailed evaluation of related approaches with specific limitations highlighted. Uses "but" to signal critique.
                        </div>
                    </div>

                    <div class="move">
                        <div class="move-title">Move 3 & 4: Gaps & Synthesis</div>
                        <div class="move-content">
                            <strong>Gaps:</strong> Temporal relation oversight, explicit timestamp dependency, non-end-to-end training
                        </div>
                        <div class="key-excerpt">
                            "[37,31,94] use language as a bridge for long-term video understanding. They first divide a long video into short clips, generate textual descriptions for each clip, and then employ an LLM to aggregate the short captions for long video analysis. However, this architecture cannot be trained end-to-end, and the long video understanding quality depends on the short clip captions."
                        </div>
                        <div class="analysis">
                            Gaps established through systematic critique. "However" signals limitation, leading to positioning of current work.
                        </div>
                    </div>
                </div>
            </div>

            <div class="flow-arrow">↓</div>

            <!-- Methods Sections -->
            <div class="section">
                <div class="section-header">
                    <span class="section-title">Section 3: VideoStreaming (Methods)</span>
                    <span class="word-count">~2000 words</span>
                </div>

                <div class="moves">
                    <div class="move">
                        <div class="move-title">Single Clip Encoding</div>
                        <div class="move-content">
                            <strong>Technical Approach:</strong> Small language model (Phi-2) for encoding, prefix task training, attention mask modification
                        </div>
                        <div class="key-excerpt">
                            "To effectively distill the information within a sequence into a compact set of tokens, we take inspiration from recent advanced decoder-only language models and employ a comparatively small language model, Phi-2, for efficient encoding. Due to the causal attention and autoregressive nature, the language model spontaneously aggregates the sequence information onto the last few tokens, which naturally serve as a compact representation."
                        </div>
                        <div class="analysis">
                            High technical precision with mathematical formalism. Explains the rationale behind design choices.
                        </div>
                    </div>

                    <div class="move">
                        <div class="move-title">Memory-Propagated Streaming Encoding</div>
                        <div class="move-content">
                            <strong>Architecture:</strong> Sequential clip encoding with historical memory propagation, fixed-length memory representation
                        </div>
                        <div class="key-excerpt">
                            "To accomplish this objective, we divide a long video into K clips, each containing T frames, and propose a memory-propagated streaming encoding mechanism to iteratively encode each clip in sequence. In each iteration, we employ the encoded results from the last iteration as historical memory and integrate them with current clip features to produce an updated memory for subsequent encoding."
                        </div>
                        <div class="analysis">
                            Clear mathematical formulation with notation (K, T, Hk). Describes the iterative process systematically.
                        </div>
                    </div>

                    <div class="move">
                        <div class="move-title">Adaptive Memory Selection</div>
                        <div class="move-content">
                            <strong>Strategy:</strong> Question-related memory selection using cosine similarity and Gumbel-Topk
                        </div>
                        <div class="key-excerpt">
                            "Given a specific question or instruction, we first generate an adaptive indicator that summarizes relevant video content for that particular instruction. We accomplish this by reusing the language model in the streaming encoder, where we concatenate the global memory from the final iteration, HK, and the instruction texts, then pass the sequence into the model."
                        </div>
                        <div class="analysis">
                            Technical solution to information loss problem. Uses mathematical notation and algorithmic description.
                        </div>
                    </div>

                    <div class="move">
                        <div class="move-title">Progressive Training</div>
                        <div class="move-content">
                            <strong>Training Strategy:</strong> Two-stage training: single clip encoding, then streaming long video training
                        </div>
                        <div class="key-excerpt">
                            "To train VideoStreaming, we design a progressive two-stage paradigm. First, we train single clip encoding on image and short video understanding tasks. Next, we train memory-propagated streaming encoding and adaptive memory selection as well as the LLM for long video understanding."
                        </div>
                        <div class="analysis">
                            Clear training methodology with data construction details. Includes dataset statistics and training procedures.
                        </div>
                    </div>
                </div>
            </div>

            <div class="flow-arrow">↓</div>

            <!-- Experiments Section -->
            <div class="section">
                <div class="section-header">
                    <span class="section-title">Section 4: Experiments</span>
                    <span class="word-count">~1500 words</span>
                </div>

                <div class="moves">
                    <div class="move">
                        <div class="move-title">Datasets</div>
                        <div class="move-content">
                            <strong>Evaluation:</strong> Multiple long video QA datasets with varying durations (42 seconds to 108 minutes)
                        </div>
                        <div class="key-excerpt">
                            "We evaluate our model on long video QA datasets and present the statistics on the temporal duration of individual datasets in Table. 1. Among them, Next-QA [86], Next-GQA [87] and VideoChatGPT [53] encompass minute-long videos with thousands of frames. EgoSchema [54] contains over 5K three-minute videos with multiple-choice questions."
                        </div>
                        <div class="analysis">
                            Comprehensive dataset description with specific statistics. Demonstrates evaluation rigor across different video lengths.
                        </div>
                    </div>

                    <div class="move">
                        <div class="move-title">Main Results</div>
                        <div class="move-content">
                            <strong>Performance:</strong> Superior results on VideoChatGPT, EgoSchema, Next-QA, Next-GQA, MovieChat-1K, MovieNet-QA
                        </div>
                        <div class="key-excerpt">
                            "Our model outperforms LLM-based video understanding methods on all five metrics, with a significant advantage in temporal understanding. It can be attributed to the memory-propagated streaming encoding architecture that explicitly captures temporal dynamics."
                        </div>
                        <div class="analysis">
                            Quantitative results with interpretation. Links performance improvements to specific architectural components.
                        </div>
                    </div>

                    <div class="move">
                        <div class="move-title">Ablation Studies</div>
                        <div class="move-content">
                            <strong>Analysis:</strong> Historical memory impact, memory selection effectiveness, encoder architecture, temporal grounding supervision
                        </div>
                        <div class="key-excerpt">
                            "We explore the influence of memory in the streaming encoding process. Typically, the historical memory significantly improves global understanding by 46.6%. This verifies our intuition that leveraging historical memory enables the model to produce a global representation that summarizes the entire video."
                        </div>
                        <div class="analysis">
                            Systematic component analysis with quantitative evidence. Validates design choices through ablation studies.
                        </div>
                    </div>
                </div>
            </div>

            <div class="flow-arrow">↓</div>

            <!-- Conclusion Section -->
            <div class="section">
                <div class="section-header">
                    <span class="section-title">Section 5: Conclusion</span>
                    <span class="word-count">~200 words</span>
                </div>

                <div class="moves">
                    <div class="move">
                        <div class="move-title">Synthesis & Impact</div>
                        <div class="move-content">
                            <strong>Contributions:</strong> Novel streaming encoding architecture, adaptive memory selection, superior performance with efficiency
                        </div>
                        <div class="key-excerpt">
                            "In this paper, we introduce a novel approach to tackle the complexities of long video understanding with large language models (LLMs). Our proposed memory-propagated streaming encoding architecture segments long videos into short clips and iteratively encodes each clip in sequence. By leveraging historical memory from preceding clips, we incorporate temporal dynamics into the encoding process and produce a fixed-length memory to encapsulate arbitrarily long videos."
                        </div>
                        <div class="analysis">
                            Clear restatement of contributions with emphasis on technical innovation and practical benefits.
                        </div>
                    </div>
                </div>
            </div>

        </div>

        <h2>Cross-Disciplinary Comparison</h2>
        <table class="comparison-table">
            <tr>
                <th>Aspect</th>
                <th>VideoStreaming Paper (CS/CV)</th>
                <th>Traditional Academic</th>
                <th>Key Learning</th>
            </tr>
            <tr>
                <td>Literature Review</td>
                <td class="highlight">Separate section after introduction</td>
                <td>Integrated into introduction</td>
                <td>CS papers often dedicate full sections to related work</td>
            </tr>
            <tr>
                <td>Technical Detail</td>
                <td class="highlight">High (algorithms, math, architectures)</td>
                <td>Medium (methods overview)</td>
                <td>Mathematical formalism crucial for CS credibility</td>
            </tr>
            <tr>
                <td>Evaluation</td>
                <td class="highlight">Quantitative metrics, ablation studies, multiple datasets</td>
                <td>Theoretical validation, qualitative</td>
                <td>Empirical rigor through comprehensive evaluation</td>
            </tr>
            <tr>
                <td>Contribution Claims</td>
                <td class="highlight">Technical novelty + efficiency gains</td>
                <td>Theoretical advancement + evidence</td>
                <td>CS emphasizes computational efficiency and scalability</td>
            </tr>
            <tr>
                <td>Training Methodology</td>
                <td class="highlight">Detailed two-stage training, data construction</td>
                <td>General methodology description</td>
                <td>Reproducibility through detailed training procedures</td>
            </tr>
        </table>

        <h2>Imitation Framework for Future Papers</h2>
        <div class="analysis">
            <h3>Structural Elements to Adapt:</h3>
            <ul>
                <li><strong>Streaming Architecture:</strong> Sequential processing with memory propagation for long sequences</li>
                <li><strong>Adaptive Selection:</strong> Question-dependent information retrieval mechanisms</li>
                <li><strong>Progressive Training:</strong> Multi-stage training paradigm for complex systems</li>
                <li><strong>Comprehensive Evaluation:</strong> Multiple datasets with varying characteristics</li>
            </ul>

            <h3>Rhetorical Strategies:</h3>
            <ul>
                <li><strong>Problem Identification:</strong> Systematic critique of existing limitations with specific examples</li>
                <li><strong>Technical Positioning:</strong> Clear differentiation through architectural innovations</li>
                <li><strong>Efficiency Emphasis:</strong> Highlighting computational advantages alongside performance gains</li>
            </ul>

            <h3>Quality Indicators:</h3>
            <ul>
                <li><strong>Mathematical Rigor:</strong> Formal notation and algorithmic descriptions</li>
                <li><strong>Empirical Validation:</strong> Multiple evaluation metrics across diverse datasets</li>
                <li><strong>Ablation Studies:</strong> Systematic component analysis</li>
                <li><strong>Scalability:</strong> Handling arbitrarily long inputs with constant memory</li>
            </ul>
        </div>

        <div style="text-align: center; margin-top: 30px; color: #7f8c8d; font-size: 0.9em;">
            <p>Analysis based on macro-level structure framework from academic writing pedagogy</p>
            <p>Interactive visualization for learning paper organization patterns</p>
        </div>
    </div>
</body>
</html>
