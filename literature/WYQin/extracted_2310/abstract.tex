%\section{Abstract}
\begin{abstract}
%Constrained online convex optimization problem (COCO) is considered where after an online policy chooses an action $x_t$ on round $t$, the adversary reveals a convex cost function $f_t$, and a set of $k$ convex constraints of the form $g_{t,i}(x) \leq 0, i \in [k]$. The cost function $f_t$ and the constraint functions $g_{t,i}$'s could change arbitrarily with time, and no information about the future functions is assumed to be available. 
A well-studied generalization of the standard online convex optimization (OCO) framework is constrained online convex optimization (COCO). In COCO, on every round, a convex cost function and a convex constraint function are revealed to the learner after it chooses the action for that round. The objective is to design an online learning policy that simultaneously achieves a small regret while ensuring a small cumulative constraint violation (CCV) against an adaptive adversary interacting over a horizon of length $T$. A long-standing open question in COCO is whether an online policy can simultaneously achieve $O(\sqrt{T})$ regret and $\tilde{O}(\sqrt{T})$ CCV without any restrictive assumptions. For the first time, we answer this in the affirmative and show that a simple first-order policy can simultaneously achieve these bounds. Furthermore, in the case of strongly convex cost and convex constraint functions, the regret guarantee can be improved to $O(\log T)$ while keeping the CCV bound the same as above.
We establish these results by effectively combining adaptive OCO policies as a blackbox with Lyapunov optimization - a classic tool from control theory. Surprisingly, the analysis is short and elegant. 
%A potential function-based proof technique is presented that reveals a connection between regret and certain sequential inequalities through a novel decomposition result. The resulting guarantees either match the best-known results or provide new results in certain cases.
%We show that optimal performance bounds can be achieved by solving the surrogate problem using \emph{any} adaptive OCO policy that enjoys a standard data-dependent regret bound.  We conclude the paper by highlighting the application of the above framework to online multi-task learning and network control problems.
\end{abstract}
%\begin{keywords}
% Online learning, Constrained optimization, Regret bounds
%\end{keywords}

\iffalse
We consider a generalization of the classic Online Convex Optimization (OCO) problem with long-term budget constraints. Specifically, we assume that on round $t$ the policy takes action $x_t$ from an admissible set $\Omega,$ and the adversary reveals a convex cost function $f_t: \Omega \mapsto \mathbb{R}$ and a collection of $k$ constraints of the form $g_{t,i}(x)\leq 0, 1\leq i\leq k $ where each $g_{t,i}: \Omega \mapsto \mathbb{R}$ is a convex function. The cost and the constraint functions could be arbitrarily varying with time, and no information about the future cost and constraint functions (including bounds to the norm of the sub-gradients and/or strong-convexity parameters) is available.
%The cost and the constraint functions could be decided arbitrarily by an adversary and \emph{no} information about the future cost and constraint functions (including upper-bounds to the norm of the sub-gradients and/or strong convexity parameters) are available. 
We design a meta-policy that achieves a sublinear constraint violation penalty and sublinear regret against any feasible set of fixed actions. This is achieved by constructing a black box reduction of the problem to an OCO problem with a carefully constructed sequence of surrogate cost functions. Optimal performance bounds are achieved by solving the surrogate problem using any adaptive online policy with a standard data-dependent regret bound. A new potential-based proof technique is presented that reveals a new connection between deriving regret bounds and solving certain sequential inequalities through a novel regret decomposition result.
\fi
%Our analysis introduces a new general technique for deriving performance bounds by bounding the growth of a sequence of real numbers satisfying certain recursive inequalities.
% Our proof technique is flexible and can be used to extend a wide array of unconstrained online problems to their constrained version in almost a routine fashion. To be more specific, our reduction enables one to translate the problem of bounding the regret and constraint violation penalties into mere calculus exercise of bounding the order of growth of real sequences satisfying certain recursive inequalities.  
 %Compared to previous works on this problem which use a constant \citep{yuan2018online}, or a fixed sequence of step sizes \citep{jenatton2016adaptive} for the primal problem, our proposed policy uses a sequence of \emph{adaptive} step-sizes which explicitly depend on the magnitude of the past gradients.. 