<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DHE Paper Macro-Level Structure Visualization</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            background-color: #f5f5f5;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
        }
        h1 {
            text-align: center;
            color: #2c3e50;
            margin-bottom: 30px;
        }
        .structure-diagram {
            display: flex;
            flex-direction: column;
            gap: 20px;
        }
        .section {
            border: 2px solid #3498db;
            border-radius: 8px;
            padding: 15px;
            background: #ecf0f1;
            position: relative;
        }
        .section-header {
            background: #3498db;
            color: white;
            padding: 10px;
            margin: -15px -15px 15px -15px;
            border-radius: 6px 6px 0 0;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        .section-title {
            font-weight: bold;
            font-size: 1.2em;
        }
        .word-count {
            font-size: 0.9em;
            opacity: 0.9;
        }
        .moves {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 10px;
            margin-top: 10px;
        }
        .move {
            background: white;
            padding: 10px;
            border-radius: 5px;
            border-left: 4px solid #e74c3c;
        }
        .move-title {
            font-weight: bold;
            color: #e74c3c;
            margin-bottom: 5px;
        }
        .move-content {
            font-size: 0.9em;
            color: #555;
        }
        .key-excerpt {
            background: #fff3cd;
            border: 1px solid #ffeaa7;
            border-radius: 5px;
            padding: 10px;
            margin: 10px 0;
            font-style: italic;
        }
        .analysis {
            background: #d4edda;
            border: 1px solid #c3e6cb;
            border-radius: 5px;
            padding: 10px;
            margin: 10px 0;
            font-size: 0.9em;
        }
        .flow-arrow {
            text-align: center;
            font-size: 2em;
            color: #7f8c8d;
            margin: 10px 0;
        }
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: white;
        }
        .comparison-table th, .comparison-table td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        .comparison-table th {
            background-color: #f2f2f2;
            font-weight: bold;
        }
        .highlight {
            background-color: #fff3cd;
            font-weight: bold;
        }
        .legend {
            background: #ecf0f1;
            padding: 15px;
            border-radius: 5px;
            margin: 20px 0;
        }
        .legend-item {
            display: inline-block;
            margin: 5px 15px 5px 0;
            padding: 5px 10px;
            border-radius: 3px;
            font-size: 0.9em;
        }
        .cs-convention { background: #3498db; color: white; }
        .traditional { background: #e74c3c; color: white; }
        .universal { background: #27ae60; color: white; }
    </style>
</head>
<body>
    <div class="container">
        <h1>DHE Paper: Macro-Level Structure Analysis</h1>
        <p style="text-align: center; color: #7f8c8d;"><strong>Provable Discriminative Hyperspherical Embedding for Out-of-Distribution Detection</strong><br>
        AAAI 2025 | Machine Learning / Out-of-Distribution Detection</p>

        <div class="legend">
            <h3>Analysis Framework</h3>
            <div class="legend-item cs-convention">Computer Science Convention</div>
            <div class="legend-item traditional">Traditional Academic</div>
            <div class="legend-item universal">Universal Best Practice</div>
        </div>

        <div class="structure-diagram">

            <!-- Introduction Section -->
            <div class="section">
                <div class="section-header">
                    <span class="section-title">Section 1: Introduction (CARS Model)</span>
                    <span class="word-count">~700 words</span>
                </div>

                <div class="moves">
                    <div class="move">
                        <div class="move-title">Move 1: Establishing Territory</div>
                        <div class="move-content">
                            <strong>Centrality Claims:</strong> OOD detection is "crucial" for deployed ML models. Distance-based methods show "very encouraging performance."
                        </div>
                        <div class="key-excerpt">
                            "Machine learning models are typically trained with the implicit assumption that training data and test data share the same distribution, which forms in-distribution (ID) scenario. However, in many practical scenarios, a deployed neural model could be inevitably exposed to the out-of-distribution (OOD) examples that deviate from the training distribution (Rawat and Wang 2017). As a result, the model will be confused and incorrectly attribute the OOD examples into ID classes, leading to risks in practically implementing AI algorithms (Ulmer, Meijerink, and Cinà 2020; Yang et al. 2022)."
                        </div>
                        <div class="analysis">
                            Strong centrality claims supported by practical deployment scenarios. Notice the progression: standard assumption → practical reality → negative consequences ("risks in practically implementing AI algorithms"). The ethical framing with "risks" establishes importance.
                        </div>
                        <div class="key-excerpt">
                            "Among these, the distance-based methods have shown very encouraging performance by assuming that OOD examples should be distant from the clusters of ID data in the embedding space. This assumption enables the learning of discriminative embeddings, which facilitates the accurate identification of OOD examples."
                        </div>
                        <div class="analysis">
                            Position statement favoring distance-based methods. Notice the explicit assumption statement and the connection between assumption and benefit ("enables the learning...facilitates the accurate identification").
                        </div>
                    </div>

                    <div class="move">
                        <div class="move-title">Move 2: Establishing Niche</div>
                        <div class="move-content">
                            <strong>Gaps Identified:</strong> Lack of theoretical foundations, trapped in local optima, computational inefficiency
                        </div>
                        <div class="key-excerpt">
                            "Previous works (Sehwag, Chiang, and Mittal 2020; Sun et al. 2022; Ming et al. 2023) have demonstrated that large inter-class dispersion helps to improve the performance of OOD detection. Nevertheless, current distance-based OOD detection methods did not adequately explore the theoretical foundations for the effectiveness of prototype dispersion. As a result, the existing distance-based methods could be trapped in local optima when conducting inter-class dispersion, which leads to performance degradation and considerable waste of computing resources."
                        </div>
                        <div class="analysis">
                            Strong gap identification through theoretical limitation. Notice the progression: acknowledges previous findings ("have demonstrated") → identifies gap ("did not adequately explore the theoretical foundations") → negative consequences ("trapped in local optima...performance degradation...waste of computing resources"). The connection to computational resources adds practical significance.
                        </div>
                        <div class="key-excerpt">
                            "It is worth noting that although our proposed DHE looks similar to CIDER (Ming et al. 2023), they diverges fundamentally in multiple key aspects. Specifically, our DHE theoretically ensures the maximization of dispersion among different class prototypes. In contrast, CIDER cannot guarantee such maximal inter-class dispersion among prototypes. Additionally, we theoretically prove that increasing inter-class distance can enhance the ability to detect OOD examples when a distance-based scoring function is adopted. However, such theoretical justification is absent in CIDER. Consequently, when compared with CIDER, our DHE achieves enhanced training efficiency and superior OOD detection performance."
                        </div>
                        <div class="analysis">
                            Direct comparison with most related work (CIDER). Notice the explicit differentiation ("looks similar...but diverges fundamentally") and the emphasis on theoretical guarantees ("theoretically ensures," "theoretically prove"). The connection between theory and practice ("achieves enhanced training efficiency and superior OOD detection performance") is explicit.
                        </div>
                    </div>

                    <div class="move">
                        <div class="move-title">Move 3: Occupying Niche</div>
                        <div class="move-content">
                            <strong>Solution:</strong> DHE with theoretical guarantees, angular spread loss, pre-computation efficiency
                        </div>
                        <div class="key-excerpt">
                            "Therefore, in this work, we propose a simple yet effective distance-based OOD detection method called provable Discriminative Hyperspherical Embedding (DHE) to obtain the embeddings that are highly discriminative in distinguishing ID and OOD examples. Specifically, we conduct an in-depth theoretical analysis of inter-class dispersion, which demonstrates that increasing inter-class dispersion is beneficial for reducing the false positive rate (FPR) of model on OOD examples. Inspired by these theoretical insights, we introduce an angular spread loss to maximize prototype dispersion. Additionally, a prototype-enhanced contrastive (PEC) loss is utilized to ensure that the embeddings of ID examples are closely around their corresponding prototypes, which further enhances the discriminability of feature embeddings."
                        </div>
                        <div class="analysis">
                            Strong positioning with theoretical foundation emphasis. Notice the phrase "simple yet effective" (common in ML papers) and the explicit connection between theory and method ("Inspired by these theoretical insights"). The progression from theoretical analysis → loss design → embedding alignment is clear.
                        </div>
                    </div>
                </div>
            </div>

            <div class="flow-arrow">↓</div>

            <!-- Theoretical Implication Section -->
            <div class="section">
                <div class="section-header">
                    <span class="section-title">Section 2: Theoretical Implication (Novel Section)</span>
                    <span class="word-count">~1000 words</span>
                </div>

                <div class="moves">
                    <div class="move">
                        <div class="move-title">Formal Theoretical Analysis</div>
                        <div class="move-content">
                            <strong>Theoretical Framework:</strong> Definitions, lemmas, theorems with proofs in appendices
                        </div>
                        <div class="key-excerpt">
                            "Theorem 3. If the scoring function Sθ : RD → R is distance-based, then the FPR for estimating P (Gτα (x0 ) = ID data) has F P R ∝ r̂n /r̂o , where x0 refers to an OOD example, r̂n = E (E (∥zi − µc ∥2 | yi = c)) denotes the average intra-class distances, and r̂o = Ec1 ̸=c2 ∥µc1 − µc2 ∥2 , c1 , c2 ∈ {1, 2, ..., K} represents the average inter-class distances."
                        </div>
                        <div class="analysis">
                            Key theoretical result connecting FPR to intra-class and inter-class distances. Notice the proportionality relationship (FPR ∝ r̂n /r̂o) which provides clear guidance: reduce intra-class distance (r̂n) and increase inter-class distance (r̂o). This directly motivates the method design.
                        </div>
                        <div class="key-excerpt">
                            "Theorem 3 indicates that the FPR is proportional to r̂n and is inversely proportional to r̂o when using a distance-based scoring function. To reduce the FPR in OOD detection, we can decrease r̂n and increase r̂o . Given that no OOD data are involved during the training phase, Theorem 3 offers strong theoretical guidance on constructing discriminative embeddings z for the data x."
                        </div>
                        <div class="analysis">
                            Clear interpretation of theoretical result and connection to method design. Notice the explicit guidance ("decrease r̂n and increase r̂o") and the emphasis on "strong theoretical guidance." The connection to training constraints ("no OOD data are involved") is important.
                        </div>
                    </div>
                </div>
            </div>

            <div class="flow-arrow">↓</div>

            <!-- Method Section -->
            <div class="section">
                <div class="section-header">
                    <span class="section-title">Section 3: Method</span>
                    <span class="word-count">~1500 words</span>
                </div>

                <div class="moves">
                    <div class="move">
                        <div class="move-title">Two-Stage Framework</div>
                        <div class="move-content">
                            <strong>Stage 1:</strong> Prototype dispersion maximization (pre-computation)
                            <br><strong>Stage 2:</strong> Embedding alignment with prototypes
                        </div>
                        <div class="key-excerpt">
                            "Building on the insights from Theorem 3, we propose a training framework for acquiring suitable data embedding, so that the second challenge of OOD detection can be addressed from two crucial aspects, namely: 1) maximizing inter-class distances to enhance category distinction; and 2) ensuring the feature embeddings are closely around the corresponding prototypes of the same class."
                        </div>
                        <div class="analysis">
                            Clear connection to theoretical results ("Building on the insights from Theorem 3"). Notice the explicit enumeration of two objectives, which directly correspond to increasing r̂o and decreasing r̂n from Theorem 3.
                        </div>
                        <div class="key-excerpt">
                            "The framework of our method is shown in Figure 1. Firstly, prior to classifier training, we initialize the prototypes by averaging the embeddings zi of each class. Subsequently, we optimize the dispersion among prototypes to obtain a set of maximally dispersed prototypes M = {µc ∈ RD , c ∈ {1, 2, ..., K}}. During the classifier training phase, the encoder fθ is trained to ensure that the embeddings of ID examples are closely around their corresponding prototypes."
                        </div>
                        <div class="analysis">
                            Clear two-stage framework description. Notice the temporal organization ("Firstly...Subsequently...During") and the emphasis on "prior to classifier training" which highlights the pre-computation advantage. The formal set notation (M = {µc ∈ RD , c ∈ {1, 2, ..., K}}) maintains mathematical rigor.
                        </div>
                        <div class="key-excerpt">
                            "Theorem 4. For any two classes i, j ∈ {1, 2, ..., K}, the sum of squared distances between prototypes µi and µj is upper bounded by
                            1
                            2
                            P
                            i̸=j ∥µi − µj ∥2
                            2 ≤ K
                            2
                            ,
                            where the equality holds if and only if
                            µ⊤
                            i µj = (
                            1, i = j
                            1/(1 − K), i ̸= j
                            "
                        </div>
                        <div class="analysis">
                            Key theoretical result for prototype dispersion. Notice the upper bound and the "if and only if" condition which provides the optimal configuration. This theorem provides the theoretical foundation for the angular spread loss.
                        </div>
                        <div class="key-excerpt">
                            "Minimizing the angular spread loss Las is equivalent to finding the global optimum of a quadratic function, which ensures that the maximal dispersion of prototypes can be achieved efficiently and reliably. Therefore, the maximal dispersion of class prototypes can be guaranteed both theoretically and empirically."
                        </div>
                        <div class="analysis">
                            Emphasis on theoretical guarantee ("can be guaranteed both theoretically and empirically"). Notice the claim about global optimum and efficiency. The phrase "both theoretically and empirically" addresses both aspects of validation.
                        </div>
                    </div>
                </div>
            </div>

            <div class="flow-arrow">↓</div>

            <!-- Experiments Section -->
            <div class="section">
                <div class="section-header">
                    <span class="section-title">Section 4: Experiments</span>
                    <span class="word-count">~1200 words</span>
                </div>

                <div class="moves">
                    <div class="move">
                        <div class="move-title">Empirical Validation</div>
                        <div class="move-content">
                            <strong>Metrics:</strong> FPR95, AUROC
                            <br><strong>Efficiency:</strong> Training time comparisons (62% reduction vs. CIDER)
                        </div>
                        <div class="key-excerpt">
                            "As shown in Table 1, the proposed DHE significantly enhances the OOD detection performance and achieves superior performance than the baseline methods. Unlike the existing distance-based approaches that employ contrastive loss such as KNN+ and SSD+, DHE can effectively maximize the inter-class dispersion specifically for OOD detection. As a result, DHE achieves a reduction of 18.81% compared with SSD+ and 18.69% compared with KNN+ on FPR95, respectively."
                        </div>
                        <div class="analysis">
                            Quantitative comparison with specific percentages. Notice the explicit connection between method capability ("can effectively maximize the inter-class dispersion") and performance ("achieves a reduction of 18.81%"). The comparison with specific methods (SSD+, KNN+) is clear.
                        </div>
                        <div class="key-excerpt">
                            "Moreover, DHE outperforms the latest baseline methods T2FNORM and ReweightOOD, reducing FPR95 by 25.55% and 13.22%, respectively. Besides, DHE surpasses the most relevant baseline method, i.e., CIDER, by 5.37% on FPR95. Note that the main difference between the proposed DHE and CIDER lies in that DHE is provable to obtain the prototypes with maximal dispersion, which can greatly enhance the discriminative power of embeddings."
                        </div>
                        <div class="analysis">
                            Multiple comparisons with quantitative claims. Notice the emphasis on the most related work (CIDER) and the explicit differentiation ("is provable to obtain the prototypes with maximal dispersion"). The connection between theoretical property and performance ("can greatly enhance") is explicit.
                        </div>
                        <div class="key-excerpt">
                            "DHE exhibits high computational efficiency. Figure 4 exhibits the training time (seconds per epoch) of different methods when using the CIFAR-100 dataset (ID) with a ResNet-34 model. Here, we compare our proposed DHE with the CE loss and the popular contrastive learning method SupCon (Khosla et al. 2020) that is utilized in KNN+ (Sun et al. 2022) and SSD+ (Sehwag et al. 2021). We also compare our proposed DHE with CIDER (Ming et al. 2023), which is specially designed for OOD detection. The results clearly show that DHE maintains competitive computational efficiency, when compared with CE and SupCon. It is worth noting that our method reduces training time by 62% when compared with CIDER."
                        </div>
                        <div class="analysis">
                            Efficiency evaluation with quantitative claim ("reduces training time by 62%"). Notice the comparison with multiple baselines and the emphasis on computational efficiency, which is a key contribution. The large percentage reduction (62%) is significant.
                        </div>
                    </div>
                </div>
            </div>

            <div class="flow-arrow">↓</div>

            <!-- Conclusion Section -->
            <div class="section">
                <div class="section-header">
                    <span class="section-title">Section 5: Conclusion</span>
                    <span class="word-count">~200 words</span>
                </div>

                <div class="moves">
                    <div class="move">
                        <div class="move-title">Synthesis & Impact</div>
                        <div class="move-content">
                            <strong>Contributions:</strong> Theoretical guarantees, angular spread loss, computational efficiency
                            <br><strong>Key Achievement:</strong> Provable maximal prototype dispersion + 62% training time reduction
                        </div>
                        <div class="key-excerpt">
                            "In this work, we propose DHE, a simple yet effective prototype-based contrastive learning framework for OOD detection. Our theoretical analysis demonstrates that inter-class dispersion is crucial for effectively distinguishing between ID and OOD examples. Inspired by this, we devise an angular spread loss to provably maximize the dispersion among prototypes. Furthermore, we introduce a prototype-enhanced contrastive loss to ensure that embeddings are tightly clustered around their corresponding class prototypes. By simultaneously maximizing the inter-class distances and minimizing the intra-class distance, the ID-OOD separability can be greatly enhanced."
                        </div>
                        <div class="analysis">
                            Strong conclusion that summarizes contributions and connects theory to practice. Notice the emphasis on "provably maximize" and the explicit connection between theoretical insights and method design ("Inspired by this"). The summary of the approach ("simultaneously maximizing...and minimizing...") is clear.
                        </div>
                    </div>
                </div>
            </div>

        </div>

        <h2>Cross-Disciplinary Comparison</h2>
        <table class="comparison-table">
            <tr>
                <th>Aspect</th>
                <th>DHE Paper (ML)</th>
                <th>Traditional Academic</th>
                <th>Key Learning</th>
            </tr>
            <tr>
                <td>Theoretical Section</td>
                <td class="highlight">Separate dedicated section with formal proofs</td>
                <td>Integrated theoretical discussion</td>
                <td>ML papers emphasize theoretical rigor with separate sections</td>
            </tr>
            <tr>
                <td>Technical Detail</td>
                <td class="highlight">Very High (theorems, proofs, loss functions)</td>
                <td>Medium (theoretical frameworks)</td>
                <td>Mathematical formalism crucial for CS credibility</td>
            </tr>
            <tr>
                <td>Evaluation Focus</td>
                <td class="highlight">Quantitative metrics + computational efficiency</td>
                <td>Theoretical validation, qualitative</td>
                <td>Empirical rigor through performance + efficiency</td>
            </tr>
            <tr>
                <td>Contribution Claims</td>
                <td class="highlight">Theoretical guarantees + empirical validation + efficiency</td>
                <td>Theoretical advancement + evidence</td>
                <td>CS emphasizes provable guarantees + practical efficiency</td>
            </tr>
            <tr>
                <td>Efficiency Emphasis</td>
                <td class="highlight">Strong (pre-computation, training time comparisons)</td>
                <td>Moderate (computational considerations)</td>
                <td>Computational efficiency as key contribution</td>
            </tr>
        </table>

        <h2>Imitation Framework for Future Papers</h2>
        <div class="analysis">
            <h3>Structural Elements to Adapt:</h3>
            <ul>
                <li><strong>Separate Theoretical Section:</strong> Dedicated section for formal theoretical analysis</li>
                <li><strong>Two-Stage Framework:</strong> Clear separation of prototype optimization and embedding learning</li>
                <li><strong>Theorem Statements:</strong> Formal presentation of theoretical results with proofs</li>
                <li><strong>Efficiency Evaluation:</strong> Quantitative comparisons of computational efficiency</li>
                <li><strong>Pre-computation Design:</strong> Methods that enable pre-computation for efficiency gains</li>
            </ul>

            <h3>Rhetorical Strategies:</h3>
            <ul>
                <li><strong>Theoretical Positioning:</strong> Emphasis on "theoretically guarantees," "provable," "theoretical foundation"</li>
                <li><strong>Efficiency Emphasis:</strong> Highlighting computational advantages alongside performance</li>
                <li><strong>Direct Comparison:</strong> Explicit differentiation from most related work (CIDER)</li>
                <li><strong>Theory-Practice Bridge:</strong> Clear statements connecting theoretical insights to method design</li>
                <li><strong>Quantitative Claims:</strong> Specific percentages for performance and efficiency improvements</li>
            </ul>

            <h3>Quality Indicators:</h3>
            <ul>
                <li><strong>Theoretical Rigor:</strong> Formal definitions, lemmas, theorems with proofs</li>
                <li><strong>Empirical Validation:</strong> Multiple benchmarks and metrics</li>
                <li><strong>Efficiency Validation:</strong> Quantitative computational efficiency comparisons</li>
                <li><strong>Reproducibility:</strong> Detailed experimental setup</li>
                <li><strong>Theory-Practice Alignment:</strong> Clear connection between theoretical results and method design</li>
            </ul>
        </div>

        <div style="text-align: center; margin-top: 30px; color: #7f8c8d; font-size: 0.9em;">
            <p>Analysis based on macro-level structure framework from academic writing pedagogy</p>
            <p>Interactive visualization for learning paper organization patterns</p>
        </div>
    </div>
</body>
</html>
