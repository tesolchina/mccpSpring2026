# Realizing Scaling Laws in Recommender Systems: A Foundation–Expert Paradigm for Hyperscale Model Deployment

## Abstract

While scaling laws promise significant performance gains for recommender systems, efficiently deploying hyperscale models remains a major unsolved challenge. In contrast to fields where FMs are already widely adopted such as natural language processing and computer vision, progress in recommender systems is hindered by unique challenges including the need to learn from online streaming data under shifting data distributions, the need to adapt to different recommendation surfaces with a wide diversity in their downstream tasks and their input distributions, and stringent latency and computational constraints. To bridge this gap, we propose to leverage the Foundation-Expert Paradigm: a framework designed for the development and deployment of hyperscale recommendation FMs. In our approach, a central FM is trained on lifelong, cross-surface, multi-modal user data to learn generalizable knowledge. This knowledge is then efficiently transferred to various lightweight, surface-specific "expert" models via target-aware embeddings, allowing them to adapt to local data distributions and optimization goals with minimal overhead. To meet our training, inference and development needs, we built HyperCast, a production-grade infrastructure system that re-engineers training, serving, logging and iteration to power this decoupled paradigm. Our approach is now deployed at Meta serving tens of billions of user requests daily, demonstrating online metric improvements over our previous one-stage production system while improving developer velocity and maintaining infrastructure efficiency. To the best of our knowledge, this work represents the first successful deployment of a Foundation-Expert paradigm at this scale, offering a proven, compute-efficient, and developer-friendly blueprint to realize the promise of scaling laws in recommender systems.

## Introduction

The identification and systematic characterization of scaling laws in deep learning models has fundamentally transformed industrial practice [citation]. While these principles originated in the study of large language models, they have since been validated and applied to the study recommender systems [citation]. Scale now plays a fundamental role in driving recommender system performance toward the end of goal of delivering delightful and engaging user experiences. Despite the potential offered by scaling recommender models, their deployment in large-scale production environments presents a significant challenge. First, training large recommendation models often requires hundreds or even thousands of high-performance GPUs, making efficient iteration challenging for researchers and developers. Second, recommendation systems typically consist of multiple applications and surfaces, each requiring dedicated development and tuning, making scaling and maintaining of dedicated large models for each impractical. In the present work, we demonstrate how to overcome these challenges by leveraging an adapter/expert paradigm [citation] for training foundation models (FMs) coupled with our novel serving and deployment stack. Together these innovations allow us to deploy hyper-scale recommendation models in production systems efficiently; thereby laying the groundwork for realizing the full potential of scaling laws in recommendation systems. FMs have emerged as a transformative paradigm for solving challenges in machine learning over the past years. In fields such as computer vision [citation], time-series forecasting [citation], and natural language processing [citation], FMs have eclipsed performance benchmarks through their ability to generalize from pretraining on massive datasets. In the broadest terms, a FM can be defined as a deep learning based model which takes advantage of { transfer learning at scale} [citation]. In practice, leveraging FMs to solve problems typically involves a two-phase training process: Despite their success across a variety of domains, the application of FMs in large-scale recommendation systems remains nascent due, in large part, to two challenges: (i) that traditional supervised fine-tuning (SFT) is not well-suited to the streaming data setting and (ii) that teacher-student paradigms often results in a low percentage of gains transferring from the teacher to the student. While SFT is well-suited to problem settings where the FM can be trained using mostly static data [citation], most industrial scale recommendation engines are trained with online one-epoch streaming data based on sparse IDs. In the streaming data setting, SFT suffers from significant challenges including catastrophic forgetting during fine-tuning [citation], difficulty maintaining performance when fine-tuning on shifting data distributions [citation], and suboptimal strategies for coordinating updates between the foundation model and task-specific layers. For these reasons, another popular approach for leveraging FMs in industrial recommendation systems is knowledge-distillation [citation]. In standard knowledge distillation, a large ``teacher'' FM generates predictions as soft labels to help train a smaller ``student'' model [citation]. Online production traffic is then served only by the student model. While this avoids serving computational bottlenecks and is well-suited to the streaming data setting because both the teacher and student can be continually trained on incoming data, it can be challenging to ensure that improvements to the teacher are effectively transferred to the student. For example, a number of recent works focus on designing specialized losses to mitigate bias from the teacher [citation] and recent work on uncovering scaling laws for knowledge distillation found that we can expect student performance to be harmed by the teacher in the large data regime [citation]. These empirical results are supported by well-known results from the classical statistics literature showing that maximum likelihood estimation is consistent and asymptotically efficient [citation]. In this work, we propose the **Foundation-Expert paradigm**, an alternative to methods like SFT or knowledge distillation. Our approach integrates a large, general-purpose foundation model (FM) with smaller, specialized expert models, decoupling general knowledge learning from task-specific adaptation. This separation addresses production bottlenecks and ensures computational feasibility in demanding online streaming environments. The core knowledge transfer mechanism in this paradigm is **target-aware embeddings**. The FM, continuously trained on lifelong multi-modal user histories spanning multiple recommendation surfaces, generates target-aware embeddings embeddings for each candidate item (target). Unlike traditional relatively stable user embeddings [citation] that offer { a general summary of user behavior}, the target-aware embeddings { dynamically capture a user's contextual interest in a specific item, given the user's interaction history and item information}, providing a more effective signal for downstream tasks. These FM embeddings are then ingested by the expert models, which use them as input features and optimize on surface-specific objectives. While expert/adapter based approaches have shown great success in a wide-variety of learning contexts [citation] including computer vision [citation] and NLP [citation] we believe that this is the first time such an approach has been applied to an industrial recommendation system of this scale. The entire paradigm is enabled by **HyperCast**, our production-grade infrastructure designed for decoupled, multi-tier model training, serving, deployment and iteration. Comprehensive offline and online A/B tests demonstrate significant improvements over the traditional one-stage paradigm across multiple recommendation surfaces. Infrastructure metrics such as end-to-end serving latency and CPU remains neutral, with model freshness on the order of minutes and an average data-to-trainer latency of 30 minutes, benefiting from the systematic optimizations from HyperCast. Taken together, the key contributions of this work are summarized as follows: Currently deployed across several core recommendation surfaces at Meta and serving tens of billions of daily requests, our paradigm achieves statistically significant user experience improvements while enhancing developer velocity and infrastructure efficiency. To the best of our knowledge, this work represents the first successful deployment of a Foundation-Expert paradigm at this scale, offering a proven, compute-efficient, and developer-friendly blueprint for realizing the promise of scaling laws in industrial recommender systems.

## Related works

In the previous section we discussed connections to SFT and teacher-student paradigms. In this section we focus on connections to long user history modeling and methods for learning rich user representations for recommendation systems. **Long User History Modeling** Over the past two years much of the improvement in industry content recommendation quality was arguably driven by systems which learn from long user interaction histories; for example, see recent works from Meta [citation], LinkedIn [citation], ByteDance [citation], Xiaohongshu [citation], and Alibaba [citation]. These works introduced efficient architectures for sequence modeling and demonstrated the effectiveness of scaling up user history learning in recommender models. Our work is orthogonal to these previous work, as we focus on how to efficiently productionalize the scaled model via a Foundation-Expert framework. Most of those innovations can be applied to our FM design. In this work, we leverage the architecture introduced in [citation], the first generative recommendation system in the literature. **Learning rich representations for downstream tasks.** Closely related to our work are methods which utilize models to learn representations of user or item to improve predictive performance on downstream tasks [citation]. These methods largely focus on learning general user or item summarization independently, without focusing on representation of user and item pair --user's target-aware representation is about the user's interest in a specific item based on his/her behavior sequence and the item information). While this approach is beneficial in terms of computational efficiency, it is inherently limited in the expressiveness of the representations it can learn. As a result, it struggles to achieve a high transfer ratio from the FM model to the expert model. Recent studies have shown that target-aware modeling is important for enhancing the performance of recommender models [citation]. In the context of recommendation systems, [citation] developed an approach for training FMs to learn from long user histories offline. As compared to the approach developed in the present work, our FM focuses on learning target-aware embeddings for each candidate item. In addition, our FM is trained in online streaming setup and updated at a high frequency (on the order of several minutes), continuously adapting to latest user interactions.

## Methods

In this section, we introduce the design of our proposed Foundation-Expert paradigm, a two-stage architecture designed to overcome the inefficiencies in the traditional one-stage per-surface scaling of recommender systems. In this paradigm, a central, compute-intensive FM learns general knowledge from lifelong user histories, multi-modal content understanding, and cross surface techniques. The FM generates target-aware embeddings for each candidate item which are then consumed as input features by lightweight Expert models (typically 20-40 In the following subsections, we will detail the architecture of the Foundation Model and the Experts, followed by a description of HyperCast, the end-to-end infrastructure system that enables this paradigm. ### Foundation Model Design #### Input As depicted in Figure [reference], the FM is trained on a dataset comprising of cross-surface, lifelong user histories and multi-modal content. The input features are organized into two categories: **Main Features** are used for target-aware sequential modeling to generate the FM embeddings. These include the user's interaction history and information about the target items. Each item (historical or target) is represented by its categorical features such as item ID $p$, contextual features $c$ (which includes but not limited to surface type, timestamp, LLM-powered multi-modal representations), and the associated user action $a$. Each of these inputs is represented as vector or embedding, $Emb_p$, $Emb_c$, and $Emb_a$, respectively. **Auxiliary Features** consist of non-sequential data, such as common categorical, continuous and embedding features used in recommender systems. These features, selected based on their importance in each surface, are used to aid the alignment of the FM embeddings during training for better generalizability on downstream experts. #### Target-aware Sequential Modeling To enable effective target-aware modeling of lifelong user behaviors, we leverage Hierarchical Sequential Transduction Units (HSTU) [citation], a transformer variant engineered for industrial-scale recommendation systems. Building upon the original HSTU architecture, we introduce an architectural simplification depicted in Figure [reference]: instead of interleaving item and action embeddings, we combine them via direct summation. Furthermore, to prevent label leakage from user history, we remove the auto-regressive auxiliary losses. This optimization effectively halves the input sequence length, yielding a 50 In practice, the inputs are a sequence of $N$ past impressions in user history $x_0, x_1,$ $ , x_{N-1} (x_i X)$ ordered chronologically, and a sequence of $M$ target items in one request $y_0, y_1, , y_{M-1}$ ($y_j X$), where $X$ denotes the set of all items in the recommendation product pool. After initial preprocessing, we get a joint unified sequence of $Emb_{x_0}, Emb_{x_1},$ $ , Emb_{x_{N-1}},$ $Emb_{y_0}, Emb_{y_1}, , Emb_{y_{M-1}}$: [ Emb_{x_i} = f(Emb_{p, i}, Emb_{c, i}) + Emb_{a, i} ] [ Emb_{y_j} = f(Emb_{p, j}, Emb_{c, j}) ] where $Emb$ is the embedding representation of the corresponding item abtained from eq:emb_1, eq:emb_2, $f()$ is a simple transformation like multilayer perceptron. With this unified sequence as input, sequential modeling in standard retrieval and ranking models can be formulated as shown in Figure [reference]. #### Foundation Model Alignment Similar to many recommendation models, our FM is optimized using a multi-task multi-label (MTML) learning objective. The overall loss function $L$ consists of two components, [ L = _{s=1}^S _s L_{main_s} + _{t=1}^T _t L_{aux_t} ] where $L_{main_s}$ and $L_{aux_t}$ denote the loss of each shared main task and surface-specific auxiliary task respectively, $_s$ and $_t$ denote the weight of the corresponding task $s$ and $t$, $S$ denote total main tasks and $T$ denote total auxiliary tasks. **Main Loss ($L_{main**$)} This loss is derived from generalizable, cross-surface objectives such as likes, shares, and video completions. This supervision is applied directly to the HSTU module's output embeddings after a simple multi-task (mt) module, ensuring it can learn powerful and broadly applicable target-aware representations. **Auxiliary Loss ($L_{aux**$)} This loss is designed for surface-specific alignment using crucial tasks from each domain. The target-aware embeddings are passed to a lightweight Alignment Module for interactions with auxiliary features. To handle the heterogeneous nature of these tasks (e.g., engagement with video only happens on a product surface that presents videos). the loss for each auxiliary task is calculated only over its respective valid sample space: [ L_{aux_t}(_H, _{aux_t}) = 1{_i ^i_t} ^i_t loss_t(y^i_t(_H, _{aux_t}), y_t^i) ] where $_{aux_t}$ is the heterogeneous Alignment Module for each specific surface, $loss_t$ is task $t$'s loss of sample i computed based on prediction $y_t^i$ ground truth $y_t^i$, $_t^i {0, 1}$ indicates whether the sample is in the sample space of task $t$. In this way, the surface-specific features, tasks and architectures serve as auxiliary to better align the FM with experts to their individual objectives. #### Efficiency Optimizations of Scalable Foundation Model A central goal of our design is to ensure the FM can be scaled efficiently in the online streaming and real-time inference environment. Building upon the efficient scaling properties of the HSTU architecture, we further developed several optimizations including compute de-duplication, sparse attention mechanisms [citation] for HSTU self-attention, Triton kernel co-design and various caching techniques. These optimizations are critical for making trillion-parameter scale FMs practical by significantly reducing resources required for training, serving, and logging. While a detailed analysis of these optimizations is beyond the scope of this paper, they are crucial to the success of the paradigm. ### Expert Design In the Foundation-Expert paradigm, the traditional one-stage model for each production surface is replaced by a lightweight Expert model. By offloading the compute-heavy task of general knowledge acquisition to the Foundation Model (FM), experts can be substantially smaller than their one-stage counterparts. This enables rapid iteration cycles focused exclusively on surface-specific optimizations. The primary architectural difference from their one-stage counterparts is the inclusion of three components: a FM Embedding Module, a FM Fusion Module, and a lightweight HSTU module dedicated to capturing short-term, real-time user interests. The data flow is as follows: first, the expert ingests the target-aware embeddings from the FM. These embeddings undergo preprocessing and robustness enhancements (e.g., regularization, denoising) within the FM Embedding Module. Subsequently, the FM Fusion Module combines these processed embeddings—representing long-term interests—with the output of the expert's own HSTU module, which represents short-term interests. This fused representation then interacts with other parts of the expert model via the Expert Fusion Module to generate final predictions for its surface-specific, multi-task learning objectives. The Expert Fusion Module's architecture is flexible, ranging from a simple MLP to more advanced structures, to meet the specific needs of different surface experts. ### System Deployment In industrial recommender systems, an online streaming setup is critical for delivering highly relevant and timely recommendations, as it allows the system to continuously ingest, process, and react to the latest user interactions. However, deploying our two-stage paradigm in such a real-time environment introduces challenges in managing high-frequency updates, low-latency inference, and agile development. To address these, we designed and built HyperCast, the end-to-end infrastructure system depicted in Figure [reference]. HyperCast powers the entire Foundation-Expert lifecycle and is engineered with the following components: #### Decoupled Training Architecture A core design principle of our paradigm is the complete decoupling of the FM and expert model iterations. This is achieved by materializing the FM's target-aware embeddings and logging them as candidate-level features available in the training data. Consequently, the FM and expert training jobs can operate independently, each consuming its own data and updating its weights without direct dependencies on the other's training state. #### High Freshness HyperCast enables exceptional model and data freshness, which is critical in a real-time recommendation environment. For model freshness, both the FM and experts are trained in online streaming fashion. HyperCast facilitates independent and high-frequency model updates, employing a component-wise streaming synchronization mechanism. Specifically, instead of publishing and updating a full model snapshot which can be time-consuming, only part of (e.g. 30 For data freshness, a real-time pipeline logs user interaction events immediately as they occur. A dynamic joining strategy then makes this data available to the online streaming trainers, reducing the average data-to-trainer latency to approximately 30 minutes. #### Multi-tier Inference Service Deployment and Optimization The Foundation-Expert paradigm necessitates three distinct inference workloads with different operational requirements: (1) online FM Serving, which provides embeddings for hundreds of ranking candidates under strict latency constraints; (2) offline FM logging, which generates embeddings only for a small subset of served items for training data and has relaxed latency requirements; and (3) online expert serving. To manage these heterogeneous requirements, HyperCast implements a multi-tier deployment architecture. Each workload is handled by an independent, purpose-built inference service tier, allowing for specialized optimization. For instance, the FM logging tier requires only one-third of the hosts compared to the online FM serving tier. Similarly, the expert tier can be configured flexibly, as the expressive power of the FM embeddings allows for substantially more light-weight expert models for each surface. This scheme enables us to tailor GPU runtime setups, latency targets, and hardware types for each tier, maximizing hardware utilization and inference efficiency. We mitigate the latency impact of the sequential two-stage serving through several major optimizations. First, HyperCast's data-flow engine merges and parallelizes feature fetching steps across the FM and expert models, ensuring these operations introduce no additional overhead. Second, GPU execution time is inherently reduced due to the lightweight nature of the experts compared with one-stage models. We further improve its efficiency by implementing a "Inference Pruning" strategy, where only the subset of the FM needed for target-aware embedding inference is deployed. These end-to-end optimizations make the two-stage serving highly efficient. #### Agile Development and Version Management The decoupled architecture significantly accelerates the development lifecycle. Experts can be iterated upon rapidly and independently because the powerful FM knowledge is materialized as input features, obviating the need for heavy joint training. To further speed up experimentation with the FM itself, HyperCast provides a mechanism which can recursively load FM checkpoints into an expert's training flow for generating FM embeddings on the fly, enabling quick evaluation without a full, resource-intensive production deployment. To manage the complexity of this decoupled environment, HyperCast includes a dedicated multi-version control framework. During data generation, embeddings from all active FM versions are logged. Each expert is then configured to select embeddings from a single, specific FM version for its training and deployment. This mechanism isolates the model lifecycles, enabling scalable and safe testing of various Foundation-Expert combinations.

## Experiment

In this section, we present a series of experiments to validate our proposed Foundation-Expert paradigm. We begin by demonstrating the effectiveness of the target-aware embeddings, the central component of our approach. Next, we show that performance improvements in the Foundation Model (FM) transfer effectively to expert models across multiple recommendation surfaces, and we analyze the generalization capabilities of the embeddings on tasks for which the FM was not explicitly trained. Finally, we present results from online A/B tests to validate the paradigm's feasibility and performance in a live production environment. ### Experiment Setup **Data** All experiments were conducted on industrial datasets. Since this work required a tight coupling between infrastructure and modeling improvements to ensure the practical relevance and scalability, we did not apply our approach to public benchmarks. **Evaluation Metrics** In the present work, we estimate the offline performance of our approach using the Normalized Entropy (NE). NE is the usual cross-entropy loss normalized by the the entropy of the data distribution [citation]. For example, given $N$ training examples and letting $y_i {0, 1 }$ be the label of the $i^{ th}$ training example, the NE is estimated as, [ NE = {1{N} _{i=1} y_i p_i + (1- y_i) (1 - p_i)} { p p + (1-p) (1-p)} ] where $p=1{N}_{i=1}^N y_i$ and $p_i$ is predicted probability for example $i$. The utility of this metric is that it is less sensitive to datasets where the number of negative examples greatly outnumbers the number of positive examples and vice-versa than the standard cross-entropy loss. We also note that an improvement to the NE of $ 0.05 In the offline evaluation we assess model performance across several important tasks: (i) ``video complete" which indicates whether or not a user watches a video from start to finish; (ii) ``video view duration" which measures how long a user watches a particular video for; and (iii) ``like" and ``share" both of which are self-explanatory. In addition to these broadly applicable tasks for both FM and most experts, the evaluation also incorporates surface-specific critical tasks, which follow the naming scheme of "Surface_X_Task_i". table[h!] tabular{lcccc} **Model** & 4{c}{**NE Diff ( (lr){2-5** & **Like** & **Share** & **VVD** & **VC** Baseline & 0 & 0 & 0 & 0 Baseline + UE & -0.64 & -1.15 & -0.81 & -0.78 Baseline + **TAE (ours)** & -2.13 & -3.02 & -2.97 & -2.96 Baseline + UE + **TAE (ours)** & -2.14 & -3.15 & -2.98 & -2.97 tabular Effectiveness of our proposed Target-Aware Embedding (TAE). UE here is the strongest internal User Embedding (UE) method. "VVD" and "VC" are the short forms of "Video View Duration" and "Video Complete" metrics. To ensure a controlled comparison, both embedding features are derived from the same temporal range of user history. NE Diff ( {table **Foundation Model** The FMs evaluated in this study are trained in standard online streaming setup, utilizing data from four important recommendation surfaces. We evaluate two FM variants, designated HSTU-0.5B (30G inference FLOPs) and HSTU-1B (80G inference FLOPs). It is important to note that the 0.5B and 1B model sizes here refer exclusively to the dense parameters; when including the sparse embedding tables, the models operate on a trillion-parameter scale. Training is conducted on 160 and 512 NVIDIA H100 GPUs for the HSTU-0.5B and HSTU-1B models, respectively. To enhance data freshness, per-surface downsampling is employed on the training data. **Experts** The expert FM Fusion Module utilizes a simple MLP as a robust baseline. While more advanced fusion strategies may yield further improvements, an exploration of these is beyond the scope of this work. Similar to the FM, the experts also utilize data downsampling; however, the specific ratios for each expert are tailored to individual surface requirements and may differ from those of the FM. table*[t] tabular{l l l c c c } **Surface** & **Task Type** & **Task Name** & **FM NE Diff Surface A** & Main & Like & -0.73 & -0.54 & 0.7397 & Main & Share & -0.50 & -0.50 & 1.0000 & Main & Video View Duration & -1.14 & -1.05 & 0.9211 & Main & Video Complete & -1.17 & -1.06 & 0.9060 **Surface B** & Main & Like & -0.83 & -0.60 & 0.7228 & Main & Share & -0.60 & -0.48 & 0.8000 & Main & Video View Duration & -1.16 & -0.91 & 0.7844 & Main & Video Complete & -1.36 & -0.92 & 0.6765 & Aux & Surface_B_Task_1 & -1.74 & -1.12 & 0.6437 & Aux & Surface_B_Task_2 & -1.03 & -0.92 & 0.8932 & Aux & Surface_B_Task_3 & -1.44 & -1.06 & 0.7361 **Surface C** & Main & Like & -0.77 & -0.60 & 0.7792 & Main & Share & -0.51 & -0.46 & 0.9020 & Main & Video View Duration & -0.99 & -0.88 & 0.8889 & Main & Video Complete & -1.23 & -0.89 & 0.7236 & Aux & Surface_C_Task_1 & -0.26 & -0.24 & 0.9231 & Aux & Surface_C_Task_2 & -0.43 & -0.40 & 0.9302 & Aux & Surface_C_Task_3 & -1.20 & -0.99 & 0.8250 {tabular Foundation-to-Expert Transfer Efficiency across Surfaces. This table presents the Transfer Ratio (higher is better) and evaluation NE performance (lower is better) on important tasks across four recommendation surfaces. Here "FM NE Diff" and "Expert NE Diff" means $NE(HSTU-1B) -NE(HSTU-0.5B)$ and $NE({Expert_{HSTU-1B}) - NE(Expert_{HSTU-0.5B})$ respectively. For "Task Type", "Main" means that task is main task for both FMs and Experts. "Aux" means that task is auxiliary task for FMs while main task for Experts. The results demonstrate that our approach achieves high transfer ratios in the range of $[0.64, 1.0]$ } table* ### Effectiveness of Target-Aware Embeddings As discussed previously, two-stage, embedding-based methods are a popular paradigm in recommender systems, offering an efficient mechanism to share knowledge from a powerful, centralized FM to various downstream models. However, these methods traditionally focus on relatively stable embeddings, such as user-only or item-only embeddings. While this reduces the required FM update frequency and infrastructure optimizations, it limits the expressive power of the embeddings, thereby failing to fully realize the benefits of scaling laws. To validate the effectiveness of our proposed target-aware embeddings we conduct an ablation study against the strongest internal user embeddings. The FM that produces the baseline user embeddings is trained on the same cross-surface dataset and user history time-range as our HSTU-1B FM. The user embeddings have a dimension 32x larger than our target-aware embeddings and have an embedding freshness of several hours. In the expert models, the user embeddings are processed by a dedicated Fusion Module that uses target-aware attention before interacting with the other components. We note that this user embedding fusion module introduces an additional 5-7 The results are summarized in Table [reference]. The "Baseline" model is a production model that excludes both user embeddings and our proposed target-aware embeddings. It clearly shows that adding our target-aware embeddings to the baseline yields substantial NE improvements across all tasks, significantly outperforming the gains achieved by adding the user embeddings. Furthermore, an ablation study adding the user embeddings on top of our system shows only minor additional improvements. This indicates that our approach efficiently captures the necessary signals for modeling a user's interest in a specific candidate, validating the efficacy of our strategy. The expressiveness of the target-aware embeddings has a direct impact on the expert models. It enables the experts to be exceptionally lightweight (requiring just 20-40 ### Foundation-to-Expert Transfer Efficiency One advantage of the FM-expert design is its transfer efficiency: the FM can be improved centrally, with performance gains transferring at a high ratio to numerous downstream experts simultaneously. This approach directly addresses a known challenge of knowledge distillation where, in large-data regimes, improvements to a teacher model no longer transfer effectively to the student [citation]. To investigate this transfer capability, we conducted an experiment training two architecturally identical expert models on billions of examples. The sole difference between the them was the source of their input FM embeddings: one utilized the HSTU-0.5B FM, and the other, the HSTU-1B FM. For both expert models we initialized the parameters of the expert model (except for the FM Embedding Module and FM Fusion Module) from an expert that had been trained on the HSTU-0.5B FM for more than 1-month. We define the Transfer Ratio (TR) between a pair of FMs for a given expert as, [ TR = NE({Expert_{FM1}) - NE(Expert_{FM2})}{NE(FM1) -NE(FM2)} ] Here, NE represents the Normalized Entropy, our primary offline performance metric. The TR measures the proportional improvement in the expert model relative to the underlying improvement in the foundation model. A higher TR value signifies a more efficient paradigm, ensuring that investments in scaling the FM yield corresponding performance gains in downstream expert models. We note that because both the FM and expert models are trained using a different feature and task set, a transfer ratio of $>=1$ is theoretically possible due to higher-order interactions between the union of the feature and task set in the overall model in this paradigm design. We summarize our results in Table [reference]. These results demonstrate that scaling gains from the FM are efficiently propagated to expert models across various surfaces, which can save considerable training resources and engineering effort that would otherwise be dedicated to scaling each model independently. ### Generalization to Unseen Tasks While the previous experiments in Section [reference] demonstrated FM's strong generalizability across surfaces, the FM has been exposed to all the surface-specific tasks as either main or auxiliary objectives. In this section, we investigate a more challenging scenario: the FM's ability to generalize to expert tasks on which it has no direct training supervision. For this experiment, we established a baseline using the production model of "Surface D" without our FM embeddings. The expert model is architecturally identical to the baseline but incorporates embeddings from the HSTU-0.5B FM. Notably, the FM was trained using around 20 As shown in Table [reference], the expert model with FM embeddings achieved statistically significant gains on all four "unseen" tasks over the baseline. This result underscores the FM's powerful generalization ability, proving it can learn and transfer knowledge that is broadly useful, even for tasks beyond its explicit optimization objectives. This capability is a cornerstone of our "build once, use everywhere" vision, enabling a single FM to benefit an entire ecosystem of diverse and evolving tasks. table[h!] tabular{lcc} **Task Name** & **NE Diff ( Surface_D_Task_1** & -0.60 **Surface_D_Task_2** & -0.53 **Surface_D_Task_3** & -0.40 **Surface_D_Task_4** & -0.51 {tabular Expert performance improvements on Surface D tasks that were not seen by the FM during its training. The baseline is the production model. table ### Online Performance We validated our proposed paradigm through extensive online A/B tests on several core recommendation surfaces. The expert model, which utilizes embeddings from the HSTU-0.5B FM, was benchmarked against directly serving the FM. This setup provides a direct comparison between our two-stage Foundation-Expert paradigm and the traditional one-stage approach. The results demonstrated statistically significant improvements on all surfaces across both engagement and consumption metrics, including a notable shift in engagement towards fresher content. We attribute these gains to our architecture's explicit separation of concerns, where the FM captures general, long-term knowledge, enabling the expert to specialize in surface-specific optimizations and real-time user interests. Moreover, these user-facing improvements were achieved without compromising system performance. The infrastructure metrics such as end-to-end serving latency and CPU performance remained neutral. This is attributed to optimizations within our new infrastructure, HyperCast.

## Conclusion

In this paper, we introduced the Foundation-Expert paradigm, a novel approach for deploying hyperscale recommender systems. By decoupling a central, compute-heavy FM from lightweight, surface-specific Experts, our framework facilitates highly efficient and generalizable knowledge transfer at a massive scale via target-aware embeddings. We demonstrated that this paradigm, powered by our HyperCast infrastructure, overcomes the limitations of traditional knowledge distillation and provides statistically significant improvements in online metrics in A/B testing. Currently, the proposed paradigm is fully deployed across multiple core recommendation surfaces at Meta, serving tens of billions of daily user requests. This work provides a proven blueprint for realizing the benefits of scaling laws in complex, real-time recommendation environments. acks This work represents the joint efforts of many of engineers, researchers, data scientists, and leaders and would not be possible without the following individuals (listed alphabetically): Banit Agrawal, Bin Kuang, Bugra Akyildiz, Chao Deng, Charlie Li, Chelsea Pan, Chenran Li, Chloe Liu, Chufeng Hu, Chunxing Yin, Colin Peppler, Cong Shen, Daisy Shi He, Franco Mo, Han Li, Hao Lin, Hao Wan, Hong Yan, Hongyi Jia, Hongzheng Shi, Huihui Cheng, Ilina Mitra, Jack Chai, James Zuo, Jeet Kanjani, Jiahao Luo, Jiaqi Zhai, Jing Ma, Jing Shan, Ke Gong, Lars Backstrom, Linjian Ma, Lu Fang, Lu Zhang, Marcus Gao, Meihong Wang, Michael Chen, Michael He, Mike Ching, Min Ni, Nikki Zhang, Ning Jiang, Pai-Wei Lai, Qianqian Zhong, Rajasi Saha, Ram Ramanathan, Rex Cheung, Rui Jian, Rui Zhang, Runming Lu, Shikha Kapoor, Shilin Ding, Shiyan Deng, Shouwei Chen, Siqiao Chen, Sophia (Xueyao) Liang, Wen-Yun Yang, Xiaoxing Zhu, Xinyao Hu, Xinye Zheng, Xudong Ma, Yanhong Wu, Yifan Shao, Yisong Song, Yuting Zhang, Zhe (Joe) Wang, Zhuoran Zhao, Zimeng Yang. acks ACM-Reference-Format paper

