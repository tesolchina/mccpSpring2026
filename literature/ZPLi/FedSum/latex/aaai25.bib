@book{em:86,
  editor  = "Engelmore, Robert and Morgan, Anthony",
  title   = "Blackboard Systems",
  year    = 1986,
  address = "Reading, Mass.",
  publisher = "Addison-Wesley",
}

@inproceedings{c:83,
  author  = "Clancey, William J.",
  year    = 1983,
  title   = "{Communication, Simulation, and Intelligent
Agents: Implications of Personal Intelligent Machines
for Medical Education}",
  booktitle="Proceedings of the Eighth International Joint Conference on Artificial Intelligence {(IJCAI-83)}", 
  pages   = "556-560",
  address = "Menlo Park, Calif",
  publisher = "{IJCAI Organization}",
}
@inproceedings{c:84,
  author  = "Clancey, William J.",
  year    = 1984,
  title   = "{Classification Problem Solving}",
  booktitle = "Proceedings of the Fourth National 
              Conference on Artificial Intelligence",
  pages   = "45-54",
  address = "Menlo Park, Calif.",
  publisher="AAAI Press",
}
@article{r:80,
  author = {Robinson, Arthur L.},
  title = {New Ways to Make Microcircuits Smaller},
  volume = {208},
  number = {4447},
  pages = {1019--1022},
  year = {1980},
  doi = {10.1126/science.208.4447.1019},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075},
  URL = {https://science.sciencemag.org/content/208/4447/1019},
  eprint = {https://science.sciencemag.org/content/208/4447/1019.full.pdf},
  journal = {Science},
}
@article{r:80x,
  author  = "Robinson, Arthur L.",
  year    = 1980,
  title   = "{New Ways to Make Microcircuits Smaller---Duplicate Entry}",
  journal = "Science",
  volume  =  208,
  pages   = "1019-1026",
}
@article{hcr:83,
title = {Strategic explanations for a diagnostic consultation system},
journal = {International Journal of Man-Machine Studies},
volume = {20},
number = {1},
pages = {3-19},
year = {1984},
issn = {0020-7373},
doi = {https://doi.org/10.1016/S0020-7373(84)80003-6},
url = {https://www.sciencedirect.com/science/article/pii/S0020737384800036},
author = {Diane Warner Hasling and William J. Clancey and Glenn Rennels},
abstract = {This article examines the problem of automatte explanation of reasoning, especially as it relates to expert systems. By explanation we mean the ability of a program to discuss what it is doing in some understandable way. We first present a general framework in which to view explanation and review some of the research done in this area. We then focus on the explanation system for NEOMYCIN, a medical consultation program. A consultation program interactively helps a user to solve a problem. Our goal is to have NEOMYCIN explain its problem-solving strategies. An explanation of strategy describes the plan the program is using to reach a solution. Such an explanation is usually concrete, referring to aspects of the current problem situation. Abstract explanations articulate a general principle, which can be applied in different situations; such explanations are useful in teaching and in explaining by analogy. We describe the aspects of NEOMYCIN that make abstract strategic explanations possible—the representation of strategic knowledge explicitly and separately from domain knowledge— and demonstrate how this representation can be used to generate explanations.}
}
@article{hcrt:83,
  author  = "Hasling, Diane Warner and Clancey, William J. and Rennels, Glenn R. and Test, Thomas",
  year    = 1983,
  title   = "{Strategic Explanations in Consultation---Duplicate}",
  journal = "The International Journal of Man-Machine Studies",
  volume  = 20,
  number  = 1,
  pages   = "3-19",
}
@techreport{r:86,
  author  = "Rice, James",
  year    = 1986,
  title   = "{Poligon: A System for Parallel Problem Solving}",
  type    = "Technical Report", 
  number  = "KSL-86-19", 
  institution = "Dept.\ of Computer Science, Stanford Univ.",
}
@phdthesis{c:79,
  author  = "Clancey, William J.",
  year    = 1979,
  title   = "{Transfer of Rule-Based Expertise
through a Tutorial Dialogue}",
  type    = "{Ph.D.} diss.",
  school  = "Dept.\ of Computer Science, Stanford Univ.",
  address = "Stanford, Calif.",
}
@unpublished{c:21,
  author  = "Clancey, William J.",
  title   = "{The Engineering of Qualitative Models}",
  year    = 2021,
  note    = "Forthcoming",
}
@misc{c:22,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{c:23,
  title        = "Pluto: The 'Other' Red Planet",
  author       = "{NASA}",
  howpublished = "\url{https://www.nasa.gov/nh/pluto-the-other-red-planet}",
  year         = 2015,
  note         = "Accessed: 2018-12-06"
}



#### 自己的内容

@inproceedings{Few_shot_Federated_NLP, 
   series={EuroMLSys ’23},
   title={Towards Practical Few-shot Federated NLP},
   url={http://dx.doi.org/10.1145/3578356.3592575},
   DOI={10.1145/3578356.3592575},
   booktitle={Proceedings of the 3rd Workshop on Machine Learning and Systems},
   publisher={ACM},
   author={Cai, Dongqi and Wu, Yaozong and Yuan, Haitao and Wang, Shangguang and Lin, Felix Xiaozhu and Xu, Mengwei},
   year={2023},
   month=may, collection={EuroMLSys ’23} 
}

@inproceedings{Few_shot_Mobile_NLP, 
    series={ACM MobiCom ’23},
   title={Federated Few-Shot Learning for Mobile NLP},
   url={http://dx.doi.org/10.1145/3570361.3613277},
   DOI={10.1145/3570361.3613277},
   booktitle={Proceedings of the 29th Annual International Conference on Mobile Computing and Networking},
   publisher={ACM},
   author={Cai, Dongqi and Wang, Shangguang and Wu, Yaozong and Lin, Felix Xiaozhu and Xu, Mengwei},
   year={2023},
   month=oct, collection={ACM MobiCom ’23} }



@ARTICLE{LancichinettiFortunato2009,
  author  = {Lancichinetti, A. and Fortunato, S.},
  title   = {Benchmarks for testing community detection algorithms on directed and weighted graphs with overlapping communities},
  journal = {Phys. Rev. E.}, 
  volume  = {80},
  year    = {2009},
  pages   = {016118}
}

@CONFERENCE{HullermeierRifqi2009,
  author  = {Hullermeier, E. and Rifqi, M.},
  title   = {A Fuzzy Variant of the Rand Index for Comparing Clustering Structures},
  booktitle = {in Proc. IFSA/EUSFLAT Conf.}, 
  year    = {2009},
  pages   = {1294-1298}
}

@ARTICLE{wikihow,
  title={Wikihow: A large scale text summarization dataset},
  author={Koupaee, Mahnaz and Wang, William Yang},
  journal={arXiv preprint arXiv:1810.09305},
  url={http://arxiv.org/abs/1810.09305},
  year={2018}
}

@CONFERENCE{huang-etal-2021-efficient,
    title = "Efficient Attentions for Long Document Summarization",
    author = "Huang, Luyang  and
      Cao, Shuyang  and
      Parulian, Nikolaus  and
      Ji, Heng  and
      Wang, Lu",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.112",
    doi = "10.18653/v1/2021.naacl-main.112",
    pages = "1419--1436",
    
}

@CONFERENCE{cohan-etal-2018-discourse,
    title = "A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents",
    author = "Cohan, Arman  and
      Dernoncourt, Franck  and
      Kim, Doo Soon  and
      Bui, Trung  and
      Kim, Seokhwan  and
      Chang, Walter  and
      Goharian, Nazli",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2097",
    doi = "10.18653/v1/N18-2097",
    pages = "615--621",
    abstract = "Neural abstractive summarization models have led to promising results in summarizing relatively short documents. We propose the first model for abstractive summarization of single, longer-form documents (e.g., research papers). Our approach consists of a new hierarchical encoder that models the discourse structure of a document, and an attentive discourse-aware decoder to generate the summary. Empirical results on two large-scale datasets of scientific papers show that our model significantly outperforms state-of-the-art models.",
}

@CONFERENCE{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W04-1013",
    pages = "74--81",
}

@ARTICLE{zhang-2019-bertscore,
  title={BERTScore: Evaluating Text Generation with BERT}, 
  author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
  year={2020},
  eprint={1904.09675},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@inproceedings{liu-etal-2021-noisy,
    title = "Noisy Self-Knowledge Distillation for Text Summarization",
    author = "Liu, Yang  and
      Shen, Sheng  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.56",
    doi = "10.18653/v1/2021.naacl-main.56",
    pages = "692--703",
    abstract = "In this paper we apply self-knowledge distillation to text summarization which we argue can alleviate problems with maximum-likelihood training on single reference and noisy datasets. Instead of relying on one-hot annotation labels, our student summarization model is trained with guidance from a teacher which generates smoothed labels to help regularize training. Furthermore, to better model uncertainty during training, we introduce multiple noise signals for both teacher and student models. We demonstrate experimentally on three benchmarks that our framework boosts the performance of both pretrained and non-pretrained summarizers achieving state-of-the-art results.",
}

@conference{liu-lapata-2019-text,
    title = "Text Summarization with Pretrained Encoders",
    author = "Liu, Yang  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1387",
    doi = "10.18653/v1/D19-1387",
    pages = "3730--3740",
}

@CONFERENCE{cheng2016neural,
      title={bottom-up by Extracting Sentences and Words}, 
      author = "Cheng, Jianpeng  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1046",
    doi = "10.18653/v1/P16-1046",
    pages = "484--494",
}

@CONFERENCE{nallapati2017summarunner,
  title={Summarunner: A recurrent neural network based sequence model for extractive summarization of documents},
  author={Nallapati, Ramesh and Zhai, Feifei and Zhou, Bowen},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={31},
  number={1},
  year={2017}
}

@ARTICLE{narayan2018ranking,
  title={Ranking sentences for extractive summarization with reinforcement learning},
  author={Narayan, Shashi and Cohen, Shay B and Lapata, Mirella},
  journal={arXiv preprint arXiv:1802.08636},
  year={2018}
}

@ARTICLE{dong2018banditsum,
  title={Banditsum: Extractive summarization as a contextual bandit},
  author={Dong, Yue and Shen, Yikang and Crawford, Eric and van Hoof, Herke and Cheung, Jackie Chi Kit},
  journal={arXiv preprint arXiv:1809.09672},
  year={2018}
}

@CONFERENCE{2020Empirical,
  title={Empirical Studies of Institutional Federated Learning For Natural Language Processing},
  author={ Zhu, X.  and  Wang, J.  and  Hong, Z.  and  Xiao, J. },
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  year={2020},
}

@ARTICLE{DBLP:journals/corr/abs-2106-13973,
  author       = {Priyam Basu and
                  Tiasa Singha Roy and
                  Rakshit Naidu and
                  Z{\"{u}}mr{\"{u}}t M{\"{u}}ft{\"{u}}oglu and
                  Sahib Singh and
                  Fatemehsadat Mireshghallah},
  title        = {Benchmarking Differential Privacy and Federated Learning for {BERT}
                  Models},
  journal      = {CoRR},
  volume       = {abs/2106.13973},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.13973},
  eprinttype    = {arXiv},
  eprint       = {2106.13973},
  timestamp    = {Wed, 30 Jun 2021 16:14:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-13973.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/taslp/ChenLCWJHC15,
  author    = {Kuan{-}Yu Chen and
               Shih{-}Hung Liu and
               Berlin Chen and
               Hsin{-}Min Wang and
               Ea{-}Ee Jan and
               Wen{-}Lian Hsu and
               Hsin{-}Hsi Chen},
  title     = {Extractive Broadcast News Summarization Leveraging Recurrent Neural
               Network Language Modeling Techniques},
  journal   = {{IEEE} {ACM} Trans. Audio Speech Lang. Process.},
  volume    = {23},
  number    = {8},
  pages     = {1322--1334},
  year      = {2015},
  url       = {https://doi.org/10.1109/TASLP.2015.2432578},
  doi       = {10.1109/TASLP.2015.2432578},
  timestamp = {Fri, 13 Mar 2020 14:40:46 +0100},
  biburl    = {https://dblp.org/rec/journals/taslp/ChenLCWJHC15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{0A,
  title={A semantic approach to extractive multi-document summarization: Applying sentence expansion for tuning of conceptual densities - ScienceDirect},
  author={ Bidoki, M.  and  Moosavi, M. R.  and  Fakhrahmad, M. },
  journal={Information Processing & Management},
  volume={57},
  year      = {2022},
  number={ 6},
}
@CONFERENCE{2019TowardsEfficient,
  title={Towards Efficient and Privacy-Preserving Federated Deep Learning},
  author={ Hao, M.  and  Li, H.  and  Xu, G.  and  Liu, S.  and  Yang, H. },
  booktitle={ICC 2019 - 2019 IEEE International Conference on Communications (ICC)},
  year={2019},
}
@inproceedings{2016Abstractive2,
  title={Abstractive Sentence Summarization with Attentive Recurrent Neural Networks},
  author={ Chopra, Sumit  and  Auli, Michael  and  Rush, Alexander M. },
  booktitle={Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  year={2016},
}
@article{2016Abstractive,
  title={Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond},
  author={ Nallapati, Ramesh  and  Zhou, Bowen  and  Santos, Cicero Nogueira Dos  and  Gulcehre, Caglar  and  Xiang, Bing },
  year={2016},
}

@inproceedings{2018Neural,
  title={Neural Latent Extractive Document Summarization},
  author={ Zhang, Xingxing  and  Lapata, Mirella  and  Wei, Furu  and  Zhou, Ming },
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  year={2018},
}
@inproceedings{DBLP:conf/acl/ZhouR19,
  author    = {Jiawei Zhou and
               Alexander M. Rush},
  editor    = {Anna Korhonen and
               David R. Traum and
               Llu{\'{\i}}s M{\`{a}}rquez},
  title     = {Simple Unsupervised Summarization by Contextual Matching},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {5101--5106},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/p19-1503},
  doi       = {10.18653/v1/p19-1503},
  timestamp = {Tue, 28 Jan 2020 10:27:51 +0100},
  biburl    = {https://dblp.org/rec/conf/acl/ZhouR19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{2020AREDSUM,
author = {Bi, Keping and Jha, Rahul and Croft, W. Bruce and Celikyilmaz, Asli},
title = {AREDSUM: Adaptive Redundancy-Aware Iterative Sentence Ranking for Extractive Document Summarization},
booktitle = {EACL},
year = {2021},
month = {April},
}
@inproceedings{2017Combining,
  title={Combining Graph Degeneracy and Submodularity for Unsupervised Extractive Summarization},
  author={ Tixier, A.  and  Meladianos, P.  and  Vazirgiannis, M. },
  booktitle={Proceedings of the Workshop on New Frontiers in Summarization},
  year={2017},
}
@inproceedings{2017GraphMulti,
  title={Graph-based Neural Multi-Document Summarization},
  author={ Yasunaga, M.  and  Zhang, R.  and  Meelu, K.  and  Pareek, A.  and  Srinivasan, K.  and  Radev, D. },
  booktitle={Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)},
  year={2017},
}
@inproceedings{2020Leveraging,
  title={Leveraging Graph to Improve Abstractive Multi-Document Summarization},
  author={ Li, W.  and  Xiao, X.  and  Liu, J.  and  Wu, H.  and  Du, J. },
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  year={2020},
}
@article{2019Scoring,
  title={Scoring Sentence Singletons and Pairs for Abstractive Summarization},
  author={ Lebanoff, L.  and  Song, K.  and  Dernoncourt, F.  and  Kim, D. S.  and  Liu, F. },
  year={2019},
}

@inproceedings{cao2022hibrids,
    title = "{HIBRIDS}: Attention with Hierarchical Biases for Structure-aware Long Document Summarization",
    author = "Cao, Shuyang  and
      Wang, Lu",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2022.acl-long.58",
    pages = "786--807",
}

@article{DBLP:journals/talip/WeiRZCSS19,
  author    = {Bingzhen Wei and
               Xuancheng Ren and
               Yi Zhang and
               Xiaoyan Cai and
               Qi Su and
               Xu Sun},
  title     = {Regularizing Output Distribution of Abstractive Chinese Social Media
               Text Summarization for Improved Semantic Consistency},
  journal   = {{ACM} Trans. Asian Low Resour. Lang. Inf. Process.},
  volume    = {18},
  number    = {3},
  pages     = {31:1--31:15},
  year      = {2019},
  url       = {https://doi.org/10.1145/3314934},
  doi       = {10.1145/3314934},
  timestamp = {Thu, 22 Oct 2020 08:33:34 +0200},
  biburl    = {https://dblp.org/rec/journals/talip/WeiRZCSS19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{kairouz2021advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={14},
  number={1--2},
  pages={1--210},
  year={2021},
  publisher={Now Publishers, Inc.}
}
@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}
@article{WANG2023103167,
title = {A privacy preserving framework for federated learning in smart healthcare systems},
journal = {Information Processing & Management},
volume = {60},
number = {1},
pages = {103167},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.103167},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322002680},
author = {Wenshuo Wang and Xu Li and Xiuqin Qiu and Xiang Zhang and Vladimir Brusic and Jindong Zhao},
keywords = {Federated learning, Ring signature, Privacy preserving, Source inference attack, Smart healthcare system},
abstract = {Federated Learning (FL) is a platform for smart healthcare systems that use wearables and other Internet of Things enabled devices. However, source inference attacks (SIAs) can infer the connection between physiological data in training datasets with FL clients and reveal the identities of participants to the attackers. We propose a comprehensive smart healthcare framework for sharing physiological data, named FRESH, that is based on FL and ring signature defense from the attacks. In FRESH, physiological data are collected from individuals by wearable devices. These data are processed by edge computing devices (e.g., mobile phones, tablet PCs) that train ML models using local data. The model parameters are uploaded by edge computing devices to the central server for joint training of FL models of disease prediction. In this procedure, certificateless ring signature is used to hide the source of parameter updates during joint training for FL to effectively resist SIAs. In the proposed ring signature schema, an improved batch verification algorithm is designed to leverage additivity of linear operations on elliptic curves and to help reduce the computing workload of the server. Experimental results demonstrate that FRESH effectively reduces the success rate of SIAs and the batch verification method significantly improves the efficiency of signature verification. FRESH can be applied to large scale smart healthcare systems with FL involving large numbers of users.}
}
@article{REGUEIRO2021102745,
title = {Privacy-enhancing distributed protocol for data aggregation based on blockchain and homomorphic encryption},
journal = {Information Processing & Management},
volume = {58},
number = {6},
pages = {102745},
year = {2021},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102745},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321002272},
author = {Cristina Regueiro and Iñaki Seco and Santiago {de Diego} and Oscar Lage and Leire Etxebarria},
keywords = {Homomorphic encryption, Blockchain, Privacy, Confidentiality, Security, Data aggregation},
abstract = {The recent increase in reported incidents of security breaches compromising users' privacy call into question the current centralized model in which third-parties collect and control massive amounts of personal data. Blockchain has demonstrated that trusted and auditable computing is possible using a decentralized network of peers accompanied by a public ledger. Furthermore, Homomorphic Encryption (HE) guarantees confidentiality not only on the computation but also on the transmission, and storage processes. The synergy between Blockchain and HE is rapidly increasing in the computing environment. This research proposes a privacy-enhancing distributed and secure protocol for data aggregation backboned by Blockchain and HE technologies. Blockchain acts as a distributed ledger which facilitates efficient data aggregation through a Smart Contract. On the top, HE will be used for data encryption allowing private aggregation operations. The theoretical description, potential applications, a suggested implementation and a performance analysis are presented to validate the proposed solution.}
}
@article{zhang2023federated,
  title={Federated Compositional Deep AUC Maximization},
  author={Zhang, Xinwen and Zhang, Yihan and Yang, Tianbao and Souvenir, Richard and Gao, Hongchang},
  journal={arXiv preprint arXiv:2304.10101},
  year={2023}
}
@inproceedings{li2022federated,
      title={Federated Learning on Non-IID Data Silos: An Experimental Study},
      author={Li, Qinbin and Diao, Yiqun and Chen, Quan and He, Bingsheng},
      booktitle={IEEE International Conference on Data Engineering},
      year={2022}
}
@inproceedings{FedProx,
 author = {Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
 booktitle = {Proceedings of Machine Learning and Systems},
 editor = {I. Dhillon and D. Papailiopoulos and V. Sze},
 pages = {429--450},
 title = {Federated Optimization in Heterogeneous Networks},
 url = {https://proceedings.mlsys.org/paper_files/paper/2020/file/38af86134b65d0f10fe33d30dd76442e-Paper.pdf},
 volume = {2},
 year = {2020}
}
@misc{liu2021federated_NLP_Survey,
      title={Federated Learning Meets Natural Language Processing: A Survey}, 
      author={Ming Liu and Stella Ho and Mengqi Wang and Longxiang Gao and Yuan Jin and He Zhang},
      year={2021},
      eprint={2107.12603},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@InProceedings{FedRep,
  title = 	 {Exploiting Shared Representations for Personalized Federated Learning},
  author =       {Collins, Liam and Hassani, Hamed and Mokhtari, Aryan and Shakkottai, Sanjay},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2089--2099},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/collins21a/collins21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/collins21a.html},
  abstract = 	 {Deep neural networks have shown the ability to extract universal feature representations from data such as images and text that have been useful for a variety of learning tasks. However, the fruits of representation learning have yet to be fully-realized in federated settings. Although data in federated settings is often non-i.i.d. across clients, the success of centralized deep learning suggests that data often shares a global {\em feature representation}, while the statistical heterogeneity across clients or tasks is concentrated in the {\em labels}. Based on this intuition, we propose a novel federated learning framework and algorithm for learning a shared data representation across clients and unique local heads for each client. Our algorithm harnesses the distributed computational power across clients to perform many local-updates with respect to the low-dimensional local parameters for every update of the representation. We prove that this method obtains linear convergence to the ground-truth representation with near-optimal sample complexity in a linear setting, demonstrating that it can efficiently reduce the problem dimension for each client. Further, we provide extensive experimental results demonstrating the improvement of our method over alternative personalized federated learning approaches in heterogeneous settings.}
}
@inproceedings{NEURIPS2020_pFedMe,
 author = {T. Dinh, Canh and Tran, Nguyen and Nguyen, Josh},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {21394--21405},
 publisher = {Curran Associates, Inc.},
 title = {Personalized Federated Learning with Moreau Envelopes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f4f1f13c8289ac1b1ee0ff176b56fc60-Paper.pdf},
 volume = {33},
 year = {2020}
}

@conference{zhong-etal-2020-extractive,
    title = "Extractive Summarization as Text Matching",
    author = "Zhong, Ming  and
      Liu, Pengfei  and
      Chen, Yiran  and
      Wang, Danqing  and
      Qiu, Xipeng  and
      Huang, Xuanjing",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.552",
    doi = "10.18653/v1/2020.acl-main.552",
    pages = "6197--6208",
    
}

@inproceedings{Cohan_2019,
   title={Pretrained Language Models for Sequential Sentence Classification},
   url={http://dx.doi.org/10.18653/v1/D19-1383},
   DOI={10.18653/v1/d19-1383},
   booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
   publisher={Association for Computational Linguistics},
   author={Cohan, Arman and Beltagy, Iz and King, Daniel and Dalvi, Bhavana and Weld, Dan},
   year={2019} }

@inproceedings{sun-etal-2019-utilizing,
    title = "Utilizing {BERT} for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence",
    author = "Sun, Chi  and
      Huang, Luyao  and
      Qiu, Xipeng",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1035",
    doi = "10.18653/v1/N19-1035",
    pages = "380--385",
    abstract = "Aspect-based sentiment analysis (ABSA), which aims to identify fine-grained opinion polarity towards a specific aspect, is a challenging subtask of sentiment analysis (SA). In this paper, we construct an auxiliary sentence from the aspect and convert ABSA to a sentence-pair classification task, such as question answering (QA) and natural language inference (NLI). We fine-tune the pre-trained model from BERT and achieve new state-of-the-art results on SentiHood and SemEval-2014 Task 4 datasets. The source codes are available at \url{https://github.com/HSLCY/ABSA-BERT-pair}.",
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@conference{gehrmann2018bottomup,
      title={Bottom-Up Abstractive Summarization}, 
      author = "Gehrmann, Sebastian  and
      Deng, Yuntian  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1443",
    doi = "10.18653/v1/D18-1443",
    pages = "4098--4109",
}

@InProceedings{Ditto,
  title = 	 {Ditto: Fair and Robust Federated Learning Through Personalization},
  author =       {Li, Tian and Hu, Shengyuan and Beirami, Ahmad and Smith, Virginia},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {6357--6368},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/li21h/li21h.pdf},
  url = 	 {https://proceedings.mlr.press/v139/li21h.html},
  abstract = 	 {Fairness and robustness are two important concerns for federated learning systems. In this work, we identify that robustness to data and model poisoning attacks and fairness, measured as the uniformity of performance across devices, are competing constraints in statistically heterogeneous networks. To address these constraints, we propose employing a simple, general framework for personalized federated learning, Ditto, that can inherently provide fairness and robustness benefits, and develop a scalable solver for it. Theoretically, we analyze the ability of Ditto to achieve fairness and robustness simultaneously on a class of linear problems. Empirically, across a suite of federated datasets, we show that Ditto not only achieves competitive performance relative to recent personalization methods, but also enables more accurate, robust, and fair models relative to state-of-the-art fair or robust baselines.}
}

@conference{see-etal-2017-get,
    title = "Get To The Point: Summarization with Pointer-Generator Networks",
    author = "See, Abigail  and
      Liu, Peter J.  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1099",
    doi = "10.18653/v1/P17-1099",
    pages = "1073--1083",
}
@ARTICLE{9743558,
  author={Tan, Alysa Ziying and Yu, Han and Cui, Lizhen and Yang, Qiang},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Towards Personalized Federated Learning}, 
  year={2022},
  volume={},
  number={},
  pages={1-17},
  doi={10.1109/TNNLS.2022.3160699}}

@conference{bi-etal-2021-aredsum,
    title = "{AREDSUM}: Adaptive Redundancy-Aware Iterative Sentence Ranking for Extractive Document Summarization",
    author = "Bi, Keping  and
      Jha, Rahul  and
      Croft, Bruce  and
      Celikyilmaz, Asli",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.22",
    doi = "10.18653/v1/2021.eacl-main.22",
    pages = "281--291",
}
@inproceedings{SuPerFed,
author = {Hahn, Seok-Ju and Jeong, Minwoo and Lee, Junghye},
title = {Connecting Low-Loss Subspace for Personalized Federated Learning},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539254},
doi = {10.1145/3534678.3539254},
abstract = {Due to the curse of statistical heterogeneity across clients, adopting a personalized federated learning method has become an essential choice for the successful deployment of federated learning-based services. Among diverse branches of personalization techniques, a model mixture-based personalization method is preferred as each client has their own personalized model as a result of federated learning. It usually requires a local model and a federated model, but this approach is either limited to partial parameter exchange or requires additional local updates, each of which is helpless to novel clients and burdensome to the client's computational capacity. As the existence of a connected subspace containing diverse low-loss solutions between two or more independent deep networks has been discovered, we combined this interesting property with the model mixture-based personalized federated learning method for improved performance of personalization. We proposed SuPerFed, a personalized federated learning method that induces an explicit connection between the optima of the local and the federated model in weight space for boosting each other. Through extensive experiments on several benchmark datasets, we demonstrated that our method achieves consistent gains in both personalization performance and robustness to problematic scenarios possible in realistic services.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {505–515},
numpages = {11},
keywords = {label noise, personalization, personalized federated learning, mode connectivity, federated learning, non-iid data},
location = {Washington DC, USA},
series = {KDD '22}
}

@conference{xu-etal-2020-unsupervised,
    title = "Unsupervised Extractive Summarization by Pre-training Hierarchical Transformers",
    author = "Xu, Shusheng  and
      Zhang, Xingxing  and
      Wu, Yi  and
      Wei, Furu  and
      Zhou, Ming",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.161",
    doi = "10.18653/v1/2020.findings-emnlp.161",
    pages = "1784--1795",
}

@conference{cui-hu-2021-sliding,
    title = "Sliding Selector Network with Dynamic Memory for Extractive Summarization of Long Documents",
    author = "Cui, Peng  and
      Hu, Le",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.470",
    doi = "10.18653/v1/2021.naacl-main.470",
    pages = "5881--5891",
}

@conference{guan-etal-2021-frame,
    title = "Frame Semantic-Enhanced Sentence Modeling for Sentence-level Extractive Text Summarization",
    author = "Guan, Yong  and
      Guo, Shaoru  and
      Li, Ru  and
      Li, Xiaoli  and
      Tan, Hongye",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.331",
    doi = "10.18653/v1/2021.emnlp-main.331",
    pages = "4045--4052",
}
@InProceedings{10.1007/978-3-031-28996-5_1,
author="Isaksson, Martin
and Listo Zec, Edvin
and C{\"o}ster, Rickard
and Gillblad, Daniel
and Girdzijauskas, Sarunas",
editor="Goebel, Randy
and Yu, Han
and Faltings, Boi
and Fan, Lixin
and Xiong, Zehui",
title="Adaptive Expert Models for Federated Learning",
booktitle="Trustworthy Federated Learning",
year="2023",
publisher="Springer International Publishing",
address="Cham",
pages="1--16",
abstract="Federated Learning (FL) is a promising framework for distributed learning when data is private and sensitive. However, the state-of-the-art solutions in this framework are not optimal when data is heterogeneous and non-IID. We propose a practical and robust approach to personalization in FL that adjusts to heterogeneous and non-IID data by balancing exploration and exploitation of several global models. To achieve our aim of personalization, we use a Mixture of Experts (MoE) that learns to group clients that are similar to each other, while using the global models more efficiently. We show that our approach achieves an accuracy up to 29.78{\%} better than the state-of-the-art and up to 4.38{\%} better compared to a local model in a pathological non-IID setting, even though we tune our approach in the IID setting.",
isbn="978-3-031-28996-5"
}

@conference{zhong2022dialoglm,
  title={Dialoglm: Pre-trained model for long dialogue understanding and summarization},
  author={Zhong, Ming and Liu, Yang and Xu, Yichong and Zhu, Chenguang and Zeng, Michael},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={10},
  pages={11765--11773},
  year={2022}
}

@article{FedBERT,
author = {Tian, Yuanyishu and Wan, Yao and Lyu, Lingjuan and Yao, Dezhong and Jin, Hai and Sun, Lichao},
title = {FedBERT: When Federated Learning Meets Pre-Training},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3510033},
doi = {10.1145/3510033},
abstract = {The fast growth of pre-trained models (PTMs) has brought natural language processing to a new era, which has become a dominant technique for various natural language processing (NLP) applications. Every user can download the weights of PTMs, then fine-tune the weights for a task on the local side. However, the pre-training of a model relies heavily on accessing a large-scale of training data and requires a vast amount of computing resources. These strict requirements make it impossible for any single client to pre-train such a model. To grant clients with limited computing capability to participate in pre-training a large model, we propose a new learning approach, FedBERT, that takes advantage of the federated learning and split learning approaches, resorting to pre-training BERT in a federated way. FedBERT can prevent sharing the raw data information and obtain excellent performance. Extensive experiments on seven GLUE tasks demonstrate that FedBERT can maintain its effectiveness without communicating to the sensitive local data of clients.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {aug},
articleno = {66},
numpages = {26},
keywords = {NLP, Federated learning, pre-training, BERT}
}
@inproceedings{FedED,
    title = "{F}ed{ED}: Federated Learning via Ensemble Distillation for Medical Relation Extraction",
    author = "Sui, Dianbo  and
      Chen, Yubo  and
      Zhao, Jun  and
      Jia, Yantao  and
      Xie, Yuantao  and
      Sun, Weijian",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.165",
    doi = "10.18653/v1/2020.emnlp-main.165",
    pages = "2118--2128",
    abstract = "Unlike other domains, medical texts are inevitably accompanied by private information, so sharing or copying these texts is strictly restricted. However, training a medical relation extraction model requires collecting these privacy-sensitive texts and storing them on one machine, which comes in conflict with privacy protection. In this paper, we propose a privacy-preserving medical relation extraction model based on federated learning, which enables training a central model with no single piece of private local data being shared or exchanged. Though federated learning has distinct advantages in privacy protection, it suffers from the communication bottleneck, which is mainly caused by the need to upload cumbersome local parameters. To overcome this bottleneck, we leverage a strategy based on knowledge distillation. Such a strategy uses the uploaded predictions of ensemble local models to train the central model without requiring uploading local parameters. Experiments on three publicly available medical relation extraction datasets demonstrate the effectiveness of our method.",
}
@inproceedings{passban-etal-2022-training,
    title = "Training Mixed-Domain Translation Models via Federated Learning",
    author = "Passban, Peyman  and
      Roosta, Tanya  and
      Gupta, Rahul  and
      Chadha, Ankit  and
      Chung, Clement",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.186",
    doi = "10.18653/v1/2022.naacl-main.186",
    pages = "2576--2586",
    abstract = "Training mixed-domain translation models is a complex task that demands tailored architec- tures and costly data preparation techniques. In this work, we leverage federated learning (FL) in order to tackle the problem. Our investiga- tion demonstrates that with slight modifications in the training process, neural machine trans- lation (NMT) engines can be easily adapted when an FL-based aggregation is applied to fuse different domains. Experimental results also show that engines built via FL are able to perform on par with state-of-the-art baselines that rely on centralized training techniques.We evaluate our hypothesis in the presence of five datasets with different sizes, from different domains, to translate from German into English and discuss how FL and NMT can mutually benefit from each other. In addition to provid- ing benchmarking results on the union of FL and NMT, we also propose a novel technique to dynamically control the communication band- width by selecting impactful parameters during FL updates. This is a significant achievement considering the large size of NMT engines that need to be exchanged between FL parties.",
}

@inproceedings{MAML,
author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
title = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {1126–1135},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}


@inproceedings{NEURIPS2020_24389bfe,
 author = {Fallah, Alireza and Mokhtari, Aryan and Ozdaglar, Asuman},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {3557--3568},
 publisher = {Curran Associates, Inc.},
 title = {Personalized Federated Learning with Theoretical Guarantees: A Model-Agnostic Meta-Learning Approach},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/24389bfe4fe2eba8bf9aa9203a44cdad-Paper.pdf},
 volume = {33},
 year = {2020}
}
@inproceedings{NEURIPS2021_82599a4e,
 author = {Marfoq, Othmane and Neglia, Giovanni and Bellet, Aur\'{e}lien and Kameni, Laetitia and Vidal, Richard},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {15434--15447},
 publisher = {Curran Associates, Inc.},
 title = {Federated Multi-Task Learning under a Mixture of Distributions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/82599a4ec94aca066873c99b4c741ed8-Paper.pdf},
 volume = {34},
 year = {2021}
}
@InProceedings{Xu_2022_CVPR,
    author    = {Xu, An and Li, Wenqi and Guo, Pengfei and Yang, Dong and Roth, Holger R. and Hatamizadeh, Ali and Zhao, Can and Xu, Daguang and Huang, Heng and Xu, Ziyue},
    title     = {Closing the Generalization Gap of Cross-Silo Federated Medical Image Segmentation},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {20866-20875}
}
@inproceedings{XSum,
    title = "Don{'}t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization",
    author = "Narayan, Shashi  and
      Cohen, Shay B.  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1206",
    doi = "10.18653/v1/D18-1206",
    pages = "1797--1807",
    abstract = "We introduce {``}extreme summarization{''}, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question {``}What is the article about?{''}. We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the article{'}s topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans.",
}

@misc{deng2020adaptive,
      title={Adaptive Personalized Federated Learning}, 
      author={Yuyang Deng and Mohammad Mahdi Kamani and Mehrdad Mahdavi},
      year={2020},
      eprint={2003.13461},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{NEURIPS2020_ac450d10,
 author = {Deng, Yuyang and Kamani, Mohammad Mahdi and Mahdavi, Mehrdad},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {15111--15122},
 publisher = {Curran Associates, Inc.},
 title = {Distributionally Robust Federated Averaging},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/ac450d10e166657ec8f93a1b65ca1b14-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{FedPCL,
 author = {Tan, Yue and Long, Guodong and Ma, Jie and LIU, LU and Zhou, Tianyi and Jiang, Jing},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {19332--19344},
 publisher = {Curran Associates, Inc.},
 title = {Federated Learning from Pre-Trained Models: A Contrastive Learning Approach},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/7aa320d2b4b8f6400b18f6f77b6c1535-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@InProceedings{Li_2021_CVPR,
    author    = {Li, Qinbin and He, Bingsheng and Song, Dawn},
    title     = {Model-Contrastive Federated Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {10713-10722}
}

@InProceedings{2023FedALA,
  title={{FedALA: Adaptive Local Aggregation for Personalized Federated Learning}},
  author={Zhang, Jianqing and Hua, Yang and Wang, Hao and Song, Tao and Xue, Zhengui and Ma, Ruhui and Guan, Haibing},
  booktitle={AAAI Conference on Artificial Intelligence (AAAI)},
  year={2023}
}

@inproceedings{qin-etal-2021-improving-federated,
    title = "Improving Federated Learning for Aspect-based Sentiment Analysis via Topic Memories",
    author = "Qin, Han  and
      Chen, Guimin  and
      Tian, Yuanhe  and
      Song, Yan",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.321",
    doi = "10.18653/v1/2021.emnlp-main.321",
    pages = "3942--3954"
}

@misc{liu2023communication,
  title={Communication Efficient Federated Learning for Multilingual Neural Machine Translation with Adapter}, 
  author={Yi Liu and Xiaohan Bi and Lei Li and Sishuo Chen and Wenkai Yang and Xu Sun},
  year={2023},
  eprint={2305.12449},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@article{FairFed,
  author       = {Yahya H. Ezzeldin and
                  Shen Yan and
                  Chaoyang He and
                  Emilio Ferrara and
                  Salman Avestimehr},
  title        = {FairFed: Enabling Group Fairness in Federated Learning},
  journal      = {CoRR},
  volume       = {abs/2110.00857},
  year         = {2021},
  url          = {https://arxiv.org/abs/2110.00857},
  eprinttype    = {arXiv},
  eprint       = {2110.00857},
  timestamp    = {Fri, 08 Oct 2021 15:47:55 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2110-00857.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@InProceedings{FedDisco,
  author       = {Rui Ye and
                  Mingkai Xu and
                  Jianyu Wang and
                  Chenxin Xu and
                  Siheng Chen and
                  Yanfeng Wang},
  title        = {FedDisco: Federated Learning with Discrepancy-Aware Collaboration},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  year = 	 {2023},
  series = 	 {Proceedings of Machine Learning Research},
  pdf = 	 {https://doi.org/10.48550/arXiv.2305.19229},
  
}

@InProceedings{FedPAC,
  author       = {Jian Xu and Xinyi Tong and Shao-Lun Huang},
  title        = {Personalized Federated Learning with Feature Alignment and Classifier Collaboration},
  booktitle =    {Proceedings of the 40th International Conference on Machine Learning},
  year =   {2023},
  series =   {Proceedings of Machine Learning Research},
  pdf =    {https://openreview.net/pdf?id=SXZr8aDKia},
  
}
@inproceedings{Park_2024,
   title={Unsupervised Extractive Dialogue Summarization in Hyperdimensional Space},
   url={http://dx.doi.org/10.1109/ICASSP48485.2024.10446698},
   DOI={10.1109/icassp48485.2024.10446698},
   booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
   publisher={IEEE},
   author={Park, Seongmin and Kim, Kyungho and Seo, Jaejin and Lee, Jihwa},
   year={2024},
   month=apr }

@inproceedings{liu2019hierarchical,
      title={Hierarchical Transformers for Multi-Document Summarization}, 
      author={Yang Liu and Mirella Lapata},
      year={2019},
      publisher={Association for Computational Linguistics}
}
@inproceedings{wang2020heterogeneous,
      title={Heterogeneous Graph Neural Networks for Extractive Document Summarization}, 
      author={Danqing Wang and Pengfei Liu and Yining Zheng and Xipeng Qiu and Xuanjing Huang},
      year={2020},
      publisher={Association for Computational Linguistics}
}
@inproceedings{specter2020cohan,
  title={{SPECTER: Document-level Representation Learning using Citation-informed Transformers}},
  author={Arman Cohan and Sergey Feldman and Iz Beltagy and Doug Downey and Daniel S. Weld},
  publisher={Association for Computational Linguistics},
  year={2020}
}

@inproceedings{gu2022memsum,
      title={MemSum: Extractive Summarization of Long Documents Using Multi-Step Episodic Markov Decision Processes}, 
      author={Nianlong Gu and Elliott Ash and Richard H. R. Hahnloser},
      year={2022},
    publisher={Association for Computational Linguistics},
    year={2020}
}

@misc{zhang2023diffusum,
      title={DiffuSum: Generation Enhanced Extractive Summarization with Diffusion}, 
      author={Haopeng Zhang and Xiao Liu and Jiawei Zhang},
      year={2023},
    publisher={Association for Computational Linguistics},
    year={2020}
}



@misc{liu2021simcls,
      title={SimCLS: A Simple Framework for Contrastive Learning of Abstractive Summarization}, 
      author={Yixin Liu and Pengfei Liu},
      year={2021},
      publisher={Association for Computational Linguistics},
}

@article{an2022colo,
  title={Colo: A contrastive learning based re-ranking framework for one-stage summarization},
  author={An, Chenxin and Zhong, Ming and Wu, Zhiyong and Zhu, Qin and Huang, Xuanjing and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2209.14569},
  year={2022}
}

@misc{kim2019abstractive,
      title={Abstractive Summarization of Reddit Posts with Multi-level Memory Networks}, 
      author={Byeongchang Kim and Hyunwoo Kim and Gunhee Kim},
      year={2019},
        booktitle = "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
        publisher = "Association for Computational Linguistics",
}

@article{RN1,
   author = {Cheng, W. C. and Tsui, Y. C. and Ragusa, S. and Koelzer, V. H. and Mina, M. and Franco, F. and Laubli, H. and Tschumi, B. and Speiser, D. and Romero, P. and Zippelius, A. and Petrova, T. V. and Mertz, K. and Ciriello, G. and Ho, P. C.},
   title = {Uncoupling protein 2 reprograms the tumor microenvironment to support the anti-tumor immune cycle},
   journal = {Nat Immunol},
   volume = {20},
   number = {2},
   pages = {206-217},
   ISSN = {1529-2916 (Electronic)
1529-2908 (Linking)},
   DOI = {10.1038/s41590-018-0290-0},
   url = {https://www.ncbi.nlm.nih.gov/pubmed/30664764},
   year = {2019},
   type = {Journal Article}
}

@article{RN2,
   author = {Liu, P. S. and Wang, H. and Li, X. and Chao, T. and Teav, T. and Christen, S. and Di Conza, G. and Cheng, W. C. and Chou, C. H. and Vavakova, M. and Muret, C. and Debackere, K. and Mazzone, M. and Huang, H. D. and Fendt, S. M. and Ivanisevic, J. and Ho, P. C.},
   title = {alpha-ketoglutarate orchestrates macrophage activation through metabolic and epigenetic reprogramming},
   journal = {Nat Immunol},
   volume = {18},
   number = {9},
   pages = {985-994},
   ISSN = {1529-2916 (Electronic)
1529-2908 (Linking)},
   DOI = {10.1038/ni.3796},
   url = {https://www.ncbi.nlm.nih.gov/pubmed/28714978},
   year = {2017},
   type = {Journal Article}
}

@article{RN4,
   author = {Roychoudhuri, R. and Eil, R. L. and Restifo, N. P.},
   title = {The interplay of effector and regulatory T cells in cancer},
   journal = {Curr Opin Immunol},
   volume = {33},
   pages = {101-11},
   ISSN = {1879-0372 (Electronic)
0952-7915 (Linking)},
   DOI = {10.1016/j.coi.2015.02.003},
   url = {https://www.ncbi.nlm.nih.gov/pubmed/25728990},
   year = {2015},
   type = {Journal Article}
}

@article{RN3,
   author = {Thommen, D. S. and Koelzer, V. H. and Herzig, P. and Roller, A. and Trefny, M. and Dimeloe, S. and Kiialainen, A. and Hanhart, J. and Schill, C. and Hess, C. and Savic Prince, S. and Wiese, M. and Lardinois, D. and Ho, P. C. and Klein, C. and Karanikas, V. and Mertz, K. D. and Schumacher, T. N. and Zippelius, A.},
   title = {A transcriptionally and functionally distinct PD-1(+) CD8(+) T cell pool with predictive potential in non-small-cell lung cancer treated with PD-1 blockade},
   journal = {Nat Med},
   volume = {24},
   number = {7},
   pages = {994-1004},
   ISSN = {1546-170X (Electronic)
1078-8956 (Linking)},
   DOI = {10.1038/s41591-018-0057-z},
   url = {https://www.ncbi.nlm.nih.gov/pubmed/29892065},
   year = {2018},
   type = {Journal Article}
}


@article{LiteratureReview1,
  title={Federated learning: Challenges, methods, and future directions},
  author={Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith, Virginia},
  journal={IEEE signal processing magazine},
  volume={37},
  number={3},
  pages={50--60},
  year={2020},
  publisher={IEEE}
}

@ARTICLE{LiteratureReview2,
  author={Pei, Jiaming and Liu, Wenxuan and Li, Jinhai and Wang, Lukun and Liu, Chao},
  journal={IEEE Transactions on Consumer Electronics}, 
  title={A Review of Federated Learning Methods in Heterogeneous scenarios}, 
  year={2024},
  volume={},
  number={},
  pages={1-1},
  keywords={Federated learning;Data models;Mathematical models;Computational modeling;Consumer electronics;Training;Surveys;Federated learning;device heterogeneity;data heterogeneity;non-IID data distribution;model heterogeneity},
  doi={10.1109/TCE.2024.3385440}}

@misc{LiteratureReview3,
      title={Heterogeneous Federated Learning: State-of-the-art and Research Challenges}, 
      author={Mang Ye and Xiuwen Fang and Bo Du and Pong C. Yuen and Dacheng Tao},
      year={2023},
      eprint={2307.10616},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{LiteratureReview4,
  title={Client selection in federated learning: Principles, challenges, and opportunities},
  author={Fu, Lei and Zhang, Huanle and Gao, Ge and Zhang, Mi and Liu, Xin},
  journal={IEEE Internet of Things Journal},
  year={2023},
  publisher={IEEE}
}

@inproceedings{FedAvg,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@inproceedings{Scaffold,
  title={Scaffold: Stochastic controlled averaging for federated learning},
  author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
  booktitle={International conference on machine learning},
  pages={5132--5143},
  year={2020},
  organization={PMLR}
}

@article{FedNova,
  title={Tackling the objective inconsistency problem in heterogeneous federated optimization},
  author={Wang, Jianyu and Liu, Qinghua and Liang, Hao and Joshi, Gauri and Poor, H Vincent},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={7611--7623},
  year={2020}
}

@article{FedPer,
  title={Federated learning with personalization layers},
  author={Arivazhagan, Manoj Ghuhan and Aggarwal, Vinay and Singh, Aaditya Kumar and Choudhary, Sunav},
  journal={arXiv preprint arXiv:1912.00818},
  year={2019}
}

@inproceedings{FedRep,
  title={Exploiting shared representations for personalized federated learning},
  author={Collins, Liam and Hassani, Hamed and Mokhtari, Aryan and Shakkottai, Sanjay},
  booktitle={International conference on machine learning},
  pages={2089--2099},
  year={2021},
  organization={PMLR}
}

@inproceedings{Ditto,
  title={Ditto: Fair and robust federated learning through personalization},
  author={Li, Tian and Hu, Shengyuan and Beirami, Ahmad and Smith, Virginia},
  booktitle={International conference on machine learning},
  pages={6357--6368},
  year={2021},
  organization={PMLR}
}

@article{pFedMe,
  title={Personalized federated learning with moreau envelopes},
  author={T Dinh, Canh and Tran, Nguyen and Nguyen, Josh},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21394--21405},
  year={2020}
}


@article{FedDF,
  title={Ensemble distillation for robust model fusion in federated learning},
  author={Lin, Tao and Kong, Lingjing and Stich, Sebastian U and Jaggi, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={2351--2363},
  year={2020}
}

@inproceedings{FedGen,
  title={Data-free knowledge distillation for heterogeneous federated learning},
  author={Zhu, Zhuangdi and Hong, Junyuan and Zhou, Jiayu},
  booktitle={International conference on machine learning},
  pages={12878--12889},
  year={2021},
  organization={PMLR}
}



@article{FedLGD,
  title={Federated Virtual Learning on Heterogeneous Data with Local-global Distillation},
  author={Huang, Chun-Yin and Jin, Ruinan and Zhao, Can and Xu, Daguang and Li, Xiaoxiao},
  journal={arXiv preprint arXiv:2303.02278},
  year={2023}
}



@inproceedings{FedProto,
  title={Fedproto: Federated prototype learning across heterogeneous clients},
  author={Tan, Yue and Long, Guodong and Liu, Lu and Zhou, Tianyi and Lu, Qinghua and Jiang, Jing and Zhang, Chengqi},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={8},
  pages={8432--8440},
  year={2022}
}

@inproceedings{FedNH,
  title={Tackling data heterogeneity in federated learning with class prototypes},
  author={Dai, Yutong and Chen, Zeyuan and Li, Junnan and Heinecke, Shelby and Sun, Lichao and Xu, Ran},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={6},
  pages={7314--7322},
  year={2023}
}
@article{FedHKD,
  title={The best of both worlds: Accurate global and personalized models through federated learning with data-free hyper-knowledge distillation},
  author={Chen, Huancheng and Vikalo, Haris and others},
  journal={arXiv preprint arXiv:2301.08968},
  year={2023}
}
@inproceedings{FPL,
  title={Rethinking federated learning with domain shift: A prototype view.},
  author={Huang, Wenke and Ye, Mang and Shi, Zekun and Li, He and Du, Bo},
  booktitle={In 2023 IEEE CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={16312--16322},
  year={2023}
}

@article{FedTGP,
  title={FedTGP: Trainable Global Prototypes with Adaptive-Margin-Enhanced Contrastive Learning for Data and Model Heterogeneity in Federated Learning},
  author={Zhang, Jianqing and Liu, Yang and Hua, Yang and Cao, Jian},
  journal={arXiv preprint arXiv:2401.03230},
  year={2024}
}


@inproceedings{FedCAC,
  title={Bold but cautious: Unlocking the potential of personalized federated learning through cautiously aggressive collaboration},
  author={Wu, Xinghao and Liu, Xuefeng and Niu, Jianwei and Zhu, Guogang and Tang, Shaojie},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={19375--19384},
  year={2023}
}

@article{FedBABU,
  title={Fedbabu: Towards enhanced representation for federated image classification},
  author={Oh, Jaehoon and Kim, Sangmook and Yun, Se-Young},
  journal={arXiv preprint arXiv:2106.06042},
  year={2021}
}

@article{Fed-CO2,
  title={Fed-CO $ \_ $\{$2$\}$ $: Cooperation of Online and Offline Models for Severe Data Heterogeneity in Federated Learning},
  author={Cai, Zhongyi and Shi, Ye and Huang, Wei and Wang, Jingya},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{kNN-Per,
  title={Personalized federated learning through local memorization},
  author={Marfoq, Othmane and Neglia, Giovanni and Vidal, Richard and Kameni, Laetitia},
  booktitle={International Conference on Machine Learning},
  pages={15070--15092},
  year={2022},
  organization={PMLR}
}

@article{IFCA,
  title={An efficient framework for clustered federated learning},
  author={Ghosh, Avishek and Chung, Jichan and Yin, Dong and Ramchandran, Kannan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={19586--19597},
  year={2020}
}


@inproceedings{FedCluster,
  title={Fedcluster: Boosting the convergence of federated learning via cluster-cycling},
  author={Chen, Cheng and Chen, Ziyi and Zhou, Yi and Kailkhura, Bhavya},
  booktitle={2020 IEEE International Conference on Big Data (Big Data)},
  pages={5017--5026},
  year={2020},
  organization={IEEE}
}

@article{FLT,
  title={Federated learning with taskonomy for non-IID data},
  author={Jamali-Rad, Hadi and Abdizadeh, Mohammad and Singh, Anuj},
  journal={IEEE transactions on neural networks and learning systems},
  year={2022},
  publisher={IEEE}
}

@inproceedings{FedConcat,
  title={Exploiting Label Skews in Federated Learning with Model Concatenation},
  author={Diao, Yiqun and Li, Qinbin and He, Bingsheng},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={10},
  pages={11784--11792},
  year={2024}
}

@inproceedings{Taskonomy,
  title={Taskonomy: Disentangling task transfer learning},
  author={Zamir, Amir R and Sax, Alexander and Shen, William and Guibas, Leonidas J and Malik, Jitendra and Savarese, Silvio},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3712--3722},
  year={2018}
}

@article{FedDC,
  title={Federated learning from small datasets},
  author={Kamp, Michael and Fischer, Jonas and Vreeken, Jilles},
  journal={arXiv preprint arXiv:2110.03469},
  year={2021}
}

@article{DBE,
  title={Eliminating domain bias for federated learning in representation space},
  author={Zhang, Jianqing and Hua, Yang and Cao, Jian and Wang, Hao and Song, Tao and Xue, Zhengui and Ma, Ruhui and Guan, Haibing},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{FedFed,
  title={FedFed: Feature distillation against data heterogeneity in federated learning},
  author={Yang, Zhiqin and Zhang, Yonggang and Zheng, Yu and Tian, Xinmei and Peng, Hao and Liu, Tongliang and Han, Bo},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{FedPAC,
  title={Personalized federated learning with feature alignment and classifier collaboration},
  author={Xu, Jian and Tong, Xinyi and Huang, Shao-Lun},
  journal={arXiv preprint arXiv:2306.11867},
  year={2023}
}

@article{nguyen2022begin,
  title={Where to begin? on the impact of pre-training and initialization in federated learning},
  author={Nguyen, John and Wang, Jianyu and Malik, Kshitiz and Sanjabi, Maziar and Rabbat, Michael},
  journal={arXiv preprint arXiv:2206.15387},
  year={2022}
}


@misc{FedCLIP,
      title={FedCLIP: Fast Generalization and Personalization for CLIP in Federated Learning}, 
      author={Wang Lu and Xixu Hu and Jindong Wang and Xing Xie},
      year={2023},
      eprint={2302.13485},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{CLIP2FL,
  title={CLIP-Guided Federated Learning on Heterogeneity and Long-Tailed Data},
  author={Shi, Jiangming and Zheng, Shanshan and Yin, Xiangbo and Lu, Yang and Xie, Yuan and Qu, Yanyun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={13},
  pages={14955--14963},
  year={2024}
}

@inproceedings{FedBEAL,
  title={Bias-eliminating augmentation learning for debiased federated learning},
  author={Xu, Yuan-Yi and Lin, Ci-Siang and Wang, Yu-Chiang Frank},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={20442--20452},
  year={2023}
}

@article{FedSIM,
  title={Personalized federated learning with server-side information},
  author={Song, Jaehun and Oh, Min-Hwan and Kim, Hyung-Sin},
  journal={IEEE Access},
  volume={10},
  pages={120245--120255},
  year={2022},
  publisher={IEEE}
}

@article{FedSampling,
  title={Fedsampling: A better sampling strategy for federated learning},
  author={Qi, Tao and Wu, Fangzhao and Lyu, Lingjuan and Huang, Yongfeng and Xie, Xing},
  journal={arXiv preprint arXiv:2306.14245},
  year={2023}
}

@inproceedings{ClusteredSampling,
  title={Clustered sampling: Low-variance and improved representativity for clients selection in federated learning},
  author={Fraboni, Yann and Vidal, Richard and Kameni, Laetitia and Lorenzi, Marco},
  booktitle={International Conference on Machine Learning},
  pages={3407--3416},
  year={2021},
  organization={PMLR}
}

@article{ConvergenceAnalysis1,
  title={On the convergence of fedavg on non-iid data},
  author={Li, Xiang and Huang, Kaixuan and Yang, Wenhao and Wang, Shusen and Zhang, Zhihua},
  journal={arXiv preprint arXiv:1907.02189},
  year={2019}
}

@inproceedings{Prototypical1,
    author = {Snell, Jake and Swersky, Kevin and Zemel, Richard},
    title = {Prototypical networks for few-shot learning},
    year = {2017},
    isbn = {9781510860964},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {We propose Prototypical Networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical Networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend Prototypical Networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.},
    booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
    pages = {4080–4090},
    numpages = {11},
    location = {Long Beach, California, USA},
    series = {NIPS'17}
}

@misc{Prototypical2,
    title={Prototypical Contrastive Learning of Unsupervised Representations},
    author={Junnan Li and Pan Zhou and Caiming Xiong and Steven C. H. Hoi},
    year={2020},
    eprint={2005.04966},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@inproceedings{FedBFPT,
  title={FEDBFPT: An efficient federated learning framework for BERT further pre-training},
  author={Xin’ao Wang, Huan Li and Chen, Ke and Shou, Lidan},
  booktitle={Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence},
  pages={4344--4352},
  year={2023}
}

@inproceedings{FedFTG,
  title={Fine-tuning global model via data-free knowledge distillation for non-iid federated learning},
  author={Zhang, Lin and Shen, Li and Ding, Liang and Tao, Dacheng and Duan, Ling-Yu},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10174--10183},
  year={2022}
}

@inproceedings{Astraea,
  title={Astraea: Self-balancing federated learning for improving classification accuracy of mobile deep learning applications},
  author={Duan, Moming and Liu, Duo and Chen, Xianzhang and Tan, Yujuan and Ren, Jinting and Qiao, Lei and Liang, Liang},
  booktitle={2019 IEEE 37th international conference on computer design (ICCD)},
  pages={246--254},
  year={2019},
  organization={IEEE}
}

@misc{lin2022fednlp,
      title={FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks}, 
      author={Bill Yuchen Lin and Chaoyang He and Zihang Zeng and Hulin Wang and Yufen Huang and Christophe Dupuy and Rahul Gupta and Mahdi Soltanolkotabi and Xiang Ren and Salman Avestimehr},
      year={2022},
      eprint={2104.08815},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{FedNER,
      title={FedNER: Privacy-preserving Medical Named Entity Recognition with Federated Learning}, 
      author={Suyu Ge and Fangzhao Wu and Chuhan Wu and Tao Qi and Yongfeng Huang and Xing Xie},
      year={2020},
      eprint={2003.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{du2023federated,
      title={Federated Nearest Neighbor Machine Translation}, 
      author={Yichao Du and Zhirui Zhang and Bingzhe Wu and Lemao Liu and Tong Xu and Enhong Chen},
      year={2023},
      eprint={2302.12211},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{roosta2021communicationefficient,
      title={Communication-Efficient Federated Learning for Neural Machine Translation}, 
      author={Tanya Roosta and Peyman Passban and Ankit Chadha},
      year={2021},
      eprint={2112.06135},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{cai2023efficient,
  title={Efficient federated learning for modern nlp},
  author={Cai, Dongqi and Wu, Yaozong and Wang, Shangguang and Lin, Felix Xiaozhu and Xu, Mengwei},
  booktitle={Proceedings of the 29th Annual International Conference on Mobile Computing and Networking},
  pages={1--16},
  year={2023}
}
@misc{reisizadeh2020stragglerresilient,
      title={Straggler-Resilient Federated Learning: Leveraging the Interplay Between Statistical Accuracy and System Heterogeneity}, 
      author={Amirhossein Reisizadeh and Isidoros Tziotis and Hamed Hassani and Aryan Mokhtari and Ramtin Pedarsani},
      year={2020},
      eprint={2012.14453},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{
nguyen2023where,
title={Where to Begin? On the Impact of Pre-Training and Initialization in Federated Learning},
author={John Nguyen and Jianyu Wang and Kshitiz Malik and Maziar Sanjabi and Michael Rabbat},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Mpa3tRJFBb}
}
@inproceedings{Wang2023FedBFPTAE, 
title={FedBFPT: An Efficient Federated Learning Framework for Bert Further Pre-training}, author={Xin'ao Wang and Huan Li and Ke Chen and Lidan Shou}, booktitle={International Joint Conference on Artificial Intelligence}, year={2023}, url={https://api.semanticscholar.org/CorpusID:260859911} }
@misc{FedDiv,
      title={FedDiv: Collaborative Noise Filtering for Federated Learning with Noisy Labels}, 
      author={Jichang Li and Guanbin Li and Hui Cheng and Zicheng Liao and Yizhou Yu},
      year={2024},
      eprint={2312.12263},
      archivePrefix={arXiv},
      primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}
}

@misc{zhong2019closer,
      title={A Closer Look at Data Bias in Neural Extractive Summarization Models}, 
      author={Ming Zhong and Danqing Wang and Pengfei Liu and Xipeng Qiu and Xuanjing Huang},
      year={2019},
      eprint={1909.13705},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}
@misc{xing2021demoting,
      title={Demoting the Lead Bias in News Summarization via Alternating Adversarial Learning}, 
      author={Linzi Xing and Wen Xiao and Giuseppe Carenini},
      year={2021},
      eprint={2105.14241},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}
@misc{ko2021look,
      title={Look at the First Sentence: Position Bias in Question Answering}, 
      author={Miyoung Ko and Jinhyuk Lee and Hyunjae Kim and Gangwoo Kim and Jaewoo Kang},
      year={2021},
      eprint={2004.14602},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}
@misc{shrivastava2016training,
      title={Training Region-based Object Detectors with Online Hard Example Mining}, 
      author={Abhinav Shrivastava and Abhinav Gupta and Ross Girshick},
      year={2016},
      eprint={1604.03540},
      archivePrefix={arXiv},
      primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}
}
@misc{wang2023hard,
      title={Hard Patches Mining for Masked Image Modeling}, 
      author={Haochen Wang and Kaiyou Song and Junsong Fan and Yuxi Wang and Jin Xie and Zhaoxiang Zhang},
      year={2023},
      eprint={2304.05919},
      archivePrefix={arXiv},
      primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}
}
@article{Tang2021FedCorCA, 
title={FedCor: Correlation-Based Active Client Selection Strategy for Heterogeneous Federated Learning}, author={Minxue Tang and Xuefei Ning and Yitu Wang and Jingwei Sun and Yu Wang and Hai Helen Li and Yiran Chen}, journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, year={2021}, pages={10092-10101}, url={https://api.semanticscholar.org/CorpusID:245500979} }

@misc{FedMix,
      title={FedMix: Approximation of Mixup under Mean Augmented Federated Learning}, 
      author={Tehrim Yoon and Sumin Shin and Sung Ju Hwang and Eunho Yang},
      year={2021},
      eprint={2107.00233},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}
@misc{FedTDA,
      title={Fed-TDA: Federated Tabular Data Augmentation on Non-IID Data}, 
      author={Shaoming Duan and Chuanyi Liu and Peiyi Han and Tianyu He and Yifeng Xu and Qiyuan Deng},
      year={2023},
      eprint={2211.13116},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}
@misc{FedFA,
      title={FedFA: Federated Feature Augmentation}, 
      author={Tianfei Zhou and Ender Konukoglu},
      year={2023},
      eprint={2301.12995},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}
@misc{zhang2018mixup,
      title={mixup: Beyond Empirical Risk Minimization}, 
      author={Hongyi Zhang and Moustapha Cisse and Yann N. Dauphin and David Lopez-Paz},
      year={2018},
      eprint={1710.09412},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}
@misc{cubuk2019autoaugment,
      title={AutoAugment: Learning Augmentation Policies from Data}, 
      author={Ekin D. Cubuk and Barret Zoph and Dandelion Mane and Vijay Vasudevan and Quoc V. Le},
      year={2019},
      eprint={1805.09501},
      archivePrefix={arXiv},
      primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}
}
@ARTICLE{6216380,
  author={Ricci, Elisa and Zen, Gloria and Sebe, Nicu and Messelodi, Stefano},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A Prototype Learning Framework Using EMD: Application to Complex Scenes Analysis}, 
  year={2013},
  volume={35},
  number={3},
  pages={513-526},
  keywords={Histograms;Prototypes;Image analysis;Context;Optimization;Clustering algorithms;Optical imaging;Video surveillance;complex scene analysis;earth mover's distance;parametric linear programming},
  doi={10.1109/TPAMI.2012.131}}

@misc{li2021prototypical,
      title={Prototypical Contrastive Learning of Unsupervised Representations}, 
      author={Junnan Li and Pan Zhou and Caiming Xiong and Steven C. H. Hoi},
      year={2021},
      eprint={2005.04966},
      archivePrefix={arXiv},
      primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}
}
@misc{long2015learning,
      title={Learning Transferable Features with Deep Adaptation Networks}, 
      author={Mingsheng Long and Yue Cao and Jianmin Wang and Michael I. Jordan},
      year={2015},
      eprint={1502.02791},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}
@misc{zhuang2020comprehensive,
      title={A Comprehensive Survey on Transfer Learning}, 
      author={Fuzhen Zhuang and Zhiyuan Qi and Keyu Duan and Dongbo Xi and Yongchun Zhu and Hengshu Zhu and Hui Xiong and Qing He},
      year={2020},
      eprint={1911.02685},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}
@misc{lin2018focallossdenseobject,
      title={Focal Loss for Dense Object Detection}, 
      author={Tsung-Yi Lin and Priya Goyal and Ross Girshick and Kaiming He and Piotr Dollár},
      year={2018},
      eprint={1708.02002},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1708.02002}, 
}
@misc{kawaguchi2020orderedsgdnewstochastic,
      title={Ordered SGD: A New Stochastic Optimization Framework for Empirical Risk Minimization}, 
      author={Kenji Kawaguchi and Haihao Lu},
      year={2020},
      eprint={1907.04371},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1907.04371}, 
}
@article{chen2023bias,
  title={Bias and debias in recommender system: A survey and future directions},
  author={Chen, Jiawei and Dong, Hande and Wang, Xiang and Feng, Fuli and Wang, Meng and He, Xiangnan},
  journal={ACM Transactions on Information Systems},
  volume={41},
  number={3},
  pages={1--39},
  year={2023},
  publisher={ACM New York, NY}
}
@misc{chen2023evolvingsemanticprototypeimproves,
      title={Evolving Semantic Prototype Improves Generative Zero-Shot Learning}, 
      author={Shiming Chen and Wenjin Hou and Ziming Hong and Xiaohan Ding and Yibing Song and Xinge You and Tongliang Liu and Kun Zhang},
      year={2023},
      eprint={2306.06931},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.06931}, 
}
@inproceedings{zhang2022unsupervised,
  title={Unsupervised sentence representation via contrastive learning with mixing negatives},
  author={Zhang, Yanzhao and Zhang, Richong and Mensah, Samuel and Liu, Xudong and Mao, Yongyi},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={10},
  pages={11730--11738},
  year={2022}
}
@misc{shrivastava2016trainingregionbasedobjectdetectors,
      title={Training Region-based Object Detectors with Online Hard Example Mining}, 
      author={Abhinav Shrivastava and Abhinav Gupta and Ross Girshick},
      year={2016},
      eprint={1604.03540},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1604.03540}, 
}
@inproceedings{arpit2017closer,
  title={A closer look at memorization in deep networks},
  author={Arpit, Devansh and Jastrz{\k{e}}bski, Stanis{\l}aw and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and others},
  booktitle={International conference on machine learning},
  pages={233--242},
  year={2017},
  organization={PMLR}
}
@inproceedings{girshick2014rich,
  title={Rich feature hierarchies for accurate object detection and semantic segmentation},
  author={Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={580--587},
  year={2014}
}
@inproceedings{mindermann2022prioritized,
  title={Prioritized training on points that are learnable, worth learning, and not yet learnt},
  author={Mindermann, S{\"o}ren and Brauner, Jan M and Razzak, Muhammed T and Sharma, Mrinank and Kirsch, Andreas and Xu, Winnie and H{\"o}ltgen, Benedikt and Gomez, Aidan N and Morisot, Adrien and Farquhar, Sebastian and others},
  booktitle={International Conference on Machine Learning},
  pages={15630--15649},
  year={2022},
  organization={PMLR}
}
@article{kaddour2024no,
  title={No train no gain: Revisiting efficient training algorithms for transformer-based language models},
  author={Kaddour, Jean and Key, Oscar and Nawrot, Piotr and Minervini, Pasquale and Kusner, Matt J},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{FedSum1,
  title={Adapter-based Selective Knowledge Distillation for Federated Multi-domain Meeting Summarization},
  author={Xiachong Feng and Xiaocheng Feng and Xiyuan Du and MingSung Kan and Bing Qin},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.03275},
  url={https://api.semanticscholar.org/CorpusID:260680370}
}

@article{FedSum2,
  title={Personalized Federated Learning via Gradient Modulation for Heterogeneous Text Summarization},
  author={Rong Pan and Jianzong Wang and Lingwei Kong and Zhangcheng Huang and Jing Xiao},
  journal={2023 International Joint Conference on Neural Networks (IJCNN)},
  year={2023},
  pages={1-7},
  url={https://api.semanticscholar.org/CorpusID:258298034}
}