% \begin{table*}[t]
% % \begin{tabular}{@{}lllllcccccc@{}}
% \centering
% \renewcommand{\arraystretch}{1.05} % 设置行高为默认的1.5倍
% \resizebox{0.80\linewidth}{!}{
% \begin{tabular}{cccccccccccc}
% \toprule
% \multicolumn{2}{c}{}                         & \multicolumn{3}{c}{}             & \multicolumn{3}{c}{(\OurDATA{})}                                   & \multicolumn{3}{c}{} \\ \cmidrule(l){3-11} 
% \multicolumn{2}{c}{}                         & \multicolumn{3}{c}{(1-shot)}             & \multicolumn{3}{c}{(2-shot)}                                   & \multicolumn{3}{c}{(4-shot)}                                          \\ \cmidrule(l){3-11} 
% \multicolumn{2}{c}{\multirow{-4}{*}{\textbf{Method}}} & EM                           & Tokens & $1/\eta$    & EM                      & Tokens                & $1/\eta$                      & EM                      & Tokens                 & $1/\eta$                      \\ \midrule
% % \rowcolor[HTML]{DFDFDF} 
% \multirow{4}{*}{Original}                 
%                           &  (1-shot)        &71.04                        & 422    & 1.00 &                         &                       &                        &                         &                        &                        \\
%                           &  (1-shot)        & 74.98                        & 311    & 1.00 & \multirow{-2}{*}{76.81} & \multirow{-2}{*}{733} & \multirow{-2}{*}{1.00} &                         &                        &                        \\
%                           &  (1-shot)        & 73.92 & 249    & 1.00 &                         &                       &                        &                         &                        &                        \\
                          
%                           &  (1-shot)        & 73.39                        & 242    & 1.00 & \multirow{-2}{*}{{79.53}} & \multirow{-2}{*}{491} & \multirow{-2}{*}{1.00} & \multirow{-4}{*}{78.92} & \multirow{-4}{*}{1224} & \multirow{-4}{*}{1.00} \\ \midrule
% % \rowcolor[HTML]{DFDFDF} 
% \multirow{4}{*}{Selective-Context}        
%                           &  (1-shot)        & 53.28                        & 317    & 1.33 &                         &                       &                        &                         &                        &                        \\
%                           &  (1-shot)        & 54.74                        & 231    & 1.35 & \multirow{-2}{*}{57.34} & \multirow{-2}{*}{550} & \multirow{-2}{*}{1.33} &                         &                        &                        \\
%                           &  (1-shot)        & 58.40                        & 190    & 1.32 &                         &                       &                        &                         &                        &                        \\
%                           &  (1-shot)        & 55.78                        & 175    & 1.39 & \multirow{-2}{*}{52.17} & \multirow{-2}{*}{349} & \multirow{-2}{*}{1.41} & \multirow{-4}{*}{58.13} & \multirow{-4}{*}{881}  & \multirow{-4}{*}{1.39} \\ \midrule
% % \rowcolor[HTML]{DFDFDF} 

% \multirow{4}{*}{LLMLLingua}                
%                           &  (1-shot)        & \underline{68.43}(0.5\%)                        & 321    & 1.32 &                         &                       &                        &                         &                        &                        \\
%                           &  (1-shot)        & 60.02                        & 234    & 1.33 & \multirow{-2}{*}{65.71} & \multirow{-2}{*}{498} & \multirow{-2}{*}{1.47} &                         &                        &                        \\
%                           &  (1-shot)        & 66.53                        & 202    & 1.23 &                         &                       &                        &                         &                        &                        \\
%                           &  (1-shot)        & \underline{62.38}(4.7\%)                        & 172    & 1.41 & \multirow{-2}{*}{59.29} & \multirow{-2}{*}{358} & \multirow{-2}{*}{1.37} & \multirow{-4}{*}{\underline{63.41}(13.7\%)} & \multirow{-4}{*}{906}  & \multirow{-4}{*}{1.35} \\ \midrule
% % \rowcolor[HTML]{DFDFDF} 
% \multirow{4}{*}{GPT4-Generation}           
%                           &  (1-shot)        & 62.86                        & 283    & 1.49 &                         &                       &                        &                         &                        &                        \\
%                           &  (1-shot)        & \underline{65.71}(9.5\%)                        & 221    & 1.41 & \multirow{-2}{*}{\underline{68.57}(4.3\%)} & \multirow{-2}{*}{469} & \multirow{-2}{*}{1.56} &                         &                        &                        \\
%                           &  (1-shot)        & \underline{66.57}(1.1\%)                        & 199    & 1.25 &                         &                       &                        &                         &                        &                        \\
%                           &  (1-shot)        & 60.18                        & 159    & 1.52 & \multirow{-2}{*}{\underline{67.00}(8.2\%)} & \multirow{-2}{*}{354} & \multirow{-2}{*}{1.39} & \multirow{-4}{*}{62.68} & \multirow{-4}{*}{956}  & \multirow{-4}{*}{1.28} \\ \midrule
% % \rowcolor[HTML]{DFDFDF} 
% \multirow{4}{*}{\OurMODEL{}(Ours)}                      
%                           &  (1-shot)        & \textbf{68.78}                        & 292    & 1.45 &                         &                       &                        &                         &                        &                        \\
%                           &  (1-shot)        & \textbf{71.98}                        & 212    & 1.47 & \multirow{-2}{*}{\textbf{71.53}} & \multirow{-2}{*}{469} & \multirow{-2}{*}{1.56} &                         &                        &                        \\
%                           & (1-shot)        & \textbf{67.27}                        & 160    & 1.56 &                         &                       &                        &                         &                        &                        \\
%                           & (1-shot)        & \textbf{65.32}                        & 165    & 1.47 & \multirow{-2}{*}{\textbf{72.47}} & \multirow{-2}{*}{329} & \multirow{-2}{*}{1.49} & \multirow{-4}{*}{\textbf{72.12}} & \multirow{-4}{*}{820}  & \multirow{-4}{*}{1.49} \\ 
% \bottomrule[1.0pt]
% \end{tabular}
% }
% \caption{Experimental results on~\OurDATA{}. We report the number 
% of tokens in original and compressed prompts along with EM and 
% compression ratio $(1/\eta)$. We bold-face the overall 
% best-performing scores with existing state-of-art underlined. \di{What is $\eta^*$, tokens, and what is the percentage number? Why there are four 1-shot results? }}
% \vspace{-2ex}
% \label{tab:exp1}
% \end{table*}



% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[t]
% \begin{tabular}{@{}lllllcccccc@{}}
\centering
\renewcommand{\arraystretch}{1.05} % 设置行高为默认的1.5倍
\resizebox{0.80\linewidth}{!}{
\begin{tabular}{ccccccccccccccc}
\toprule
\multicolumn{2}{c}{\multirow{2}{*}{Method}} & \multicolumn{3}{c}{\OurDATA{}(1-shot)} & \multicolumn{3}{c}{\OurDATA{}(2-shot)} & \multicolumn{3}{c}{\OurDATA{}(4-shot)} & \multicolumn{3}{c}{\OurDATA{}(8shot)}\\ \cline{3-14} 

\multicolumn{2}{c}{}                        & EM         & Tokens      & 1/$\eta$       & EM         & Tokens      & 1/$\eta$       & EM         & Tokens      & 1/$\eta$      & EM         & Tokens      & 1/$\eta$ \\ \hline
\multicolumn{2}{c}{Original}                & 73.33      & 306         & 1.00      & 78.17      & 612         & 1.00      & 78.92      & 1224        & 1.00     & 82.53      & 2365        & 1.00 \\

\multicolumn{2}{c}{Selective-Context}       & 55.55      & 228         & 1.34      & 58.01      & 449         & 1.36      & 58.13      & 881         & 1.39     & 61.47      & 1752        & 1.35 \\

\multicolumn{2}{c}{LLMLLingua}              & \underline{63.25(7.3\%)}      & 232         & 1.32      & 65.70      & 428         & 1.43      & \underline{67.41(5.5\%)}      & 906         & 1.35     & \underline{73.02(4.2\%)}      & 1799        & 1.31 \\

\multicolumn{2}{c}{GPT4-Generation}         & 60.51      & 215         & 1.42      & \underline{66.44(10.1\%)}      & 411         & 1.49      & 66.39      & 956         & 1.28     & 71.41      & 1273        & 1.86 \\

\multicolumn{2}{c}{\OurMODEL{}}                    & \textbf{67.84}      & 207         & 1.48      & \textbf{73.17}      & 399         & 1.53      & \textbf{71.14}      & 820         & 1.49     & \textbf{76.13}      & 1586        & 1.49  \\ 
\bottomrule[1.0pt]
\end{tabular}}
\vspace{-1.7ex}
\caption{Experimental results on~\OurDATA{}. We report the avg., number of \emph{Tokens} in original and compressed prompts along with EM and compression ratio $(1/\eta)$. For these results, we use GPT3.5-turbo as target LLM. $\eta^{*}$ is set equal to 0.7. We bold-face overall best scores, and
underline the state-of-art along with relative \%-age performance improvement.}
\vspace{-3.1ex}
\label{tab:exp1}
\end{table*}