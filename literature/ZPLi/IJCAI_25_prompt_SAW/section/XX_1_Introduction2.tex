\section{Introduction}
% background介绍
Large Language Models (LLMs) have attracted considerable attention for 
their superior performance across a wide range of applications. For 
this task-specific instructions (aka prompts) play a crucial role 
for extending the capabilities of LLMs for multiple different tasks.
The prompts provide the provision to guide the model to elucidate
desired model behavior without perturbing the model parameters.
This is also highlighted in recent studies that show well-designed 
prompts and the integration of external knowledge is significant 
to enhance the effectiveness of LLMs' (\citet{PromptEngineeringSurvey},~\citep{PromptEngineeringSurvey}).
Different LLMs-related techniques directly benefiting from prompts 
include but are not limited to: 
In-Context Learning (\citet{ICL},~\citep{ICL}), 
Chain-of-Thought (\citet{CoT},~\citep{CoT}), 
Retrieval Augmented Generation (\citet{RAG},~\citep{RAG}), 
and Agents (\citet{Agent},~\citep{Agent}) etc. 
\eat{play a crucial role in this process.}







% 引出prompt压缩
%(\citet{softPrompt}~\citep{softPrompt})
At the same time, the abilities of LLMs are significantly 
compromised/constrained by increasingly lengthy prompts, 
even comprising thousands of tokens. 
Lengthy prompts 
not only obscure requisite/essential information, but
also increase the computational costs and incur inference latency.
To tackle this challenge, \textit{prompt compression} 
techniques, e.g.,~\cite{selectiveContext}, have garnered significant interests.
These approaches are based on the fact that natural language is 
inherently redundant~(\citet{rongyu}~\citep{rongyu}) and we may 
substantially compress the length of original textual prompts by preserving 
requisite/essential information (\citet{softPrompt},~\citep{softPrompt})
in way that it maintains and/or improves the end-utility of the prompt.

\eat{These approaches are \eat{predicated}\asif{based} on the \eat{notion}\asif{fact} 
that natural language inherently contains redundancies~\cite{rongyu}, 
seeks to diminish the length of the original prompts.
~\cite{softPrompt} observed that such compressed 
prompts are capable of preserving essential information.}

% 介绍已有方法
Existing prompt compression approaches may be divided into 
two major categories: soft prompt compression
~(\citet{softPrompt},~\citep{softPrompt}) and 
discrete prompt compression~(\citet{Jung2023DiscretePC},~\citep{Jung2023DiscretePC}).
Soft prompt compression employs back-propagation to incorporate information from 
labeled training instances. It suffers from lack of cross-model reusability 
and is ineffective for proprietary LLMs (only accessible via APIs). 
On the other hand, discrete prompt compression exploits the discrete nature 
of the text to design strategies that directly changes/edits the prompt tokens.
For instance,~\cite{selectiveContext} proposed Selective-Context that uses 
a compact language model to evaluate context's lexical units, enabling 
compression by eliminating units with minimal information. 
Also, LLMLingua~(\citet{llmlingua},~\citep{llmlingua}) 
and LongLLMLingua~(\citet{longllmlingua},~\citep{longllmlingua}) 
developed budget control mechanisms to compresses prompts based on their perplexity.


\fixme{some limitations of discrete prompting methods..? \warn{Add in red plz.}}

\eat{Prompt compression technologies are currently divided into two main categories: soft prompt compression~\cite{softPrompt} and discrete prompt compression~\cite{Jung2023DiscretePC}. Soft prompt compression, achieved by training embeddings inclusive of the original context, suffers from a lack of cross-model reusability and is ineffective when Large Language Models (LLMs) are accessible only via APIs. Conversely, a more promising strategy involves directly compressing discrete prompts that consist of specific tokens from the vocabulary.
~\cite{selectiveContext} propose Selective-Context, employs a compact language model to evaluate the self-information of context's lexical units, enabling compression through the elimination of units with minimal informational value. Subsequently, LLMLingua~\cite{llmlingua} and LongLLMLingua~\cite{longllmlingua} developed an innovative budget control mechanism and compresses prompts based on their perplexity.} 

% 引出limitation
\warn{We observe the existing research on prompt compression \eat{encounters}
lacks two key/primary aspects, i.e., evaluation fairness and grammatical structural coherence.}

\eat{These methodologies encounter two primary challenges: evaluation fairness and 
\eat{structural deficiency}~\asif{grammatical structural coherence}.}
% Prompt压缩算法对不同的Prompt压缩结果有较大的方差，但是目前Prompt压缩算法选择的基准Prompt数量较少，这会导致结果的偏移。而且他们没有基于相同的基准Prompt进行压缩，这也进一步加剧了不公平性。
\warn{The evaluation fairness implies absence of a universally accepted benchmark 
for rigorous \eat{prompt compression results in} evaluation 
\eat{not being conducted}under equitable conditions.}
\eat{\asif{Lacking evaluation fairness implies absence of a universally accepted benchmark 
for rigorous \eat{prompt compression results in} evaluation 
\eat{not being conducted}under equitable conditions.}}
\warn{The performance of compression algorithms\eat{for prompts} varies significantly in terms of the outcomes of compressed prompts.
However, the range of original prompts selected by these algorithms is restricted, 
resulting in biased outcomes. \fixme{Double-check. What do you mean here..?}
Furthermore, the absence of standardized benchmark prompts across algorithms 
amplifies the issue of unfair comparisons.}

\textbf{Do we also work out transfer-ability of compressed prompts
across different LLMs..??}
\fixchen{Do we really solve this...? Do we set up a standard benchmark 
+ Evaluate all existing approaches. If not we need to mention other limitations.}

% 这不仅使得压缩后的Prompt存在语义缺失的风险，而且会造成压缩后的Prompt人类进行阅读存在困难。
\warn{By structural coherence, we imply existing approaches neglect the 
syntactic and semantic structure of the compressed prompt thus hampering 
its readability for the human readers. 
It happens because the contemporary prompt compression algorithms primarily 
focus on quantifying token-level information, neglecting the overall 
grammatical structure of the compressed prompt.}
\fixchen{It would be best if we could add an output example in introduction, 
showing lack of grammatical structures for output of any existing PC techniques}

\eat{Secondly, contemporary prompt compression algorithms primarily focus on 
quantifying token-level information content, neglecting the overall structural 
information of the prompt. This not only increases the risk of semantic loss 
within the compressed prompt but also hampers its readability for human readers.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Apart from this, we observe compressing longer prompts into short 
concise and meaningful 
text is a challenging task. An effective prompt compression strategy 
has to preserve the requisite semantic content while at the same time 
maintaining the end-performance.
It is very hard to enforce stringent constraints on the
compressed prompt, as overly-concise prompts may 
result in losing requisite semantic contents, eventually 
leading to performance degradation.
Moreover, the discrete nature of the text makes it difficult to use 
back-propagation for effective gradient propagation.


% 介绍自己的方法
To address these challenges, in this paper we propose 
\textsc{\underline{\textbf{Prompt}}} 
compres\underline{\textbf{S}}ion 
using relation \underline{\textbf{AW}}are graphs (\OurMODEL{}), 
a novel method designed to cut-down un-necessary information in the 
prompt text by exploiting knowledge graph relational triplets.
\warn{Specifically,~\OurMODEL{} first extracts all possible 
relational triplets for the information contained in the prompt.
Later, it organizes the relational triplets as meaningful and 
semantically coherent sentence structures helpful for identifying 
superfluous information. Finally, it, reinstates essential knowledge 
graph triples, resulting in an optimized and compressed prompt.
}


%%%%%%%%%%%%%%%%%%


\asif{\OurMODEL{} attained state-of-the-art performance on open-source evaluation 
benchmarks, i.e.,~\textsc{GSM8K} and \textsc{BBH}. Note, the existing benchmarks 
are limited to a small range of tasks~\warn{(also explained in Section XX)}, 
we also curated a novel benchmark namely \fixchen{NAME of Benchmark} 
encompassing prompts from diverse task scenarios for a rigorous 
evaluation of \OurMODEL. We summarize the key contributions of this work as follows:}

\warn{
\begin{itemize}
    \item We propose~\OurMODEL{}, a methodology crafted for compressing prompts 
    by extracting the graph-structured information embedded within the prompts themselves.
    \item We establish a benchmark for the evaluation of prompt compression.
    \item We carried out comprehensive experiments across a range of prompt compression algorithms, demonstrating that our approach achieves state-of-the-art performance.
\end{itemize}}
\fixme{Re-write the contribution based on the new proposed methodology.}
\li{Extended prompts result in elevated costs and inference delays. Moreover, they can contribute to a potential decline in model performance}







%%%%%%%%%%%%%%%% EXTRA Text %%%%%%%%%%%%%%%%%%%%%%%%



\eat{\OurMODEL{} attained state-of-the-art performance on open-source data sets, i.e.,  
\textsc{GSM8K} and \textsc{BBH}. 
Given the fact
During the experimentation phase, it became clear that a single dataset provided a limited range of tasks and lacked a suitable original prompt for fair comparison. 
We also developed
Consequently, we developed a benchmark comprising initial prompts alongside diverse task scenarios.}



\eat{To address these challenges, in this paper we propose \textbf{K}nowledge 
\textbf{G}raph \textbf{B}ased \textbf{P}rompt \textbf{C}ompression (\OurMODEL), 
a novel method designed for prompt compression through the extraction 
of graph structure information.
\eat{, aiming to resolve the aforementioned issues}
Specifically,~\OurMODEL{} first extracts all possible 
\eat{knowledge graph triples or} relational triplets for 
the information contained in the prompt.
\eat{to eliminate redundant semantic information within the prompt.}
Later, it \eat{\textsc{KGBPC} further} organizes the relational
triplets as meaningful and semantically coherent sentence structures 
helpful for identifying\eat{and removing} superfluous information.
\eat{enhances prompt conciseness by relational triples leveraging graph structure insights.}
In the concluding phase, \textsc{KGBPC} reinstates essential knowledge graph triples, resulting in an optimized and compressed prompt.
}


\eat{Despite their advantages, LLMs sometimes face issues like logical errors and factual inaccuracies, which limits their real-world applicability (\citet{Hallucination},~\citep{Hallucination}).}


%\fixchen{Add some justifications for why we need compressed prompts? E.g., 
%(i) reduce inference latency, (ii) what else..!}

\eat{Nevertheless, these methods result in the use of increasingly lengthy prompts, 
even comprising thousands of tokens. Such lengthy prompts not only increase 
computational costs but also risk obscuring essential information, thereby 
diminishing the performance of LLMs.}
