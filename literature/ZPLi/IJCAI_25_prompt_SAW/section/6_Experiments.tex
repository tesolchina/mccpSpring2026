\vspace{-1.7ex}
\section{Experiments}
\label{sec:exp}
\vspace{-0.7ex}
In this section, we conduct comprehensive experiments for the performance 
evaluation for~\OurMODEL{} compared against different baseline models.

\subsection{Experiment Settings}

\noindent{\bf Datasets.}
To comprehensively evaluate the effectiveness of 
compressed prompts, we evaluate their performance under
both task-agnostic and task-aware data settings. 
For task-agnostic data sets, we consider~\OurDATA{}, 
\emph{i.e.,} an extended variant of the original \textsc{GSM8K}~\citep{GSM8K} 
devised by us to report the model performance  under 
$i$-shot settings with $i\in \{1,2,4,8\}$ (details in Section~\ref{sec:data_aug}). 
For the task-aware dataset, we use NaturalQuestions~\citep{LostInMiddle}, and ShareGPT\footnote{\url{https://sharegpt.com/}}.  
The statistics of dataset is given in 
Table~\ref{tab:DataStat}, and further details 
are provided in Appendix~\ref{Appendix:data}.





\noindent{\bf Baselines.} We compare the performance 
of~\OurMODEL{} against following models as baselines: 
(i) Selective-Context~\citep{selectiveContext},
(ii) LLMLingua~\citep{llmlingua}, 
(iii) LongLLMlingua~\citep{longllmlingua}, and
(iv) GPT4~\citep{GPT4}.
Details about the baselines are provided 
in Appendix~\ref{Appendix:Baseline}.
Note, in order to setup a fair platform for 
comparative evaluation, we re-compute the 
results for the baseline models as per our data 
settings.


\noindent{\bf Evaluation Metrics.} Similar to 
\citet{GSM8K} and \citet{llmlingua},
we use Exact Match (EM) as the evaluation
metric for~\OurDATA{}. For NaturalQuestions, we used 
Span Accuracy (Span-Acc) as a metric, similar to~\citet{LostInMiddle} 
and \citet{longllmlingua}. For ShareGPT, we used 
Rouge as the evaluation metric~\citep{rouge_2004}. 
Apart from these, we also use fluency (FL)~\citep{Fluency} 
to measure the readability and grammatical coherence of the 
compressed prompt. Details and mathematical
formulation of these metrics are given in Appendix~\ref{Appendix:Eval}.

\input{Table_tex/exp1}

\noindent{\bf Large Models.}
To demonstrate the generalization of our algorithm 
on different LLMs, we use {GPT3.5-turbo} 
and LLaMA2-7B-chat as our target LLMs. 

\noindent{\bf Experimental Setup.} 
Following the setting of LLMLingua~\citep{llmlingua}, 
we employ greedy decoding with the temperature set to 
0. The max number of tokens generated by LLMs are 
limited to 400.
For graph construction, we use Open-IE tooklit
~\citep{OpenIE} as the primary tool and 
Phi-3-mini~\footnote{\url{https://ollama.com/library/phi3}} as our auxiliary solution.
Note, on average 90-\% graphs were constructed using the 
Open-IE toolkit.
We use OpenAI embedding API~\footnote{\url{https://openai.com/}} as the embedding encoder $(E)$.
The value for $\eta^{*}$ is set to \{0.1, 0.3, 0.5\} for both~ShareGPT and NaturalQuestions, while $\eta^{*}$ = 0.7
for~\OurDATA{}.
In Algorithm~\ref{alg:alg2}, we use $\gamma$ = 0.001. 
All the results reported in 
this paper are averaged over five runs.
All experiments were performed using PyTorch 2.1.0 
with Nvidia RTX 4090 24GB GPU.


\subsection{Experimental Results}

{\bf Results for Task-agnostic Settings.} 
For task-agnostic settings, we report the results of~\OurMODEL{} for~\OurDATA{} in Table~\ref{tab:exp1}.
Note, unlike existing research that reports their 
performance for one fixed setting, we report these results 
for $i$-shot settings, where $i$ indicates the number 
of prompts have been employed by \OurMODEL{}, 
\emph{i.e.,} \{1, 2, 4 and 8\}-shots.

Comparing these results against the baseline models, we can observe
that~\OurMODEL{} outperforms the previous state-of-the-art by a 
significant margin. For instance, compared to the best 
performing baselines, \OurMODEL{} improves the {EM} score by 
up to 7.3\%, 10.1\%, 5.5\% and 4.2\% under 1-shot, 2-shot, 
4-shot and 8-shot settings, respectively.
{Correspondingly reduction in the prompt size is 
32.3\%, 34.9\%, 33.0\% and 32.9\%}.
We attribute such drastic performance improvement to the following factors: 
(1)~\OurMODEL{} retains the logical integrity of the prompts by sub-dividing the original prompts into smaller
comprehensive information elements; 
(2)\OurMODEL{} benefits from the workflow that allows
selecting and omitting individual information elements
for prompt compression without destroying the 
overall information structure of the compressed prompt.
These help~\OurMODEL{} to ensure the utility 
of the compressed prompt for the end task.

\input{Table_tex/exp2}
\input{Table_tex/exp_sharegpt}

\noindent{\bf Results for Task-aware Settings.} Table~\ref{tab:exp2} 
reports the performance of~\OurMODEL{} under task-aware 
settings on  NaturalQuestions using GPT3.5-turbo 
and LLaMA2-7B-chat as target LLMs. These results show that, 
\OurMODEL{} improves the Span Accuracy by 39.0\%, 40.8\% and 14.7\% for GPT3.5-turbo, and 77.1\%, 71.2\%, 72.7\% for LLaMA2-7B-chat, respectively, for different values of the target compression rates $\eta^{*} = \{0.5, 0.3, 0.1\}$, 
against the best-performing baseline (LongLLMLingua).
Correspondingly, the reduction in the prompt size is 
56.7\%, 74.0\%, and 93.7\%\ respectively.


The results of~\OurMODEL{} on ShareGPT (in Table~\ref{tab:exp_sharegpt}) show for GPT3.5-turbo as 
target LLM, \OurMODEL{} improves the Rouge-1 score 
by up to 29.3\%, 34.9\% and 38.6\%. The performance 
for Rouge-2 and Rouge-L exhibit a similar behavior. 
For these results, we also observe for LLaMA2-7B-chat, 
the improvement in performance is relatively lower 
compared to that of GPT3.5-turbo.
A probable justification in this regard is the fact 
that LLaMA2-7B-chat is more influenced by the change in context for the compressed prompt. Whereas, higher performance on GPT3.5-turbo indicates that~\OurMODEL{} preserves the critical information in the prompt.

Correlating the results for both settings, we observe 
that compared to the task-agnostic scenarios,~\OurMODEL{} yields better performance for task-aware settings, 
especially for NaturalQuestions.
This is due to 
the fact that it is more difficult to dig out the latent 
correlation between information elements within the prompt's 
internal structure rather than explicit task-aware 
correlation extraction.

From Table~\ref{tab:exp2} and Figure~\ref{fig:CompressRate} 
we can also find that the performance of~\OurMODEL{} drops 
when the value for the target compression rate $(\eta^{*})$ 
decreases from 0.5 to 0.1. A probable justification that the 
actual compression ratio of~\OurMODEL{} is significantly 
higher than the target compression ratio when the target 
compression ratio is 10, \emph{i.e.,} $\eta^{*}=0.1$. 
This is owing to the fact that~\OurMODEL{} only retains the information 
elements in $\mathcal{G}$ as the key/basic information units 
for the compression process. It will delete some entities and 
relations that may be highly similar to the problem but their 
overall structure is too long, leading to relatively poor 
performance for a higher compression ratio.

\eat{From Table~\ref{tab:exp_sharegpt} we can find that when using the ShareGPT dataset, utilizing ChatGPT3.5-turbo-0301 as the target LLM can achieve greater performance improvement than using Llama2-7B as the target LLM. A probable justification is that weaker LLMs (\emph{e.g.,} LLaMA2-7B-chat) are more influenced by the context when generating output. The great performance on GPT3.5-turbo indicates that \OurMODEL{} actually does not reduce the critical information from past conversations.}


\eat{which may be 
accredited to a higher compression ratio.}
\eat{Overall, these results show that~\OurMODEL{} exhibits 
better results compared to the baseline models.}

\eat{\textbf{Different target compress rate via LLMs}. As shown in Figure~\ref{fig:CompressRate}, the performance of \OurMODEL{} decreases relatively gently with the decrease of compression rate on \OurDATA{}. [Reason]. The compression rate has a greater impact for \OurMODEL{} on NatrualQuestions. At a compression rate of 0.1, since our compression granularity is not token, but triplet, the actual compression rate is much lower than 0.1. We check the compression results and find that .}

\subsection{Further Analysis}
In this section, we perform an in-depth analysis
of the performance of~\OurMODEL{}. 

%\subsubsection{Compressed Prompt Utility}
\eat{ 
{\bf Computational Overhead.} We partition the overall computational 
overhead of~\OurMODEL{} into compression overhead and subsequent 
inference overhead. This may be computed as follows:
\begin{equation}
    c_{\OurMODEL{}} = \underbrace{(P+N)\cdot c_{\text{small}}+N^{\prime}\cdot
    c_{\text{encoder}}}_{Compressed\ Overhead}+\widetilde{N}\cdot c_{\text{LLMs}},
\end{equation}
where $c_{small}$, $c_{encoders}$, and $c_{LLM}$ represent the per token overhead of the small-scale language \warn{model $M$ in \ref{eq:Stage1}}, encoder $E$, and LLM used to answer, respectively. $P$ 
is the length of prompt text used to extract graph structure, $N$ is the length of the original prompt, $N^{\prime}$ is the total length of elements included in the graph. \di{No analysis and results}
\warn{We also discuss using additional approaches to replace small 
LLMs in Appendix~\ref{Appendix:graph-construct}, which will 
further reduce the compression overhead of our method.}
} 

\eat{\textbf{The Overhead of LLMLingua}\\
\begin{equation}
    c_{LLMLingua}=(L+kL/\tau+L/\tau)\cdot c_{\text{small}}+L/\tau\cdot c_{\text{LLMs}}
\end{equation}
where $c_{small}$ and $c_{LLMs}$ represent the per token overhead of the small LM and LLM. $L$ is the length of original prompt, $kL/\tau$ is the perplexity calculation of tokens, and $L/\tau$ is the conditioned perplexity calculation of compressed results. The token numbers of perplexity and conditioned perplexity are consistent with those mentioned in LLMLingua.}

\eat{We formally discuss the cost of compress algorithm by setting compression threshold $\eta$, which mean the compression ratio $\tau$ is $1/\eta$. \\}



\noindent{\bf Different Target LLMs.}
We also analyze the performance of~\OurMODEL{} using 
different target LLMs. Comparing the results in Table~\ref{tab:exp2} and Table~\ref{tab:exp_sharegpt}, 
we can find that GPT3.5-turbo performs better than 
LLaMA2-7B-chat.

For NaturalQuestions, GPT3.5-turbo achieves up to 19.8\% 
higher Acc scores compared to that of LLaMA2-7B-chat.
Likewise for ShareGPT, it achieve up to 47.55\% higher 
value for Rouge-1. We attribute this result to GPT3.5's 
stronger ability to understand and comprehend context, 
resulting in generating higher quality response for 
the prompts compressed by~\OurMODEL{}.

\eat{However, when we consider the NatrualQuestions dataset, 
using LLaMA-2-7B-chat as target LLM allows \OurMODEL{} to 
achieve greater performance improvements compared to the 
baseline. 

This is because \OurMODEL{} can extract vital information 
from the prompt to help answer questions, which is harder 
for weaker LLM to find.}

% \input{Table_tex/exp3}




\input{Table_tex/exp4} 


\noindent{\bf Readability  of Compressed Prompts.}
As explained in the introduction (also highlighted in 
Figure~\ref{fig:example1}), a key limitation of existing  
prompt compression approaches is the limited readability 
of the compressed prompt. In order to validate the results of~\OurMODEL{} in terms
of human readability and/or interpretability, we report some
example prompts along with prompt compressed using~\OurMODEL{}
and LLMLingua~\citep{llmlingua}
in Appendix~\ref{Appendix:exp-readability}, for a quick comparison. These examples clearly indicate that the 
prompt compressed by~\OurMODEL{} exhibit better readability 
and/or interpretability compared to compressed using 
LLMLingua. 

For instance, as shown in Example~\ref{Appendix:ex1} 
(Appendix~\ref{Appendix:exp-readability}),
the prompt compressed by LLMLingua encompasses
grammatical incoherent text, such as:
\{\emph{``List of Nobelates in The first Prize1 
Wilhelmrad, of who received82 in en prize''}\}.
Lack of grammatical coherence significantly undermines 
the readability and/or interpretability of the compressed 
prompt, thus impacting its end-utility.
Whereas, the prompt compressed by~\OurMODEL{}, relying on knowledge graph triples exhibits a consistent grammatical 
sentence structure, as shown in the lower half of Example~\ref{Appendix:ex1}.

\input{Figure_tex/fig4}


To further support our claims, we also conducted a 
quantitative comparison. Specifically, we assess the 
fluency of the compressed prompts through the computation of a weighted mean of bi-gram and tri-gram entropies~\citep{Fluency}. 
Its computational details are given in Appendix~\ref{Appendix:Eval}, and result 
is reported in Table~\ref{tab:exp4}.
These results show that~\OurMODEL{} yields relatively 
higher fluency scores than the baseline models.
A lower score for baseline models, \emph{e.g.,} LLMLingua, is attributable to loss of intrinsic semantic relationship 
between the tokens for the compressed prompt. 

% \subsubsection{Fluency.}
% \warn{\textbf{Fluency.}(should add in 5.3.3 or 5.3.4?)}


%\subsubsection{Different  target compress rate via LLMs}
%\warn{The result is shown in Figure~\ref{fig:CompressRate}, the analysis is the same as in section 5.2 (maybe this part is more suitable to fuse into section 5.2)}

\eat{
\noindent{\bf Fluency of Compressed Prompts.}
\li{We also conducted a quantitative comparison to 
support our claims that the prompts compressed by~\OurMODEL{} are more fluent.} For this, we assess the fluency of the compressed prompts 
through the computation of a weighted mean of bi-gram and tri-gram entropies~\citep{Fluency}. Its computational details are given in Appendix~\ref{Appendix:Eval}, and result 
is reported in Table~\ref{tab:exp4}. \di{This paragraph is strange, it is almots the same the above paragraph. Reorganize it. }
}



\noindent{\bf Computational Overhead.} One of 
the key objectives of prompt compression is 
to efficiently reduce the overall computational and corresponding fiscal cost associated with the proprietary LLMs, while at the same time preserving the end-utility of the prompt.

In order to compute the computational overhead of~\OurMODEL{}, we assume embedding and computing similarity takes a constant amount of time and its cost
may be ignored because it is much smaller than that of LLM.
Specifically, we use the following formulation to study 
the computational efficiency of~\OurMODEL{}:

\vspace{-10pt}
\begin{equation}
    c = L \cdot c_{\text{graph}} + (L \cdot \eta^{*}) \cdot c_{\text{LLMs}},
\end{equation}
\vspace{-10pt}

where $c_{\text{graph}}$ represents the per-token computation load to generate knowledge triples 
and $c_{\text{LLMs}}$ is per-token computation for target LLMs to generate final output respectively. 
$L$ represents length of the original prompt, $\eta^{*}$ is the target compress rate, and
$c$ represents the total computational overhead for compression.

Following the assumption of \cite{llmlingua}, we estimate $c_{\text{graph}}$ and $c_{\text{LLMs}}$ based on model parameters, as: 
$c_{\text{graph}} \approx 0.3/175c_{\text{LLMs}} \approx 0.0017 \cdot c_{\text{LLMs}}$ using OpenIE toolkit as the knowledge extraction approach. 
When $\eta^{*} = 0.2$, we have $c \approx  0.2017L \cdot c_{\text{LLMs}}$. This means we can achieve nearly 5x savings in terms of computational resources.
For the cases employing a small LLM for graph construction, $c_{\text{graph}} \approx 3.8/175c_{\text{LLMs}} \approx 0.02 \cdot c_{\text{LLMs}}$, the computational overhead is $c \approx 0.22 \cdot c_{\text{LLMs}}$, which is only slightly higher than the first case.

Overall, these results show the computation 
efficiency of our model is comparable with 
that of~\citet{llmlingua}. On the other hand,~\OurMODEL{} 
offers much higher benefits, \emph{i.e.,} compressing
prompts without distorting their readability for 
end-users, while at the same time preserving their 
end-utility to the best possible extent.

%%%%%%%%%%%%%%%%%%%% EXTRA STUFF %%%%%%%%%%%%%%%%%%
\eat{\textbf{task-aware}\\
We then test the performance of~\OurMODEL{} for the task-aware prompt on NaturalQuestions, the results are shown in Table~\ref{tab:exp2}. Compare to Table~\ref{tab:exp1}, the results show that \OurMODEL{} achieves greater improvement than baselines on task-aware prompt. We attribute this to the fact that internal structure correlation extraction is more difficult than problem structure correlation extraction.\\
Although \OurMODEL{} demonstrates good performance on task-aware prompt, }


\eat{
\eat{For example, when consider Exact Match as the evaluation metric, \OurMODEL{} achieves an average improvement of 4.02, 9.54, and 8.71 in 1shot, 2shot and 4shot scenarios compared with LLMLingua.} 
We attribute such drastic performance improvement to the following factors: (1) \OurMODEL{} retains the logical integrity of the COT example without destroying its internal structure. (2)
\OurMODEL{} benefits from the workflow that constructs the graph first, and then further compresses it. This workflow can help us compress different examples robustly without significant performance degradation on some examples.}

\eat{The No.1, No.2, No.3, No.4 shots are the different questions/examples with the inference paths. 
The GSM8K-AUG(1-shot) denotes that the No.2, No.3, and No.4 examples contain one instruction shot, i.e., No.1 shot.
GSM8K-AUG(2-shot) means that the No.3, and No.4 examples/questions contain two instruction shots (No.1 and No.2). 
GSM8K-AUG(4-shot) means that the question contain four instruction shots (No.1, No.2, No.3, No.4). 
}

\eat{
\paragraph{Implementation Details.}
In this paper, we use GPT-3.5-Turbo-0613 as
the target LLM, which can be access via OpenAI.To ensure stable and reproducible results, we adopt greedy decoding and set the temperature to 0 in all experiments.We use GPT-3.5-Turbo-0613 as our knowledge graph extraction model.

\paragraph{Datasets \& Evaluation Metric} 
We evaluate~\OurMODEL{} across three datasets, including QA task and reasoning and in-context learning (ICL) task. For QA task, we use NaturalQuestions(\citet{LostInMiddle},\citep{LostInMiddle}) and Longbench(\citet{Longbench}.\citep{Longbench}). As for reasoning and in-context learning (ICL), we use GSM8K(\citet{GSM8K},\citep{GSM8K}).

Diverging slightly from the original dataset configuration, we opt to directly extract documents containing actual answers or the closest ICL example to the problem for inclusion in the prompt, rather than incorporating all documents or all ICL examples. This approach stems from our belief that selecting a single document or single ICL example from multiple options does not accurately reflect the efficacy of prompt compression. Furthermore, it leads to an inequitable comparison between various prompt compression methodologies, given that not all such algorithms entail a document selection or an ICL example selection step from a plurality of options.

\textbf{(a) NaturalQuestions:} This benchmark comprises real-world queries from individuals utilizing Google, with responses derived from Wikipedia pages. Specifically, each query is associated with 20 pertinent documents in the initial prompt, one of which contains the correct answer. 

\textbf{(b) Longbench:} 

\textbf{(c) GSM8K:} 
}

